[{"content":"背景 在企业项目开发与测试过程中，我们经常会遇到微信支付这类 必须公网可访问 的第三方服务需要回调我们的应用接口的问题。但许多项目部署在 内网环境，公网服务无法直接访问，尤其在多套餐测试环境中，这个限制尤为突出。\n在现代分布式系统架构中，支付回调处理是一个关键但充满挑战的环节。以我们金融科技平台为例，我们遇到了以下典型问题：\n混合网络环境：开发测试环境位于线下内网，而微信支付回调必须通过公网访问 多租户隔离：每个开发人员需要独立的回调地址进行联调测试 网络穿透需求：需要安全地将公网请求透传到内网特定开发环境 动态路由：根据URL路径参数动态路由到不同后端服务 传统解决方案如端口映射或直接暴露内网服务都存在安全风险和管理复杂度问题。为此，我们设计了一套基于Nginx的智能路由方案。\n实现目标 使微信支付回调接口可通过公网地址访问。\n微信服务器回调请求穿透公网到内网服务。\n支持多个上下游项目通过统一的公网域名接收回调请求。\n动态提取账号ID，转发请求到不同子系统。\n技术架构设计 公网接入层Nginx 提供统一的公网接入域名 paycallback.example.com 基础安全防护（IP白名单等） 请求透传到线下环境网关 内网路由层Nginx 动态路径解析与路由 租户隔离与请求转发 本地静态资源服务 1 2 3 4 5 6 7 8 9 10 公网环境 [微信支付] → [公网Nginx (代理服务端)] │ ▼ [代理隧道] │ ▼ 线下环境 [内网Nginx (代理客户端)] → [开发者A服务] → [开发者B服务] 核心实现 公网Nginx配置（代理 服务端） 公网服务部署在支持代理的云服务器上，配置一个统一的公网入口，如：\n1 http://paycallback.example.com/ 微信支付配置示例：\n1 2 3 4 { \u0026#34;payNotifyUrl\u0026#34;: \u0026#34;http://paycallback.example.com/wechatPayNotify/{developerId}\u0026#34; // 示例：http://paycallback.example.com/wechatPayNotify/johndoe } 公网Nginx配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 server { listen 80; server_name paycallback.example.com; # IP白名单保护 include /etc/nginx/conf.d/whitelist/paycallback.conf; location / { # 通过隧道转发到内网 proxy_pass http://10.0.0.10:80; # 指向内网VPN网关 include proxy_params.conf; } } 网Nginx配置（代理客户端） 部署在测试环境，连接公网VPN，负责路由转发请求到具体的子系统。以 /wechatCallback/{accountId} 路由为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 server { listen 80; server_name paycallback.example.com; # 内网DNS解析 resolver 10.0.0.53 valid=30s; location ^~ /wechatPayNotify/ { # 提取开发者ID set $developer_id $uri; if ($uri ~* \u0026#34;^/wechatPayNotify/([^/]+)$\u0026#34;) { set $developer_id $1; } # 构造动态上游地址 set $upstream_host \u0026#34;dev-$developer_id.internal.example.com\u0026#34;; # 请求转发配置 proxy_pass http://$upstream_host/api/payment/notify; proxy_set_header Host $upstream_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 优化代理性能 proxy_http_version 1.1; proxy_request_buffering off; proxy_buffering off; # 日志记录 access_log /var/log/nginx/payment_notify.log main; } # 静态资源服务 location / { try_files $uri /index.html; } } 业务接入流程 以项目 foo-app 为例：\n统一配置微信支付回调地址为： http://paycallback.example.com/wechatCallback/foo 内网Nginx根据路径动态提取 foo 为账号ID，拼接成： http://subsys-foo.internal/api/pay/wechat/notify 内网应用接收到微信服务器回调的数据，进行正常处理。 测试方案 使用 curl 模拟微信回调请求：\n1 2 3 curl -X POST http://paycallback.example.com/wechatCallback/foo \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;order_id\u0026#34;: \u0026#34;123456\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PAID\u0026#34;}\u0026#39; 查看内网子系统日志是否正常接收到回调。\n关键优势 动态路由机制 URL路径参数自动映射到对应开发者环境： devstream.example.com/wechatPayCallback/alice → dev-alice.internal.example.com 安全隔离 加密隧道保障传输安全 公网Nginx IP白名单控制访问源 内网环境完全隔离于公网 高效开发 支持多开发者并行测试 无需重复配置公网暴露点 回调请求实时到达内网环境 弹性扩展 新增开发者只需配置内网服务 无需修改公网Nginx配置 自动DNS解析支持服务发现 实施效果 微信支付回调延迟从分钟级降至200ms内 开发测试效率提升300%，支持10+开发者并行工作 完全消除公网暴露内网端口的风险 配置变更减少90%，新成员接入时间从2小时降至5分钟 ","date":"2025-06-13T12:14:58Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202506131214/","title":"基于Nginx的微信支付回调跨网络环境解决方案"},{"content":"背景 公司要把重要的数据库的操作记录留存，方便查询操作可视化和告警监控等待。更能和其余相同的数据库保持一致。\n本文将详细介绍如何通过MariaDB的server_audit插件为MySQL社区版实现完整的审计功能，包括版本兼容性处理、插件配置、日志解析和存储方案。\n为什么MySQL社区版需要审计功能？ 在公司数据库管理中，审计日志是安全合规和故障排查的重要工具。然而，许多使用MySQL社区版的企业面临一个尴尬的现实：原生的审计功能仅在企业版中提供。根据MySQL官方文档，社区版从5.7.34版本后不再支持第三方审计插件，这给需要合规审计的企业带来了挑战。\n技术选型与版本兼容性分析 方案 优点 缺点 适用场景 MySQL企业版审计插件 官方支持，功能完善 需要付费许可 预算充足的企业 MariaDB server_audit插件 免费开源，功能完整 版本兼容性复杂 MySQL 5.7.34及以下版本 Percona审计插件 完全兼容MySQL 需要迁移到Percona Server 新建项目或可迁移环境 触发器+通用日志 无需额外组件 性能影响大，信息不完整 简单审计需求 在企业数据管理中，数据库操作审计是安全合规的核心需求。但MySQL社区版存在明显短板：\n缺乏原生审计功能（仅企业版支持audit_log插件） 版本兼容陷阱 MySQL 5.7.34：最后一个可原生使用MariaDB插件的版本 MySQL ≥5.7.34 无法使用MariaDB的server_audit.so插件 MySQL 8.0 完全移除插件兼容性 替代方案选择 方案1：MySQL 5.7低版本 + MariaDB插件（需版本匹配） 方案2：迁移至Percona Server（免费开源，兼容MySQL 5.6/5.7/8.0且支持审计） 方案3：全面迁移到MariaDB 实践建议：在测试环境中验证插件兼容性，生产环境部署前进行完整的回归测试。\n环境 环境如下：\nMySQL\n1 2 [root@test_db01 ~]# mysql -V mysql Ver 14.14 Distrib 5.7.27, for Linux (x86_64) using EditLine wrapper Cnetos7\n1 2 3 4 [root@test_db01 ~]# cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core) Linux test_db01 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 安装插件 查看插件安装目录 1 2 3 4 5 6 7 8 mysql\u0026gt; show global variables like \u0026#39;plugin_dir\u0026#39;; +---------------+--------------------------+ | Variable_name | Value | +---------------+--------------------------+ | plugin_dir | /usr/lib64/mysql/plugin/ | +---------------+--------------------------+ 1 row in set (0.00 sec) 提取mariadb审计插件并放置插件目录 此处需要注意版本，直接适配核对测试\n1 2 3 4 wget https://downloads.mariadb.com/MariaDB/mariadb-10.5.16/bintar-linux-x86_64/mariadb-10.5.16-linux-x86_64.tar.gz tar -zxvf mariadb-10.5.3-linux-x86_64.tar.gz cp ./mariadb-10.5.3-linux-x86_64/lib/plugin/server_audit.so /usr/lib64/mysql/plugin/ chmod +x /usr/lib64/mysql/plugin/ mysql安装server_audit.so插件 1 2 3 4 mysql\u0026gt; install plugin server_audit soname \u0026#39;server_audit.so\u0026#39;; Query OK, 0 rows affected (0.02 sec) ##也可以在my.cnf 加载插件方式安装 在my.cnf 设置 plugin_load = server_audit=server_audit.so 查看当前MySQL插件情况\n1 2 mysql\u0026gt; show plugins; | SERVER_AUDIT | ACTIVE | AUDIT | server_audit.so | GPL | 增加审计目录授权 1 2 mkdir /opt/mysqldata/auditlogs chown -R mysql:mysql /opt/mysqldata/auditlogs 开启审计，写入配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #防止server_audit 插件被卸载 进行配置文件配置 server_audit=FORCE_PLUS_PERMANENT #指定哪些操作被记录到日志文件中 server_audit_events=\u0026#39;CONNECT,QUERY,TABLE,QUERY_DDL\u0026#39; #开启审计功能 server_audit_logging=on #默认存放路径，可以不写，默认到data文件下 server_audit_file_path=/opt/mysqldata/auditlogs #设置文件大小 默认1000000，1073741824=1GB server_audit_file_rotate_size=1073741824 #指定日志文件的数量，如果为0日志将从不轮转 server_audit_file_rotations=0 下面是我的配置文件 版本太低，不支持JSON，高版本支持JSON格式，方便Python处理，后续只能re来处理 ​```bash #######server audit##### server_audit_logging=ON server_audit_file_path=/www/server/data/server_audit.log server_audit=FORCE_PLUS_PERMANENT server_audit_file_rotate_size=500M server_audit_file_rotations=15 max_allowed_packet=32M #audit_log_format=JSON #audit_log_file=server_audit.json #server_audit_logging=ON #server_audit_file_path=/data/mysql/server_audit.json #server_audit=FORCE_PLUS_PERMANENT #server_audit_file_rotate_size=1G #server_audit_file_rotations=10 #max_allowed_packet=32M FORCE_PLUS_PERMANENT：防止插件被意外卸载 日志轮转设置：避免日志文件无限增长 max_allowed_packet：确保大SQL语句能被完整记录 重启mysql生效！\n1 2 3 /etc/init.d/mysql restart Shutting down MySQL............... SUCCESS! Starting MySQL.. SUCCESS! 检查日志状态 1 2 3 4 5 -- 检查插件状态 SHOW PLUGINS WHERE NAME = \u0026#39;server_audit\u0026#39;; -- 查看审计设置 SHOW GLOBAL VARIABLES LIKE \u0026#39;server_audit%\u0026#39;; 日志格式 1 2 3 4 5 6 7 20250610 10:42:49,test_db01,root,192.168.102.207,7972452,438352983,QUERY,xxxx,\u0026#39;SELECT id,ext_id,ext_table_name,target_type,value1,value2,spread_config FROM t_target \\n \\n WHERE (ext_id = \\\u0026#39;12606411819336451111124\\\u0026#39; AND ext_table_name = \\\u0026#39;t_discount_coupon\\\u0026#39;)\u0026#39;,0 20250610 10:42:50,test_db01,root,192.168.83.18,7972073,438352984,QUERY,gazelle_model_assemble,\u0026#39;SELECT id,code,name,handler,status,retry_times,create_time,update_time FROM batch WHERE (status = \\\u0026#39;running\\\u0026#39;)\u0026#39;,0 20250610 10:42:50,test_db01,root,192.168.102.207,7969972,438352986,QUERY,xxxx,\u0026#39;select * from t_event_send where sent = 0 order by created_time asc\u0026#39;,0 20250610 10:42:50,test_db01,root,192.168.83.36,7853786,438352985,QUERY,hxxxdb,\u0026#39;SELECT\\n MAX(IF(status = 1, create_time, null)) nearBankTime\\n FROM user_bank_cards\\n WHERE create_time \u0026gt;= NOW() - INTERVAL 12 HOUR\u0026#39;,0 20250610 10:42:50,test_db01,root,192.168.83.27,7971636,438352987,QUERY,hixxxx_core,\u0026#39;SELECT id, user_id, trade_no, out_trade_no, order_sn, lease_id, period_no, amount, trade_status, alipay_close_status, trade_type, remark, trade_way, business_type, entry_param, coupon_id, coupon_amount, deleted, gmt_create, gmt_modified FROM hire_trade_flows WHERE deleted = 0 AND (trade_status = 3 AND trade_type = 1 AND trade_way IN (1, 3, 4) AND business_type IN (3, 5, 2, 1, 14, 4, 6, 10, 13))\u0026#39;,0 20250610 10:42:50,test_db01,root,192.168.83.25,7972144,438352988,QUERY,historical_customer_data,\u0026#39;SELECT 1\u0026#39;,0 20250610 10:42:50,test_db01,root,192.168.83.25,7972144,438352989,QUERY,historical_customer_data,\u0026#39;SELECT 1\u0026#39;,0 解释这条日志格式\n1 20250610 10:42:50,test_db01,root,192.168.83.36,7853786,438352985,QUERY,hxxxdb,\u0026#39;SELECT\\n MAX(IF(status = 1, create_time, null)) nearBankTime\\n FROM user_bank_cards\\n WHERE create_time \u0026gt;= NOW() - INTERVAL 12 HOUR\u0026#39;,0 字段编号 字段值 含义 1 20250610 10:42:50 时间戳，表示审计事件发生的时间 2 test_db01 服务器主机名（或 MariaDB 实例名称） 3 root 数据库用户名，执行该语句的用户 4 192.168.83.36 客户端 IP 地址 5 7853786 线程 ID（MariaDB 内部线程号） 6 438352985 查询 ID，用于区分不同的 SQL 查询 7 QUERY 事件类型（QUERY 表示 SQL 查询） 8 hxxxdb 当前数据库名（USE 的数据库） 9 'SELECT\\n MAX(IF(... SQL 语句内容，使用单引号包裹，\\n 为换行符 10 0 返回值 / 错误码，0 表示成功（无错误） 审计日志处理系统实现 系统架构设计 1 2 3 4 5 [MySQL Server] ↓ (生成审计日志) [server_audit.log] ↓ (Python监控进程) [日志解析模块] → [MySQL审计数据库] Python处理程序核心逻辑 1 20250610 10:42:50,test_db01,root,192.168.83.36,7853786,438352985,QUERY,hxxxdb,\u0026#39;SELECT\\n MAX(IF(status = 1, create_time, null)) nearBankTime\\n FROM user_bank_cards\\n WHERE create_time \u0026gt;= NOW() - INTERVAL 12 HOUR\u0026#39;,0 我们有上面的日志格式，可以进行如下操作处理\n日志监控关键特性：\n断点续传：通过state文件记录读取位置 文件轮转检测：通过inode和dev识别日志切换 异常处理：完善的错误捕获和日志记录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def process_log_entry(new_content): \u0026#34;\u0026#34;\u0026#34;处理单个审计日志条目\u0026#34;\u0026#34;\u0026#34; # 定义正则表达式模式，用于解析审计日志格式 # 模式匹配字段：时间，数据库名(1)，用户名，源IP，连接ID，查询ID，事件类型，数据库名(2)，SQL语句，执行结果 pattern = r\u0026#34;^(\\d{8} \\d{2}:\\d{2}:\\d{2}),([\\w]+),([\\w]+),(\\d+\\.\\d+\\.\\d+\\.\\d+),(\\d+),(\\d+),([\\w]+),([\\w]+),\u0026#39;(.*)\u0026#39;,(\\d+)\u0026#34; # 尝试匹配日志行 match = re.match(pattern, new_content) if match: # 提取匹配的各个字段 sql_time = match.group(1) # 时间 (格式: 年月日 时分秒) database_name = match.group(8) # 数据库名称 (从第8组获取) sql_content = match.group(9) # SQL语句内容 (包含原始转义字符) s_sql_content = match.group(9) # 保留原始SQL语句用于日志记录 sql_result = match.group(10) # SQL执行结果 (0表示成功) # 清洗SQL语句内容： # 1. 去除SQL注释（--、#和/* */类型的注释） COMMENT_PATTERN = re.compile(r\u0026#34;(--.*?\\\\r\\\\n|#.*?\\\\r\\\\n|/\\*.*?\\*/)\u0026#34;, re.DOTALL | re.MULTILINE) sql_content = COMMENT_PATTERN.sub(\u0026#34;\u0026#34;, sql_content) # 2. 替换特殊转义字符 # - 将\\n换行符替换为空格 # - 将\\\u0026#39;转义单引号替换为普通单引号 # - 将\\t制表符替换为空格 # - 将\\r回车符替换为空格 sql_content = sql_content.replace(\u0026#39;\\\\n\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\\\\\\u0026#39;\u0026#39;, \u0026#39;\\\u0026#39;\u0026#39;).replace(\u0026#39;\\\\t\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\\\r\u0026#39;, \u0026#39; \u0026#39;) # 3. 去除SQL语句前后的多余空格 sql_content = sql_content.strip() # 检查是否为需要记录的DDL语句： # 1. SQL执行成功 (sql_result == \u0026#34;0\u0026#34;) # 2. 是DDL操作 (CREATE/ALTER/DROP/RENAME开头) DDL_PATTERN = re.compile(r\u0026#34;^(CREATE|ALTER|DROP|RENAME)\u0026#34;, re.IGNORECASE) if sql_result == \u0026#34;0\u0026#34; and DDL_PATTERN.match(sql_content): # 记录处理日志 logging.info(\u0026#34;-\u0026#34; * 50) logging.info(f\u0026#34;原始SQL内容: {s_sql_content}\u0026#34;) logging.info(f\u0026#34;执行时间: {sql_time}\u0026#34;) logging.info(f\u0026#34;数据库名称: {database_name}\u0026#34;) logging.info(f\u0026#34;清洗后SQL: {sql_content}\u0026#34;) logging.info(\u0026#34;-\u0026#34; * 50) # 将审计记录保存到MySQL数据库 save_to_mysql(sql_time, database_name, sql_content) 1 2 原始: \u0026#39;SELECT\\n MAX(...) nearBankTime\\n FROM ...\u0026#39; 清洗后: \u0026#39;SELECT MAX(...) nearBankTime FROM ...\u0026#39; 完整代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 # -*- coding: utf-8 -*- import os, sys import re import time import json, logging from datetime import datetime import pymysql # 日志配置 logging.basicConfig( filename=\u0026#39;mysql_monitor.log\u0026#39;, level=logging.INFO, format=\u0026#39;[%(asctime)s] %(levelname)s: %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39; ) # 读取缓存文件，定位日志读取位置 def load_state(state_file): \u0026#34;\u0026#34;\u0026#34;Load saved state.\u0026#34;\u0026#34;\u0026#34; try: with open(state_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: state = json.load(f) return state.get(\u0026#39;position\u0026#39;, 0), state.get(\u0026#39;inode\u0026#39;, None), state.get(\u0026#39;dev\u0026#39;, None) except (IOError, ValueError): return 0, None, None except Exception as e: logging.error(f\u0026#34;Failed to load state: {str(e)}\u0026#34;) return 0, None, None # 保存位置数据到文件中 def save_state(state_file, position, inode, dev): \u0026#34;\u0026#34;\u0026#34;Save current state.\u0026#34;\u0026#34;\u0026#34; try: with open(state_file, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump({ \u0026#39;position\u0026#39;: position, \u0026#39;inode\u0026#39;: inode, \u0026#39;dev\u0026#39;: dev }, f) except Exception as e: logging.error(f\u0026#34;Failed to save state: {str(e)}\u0026#34;) # 保存sql数据到数据中，方便可视化 或者 告警 def save_to_mysql(sql_time, database_name, sql_content): \u0026#34;\u0026#34;\u0026#34;将日志信息写入到 MySQL 数据库的 audit_log 表中\u0026#34;\u0026#34;\u0026#34; connection = pymysql.connect( host=\u0026#39;192.168.102.201\u0026#39;, user=\u0026#39;root\u0026#39;, password=\u0026#39;xxxx.88\u0026#39;, database=\u0026#39;audit_db\u0026#39;, charset=\u0026#39;utf8mb4\u0026#39; ) try: with connection.cursor() as cursor: sql = \u0026#34;\u0026#34;\u0026#34; INSERT INTO audit_log (sql_time, database_name, sql_content) VALUES (%s, %s, %s) \u0026#34;\u0026#34;\u0026#34; sql_time = datetime.strptime(sql_time, \u0026#34;%Y%m%d %H:%M:%S\u0026#34;).strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) cursor.execute(sql, (sql_time, database_name, sql_content)) connection.commit() except Exception as e: logging.error(f\u0026#34;Failed to insert data into MySQL: {str(e)}\u0026#34;) finally: connection.close() # 读取文件 ，数据 匹配 和 清洗 def process_log_entry(new_content): \u0026#34;\u0026#34;\u0026#34;Process a single log entry.\u0026#34;\u0026#34;\u0026#34; pattern = r\u0026#34;^(\\d{8} \\d{2}:\\d{2}:\\d{2}),([\\w]+),([\\w]+),(\\d+\\.\\d+\\.\\d+\\.\\d+),(\\d+),(\\d+),([\\w]+),([\\w]+),\u0026#39;(.*)\u0026#39;,(\\d+)\u0026#34; match = re.match(pattern, new_content) if match: sql_time = match.group(1) database_name = match.group(8) sql_content = match.group(9) s_sql_content = match.group(9) sql_result = match.group(10) # 先匹配存在DDL语句的sql内容 # 去除-- /* # 等注释符 #logging.info(f\u0026#34;1 {sql_content}\u0026#34;) COMMENT_PATTERN = re.compile(r\u0026#34;(--.*?\\\\r\\\\n|#.*?\\\\r\\\\n|/\\*.*?\\*/)\u0026#34;, re.DOTALL | re.MULTILINE) sql_content = COMMENT_PATTERN.sub(\u0026#34;\u0026#34;, sql_content) # 去除多余的换行符/\u0026#39;等 #logging.info(f\u0026#34;2 {sql_content}\u0026#34;) sql_content = sql_content.replace(\u0026#39;\\\\n\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\\\\\\u0026#39;\u0026#39;, \u0026#39;\\\u0026#39;\u0026#39;).replace(\u0026#39;\\\\t\u0026#39;, \u0026#39; \u0026#39;).replace(\u0026#39;\\\\r\u0026#39;, \u0026#39; \u0026#39;) #logging.info(f\u0026#34;3 {sql_content}\u0026#34;) # 去除前后多余的空格 sql_content = sql_content.strip() #logging.info(f\u0026#34;4 {sql_content}\u0026#34;) # 检查是否为 DDL 语句 DDL_PATTERN = re.compile(r\u0026#34;^(CREATE|ALTER|DROP|RENAME)\u0026#34;, re.IGNORECASE) if sql_result == \u0026#34;0\u0026#34; and DDL_PATTERN.match(sql_content): logging.info(\u0026#34;-\u0026#34; * 50) logging.info(f\u0026#34;SQL Content: {s_sql_content}\u0026#34;) logging.info(f\u0026#34;Time: {sql_time}\u0026#34;) logging.info(f\u0026#34;Database Name: {database_name}\u0026#34;) logging.info(f\u0026#34;SQL Content: {sql_content}\u0026#34;) logging.info(\u0026#34;-\u0026#34; * 50) save_to_mysql(sql_time, database_name, sql_content) # 启动程序，传入 缓存 文件 和 审计日志记录 文件 位置 def monitor_log(state_file, log_file): \u0026#34;\u0026#34;\u0026#34;Monitor MySQL audit logs.\u0026#34;\u0026#34;\u0026#34; last_position, last_inode, last_dev = load_state(state_file) while True: try: current_st = os.stat(log_file) except OSError: logging.warning(\u0026#34;Log file not found, retrying...\u0026#34;) time.sleep(1) continue if (current_st.st_ino != last_inode) or (current_st.st_dev != last_dev): logging.info(\u0026#34;New log file detected\u0026#34;) last_position = 0 last_inode = current_st.st_ino last_dev = current_st.st_dev save_state(state_file, last_position, last_inode, last_dev) if current_st.st_size \u0026lt; last_position: logging.info(\u0026#34;File truncated\u0026#34;) last_position = 0 save_state(state_file, last_position, last_inode, last_dev) if current_st.st_size \u0026gt; last_position: try: with open(log_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, errors=\u0026#39;replace\u0026#39;) as f: f.seek(last_position) for line in f: process_log_entry(line.strip()) last_position = f.tell() save_state(state_file, last_position, last_inode, last_dev) except Exception as e: logging.error(f\u0026#34;Error reading log file: {str(e)}\u0026#34;) time.sleep(0.5) if __name__ == \u0026#34;__main__\u0026#34;: log_file = \u0026#39;/data/mysql/server_audit.log\u0026#39; state_file = \u0026#34;/opt/mysql_audit_monitor/mysqlMonitor.state\u0026#34; logging.info(f\u0026#34;Starting MySQL audit log monitoring: {log_file}\u0026#34;) try: monitor_log(state_file, log_file) except KeyboardInterrupt: logging.info(\u0026#34;Monitoring stopped\u0026#34;) sys.exit(0) 后记 生产环境建议 场景 推荐方案 MySQL 5.7.34以下版本 MariaDB插件方案 MySQL 8.0+ Percona Server或迁移至MariaDB 审计日志存储 独立MySQL实例，与业务库物理隔离 高频操作环境 写入Elasticsearch替代MySQL 通过server_audit插件+Python监控的组合，可在MySQL社区版低成本实现企业级审计需求。但需注意：\n📌 版本兼容性是成功前提，MySQL 8.0用户务必选择Percona/MariaDB 📌 日志清洗环节直接影响分析准确性（如转义符处理） 📌 审计日志存储分离是安全最佳实践\n最终效果：\n所有数据库操作可视化展示 实时捕获高危DDL语句 满足等保2.0/ISO 27001审计要求 ","date":"2025-06-10T11:12:48Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/202506101112/","title":"为MySQL社区版实现审计功能：从插件配置到日志监控全解析"},{"content":"背景 公司想对接AI智能体，用于客服系统，经过调研和实施，觉得DashScope 符合需求。 阿里云推出的DashScope灵积模型服务为开发者提供了便捷高效的大模型接入方案。本文将详细介绍如何基于DashScope API构建一个功能完善的智能对话系统，包含流式对话、工具调用等高级特性。\n项目背景与技术选型 我们的项目目标是构建一个企业级智能客服系统，需要满足以下核心需求：\n支持多轮自然语言对话 实现低延迟的流式响应 可扩展的工具调用能力 稳定的生产环境部署 经过技术评估，我们选择了阿里云DashScope服务，主要基于以下优势：\n模型多样性：提供QWEN系列等多种大语言模型 API兼容性：兼容OpenAI API格式，降低迁移成本 性能保障：阿里云基础设施确保服务稳定性 成本效益：相比自建模型集群更具性价比 模型地址：https://help.aliyun.com/zh/model-studio/videos/yi-large-quick-start\n核心代码实现与优化 基础对话功能实现 我们首先实现了基础的对话功能模块，这是整个系统的核心：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 import json from typing import List, Dict, Optional import requests from pydantic import BaseModel from utils.LogHandler import log # 配置管理使用Pydantic模型，便于验证和文档化 class DashScopeConfig(BaseModel): base_url: str = \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; api_key: str default_model: str = \u0026#34;qwen-plus\u0026#34; timeout: int = 30 max_retries: int = 3 class GPTChatResponse(BaseModel): content: str tool_calls: Optional[List[Dict]] = None def gpt_chat( messages: List[Dict[str, str]], config: DashScopeConfig, model: Optional[str] = None, temperature: Optional[float] = None, max_tokens: Optional[int] = 512 ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 标准对话API实现 :param messages: 对话消息列表 :param config: 服务配置 :param model: 指定模型，默认使用配置中的default_model :param temperature: 生成多样性控制 :param max_tokens: 最大输出token数 :return: 模型生成的文本内容 \u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: f\u0026#39;Bearer {config.api_key}\u0026#39; } payload = { \u0026#39;model\u0026#39;: model or config.default_model, \u0026#39;messages\u0026#39;: messages, \u0026#39;max_tokens\u0026#39;: max_tokens, } if temperature is not None: payload[\u0026#39;temperature\u0026#39;] = temperature for attempt in range(config.max_retries): try: resp = requests.post( f\u0026#34;{config.base_url}/chat/completions\u0026#34;, headers=headers, json=payload, timeout=config.timeout ) resp.raise_for_status() json_data = resp.json() if choices := json_data.get(\u0026#39;choices\u0026#39;): if len(choices) \u0026gt; 0: return choices[0][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] raise ValueError(\u0026#34;No valid response from model\u0026#34;) except requests.exceptions.RequestException as e: log.error(f\u0026#34;Attempt {attempt + 1} failed: {str(e)}\u0026#34;) if attempt == config.max_retries - 1: raise if __name__ == \u0026#34;__main__\u0026#34;: config = DashScopeConfig(api_key=\u0026#34;sk-xxxxxx\u0026#34;) messages = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一名专业客服人员\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;怎么处理客户争吵问题\u0026#34;} ] response = gpt_chat(messages, config) print(response) 流式对话高级实现 为提升用户体验，我们实现了流式对话功能，并进行了多项优化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 import json from typing import List, Dict, Optional import requests from pydantic import BaseModel from utils.LogHandler import log # 配置管理使用Pydantic模型，便于验证和文档化 class DashScopeConfig(BaseModel): base_url: str = \u0026#34;https://dashscope.aliyuncs.com/compatible-mode/v1\u0026#34; api_key: str default_model: str = \u0026#34;qwen-plus\u0026#34; timeout: int = 30 max_retries: int = 3 class GPTChatResponse(BaseModel): content: str tool_calls: Optional[List[Dict]] = None def gpt_stream_chat( messages: List[Dict[str, str]], config: DashScopeConfig, model: Optional[str] = None, tools: Optional[List[Dict]] = None, on_content: Optional[callable] = None ) -\u0026gt; GPTChatResponse: \u0026#34;\u0026#34;\u0026#34; 流式对话实现，支持实时内容处理和工具调用 :param messages: 对话消息列表 :param config: 服务配置 :param model: 指定模型 :param tools: 可用工具列表 :param on_content: 内容回调函数 :return: GPTChatResponse对象 \u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: f\u0026#39;Bearer {config.api_key}\u0026#39; } payload = { \u0026#39;model\u0026#39;: model or config.default_model, \u0026#39;messages\u0026#39;: messages, \u0026#39;stream\u0026#39;: True } if tools: payload[\u0026#39;tools\u0026#39;] = tools content_list = [] tool_calls = None try: resp = requests.post( f\u0026#34;{config.base_url}/chat/completions\u0026#34;, headers=headers, json=payload, stream=True, timeout=config.timeout ) resp.raise_for_status() for line in resp.iter_lines(): if not line or line == b\u0026#39;data: [DONE]\u0026#39;: continue try: chunk = json.loads(line.decode(\u0026#39;utf-8\u0026#39;)[6:]) delta = chunk[\u0026#39;choices\u0026#39;][0][\u0026#39;delta\u0026#39;] # 处理推理过程内容 if content := delta.get(\u0026#39;reasoning_content\u0026#39;): if on_content: on_content(content, \u0026#39;reasoning\u0026#39;) content_list.append(content) # 处理最终回复内容 if content := delta.get(\u0026#39;content\u0026#39;): if on_content: on_content(content, \u0026#39;content\u0026#39;) content_list.append(content) # 处理工具调用 if \u0026#39;tool_calls\u0026#39; in delta: tool_calls = delta[\u0026#39;tool_calls\u0026#39;] if tool_calls and \u0026#39;name\u0026#39; in str(tool_calls): break except json.JSONDecodeError: log.warning(f\u0026#34;Failed to decode chunk: {line}\u0026#34;) continue except requests.exceptions.RequestException as e: log.error(f\u0026#34;Stream request failed: {str(e)}\u0026#34;) raise return GPTChatResponse(content=\u0026#39;\u0026#39;.join(content_list), tool_calls=tool_calls) def handle_stream_content(content: str, content_type: str): print(content, end=\u0026#39;\u0026#39;, flush=True) # 实时输出，不换行 if __name__ == \u0026#34;__main__\u0026#34;: config = DashScopeConfig(api_key=\u0026#34;sk-xxxxxxxxxxxxxxxxxxxxxx\u0026#34;) messages = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一名专业客服人员\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你好？\u0026#34;} ] response = gpt_stream_chat(messages, config, on_content=handle_stream_content) # print(response.content) 结束 我们成功构建了基于阿里云DashScope的高效智能对话系统。这套方案不仅适用于客服场景，也可扩展应用于智能助手、内容生成等多种AI应用场景。DashScope服务的稳定性和易用性为中小企业快速部署AI能力提供了可靠选择。\n","date":"2025-05-22T18:40:54Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/202505221840/","title":"基于阿里云DashScope API构建智能对话指南"},{"content":"什么是向量库？ 向量库（Vector Database）是一种专门设计用来存储和检索向量数据的数据库系统。在这个文件中使用的ChromaDB就是一种向量数据库。\n向量库的核心概念：\n向量嵌入（Embeddings） ：将文本、图像等非结构化数据转换为高维数字向量 相似性搜索 ：基于向量间的距离（如余弦相似度）快速查找相似内容 高效索引 ：使用特殊的索引结构（如HNSW）加速相似性搜索 向量库的用途 在这个项目中，向量库主要用于客服问答系统：\n知识库管理 ：存储问题和答案对，并将问题转换为向量形式存储 语义搜索 ：当用户提问时，将问题转换为向量，在向量库中查找最相似的已存问题 智能匹配 ：根据语义相似度而非简单的关键词匹配，找到最相关的答案 Chroma 向量库实战 Chroma 是一个用于构建带有嵌入向量（vector embedding）的 AI 应用程序的向量数据库。它们可以表示文本、图像，很快还可以表示音频和视频。它内置了您开始使用所需的一切，并在您的计算机上运行。他是开源免费，可以本地部署，支持python和js。这是他的优点。pinecone则是付费，而且需要存储数据到pinecone服务器上面，这点对于数据比较重要的企业尤其不好，Chroma则是存储在自己的服务器\nChromaDB 是一个开源的向量数据库，专门用于存储和检索向量数据。它特别适合构建语义搜索、问答系统等 AI 应用。本文将介绍如何使用 ChromaDB 实现基本的向量数据库操作，包括数据的增删改查。\n环境准备 本示例使用了以下主要依赖：\nchromadb：向量数据库 dashscope：文本向量化服务 1 2 pip install chromadb pip install dashscope 1 2 3 4 5 6 7 8 9 10 11 12 import chromadb # chroma_client = chromadb.Client() #内存模式 # 数据保存在磁盘 client = chromadb.PersistentClient(path=\u0026#34;E:\\\\Code\\\\Python\\\\weather\\\\chromadbTest\u0026#34;) collection = client.get_collection(name=\u0026#34;fruit_collection\u0026#34;) # 插入数据 collection.add( documents=[\u0026#34;This is a document about apples\u0026#34;, \u0026#34;This is a document about oranges\u0026#34;], metadatas=[{\u0026#34;source\u0026#34;: \u0026#34;web\u0026#34;}, {\u0026#34;source\u0026#34;: \u0026#34;book\u0026#34;}], ids=[\u0026#34;id1\u0026#34;, \u0026#34;id2\u0026#34;] ) 客服向量库实战 EmbeddingClient EmbeddingClient 类负责将文本转换为向量表示。它使用单例模式确保全局只有一个实例，并通过 dashscope 服务进行文本向量化。\n1 2 3 4 5 6 7 8 9 class EmbeddingClient: def get_embedding(self, text: Union[str, List[str]]) -\u0026gt; List[float]: \u0026#34;\u0026#34;\u0026#34;获取文本的向量表示\u0026#34;\u0026#34;\u0026#34; # 通过 dashscope 服务将文本转换为向量 resp = dashscope.TextEmbedding.call( model=self.model, input=text ) return resp.output[\u0026#39;embeddings\u0026#39;][0][\u0026#39;embedding\u0026#39;] 完整代码： DashScope是阿里云的一款模型服务产品，简化了AI模型的应用与部署。 已开通服务并获得API-KEY：开通DashScope并创建API-KEY。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from http import HTTPStatus import dashscope from typing import List, Union from utils.LogHandler import log # from conf.config import dashscope_api_key dashscope_api_key = \u0026#34;sk-xxxxxxxxxxx\u0026#34; class EmbeddingClient: _instance = None # 用于保存单例对象 def __new__(cls): \u0026#34;\u0026#34;\u0026#34; 确保 `EmbeddingClient` 类只有一个实例（单例模式） \u0026#34;\u0026#34;\u0026#34; if cls._instance is None: cls._instance = super().__new__(cls) cls._instance._initialize() return cls._instance def _initialize(self): \u0026#34;\u0026#34;\u0026#34; 初始化客户端连接 \u0026#34;\u0026#34;\u0026#34; dashscope.api_key = dashscope_api_key self.model = dashscope.TextEmbedding.Models.text_embedding_v3 def get_embedding(self, text: Union[str, List[str]]) -\u0026gt; List[float]: \u0026#34;\u0026#34;\u0026#34; 获取文本的向量表示 :param text: 输入文本，可以是字符串或字符串列表 :return: 向量列表 \u0026#34;\u0026#34;\u0026#34; try: resp = dashscope.TextEmbedding.call( model=self.model, input=text ) if resp.status_code == HTTPStatus.OK: embedding = resp.output[\u0026#39;embeddings\u0026#39;][0][\u0026#39;embedding\u0026#39;] return embedding else: log.error(f\u0026#34;Embedding生成失败: status_code={resp.status_code}\u0026#34;) raise Exception(f\u0026#34;Embedding生成失败: {resp.status_code}\u0026#34;) except Exception as e: log.error(f\u0026#34;Embedding异常: {str(e)}\u0026#34;) raise # 获取 embedding_client 的方法 def get_embedding_client() -\u0026gt; EmbeddingClient: \u0026#34;\u0026#34;\u0026#34; 获取EmbeddingClient实例 :return: EmbeddingClient的单例对象 \u0026#34;\u0026#34;\u0026#34; return EmbeddingClient() ChromaClient ChromaClient 类封装了与 ChromaDB 的交互操作，提供了以下主要功能：\n向量数据的插入和更新 相似向量查询 向量数据删除 集合统计信息查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 import hashlib import os from datetime import datetime from pathlib import Path from typing import Optional, Tuple import chromadb from chromadb.config import Settings from embedding_client import get_embedding_client from utils.LogHandler import log class ChromaClient: def __init__(self, persist_directory: str = None): \u0026#34;\u0026#34;\u0026#34; 初始化Chroma客户端 :param persist_directory: 持久化存储路径，默认为项目根目录下的chroma_db \u0026#34;\u0026#34;\u0026#34; if persist_directory is None: # 获取项目根目录 root_dir = Path(__file__).parent.parent persist_directory = os.path.join(root_dir, \u0026#34;chroma_db\u0026#34;) # 确保目录存在 os.makedirs(persist_directory, exist_ok=True) # 修改初始化设置 self.client = chromadb.PersistentClient( path=persist_directory, settings=Settings( anonymized_telemetry=False, is_persistent=True # 显式指定持久化 ) ) # 创建或获取集合 self.collection = self.client.get_or_create_collection( name=\u0026#34;kefu_qa\u0026#34;, metadata={\u0026#34;hnsw:space\u0026#34;: \u0026#34;cosine\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;客服问答向量库\u0026#34;} ) self.embedding_client = get_embedding_client() from utils.LogHandler import log log.info(f\u0026#34;ChromaDB initialized at: {persist_directory}\u0026#34;) def upsert_vectors(self, question: str, answer: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 插入或更新向量 :param question: 问题文本 :param answer: 答案文本 :return: 操作结果信息 \u0026#34;\u0026#34;\u0026#34; qhash = hashlib.md5(question.encode(\u0026#39;utf-8\u0026#39;)).hexdigest() try: embedding = self.embedding_client.get_embedding(question) metadata = { \u0026#34;answer\u0026#34;: answer, \u0026#34;timestamp\u0026#34;: datetime.now().isoformat() } self.collection.upsert( ids=[qhash], embeddings=[embedding], documents=[question], metadatas=[metadata] ) log.info(f\u0026#34;Successfully upserted vector for question: {question}\u0026#34;) return \u0026#34;插入更新成功\u0026#34; except Exception as e: log.error(f\u0026#34;向量更新失败: {str(e)}\u0026#34;, exc_info=True) return f\u0026#34;向量更新失败: {str(e)}\u0026#34; def query_similar(self, query_text: str, top_k: int = 1) -\u0026gt; Tuple[str, Optional[str], Optional[str]]: \u0026#34;\u0026#34;\u0026#34; 查询相似向量 :param query_text: 查询文本 :param top_k: 返回结果数量 :return: (查询文本, 匹配问题, 匹配答案) \u0026#34;\u0026#34;\u0026#34; try: query_embedding = self.embedding_client.get_embedding(query_text) results = self.collection.query( query_embeddings=[query_embedding], n_results=top_k, include=[\u0026#34;documents\u0026#34;, \u0026#34;metadatas\u0026#34;, \u0026#34;distances\u0026#34;], where=None, # 可选的过滤条件 where_document=None # 可选的文档过滤条件 ) # 检查结果是否为空 if not results[\u0026#39;ids\u0026#39;] or not results[\u0026#39;ids\u0026#39;][0]: log.info(f\u0026#34;未找到匹配问题：{query_text}\u0026#34;) return query_text, None, None # 获取相似度分数 distance = results[\u0026#39;distances\u0026#39;][0][0] if distance \u0026gt; 0.4: # 相似度阈值，可根据需要调整 log.info(f\u0026#34;找到的匹配相似度过低：{distance}\u0026#34;) return query_text, None, None question = results[\u0026#39;documents\u0026#39;][0][0] answer = results[\u0026#39;metadatas\u0026#39;][0][0][\u0026#39;answer\u0026#39;] log.info( f\u0026#34;查询成功: question={query_text},link_query_text={question}, answer={answer}, distance={distance}\u0026#34;) return query_text, question, answer except Exception as e: log.error(f\u0026#34;查询失败: {str(e)}\u0026#34;, exc_info=True) return query_text, None, None def delete_vectors(self, qhash: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 根据问题删除向量 :param qhash: :return: 删除结果信息 \u0026#34;\u0026#34;\u0026#34; try: # 先检查集合中的数据 log.info(f\u0026#34;当前集合状态: {self.get_collection_stats()}\u0026#34;) # 尝试获取要删除的记录 existing = self.collection.get( ids=[qhash] ) log.info(f\u0026#34;要删除的记录: {existing}\u0026#34;) self.collection.delete( ids=[qhash] ) log.info(f\u0026#34;已执行删除: {qhash}\u0026#34;) log.info(f\u0026#34;当前集合状态: {self.get_collection_stats()}\u0026#34;) return \u0026#34;问题记录已删除\u0026#34; except Exception as e: log.error(f\u0026#34;删除失败: {str(e)}\u0026#34;, exc_info=True) return f\u0026#34;删除失败: {str(e)}\u0026#34; def get_collection_stats(self) -\u0026gt; dict: \u0026#34;\u0026#34;\u0026#34; 获取集合统计信息 \u0026#34;\u0026#34;\u0026#34; return { \u0026#34;total_count\u0026#34;: self.collection.count(), \u0026#34;name\u0026#34;: self.collection.name, \u0026#34;metadata\u0026#34;: self.collection.metadata } _chroma_client = None def get_chroma_client(): global _chroma_client try: # 如果是第一次创建，或者需要重新检查状态 if _chroma_client is None: _chroma_client = ChromaClient() # 获取集合统计信息进行更详细的状态检查 log.info(f\u0026#39;chroma_client_status={_chroma_client.get_collection_stats()}\u0026#39;) return _chroma_client except Exception as e: # 如果出现任何异常，记录日志并重新创建 log.error(f\u0026#34;获取ChromaClient失败，重新创建对象: {e}\u0026#34;) _chroma_client = ChromaClient() return _chroma_client if __name__ == \u0026#34;__main__\u0026#34;: chroma_client = get_chroma_client() stats = chroma_client.get_collection_stats() print(f\u0026#34;删除前的集合统计: {stats}\u0026#34;) 日志记录器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 import os import logging from logging.handlers import TimedRotatingFileHandler # 确定日志存储路径，确保路径存在 CURRENT_PATH = os.path.dirname(os.path.abspath(__file__)) ROOT_PATH = os.path.abspath(os.path.join(CURRENT_PATH, \u0026#34;..\u0026#34;, \u0026#34;..\u0026#34;, \u0026#34;..\u0026#34;)) # LOG_PATH = \u0026#34;/app/logs/app/\u0026#34; LOG_PATH = os.path.join(ROOT_PATH, \u0026#39;logs\u0026#39;) if not os.path.exists(LOG_PATH): os.makedirs(LOG_PATH) # 自定义 Filter，用于确保日志 record 中包含 traceId 属性，不存在时设为默认 \u0026#34;null\u0026#34; class TraceIdFilter(logging.Filter): def filter(self, record): if not hasattr(record, \u0026#34;traceId\u0026#34;): record.traceId = \u0026#34;null\u0026#34; return True # 自定义 Formatter，用于将异常信息中的换行符替换为分隔符，使异常信息显示为单行 class SingleLineExceptionFormatter(logging.Formatter): def formatException(self, exc_info): result = super().formatException(exc_info) # 将换行符替换为空格或其他分隔符，例如 \u0026#34; | \u0026#34; return result.replace(\u0026#39;\\n\u0026#39;, \u0026#39; | \u0026#39;) # 定义日志格式与时间格式（模仿 Java 输出格式） LOG_FORMAT = \u0026#39;%(asctime)s.%(msecs)03d [%(levelname)s] [%(traceId)s] [%(threadName)s] [%(name).36s] [%(lineno)d] [ %(filename)s] [%(funcName)s] - %(message)s\u0026#39; DATE_FORMAT = \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39; class LogHandler(logging.Logger): def __init__(self, name, level=logging.INFO, use_stream=True, use_file=True): super().__init__(name, level) self.name = name self.level = level # 添加 filter，确保 traceId 不为空 self.addFilter(TraceIdFilter()) if use_stream: self.__setStreamHandler__() if use_file: self.__setFileHandler__() def __setFileHandler__(self, level=None): # 日志文件名：LOG_PATH 下，名称为 {logger_name}.log file_name = os.path.join(LOG_PATH, f\u0026#39;{self.name}.log\u0026#39;) # 使用按天滚动（每日一个文件），保留7天的日志 file_handler = TimedRotatingFileHandler(filename=file_name, when=\u0026#39;D\u0026#39;, interval=1, backupCount=7, encoding=\u0026#34;utf-8\u0026#34;) file_handler.suffix = \u0026#39;%Y%m%d.log\u0026#39; file_handler.setLevel(level if level is not None else self.level) formatter = SingleLineExceptionFormatter(LOG_FORMAT, datefmt=DATE_FORMAT) file_handler.setFormatter(formatter) self.file_handler = file_handler self.addHandler(file_handler) def __setStreamHandler__(self, level=None): stream_handler = logging.StreamHandler() stream_handler.setLevel(level if level is not None else self.level) formatter = SingleLineExceptionFormatter(LOG_FORMAT, datefmt=DATE_FORMAT) stream_handler.setFormatter(formatter) self.addHandler(stream_handler) def resetName(self, name): self.name = name self.removeHandler(self.file_handler) self.__setFileHandler__() project_name = \u0026#39;app\u0026#39; # 初始化 logger，级别设置为 DEBUG log = LogHandler(project_name, level=logging.DEBUG) # 使用示例：设置项目名称为 \u0026#34;app\u0026#34; 并初始化日志实例 if __name__ == \u0026#39;__main__\u0026#39;: # 测试日志输出 log.info(\u0026#39;This is a test message\u0026#39;) # 模拟异常情况，测试异常打印为单行 try: 1 / 0 except Exception as e: # 这里使用 extra 参数传入 traceId，如果不传则默认显示 \u0026#34;null\u0026#34; log.exception(\u0026#34;An error occurred\u0026#34;, extra={\u0026#34;traceId\u0026#34;: \u0026#34;trace-123\u0026#34;}) 完整示例 chroma_demo.py\n提供了一个完整的示例脚本 chroma_demo.py，展示了向量数据库的基本操作：\n添加示例问答对数据 进行相似问题查询 删除指定的问答对 查看集合统计信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 import os from chroma_client import get_chroma_client def demo_chroma_operations(): # 获取ChromaDB客户端实例 chroma_client = get_chroma_client() # 1. 添加示例数据 print(\u0026#34;\\n1. 添加示例数据\u0026#34;) qa_pairs = [ (\u0026#34;如何查询订单物流？\u0026#34;, \u0026#34;登录账号后进入『我的订单』，点击对应订单的『物流详情』即可查看实时快递信息，若未更新可联系承运商查询。\u0026#34;), (\u0026#34;今日有什么限时优惠？\u0026#34;, \u0026#34;今天是2025年4月28日，周一会员日可享全场9折，部分商品参与『五一提前购』活动，满300减50。\u0026#34;), (\u0026#34;商品签收后发现有破损怎么办？\u0026#34;, \u0026#34;请于签收后48小时内拍照留存证据，通过订单页面提交『售后申请』，我们将优先处理您的补偿或换货需求。\u0026#34;), (\u0026#34;如何修改绑定的手机号？\u0026#34;, \u0026#34;在账户设置的『安全中心』中验证原手机号后，可直接输入新手机号并接收验证码完成更换。\u0026#34;), (\u0026#34;积分会过期吗？\u0026#34;, \u0026#34;积分有效期为1年，每年12月31日清零未使用的积分，当前账户积分可在『会员中心』查看到期时间。\u0026#34;), (\u0026#34;五一假期期间发货吗？\u0026#34;, \u0026#34;2025年五一假期为5月1日至5日，期间仓库暂停发货，4月30日18:00前的订单将尽量节前发出。\u0026#34;), (\u0026#34;如何开具电子发票？\u0026#34;, \u0026#34;下单时勾选『需要发票』并填写抬头信息，电子发票将在订单完成后72小时内发送至您的邮箱。\u0026#34;), (\u0026#34;新用户有哪些福利？\u0026#34;, \u0026#34;首次注册可领取100元优惠券包（含3张券），首单满199元还可额外获得双倍积分。\u0026#34;), (\u0026#34;忘记密码如何重置？\u0026#34;, \u0026#34;在登录页点击『忘记密码』，通过绑定的手机号或邮箱接收验证码，按指引设置新密码即可。\u0026#34;), (\u0026#34;今日下单何时能到货？\u0026#34;, \u0026#34;当前时间为17:27，今日内下单且选择快递配送，同城预计4月30日前送达，异地需3-5个工作日（五一假期可能延迟）。\u0026#34;) ] for question, answer in qa_pairs: result = chroma_client.upsert_vectors(question, answer) print(f\u0026#34;问题：{question}\\n回答：{answer}\\n结果：{result}\\n\u0026#34;) # # 2. 查询相似问题 print(\u0026#34;\\n2. 查询相似问题示例\u0026#34;) test_queries = [ \u0026#34;我要退款怎么操作\u0026#34;, # \u0026#34;订单地址写错了怎么改\u0026#34;, # \u0026#34;几点发货\u0026#34; ] # for query in test_queries: query_text, matched_question, matched_answer = chroma_client.query_similar(query) print(f\u0026#34;\\n用户问题：{query_text}\u0026#34;) if matched_question and matched_answer: print(f\u0026#34;匹配问题：{matched_question}\u0026#34;) print(f\u0026#34;匹配答案：{matched_answer}\u0026#34;) else: print(\u0026#34;未找到匹配的问答对\u0026#34;) # # 3. 删除示例 print(\u0026#34;\\n3. 删除示例\u0026#34;) # 获取第一个问题的hash值 import hashlib question_to_delete = qa_pairs[0][0] qhash = hashlib.md5(question_to_delete.encode(\u0026#39;utf-8\u0026#39;)).hexdigest() # 删除前查看统计信息 print(\u0026#34;删除前统计：\u0026#34;, chroma_client.get_collection_stats()) # 执行删除 result = chroma_client.delete_vectors(qhash) print(f\u0026#34;删除问题：{question_to_delete}\u0026#34;) print(f\u0026#34;删除结果：{result}\u0026#34;) # 删除后查看统计信息 print(\u0026#34;删除后统计：\u0026#34;, chroma_client.get_collection_stats()) if __name__ == \u0026#34;__main__\u0026#34;: demo_chroma_operations() 基本操作示例 1. 插入/更新向量数据 1 2 3 4 # 插入问答对 question = \u0026#34;如何申请退款？\u0026#34; answer = \u0026#34;您可以在订单详情页面点击\u0026#39;申请退款\u0026#39;按钮，填写退款原因后提交申请。\u0026#34; result = chroma_client.upsert_vectors(question, answer) 2. 查询相似问题 1 2 3 4 5 6 7 # 查询相似问题 query_text = \u0026#34;我要退款怎么操作\u0026#34; query_text, matched_question, matched_answer = chroma_client.query_similar(query_text) if matched_question and matched_answer: print(f\u0026#34;匹配问题：{matched_question}\u0026#34;) print(f\u0026#34;匹配答案：{matched_answer}\u0026#34;) 3. 删除向量数据 1 2 3 4 5 6 7 8 9 10 # 删除指定问题的向量数据 chroma_client = get_chroma_client() status = chroma_client.get_collection_stats() log.info(f\u0026#34;删除前ChromaDB状态: {status}\u0026#34;) question = \u0026#34;如何申请退款？\u0026#34; qhash = hashlib.md5(question.encode(\u0026#39;utf-8\u0026#39;)).hexdigest() result = chroma_client.delete_vectors(qhash) log.info(f\u0026#34;删除问题：{question},结果：{result}\u0026#34;) status = chroma_client.get_collection_stats() log.info(f\u0026#34;删除后ChromaDB状态: {status}\u0026#34;) 实际应用场景 在客服系统中，这个向量库的应用流程大致为：\n管理员预先录入常见问题和标准答案 系统将问题转换为向量并存储在ChromaDB中 用户提问时，系统将问题转换为向量 使用 query_similar 方法在向量库中查找语义最相似的问题 如果找到相似度足够高的匹配（距离小于0.4），返回对应的标准答案 在把答案 通过约束的Prompt 给ChatGPT 回答 ChatGPT返回更完整的答案，发送给用户 这种基于向量的匹配方式比传统的关键词匹配更智能，能够理解问题的语义而非仅仅是字面表达，大大提高了自动问答系统的准确性和用户体验。 注意事项 向量相似度阈值设置：\n当前示例中相似度阈值设置为 0.4，可以根据实际需求调整 阈值越小，匹配要求越严格 持久化存储：\nChromaDB 默认将数据存储在项目根目录的 chroma_db 文件夹中 可以通过 persist_directory 参数自定义存储路径 错误处理：\n示例代码中包含了基本的错误处理机制 建议在生产环境中添加更完善的错误处理和重试机制 参考文档： https://zhuanlan.zhihu.com/p/658217843 https://www.cnblogs.com/rude3knife/p/chroma_tutorial.html\n","date":"2025-04-29T09:01:10Z","image":"https://www.ownit.top/title_pic/76.jpg","permalink":"https://www.ownit.top/p/202504290901/","title":"Chroma向量检索实战：打造智能客服的“最强大脑”"},{"content":"背景 在持续集成和持续部署的流程中，频繁的构建和部署会生成大量的镜像版本。这些历史镜像如果不及时清理，会占用大量的存储空间，导致 Harbor 仓库膨胀，影响系统性能。 目前 公司的Harbor存储已经占用1T，好多的repo的镜像tag达到上百多，没有清理十分占用空间。 目前研究了下，可采取两种方案进行清理。（各自有各的优势） 1、采用Harbor自带的清理策略 2、写脚本，可以自定义控制清理，比较方便。\n目的 释放存储空间：​清理不再使用的镜像，释放被占用的磁盘空间。\n提高系统性能：​减少无用数据，提高 Harbor 的响应速度和稳定性。\n简化管理：​通过自动化策略，减少手动清理的工作量，提高管理效率。\n方案一 Harbor 镜像自动清理策略 Harbor 提供了基于项目的镜像保留策略（Retention Policy），允许用户根据特定的规则自动保留或删除镜像。该策略支持以下配置：（需要每个仓库分别配置）\n匹配规则：通过通配符匹配特定的仓库或镜像。 保留策略：设置保留最近推送或拉取的镜像数量或时间范围。 标签过滤：根据镜像的标签（Tag）进行筛选，如保留特定前缀的标签。 删除未打标签的镜像：选择是否删除未打标签的镜像（Untagged Artifacts）。 1、使用管理员账号登录 Harbor 的 Web 控制台。 2、在左侧导航栏选择“项目”，点击需要配置清理策略的项目名称。 3、配置保留策略\n在项目页面，选择“策略”选项卡，点击“添加规则”按钮，进行以下配置：\n规则名称：为策略命名，便于识别。 匹配仓库：使用通配符（如 **）匹配需要应用策略的仓库。 保留策略：选择保留最近推送或拉取的镜像数量或时间范围。 标签过滤：设置标签匹配规则，如保留以 release- 开头的标签。 删除未打标签的镜像：根据需要选择是否删除未打标签的镜像。 4、设置定时任务 在策略页面，点击“编辑”按钮，配置策略的执行时间。Harbor 使用 UTC 时间，需注意与本地时间的差异。例如，设置每周六凌晨 0 点执行：\n1 0 0 0 ? * SAT 这表示每周六 UTC 时间 0 点执行，若在中国时区，实际执行时间为周六上午 8 点。 5、手动执行策略（可选） 在策略页面，点击“立即执行”按钮，可以手动触发策略，立即清理符合条件的镜像。 方案二 使用Shell脚本处理 harbor_clean.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 #!/bin/bash #/********************************************************** # * Author : 南宫乘风 # * Email : 1794748404@qq.com # * Last modified : 2024-01-17 10:00 # * Filename : harbor_clean.sh # * Description : 清理指定Harbor项目的镜像，只保留最近5次 # * *******************************************************/ set -e # 日志文件配置 LOG_FILE=\u0026#34;./harbor_clean.log\u0026#34; user=$(whoami) # Harbor配置(请配置账号和密码) HARBOR_URL=\u0026#34;bdata-hub.xxxxx.cn\u0026#34; HARBOR_USERNAME=\u0026#34;admin\u0026#34; HARBOR_PASSWORD=\u0026#34;xxxxxxxxx\u0026#34; KEEP_NUM=15 # 保留最近的5个镜像 # 指定要清理的项目列表 PROJECTS=(\u0026#34;service\u0026#34; \u0026#34;pre\u0026#34;) # 创建临时目录 TMP_DIR=\u0026#34;$PWD/harbor_clean_tmp\u0026#34; # 日志函数 function log_info() { content=\u0026#34;[INFO] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; echo $content \u0026gt;\u0026gt;$LOG_FILE echo -e \u0026#34;\\033[32m${content}\\033[0m\u0026#34; } function log_warn() { content=\u0026#34;[WARN] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; echo $content \u0026gt;\u0026gt;$LOG_FILE echo -e \u0026#34;\\033[33m${content}\\033[0m\u0026#34; } function log_err() { content=\u0026#34;[ERROR] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; echo $content \u0026gt;\u0026gt;$LOG_FILE echo -e \u0026#34;\\033[31m${content}\\033[0m\u0026#34; } # 获取指定项目的仓库列表 function get_repos_list() { local project=$1 log_info \u0026#34;获取项目 ${project} 的仓库列表\u0026#34; mkdir -p \u0026#34;$TMP_DIR/repos\u0026#34; local repos_list=$(curl -s -k -u \u0026#34;${HARBOR_USERNAME}:${HARBOR_PASSWORD}\u0026#34; \\ \u0026#34;https://${HARBOR_URL}/api/v2.0/projects/${project}/repositories?page=1\u0026amp;page_size=100\u0026#34;) if [ $? -ne 0 ]; then log_err \u0026#34;获取项目 ${project} 的仓库列表失败\u0026#34; return 1 fi echo \u0026#34;${repos_list}\u0026#34; | jq \u0026#39;.[]\u0026#39; | jq -r \u0026#39;.name\u0026#39; | awk -F \u0026#34;/\u0026#34; \u0026#39;{print $2}\u0026#39; \u0026gt;\u0026#34;$TMP_DIR/repos/${project}.txt\u0026#34; log_info \u0026#34;成功获取项目 ${project} 的仓库列表\u0026#34; } # 获取镜像标签列表 function get_tag_list() { local project=$1 local repo=$2 log_info \u0026#34;获取仓库 ${project}/${repo} 的标签列表\u0026#34; mkdir -p \u0026#34;$TMP_DIR/tags/$project/$repo\u0026#34; local artifacts=$(curl -s -k -u \u0026#34;${HARBOR_USERNAME}:${HARBOR_PASSWORD}\u0026#34; \\ \u0026#34;https://${HARBOR_URL}/api/v2.0/projects/${project}/repositories/${repo}/artifacts?page=1\u0026amp;page_size=100\u0026amp;with_tag=true\u0026amp;with_label=false\u0026amp;with_scan_overview=false\u0026amp;with_signature=false\u0026amp;with_immutable_status=false\u0026#34;) if [ $? -ne 0 ]; then log_err \u0026#34;获取仓库 ${project}/${repo} 的标签列表失败\u0026#34; return 1 fi echo \u0026#34;${artifacts}\u0026#34; | jq \u0026#39;.[]\u0026#39; | jq -r \u0026#39;.digest\u0026#39; \u0026gt;\u0026#34;$TMP_DIR/tags/$project/$repo/digests.txt\u0026#34; log_info \u0026#34;成功获取仓库 ${project}/${repo} 的标签列表\u0026#34; } # 删除旧镜像 function delete_images() { local project=$1 local repo=$2 local digest_file=\u0026#34;$TMP_DIR/tags/$project/$repo/digests.txt\u0026#34; if [ ! -f \u0026#34;$digest_file\u0026#34; ]; then log_warn \u0026#34;仓库 ${project}/${repo} 的标签文件不存在，跳过\u0026#34; return fi local total_num=$(wc -l \u0026lt;\u0026#34;$digest_file\u0026#34;) if [ $total_num -gt $KEEP_NUM ]; then local delete_num=$((total_num - KEEP_NUM)) log_info \u0026#34;仓库 ${project}/${repo} 有 ${delete_num} 个镜像需要删除\u0026#34; tail -n $delete_num \u0026#34;$digest_file\u0026#34; | while read digest; do log_info \u0026#34;准备删除镜像: ${project}/${repo}@${digest}\u0026#34; curl -X DELETE -s -k -u \u0026#34;${HARBOR_USERNAME}:${HARBOR_PASSWORD}\u0026#34; \\ \u0026#34;https://${HARBOR_URL}/api/v2.0/projects/${project}/repositories/${repo}/artifacts/${digest}\u0026#34; if [ $? -eq 0 ]; then log_info \u0026#34;成功删除镜像: ${project}/${repo}@${digest}\u0026#34; else log_err \u0026#34;删除镜像失败: ${project}/${repo}@${digest}\u0026#34; fi done log_info \u0026#34;仓库 ${project}/${repo} 的镜像清理完成\u0026#34; else log_info \u0026#34;仓库 ${project}/${repo} 的镜像数量未超过保留限制，跳过清理\u0026#34; fi } # 清理临时文件 function cleanup() { log_info \u0026#34;清理临时文件\u0026#34; rm -rf \u0026#34;$TMP_DIR\u0026#34; } # 主函数 function main() { log_info \u0026#34;开始执行Harbor镜像清理脚本\u0026#34; mkdir -p \u0026#34;$TMP_DIR\u0026#34; trap cleanup EXIT # 遍历指定的项目列表 for project in \u0026#34;${PROJECTS[@]}\u0026#34;; do log_info \u0026#34;开始处理项目: ${project}\u0026#34; get_repos_list \u0026#34;$project\u0026#34; if [ -f \u0026#34;$TMP_DIR/repos/${project}.txt\u0026#34; ]; then while read repo; do get_tag_list \u0026#34;$project\u0026#34; \u0026#34;$repo\u0026#34; delete_images \u0026#34;$project\u0026#34; \u0026#34;$repo\u0026#34; done \u0026lt;\u0026#34;$TMP_DIR/repos/${project}.txt\u0026#34; else log_warn \u0026#34;项目 ${project} 的仓库列表文件不存在\u0026#34; fi done log_info \u0026#34;Harbor镜像清理脚本执行完成\u0026#34; } # 执行主函数 main 1 2 3 #定时任务 crontab -e /bin/bash /root/shell/harbor_clean.sh 运行日志： 垃圾回收（Garbage Collection） -配置并执行镜像清理策略后，镜像的元数据会被删除，但实际的存储空间不会立即释放。为彻底释放空间，需要执行垃圾回收操作。\n执行步骤\n在左侧导航栏选择“系统管理” \u0026gt; “垃圾清理”。 点击“立即清理”按钮，执行垃圾回收操作。 在“历史记录”中查看垃圾回收的执行情况和释放的空间大小。 垃圾回收操作可以配置为定时执行，确保系统定期释放无用的存储空间。 注意事项 策略优先级：多个策略可能存在冲突，Harbor 按照策略的创建顺序依次执行，建议合理规划策略的优先级。\n标签管理：合理使用标签（Tag）命名规范，有助于策略的精确匹配和管理。\n测试策略：在生产环境应用策略前，建议在测试环境验证策略的效果，避免误删重要镜像。\n备份数据：执行清理操作前，建议备份重要数据，以防止数据丢失。\n","date":"2025-04-27T22:48:25Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/202504272248/","title":"Harbor2.0仓库镜像清理策略"},{"content":"MCP简介 Model Context Protocol (MCP) 是一个专门为 LLM（大语言模型）应用设计的协议，它允许你构建服务器以安全、标准化的方式向 LLM 应用程序公开数据和功能。FastMCP 作为 Python 生态中的一款轻量级框架，利用装饰器来简化路由与工具函数的开发，帮助开发者快速构建面向工具的服务端应用。 MCP Server 提供了 3 种核心原语，每种原语都有其特定的用途和特点：\n1. Tool（工具）：\nTool 允许服务器公开可执行的函数，这些函数可由客户端调用并由 LLM 使用来执行操作。Tool 不仅人让 LLM 能从外部获取信息，还能执行写入或操作，为 LLM 提供真正的行动力。 模型控制：Tool 直接暴露给 LLM 可执行函数，让模型可以主动调用。 2. Resource（资源）：\nResource 表示服务器希望提供给客户端的任何类型的只读数据。这可能包括：文件内容、数据库记录、图片、日志等等。 应用控制：Resource 由客户端或应用管理，用于为 LLM 提供上下文内容。 3. Prompt（提示模板） ：\nPrompt 是由服务器定义的可重用的模板，用户可以选择这些模板来引导或标准化与 LLM 的交互过程。例如，Git MCP Server 可以提供一个“生成提交信息”的提示模板，用户可以用它来创建标准化的提交消息。 用户控制：Prompt 通常由用户自行选择。 MCP概述 MCP服务端当前支持两种与客户端的数据通信方式：标准输入输出(stdio) 和 基于Http的服务器推送事件(http sse)\n标准输入输出(stdio) 原理： 标准输入输出是一种用于本地通信的传输方式。在这种模式下，MCP 客户端会将服务器程序作为子进程启动，双方通过约定的标准输入和标准输出（可能是通过共享文件等方法）进行数据交换。具体而言，客户端通过标准输入发送请求，服务器通过标准输出返回响应。 适用场景： 标准输入输出方式适用于客户端和服务器在同一台机器上运行的场景（本地自行编写服务端或将别人编写的服务端代码pull到本地执行），确保了高效、低延迟的通信。这种直接的数据传输方式减少了网络延迟和传输开销，适合需要快速响应的本地应用。\n基于Http的服务器推送事件(http sse) 原理: 客户端和服务端通过 HTTP 协议进行通信，利用 SSE 实现服务端向客户端的实时数据推送，服务端定义了/see与/messages接口用于推送与接收数据。这里要注意SSE协议和WebSocket协议的区别，SSE协议是单向的，客户端和服务端建立连接后，只能由服务端向客户端进行消息推送。而WebSocket协议客户端和服务端建立连接后，客户端可以通过send向服务端发送数据，并通过onmessage事件接收服务端传过来的数据。\n适用场景： 适用于客户端和服务端位于不同物理位置的场景，尤其是对于分布式或远程部署的场景，基于 HTTP 和 SSE 的传输方式更合适。如：Prometheus，MySQL，MongoDB 数据库等等\n环境介绍 uv快速构建Python环境 Win打开 powershell 进行安装 参考文档：https://docs.astral.sh/uv/getting-started/installation/#standalone-installer\n1 2 3 4 5 6 7 8 # On macOS and Linux. curl -LsSf https://astral.sh/uv/install.sh | sh # On Windows. powershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; # With pip. pip install uv 安装之后，可以通过uv help命令检查是否安装成功： 卸载\n1 powershell -c \u0026#34;irm https://astral.sh/uv/install.ps1 | more\u0026#34; 基本使用方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 创建项目文件夹、同时初始化并指定python版本 uv init py-app -p 3.11.9 # 创建并激活虚拟环境 uv venv source .venv/bin/activate # 退出虚拟环境 deactivate uv venv --seed # 强制安装基础包（如pip, setuptools, wheel） # 同步依赖 uv sync # 安装 requirements.txt uv pip install -r requirements.txt # 安装包（使用清华源） cd py-app uv add pandas pillow --default-index \u0026#34;https://pypi.tuna.tsinghua.edu.cn/simple\u0026#34; # 安装CUDA版的torch（使用南京大学源） uv pip install torch torchvision torchaudio --index-url https://mirror.nju.edu.cn/pytorch/whl/cu126 # 执行python uv run app.py Python版本的安装和管理\n1 2 3 4 5 6 7 8 # 显示当前已经安装的和可供安装的Python版本 uv python list # 安装指定的Python版本（注意：安装的Python并非全局可用，在命令行工具中输入Python无反映） uv python install 3.10 3.11 3.12 # 卸载指定的版本 uv python uninstall 3.10 https://www.cnblogs.com/wang_yb/p/18635441\nhttps://zhuanlan.zhihu.com/p/1888904532131575259\n版本推荐 Python 版本： 推荐使用 Python 3.10 及以上版本，以便更好地支持异步函数（async/await） 我的版本：Python 3.14\nFastMCP 库： 该库需要通过 pip 或其他方式安装。安装指令如下：\n1 pip install fastmcp 运行方式： 本示例采用标准输入/输出（stdio）作为传输层，可根据实际需求选择其他传输方式，如网络通信等。\n实现步骤 下面按照步骤解析如何构建一个简单但功能完备的 MCP 服务端：\nstdio 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from mcp.server.fastmcp import FastMCP mcp = FastMCP(\u0026#34;Demo 🚀\u0026#34;) @mcp.tool() def add(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;两个数字相加\u0026#34;\u0026#34;\u0026#34; return a + b @mcp.tool() async def calculate(expression: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 计算一个简单的数学表达式。 Args: expression: 要计算的数学表达式（如\u0026#34;1 + 2\u0026#34;） Returns: str: 计算结果 \u0026#34;\u0026#34;\u0026#34; try: result = eval(expression) return f\u0026#34;计算结果: {result}\u0026#34; except Exception as e: return f\u0026#34;计算错误: {str(e)}\u0026#34; @mcp.resource(\u0026#34;greeting://{name}\u0026#34;) def get_greeting(name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;获取个性化问候语\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#39;stdio\u0026#39;) 启动调试：\n1 mcp dev main.py 错误报错：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Created web app transport Created web app transport Set up MCP proxy file:///C:/Users/admin/AppData/Local/npm-cache/_npx/5a9d879542beca3a/node_modules/@modelcontextprotocol/sdk/dist/esm/server/sse.js:103 throw new Error(\u0026#34;Not connected\u0026#34;); ^ Error: Not connected at SSEServerTransport.send (file:///C:/Users/admin/AppData/Local/npm-cache/_npx/5a9d879542beca3a/node_modules/@modelcontextprotocol/sdk/dist/esm/server/sse.js:103:19) at Socket.\u0026lt;anonymous\u0026gt; (file:///C:/Users/admin/AppData/Local/npm-cache/_npx/5a9d879542beca3a/node_modules/@modelcontextprotocol/inspector/server/build/index.js:101:33) at Socket.emit (node:events:517:28) at addChunk (node:internal/streams/readable:368:12) at readableAddChunk (node:internal/streams/readable:341:9) at Readable.push (node:internal/streams/readable:278:10) at Pipe.onStreamRead (node:internal/stream_base_commons:190:23) 解决方案：\n1 2 3 npm install -g @modelcontextprotocol/server-filesystem npm install -g @modelcontextprotocol/server-memory npm install -g @modelcontextprotocol/server-brave-search sse模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from mcp.server.fastmcp import FastMCP mcp = FastMCP(\u0026#34;Demo 🚀\u0026#34;) @mcp.tool() def add(a: int, b: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;两个数字相加\u0026#34;\u0026#34;\u0026#34; return a + b @mcp.tool() async def calculate(expression: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 计算一个简单的数学表达式。 Args: expression: 要计算的数学表达式（如\u0026#34;1 + 2\u0026#34;） Returns: str: 计算结果 \u0026#34;\u0026#34;\u0026#34; try: result = eval(expression) return f\u0026#34;计算结果: {result}\u0026#34; except Exception as e: return f\u0026#34;计算错误: {str(e)}\u0026#34; @mcp.resource(\u0026#34;greeting://{name}\u0026#34;) def get_greeting(name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;获取个性化问候语\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#39;sse\u0026#39;) 在 FastMCP 中，有几个可以设置 SSE 协议相关的参数：\nhost: 服务地址，默认为 0.0.0.0 port: 服务端口，默认为 8000。上述代码中，我设置为 9000 sse_path：sse 的路由，默认为 /sse Vscode-Cline调用 安装Vscode和Cline\nstdio模式调用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;weather\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uv\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;--directory\u0026#34;, \u0026#34;E:\\\\Code\\\\Python\\\\weather\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;main.py\u0026#34; ], \u0026#34;timeout\u0026#34;: 30 } } } sse模式 1 2 3 4 5 6 7 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;weather\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://192.168.96.19:8000/sse\u0026#34; } } } 参考文档： https://github.com/GobinFan/python-mcp-server-client?tab=readme-ov-file\nhttps://www.cnblogs.com/xiao987334176/p/18821197\nhttps://www.cnblogs.com/ryanzheng/p/18781666\nhttps://www.cnblogs.com/xiao987334176/p/18822444\nhttps://www.modb.pro/db/1878984329888542720\nhttps://blog.moontak.com/id/522381/\nhttps://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide\nhttps://zhuanlan.zhihu.com/p/28700850694\n","date":"2025-04-17T09:16:33Z","image":"https://www.ownit.top/title_pic/51.jpg","permalink":"https://www.ownit.top/p/202504170916/","title":"Python使用FastMCP开发MCP服务端"},{"content":"项目背景 在云原生时代，Kubernetes已成为容器编排的事实标准。但随着业务规模的扩大，企业往往需要管理多个Kubernetes集群。本文通过一个实际项目，详细介绍如何基于Go语言和Gin框架开发一个支持多集群管理的Kubernetes控制平台，实现Pod资源的全生命周期管理（增删改查、日志查看），并支持数据过滤、排序和分页功能。\nClient-go 介绍 client-go是kubernetes官方提供的go语言的客户端库，go应用使用该库可以访问kubernetes的API Server，这样我们就能通过编程来对kubernetes资源进行增删改查操作； 除了提供丰富的API用于操作kubernetes资源，client-go还为controller和operator提供了重要支持client-go的informer机制可以将controller关注的资源变化及时带给此controller，使controller能够及时响应变化。 通过client-go提供的客户端对象与kubernetes的API Server进行交互，而client-go提供了以下四种客户端对象： RESTClient、ClientSet、DynamicClient、DiscoveryClient 代码示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package main import ( \u0026#34;context\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func main() { // 定义kubeconfig文件路径 kubeconfig := \u0026#34;config/k8s.yaml\u0026#34; // 从kubeconfig文件中构建配置 config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, kubeconfig) if err != nil { // 如果构建配置失败，则抛出错误 panic(err.Error()) } // 使用配置创建kubernetes客户端 client, err := kubernetes.NewForConfig(config) if err != nil { // 如果创建客户端失败，则抛出错误 panic(err.Error()) } // 使用客户端获取default命名空间下的所有Pod pods, err := client.CoreV1().Pods(\u0026#34;default\u0026#34;).List(context.TODO(), metav1.ListOptions{}) if err != nil { // 如果获取Pod失败，则抛出错误 panic(err.Error()) } // 遍历所有Pod，并打印Pod名称 for _, pod := range pods.Items { println(pod.Name) } } 打印结果 常用方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 获取podList类型的pod列表 podList, err := client.CoreV1().Pods(namespace).List(context.TODO(), metav1.ListOptions{}) // 获取pod的详情 pod, err := client.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) // 删除pod err := client.CoreV1().Pods(namespace).Delete(context.TODO(), podName, metav1.DeleteOptions{}) // 更新pod _, err = client.CoreV1().Pods(namespace).Update(context.TODO(), pod, metav1.UpdateOptions{}) // 获取deployment副本数 scale, err := K8s.ClientSet.AppsV1().Deployments(namespace).GetScale(context.TODO(),deploymentName, metav1.GetOptions{}) // 创建deployment deployment, err =K8s.ClientSet.AppsV1().Deployments(data.Namespace).Create(context.TODO(), deployment,metav1.CreateOptions{}) // 更新deployment（部分yaml） deployment, err = K8s.ClientSet.AppsV1().Deployments(namespace).Patch(context.TODO(),deploymentName, \u0026#34;application/strategic-merge-patch+json\u0026#34;, patchByte,metav1.PatchOptions{}) 项目架构与核心模块 项目采用分层设计，核心模块包括：\n路由层（Router）：基于Gin框架定义API端点。 控制层（Controller）：处理HTTP请求，参数校验，调用服务层逻辑。 服务层（Service）：封装业务逻辑，与Kubernetes API交互。 数据层（Kubernetes Client）：管理多集群连接，提供客户端实例。 工具模块：数据筛选、分页、日志处理等。 技术栈 Gin框架：高性能HTTP框架，用于路由和请求处理。 Kubernetes Client-go：官方Go语言客户端，操作Kubernetes资源。 多集群管理：通过动态加载Kubeconfig支持多集群。 数据分页与过滤：自定义排序、过滤和分页逻辑。 多集群管理实现 在k8s_client.go中，项目通过K8s结构体实现了多集群管理：\n1 2 3 4 type k8s struct { ClientMap map[string]*kubernetes.Clientset // 多集群Client KubeConfMap map[string]string // 多集群配置 } 初始化时，从配置文件读取多个集群的kubeconfig，并为每个集群创建独立的ClientSet：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Init 初始化k8s client func (k *k8s) Init() { // 创建一个空的map，用于存储Kubeconfigs mp := make(map[string]string, 0) // 创建一个空的map，用于存储Kubernetes的Clientset k.ClientMap = make(map[string]*kubernetes.Clientset, 0) // 反序列化 if err := json.Unmarshal([]byte(config.Kubeconfigs), \u0026amp;mp); err != nil { // 如果反序列化失败，则抛出异常 panic(fmt.Sprintf(\u0026#34;反序列化Kubeconfigs失败,%v\\n\u0026#34;, err)) } // 将反序列化后的结果存储到KubeConfMap中 k.KubeConfMap = mp // 初始化集群Client for key, value := range mp { // 根据Kubeconfigs中的配置，初始化集群Client client, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, value) if err != nil { // 如果初始化失败，则抛出异常 panic(fmt.Sprintf(\u0026#34;初始化集群%s失败,%v\\n\u0026#34;, key, err)) } clientSet, err := kubernetes.NewForConfig(client) if err != nil { // 如果初始化失败，则抛出异常 panic(fmt.Sprintf(\u0026#34;初始化集群%s失败,%v\\n\u0026#34;, key, err)) } // 将初始化后的Clientset存储到ClientMap中 k.ClientMap[key] = clientSet // 打印初始化成功的日志 logger.Info(fmt.Sprintf(\u0026#34;初始化集群%s成功\u0026#34;, key)) } } 这种设计使得在API调用时，只需指定集群名称即可操作对应的集群资源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // GetClient 根据集群名称获取Client // 根据集群名称获取kubernetes客户端 func (k *k8s) GetClient(clusterName string) (*kubernetes.Clientset, error) { // 从ClientMap中获取指定集群名称的客户端 client, ok := k.ClientMap[clusterName] // 如果不存在，则返回错误 if !ok { logger.Error(fmt.Sprintf(\u0026#34;集群%s不存在,无法获取Client\\n\u0026#34;, clusterName)) return nil, errors.New(fmt.Sprintf(\u0026#34;集群%s不存在,无法获取Client\\n\u0026#34;, clusterName)) } // 返回客户端 return client, nil } 核心功能实现 Pod列表查询 Pod列表查询功能实现了过滤、分页和排序等高级特性，代码位于pod.go和dataselect.go中。\n数据结构定义\n首先定义了通用的数据选择器结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // dataSelect 用于封装排序、过滤、分页的数据类型 type dataSelector struct { GenericDateSelect []DataCell // 可排序的数据集合 dataSelectQuery *DataSelectQuery // 查询条件 } // DataCell 用于各种资源list的类型转换，转换后可以使用dataSelector的自定义排序方法 type DataCell interface { GetCreation() time.Time GetName() string } // DataSelectQuery 定义过滤和分页的属性，过滤：Name， 分页：Limit和Page // Limit是单页的数据条数 // Page是第几页 type DataSelectQuery struct { FilterQuery *FilterQuery // 过滤条件 PaginationQuery *PaginationQuery // 分页条件 } type FilterQuery struct { Name string } type PaginationQuery struct { Limit int Page int } 过滤实现\n过滤功能通过字符串匹配实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Filter 方法用于过滤元素，比较元素的Name属性，若包含，再返回 func (d *dataSelector) Filter() *dataSelector { //如Name的传参为空，则返回所有元素 if d.dataSelectQuery.FilterQuery.Name == \u0026#34;\u0026#34; { return d } // 若Name的传参不为空，则返回元素中包含Name的元素 var filteredList []DataCell for _, item := range d.GenericDateSelect { matched := true objName := item.GetName() if !strings.Contains(objName, d.dataSelectQuery.FilterQuery.Name) { matched = false continue } if matched { filteredList = append(filteredList, item) } } d.GenericDateSelect = filteredList // 返回过滤后的元素 return d } 分页实现 分页逻辑处理了边界情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (d *dataSelector) Paginate() *dataSelector { limit := d.dataSelectQuery.PaginationQuery.Limit page := d.dataSelectQuery.PaginationQuery.Page if limit \u0026lt; 1 || page \u0026lt; 1 { return d } startIndex := (page - 1) * limit endIndex := page * limit if len(d.GenericDateSelect) \u0026lt; endIndex { endIndex = len(d.GenericDateSelect) } d.GenericDateSelect = d.GenericDateSelect[startIndex:endIndex] return d } 排序实现 通过实现sort.Interface接口实现排序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (d *dataSelector) Len() int { return len(d.GenericDateSelect) } func (d *dataSelector) Swap(i, j int) { d.GenericDateSelect[i], d.GenericDateSelect[j] = d.GenericDateSelect[j], d.GenericDateSelect[i] } func (d *dataSelector) Less(i, j int) bool { a := d.GenericDateSelect[i].GetCreation() b := d.GenericDateSelect[j].GetCreation() return b.Before(a) // 降序排列 } dataselect.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 package service import ( \u0026#34;sort\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; ) /** * @Author: 南宫乘风 * @Description:定义数据结构 * @File: dataselect.go * @Email: 1794748404@qq.com * @Date: 2025-03-20 14:38 */ // dataSelect 用于封装排序、过滤、分页的数据类型 type dataSelector struct { GenericDateSelect []DataCell // 可排序的数据集合 dataSelectQuery *DataSelectQuery // 查询条件 } // DataCell 用于各种资源list的类型转换，转换后可以使用dataSelector的自定义排序方法 type DataCell interface { GetCreation() time.Time GetName() string } // DataSelectQuery 定义过滤和分页的属性，过滤：Name， 分页：Limit和Page // Limit是单页的数据条数 // Page是第几页 type DataSelectQuery struct { FilterQuery *FilterQuery // 过滤条件 PaginationQuery *PaginationQuery // 分页条件 } type FilterQuery struct { Name string } type PaginationQuery struct { Limit int Page int } //实现自定义结构的排序，需要重写Len、Swap、Less方法 // Len 方法用于获取数据长度 func (d *dataSelector) Len() int { return len(d.GenericDateSelect) } // Swap 方法用于数组中的元素在比较大小后的位置交换，可定义升序或降序 i j 是切片的下标 func (d *dataSelector) Swap(i, j int) { // 交换GenericDateSelect数组中的第i个和第j个元素 d.GenericDateSelect[i], d.GenericDateSelect[j] = d.GenericDateSelect[j], d.GenericDateSelect[i] } // Less 方法用于定义数组中元素排序的“大小”的比较方式 // Less 方法返回true表示第i个元素小于第j个元素，返回false表示第i个元素大于第j个元素 func (d *dataSelector) Less(i, j int) bool { a := d.GenericDateSelect[i].GetCreation() b := d.GenericDateSelect[j].GetCreation() return b.Before(a) } // Sort 重新以上3个方法，使用sort.Sort()方法进行排序 func (d *dataSelector) Sort() *dataSelector { // 使用sort.Sort()方法进行排序 sort.Sort(d) return d } // 过滤 // Filter 方法用于过滤元素，比较元素的Name属性，若包含，再返回 func (d *dataSelector) Filter() *dataSelector { //如Name的传参为空，则返回所有元素 if d.dataSelectQuery.FilterQuery.Name == \u0026#34;\u0026#34; { return d } // 若Name的传参不为空，则返回元素中包含Name的元素 var filteredList []DataCell for _, item := range d.GenericDateSelect { matched := true objName := item.GetName() if !strings.Contains(objName, d.dataSelectQuery.FilterQuery.Name) { matched = false continue } if matched { filteredList = append(filteredList, item) } } d.GenericDateSelect = filteredList // 返回过滤后的元素 return d } // 分页 // Paginate 方法用于数组分页，根据Limit和Page的传参，返回数据 func (d *dataSelector) Paginate() *dataSelector { limit := d.dataSelectQuery.PaginationQuery.Limit page := d.dataSelectQuery.PaginationQuery.Page // 验证参数合法，若不合法，则返回所有元素 if limit \u0026lt; 1 || page \u0026lt; 1 { return d } // 举例：25个元素的数组，limit是10，page是3，startIndex是20，endIndex是30（实际上endIndex是25）、 startIndex := (page - 1) * limit endIndex := page * limit // 处理最后一页，这时候就把endIndex由30改为25了 if len(d.GenericDateSelect) \u0026lt; endIndex { endIndex = len(d.GenericDateSelect) } d.GenericDateSelect = d.GenericDateSelect[startIndex:endIndex] return d } // 定义podCell 类型，实现两个方法GetCreation和GetName，可进行类型转换 type podCell corev1.Pod func (p podCell) GetCreation() time.Time { return p.CreationTimestamp.Time } func (p podCell) GetName() string { return p.Name } Pod详情查询 通过Kubernetes客户端直接获取Pod详情：\n1 2 3 4 5 6 7 8 func (p *pod) GetPodDetail(client *kubernetes.Clientset, namespace, podName string) (*corev1.Pod, error) { pod, err := client.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) if err != nil { logger.Error(errors.New(\u0026#34;获取Pod详情失败, \u0026#34; + err.Error())) return nil, errors.New(\u0026#34;获取Pod详情失败, \u0026#34; + err.Error()) } return pod, nil } Pod日志查询 日志查询功能支持指定容器和返回行数限制：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func (p *pod) GetPodLog(client *kubernetes.Clientset, namespace, podName, containerName string) (log string, err error) { lineLimit := int64(config.PodLogTailLine) options := \u0026amp;corev1.PodLogOptions{ Container: containerName, TailLines: \u0026amp;lineLimit, } req := client.CoreV1().Pods(namespace).GetLogs(podName, options) podLogs, err := req.Stream(context.TODO()) if err != nil { logger.Error(errors.New(\u0026#34;获取Pod日志失败, \u0026#34; + err.Error())) return \u0026#34;\u0026#34;, errors.New(\u0026#34;获取Pod日志失败, \u0026#34; + err.Error()) } defer podLogs.Close() buf := new(bytes.Buffer) _, err = io.Copy(buf, podLogs) if err != nil { logger.Error(errors.New(\u0026#34;复制PodLog失败, \u0026#34; + err.Error())) return \u0026#34;\u0026#34;, errors.New(\u0026#34;复制PodLog失败, \u0026#34; + err.Error()) } return buf.String(), nil } pod.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 package service import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;io\u0026#34; \u0026#34;kubea-go/config\u0026#34; \u0026#34;github.com/aryming/logger\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; ) /** * @Author: 南宫乘风 * @Description: * @File: pod.go * @Email: 1794748404@qq.com * @Date: 2025-03-20 15:42 */ var Pod pod type pod struct { } // PodsResp 定义列表的返回内容，Items是pod元素列表，Total为pod元素数量 type PodsResp struct { Items []corev1.Pod `json:\u0026#34;items\u0026#34;` Total int `json:\u0026#34;total\u0026#34;` } // GetPods 获取pod列表，支持过滤和分页,排序 func (p *pod) GetPods(client *kubernetes.Clientset, filterName, namespace string, limit, page int) (*PodsResp, error) { // 获取podList类型的pod列表 podList, err := client.CoreV1().Pods(namespace).List(context.TODO(), metav1.ListOptions{}) if err != nil { logger.Error(errors.New(\u0026#34;获取Pod列表失败, \u0026#34; + err.Error())) return nil, errors.New(\u0026#34;获取Pod列表失败, \u0026#34; + err.Error()) } // 实例化dataSelector对象 selectableData := \u0026amp;dataSelector{ GenericDateSelect: p.toCells(podList.Items), dataSelectQuery: \u0026amp;DataSelectQuery{ FilterQuery: \u0026amp;FilterQuery{Name: filterName}, PaginationQuery: \u0026amp;PaginationQuery{ Limit: limit, Page: page, }, }, } //先过滤 filtered := selectableData.Filter() total := len(filtered.GenericDateSelect) data := filtered.Sort().Paginate() //将[]DataCell类型的pod列表转为v1.pod列表 pods := p.fromCells(data.GenericDateSelect) return \u0026amp;PodsResp{Items: pods, Total: total}, nil } // GetPodDetail 获取pod详情 func (p *pod) GetPodDetail(client *kubernetes.Clientset, namespace, podName string) (*corev1.Pod, error) { pod, err := client.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) if err != nil { logger.Error(errors.New(\u0026#34;获取Pod详情失败, \u0026#34; + err.Error())) return nil, errors.New(\u0026#34;获取Pod详情失败, \u0026#34; + err.Error()) } return pod, nil } // DeletePod 删除POD func (p *pod) DeletePod(client *kubernetes.Clientset, namespace, podName string) error { // 删除pod err := client.CoreV1().Pods(namespace).Delete(context.TODO(), podName, metav1.DeleteOptions{}) if err != nil { logger.Error(errors.New(\u0026#34;删除Pod失败, \u0026#34; + err.Error())) return errors.New(\u0026#34;删除Pod失败, \u0026#34; + err.Error()) } return nil } // UpdatePod 更新POD content参数是请求中传入的pod对象的json数据 func (p *pod) UpdatePod(client *kubernetes.Clientset, namespace, podName, content string) error { var pod = \u0026amp;corev1.Pod{} // 将content参数的json数据解析到pod对象中 err := json.Unmarshal([]byte(content), pod) if err != nil { logger.Error(errors.New(\u0026#34;反序列化失败, \u0026#34; + err.Error())) return errors.New(\u0026#34;反序列化失败, \u0026#34; + err.Error()) } // 更新pod _, err = client.CoreV1().Pods(namespace).Update(context.TODO(), pod, metav1.UpdateOptions{}) if err != nil { logger.Error(errors.New(\u0026#34;更新Pod失败, \u0026#34; + err.Error())) return errors.New(\u0026#34;更新Pod失败, \u0026#34; + err.Error()) } return nil } // GetPodContainer 获取pod容器 func (p *pod) GetPodContainer(client *kubernetes.Clientset, namespace, podName string) (containers []string, err error) { // 获取pod详情 pod, err := p.GetPodDetail(client, namespace, podName) if err != nil { logger.Error(errors.New(\u0026#34;获取Pod详情失败, \u0026#34; + err.Error())) return nil, errors.New(\u0026#34;获取Pod详情失败, \u0026#34; + err.Error()) } // 从pod详情中获取容器列表 for _, container := range pod.Spec.Containers { containers = append(containers, container.Name) } return containers, nil } // GetPodLog 获取pod内容器日志 func (p *pod) GetPodLog(client *kubernetes.Clientset, namespace, podName, containerName string) (log string, err error) { //设置日志的配置，容器名、tail的行数 lineLimit := int64(config.PodLogTailLine) options := \u0026amp;corev1.PodLogOptions{ Container: containerName, TailLines: \u0026amp;lineLimit, } // 获取request实例 req := client.CoreV1().Pods(namespace).GetLogs(podName, options) // 发起request请求，返回一个io.ReadCloser类型（等同于response.body） podLogs, err := req.Stream(context.TODO()) if err != nil { logger.Error(errors.New(\u0026#34;获取Pod日志失败, \u0026#34; + err.Error())) return \u0026#34;\u0026#34;, errors.New(\u0026#34;获取Pod日志失败, \u0026#34; + err.Error()) } defer podLogs.Close() //将response body写入到缓冲区，目的是为了转成string返回 buf := new(bytes.Buffer) _, err = io.Copy(buf, podLogs) if err != nil { logger.Error(errors.New(\u0026#34;复制PodLog失败, \u0026#34; + err.Error())) return \u0026#34;\u0026#34;, errors.New(\u0026#34;复制PodLog失败, \u0026#34; + err.Error()) } return buf.String(), nil } // toCells 方法用于将pod类型数组，转换成DataCell类型数组 func (p *pod) toCells(std []corev1.Pod) []DataCell { cells := make([]DataCell, len(std)) for i := range std { cells[i] = podCell(std[i]) } return cells } // fromCells 方法用于将DataCell类型数组，转换成pod类型数组 func (p *pod) fromCells(cells []DataCell) []corev1.Pod { pods := make([]corev1.Pod, len(cells)) for i := range cells { //cells[i].(podCell)就使用到了断言,断言后转换成了podCell类型，然后又转换成了Pod类型 pods[i] = corev1.Pod(cells[i].(podCell)) } return pods } Controller层面 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 package controller import ( \u0026#34;kubea-go/service\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/aryming/logger\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) /** * @Author: 南宫乘风 * @Description: * @File: pod.go.go * @Email: 1794748404@qq.com * @Date: 2025-03-25 15:37 */ var Pod pod type pod struct{} //Controller中的方法入参是gin.Context，用于从上下文中获取请求参数及定义响应内容 //流程：绑定参数\u0026#34;,调用service代码\u0026#34;,根据调用结果响应具体内容 // GetPods 获取pod列表，支持过滤、排序、分页 func (p *pod) GetPods(c *gin.Context) { //匿名结构体，用于声明入参，get请求为form格式，其他请求为json格式 params := new( struct { FilterName string `form:\u0026#34;filter_name\u0026#34;` Namespace string `form:\u0026#34;namespace\u0026#34;` Page int `form:\u0026#34;page\u0026#34;` Limit int `form:\u0026#34;limit\u0026#34;` Cluster string `form:\u0026#34;cluster\u0026#34;` }) //绑定参数，给匿名结构体中的属性赋值，值是入参 //\tform格式使用ctx.Bind方法，json格式使用ctx.ShouldBindJSON方法 if err := c.ShouldBind(params); err != nil { logger.Error(\u0026#34;Bind请求参数失败,\u0026#34; + err.Error()) // ctx.JSON方法用于返回响应内容，入参是状态码和响应内容，响应内容放入gin.H的map中 c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } // 获取k8s的连接方式 client, err := service.K8s.GetClient(params.Cluster) if err != nil { logger.Error(\u0026#34;获取k8s连接失败,\u0026#34; + err.Error()) c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } //service中的的方法通过 包名.结构体变量名.方法名 使用，serivce.Pod.GetPods() pods, err := service.Pod.GetPods(client, params.FilterName, params.Namespace, params.Limit, params.Page) if err != nil { logger.Error(\u0026#34;获取pod列表失败,\u0026#34; + err.Error()) c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;获取pod列表成功\u0026#34;, \u0026#34;data\u0026#34;: pods, }) } // GetPodDetail 获取pod详情 func (p *pod) GetPodDetail(cxt *gin.Context) { params := new(struct { Namespace string `form:\u0026#34;namespace\u0026#34;` PodName string `form:\u0026#34;pod_name\u0026#34;` Cluster string `form:\u0026#34;cluster\u0026#34;` }) if err := cxt.ShouldBind(params); err != nil { logger.Error(\u0026#34;Bind请求参数失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } client, err := service.K8s.GetClient(params.Cluster) if err != nil { logger.Error(\u0026#34;获取k8s连接失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } data, err := service.Pod.GetPodDetail(client, params.Namespace, params.PodName) if err != nil { logger.Error(\u0026#34;获取pod详情失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } cxt.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;获取pod详情成功\u0026#34;, \u0026#34;data\u0026#34;: data, }) } // DeletePod 删除pod func (p *pod) DeletePod(cxt *gin.Context) { params := new(struct { Namespace string `form:\u0026#34;namespace\u0026#34;` PodName string `form:\u0026#34;pod_name\u0026#34;` Cluster string `form:\u0026#34;cluster\u0026#34;` }) if err := cxt.ShouldBind(params); err != nil { logger.Error(\u0026#34;Bind请求参数失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } client, err := service.K8s.GetClient(params.Cluster) if err != nil { logger.Error(\u0026#34;获取k8s连接失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } if err := service.Pod.DeletePod(client, params.Namespace, params.PodName); err != nil { logger.Error(\u0026#34;删除pod失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } cxt.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;删除pod成功\u0026#34;, \u0026#34;data\u0026#34;: nil, }) } // UpdatePod 更新pod func (p *pod) UpdatePod(cxt *gin.Context) { params := new(struct { Namespace string `form:\u0026#34;namespace\u0026#34;` PodName string `form:\u0026#34;pod_name\u0026#34;` Content string `form:\u0026#34;content\u0026#34;` Cluster string `form:\u0026#34;cluster\u0026#34;` }) //PUT请求，绑定参数方法改为ctx.ShouldBindJSON if err := cxt.ShouldBindJSON(params); err != nil { logger.Error(\u0026#34;Bind请求参数失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } client, err := service.K8s.GetClient(params.Cluster) if err != nil { logger.Error(\u0026#34;获取k8s连接失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } if err := service.Pod.UpdatePod(client, params.Namespace, params.PodName, params.Content); err != nil { logger.Error(\u0026#34;更新pod失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } cxt.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;更新pod成功\u0026#34;, \u0026#34;data\u0026#34;: nil, }) } // GetPodContainer 获取pod容器 func (p *pod) GetPodContainer(cxt *gin.Context) { params := new(struct { Namespace string `form:\u0026#34;namespace\u0026#34;` PodName string `form:\u0026#34;pod_name\u0026#34;` Cluster string `form:\u0026#34;cluster\u0026#34;` }) // GET请求，绑定参数方法改为ctx.Bind if err := cxt.Bind(params); err != nil { logger.Error(\u0026#34;Bind请求参数失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } client, err := service.K8s.GetClient(params.Cluster) if err != nil { logger.Error(\u0026#34;获取k8s连接失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } containers, err := service.Pod.GetPodContainer(client, params.Namespace, params.PodName) if err != nil { logger.Error(\u0026#34;获取pod容器失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } cxt.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;获取pod容器成功\u0026#34;, \u0026#34;data\u0026#34;: containers, }) } // GetPodLog 获取pod中容器日志 func (p *pod) GetPodLog(cxt *gin.Context) { params := new(struct { Namespace string `form:\u0026#34;namespace\u0026#34;` PodName string `form:\u0026#34;pod_name\u0026#34;` ContainerName string `form:\u0026#34;container_name\u0026#34;` Cluster string `form:\u0026#34;cluster\u0026#34;` }) // GET请求，绑定参数方法改为ctx.Bind if err := cxt.Bind(params); err != nil { logger.Error(\u0026#34;Bind请求参数失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } client, err := service.K8s.GetClient(params.Cluster) if err != nil { logger.Error(\u0026#34;获取k8s连接失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } log, err := service.Pod.GetPodLog(client, params.Namespace, params.PodName, params.ContainerName) if err != nil { logger.Error(\u0026#34;获取pod日志失败,\u0026#34; + err.Error()) cxt.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;msg\u0026#34;: err.Error(), \u0026#34;data\u0026#34;: nil, }) return } cxt.JSON(http.StatusOK, gin.H{ \u0026#34;msg\u0026#34;: \u0026#34;获取pod日志成功\u0026#34;, \u0026#34;data\u0026#34;: log, }) } API设计与路由 在router.go中定义了清晰的API路由：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (*router) InitApiRouter(r *gin.Engine) { r.GET(\u0026#34;/api/ping\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{\u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;}) }) // Pod 路由服务 podGroup := r.Group(apiBasePath) { podGroup.GET(\u0026#34;/pod\u0026#34;, Pod.GetPods) // 获取Pod列表 podGroup.GET(\u0026#34;/pod/detail\u0026#34;, Pod.GetPodDetail) // 获取Pod详情 podGroup.DELETE(\u0026#34;/pod/del\u0026#34;, Pod.DeletePod) // 删除Pod podGroup.PUT(\u0026#34;/pod/update\u0026#34;, Pod.UpdatePod) // 更新Pod podGroup.GET(\u0026#34;/pod/container\u0026#34;, Pod.GetPodContainer) // 获取容器列表 podGroup.GET(\u0026#34;/pod/log\u0026#34;, Pod.GetPodLog) // 获取容器日志 } } API设计遵循RESTful规范，使用合适的HTTP方法(GET/POST/PUT/DELETE)对应不同的操作类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package controller import \u0026#34;github.com/gin-gonic/gin\u0026#34; /** * @Author: 南宫乘风 * @Description: * @File: router.go * @Email: 1794748404@qq.com * @Date: 2025-03-17 17:18 */ // Router 实例化对象，可以在main.go中调用 var Router router type router struct { } const apiBasePath = \u0026#34;/api/k8s\u0026#34; // InitRouter 初始化路由 func (*router) InitApiRouter(r *gin.Engine) { r.GET(\u0026#34;/api/ping\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{\u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;}) }) // Pod 路由服务 podGroup := r.Group(apiBasePath) { podGroup.GET(\u0026#34;/pod\u0026#34;, Pod.GetPods) podGroup.GET(\u0026#34;/pod/detail\u0026#34;, Pod.GetPodDetail) podGroup.DELETE(\u0026#34;/pod/del\u0026#34;, Pod.DeletePod) podGroup.PUT(\u0026#34;/pod/update\u0026#34;, Pod.UpdatePod) podGroup.GET(\u0026#34;/pod/container\u0026#34;, Pod.GetPodContainer) podGroup.GET(\u0026#34;/pod/log\u0026#34;, Pod.GetPodLog) } } 优雅关闭 在main.go中实现了服务的优雅关闭：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 创建信号通道 quit := make(chan os.Signal) signal.Notify(quit, os.Interrupt) \u0026lt;-quit // 阻塞等待中断信号 // 设置5秒超时关闭 ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() // 优雅关闭服务器 if err := srv.Shutdown(ctx); err != nil { logger.Error(\u0026#34;Gin Server Shutdown:\u0026#34;, err) } logger.Info(\u0026#34;Gin Server exiting\u0026#34;) 完整代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package main import ( \u0026#34;context\u0026#34; \u0026#34;kubea-go/config\u0026#34; \u0026#34;kubea-go/controller\u0026#34; \u0026#34;kubea-go/service\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aryming/logger\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) /** * @Author: 南宫乘风 * @Description: * @File: main.go * @Email: 1794748404@qq.com * @Date: 2025-03-17 14:49 */ func main() { logger.SetLogger(\u0026#34;config/log.json\u0026#34;) // 初始化路由 r := gin.Default() // 初始化K8S客户端 service.K8s.Init() controller.Router.InitApiRouter(r) // 启动服务 srv := \u0026amp;http.Server{ Addr: config.ListenAddress, Handler: r, } go func() { // service connections if err := srv.ListenAndServe(); err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { logger.Error(\u0026#34;listen: %s\\n\u0026#34;, err) } }() //优雅关闭 // 声明一个系统信号的channel，并监听系统信号，如果没有信号，就一直阻塞，如果有，就继续执行。当接收到中断信号时，执行cancel() // 创建一个用于接收OS信号的通道 quit := make(chan os.Signal) // 配置信号通知，将OS中断信号通知到quit通道 signal.Notify(quit, os.Interrupt) // 阻塞等待，直到从quit通道接收到信号 \u0026lt;-quit // 设置上下文对象ctx，带有5秒的超时 ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) // 当函数返回时，调用cancel以取消上下文并释放资源 defer cancel() // 尝试优雅关闭GIN服务器 if err := srv.Shutdown(ctx); err != nil { // 如果关闭失败，记录致命错误 logger.Error(\u0026#34;Gin Server Shutdown:\u0026#34;, err) } // 记录服务器退出信息 logger.Info(\u0026#34;Gin Server exiting\u0026#34;) } 配置管理 配置信息集中在config.go中管理：\n1 2 3 4 5 6 const ( ListenAddress = \u0026#34;0.0.0.0:8081\u0026#34; // 服务监听地址 // 多集群kubeconfig配置 Kubeconfigs = `{\u0026#34;TST-1\u0026#34;:\u0026#34;E:\\\\GitHUB_Code_Check\\\\VUE\\\\kubea-go\\\\config\\\\k8s.yaml\u0026#34;,\u0026#34;TST-2\u0026#34;:\u0026#34;E:\\\\GitHUB_Code_Check\\\\VUE\\\\kubea-go\\\\config\\\\k8s.yaml\u0026#34;}` PodLogTailLine = 500 // 日志默认返回行数 ) ","date":"2025-03-26T15:11:01Z","image":"https://www.ownit.top/title_pic/52.jpg","permalink":"https://www.ownit.top/p/202503261511/","title":"基于Go语言和Kubernetes的多集群管理平台开发实践"},{"content":"一篇关于 Vue3 项目搭建的博客文章。下面的示例包含了以下内容：\n使用 Vue CLI 创建项目 安装并使用 axios 进行 API 请求 配置 vue-router 实现路由跳转 全局引入 ant-design-vue 组件库 Vue Cli脚手架使用 Vue CLI 是一个基于 Vue.js 进行快速开发的完整系统\n配置npm淘宝源（默认国外源，下载依赖较慢）\n1 2 3 4 5 #设置淘宝源 npm config set registry https://registry.npm.taobao.org --global #查看源 npm config get registry 1 2 3 4 5 6 命令安装: npm install -g @vue/cli@4.5.12 检查版本: vue -V 创建项目: vue create \u0026lt;项目名称\u0026gt; 选择Vue3项目 运行项目: npm run serve 访问 创建项目 首先使用 Vue CLI 创建项目（默认生成 Vue3 版本）：\n1 vue create vue-demo 进入项目目录后，安装所需的依赖包：\n1 npm install axios vue-router ant-design-vue Axios 介绍使用 官方文档: http://www.axios-js.com/zh-cn/docs/ 在前端页面展示的数据大多数都是通过访问一个API获取的，做这件事的方法有好几种，例如 jquery ajax、vue-resource、axios，而vue-resource是vue插件，但3版本不再更新，目前官方 推荐主流的axios，aixos是一个http请求库。\n1 2 3 4 5 安装axios: npm install axios 在组件中导入并使用 import axios from \u0026#39;axios\u0026#39; GET请求 使用 axios.get发起GET请求。 使用 .then 获取响应数据。 使用catch 异常处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;home\u0026#34;\u0026gt; \u0026lt;img alt=\u0026#34;Vue logo\u0026#34; src=\u0026#34;../assets/logo.png\u0026#34;/\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; @click=\u0026#34;getData()\u0026#34;\u0026gt;获取后端数据\u0026lt;/button\u0026gt; {{ data }} \u0026lt;p v-if=\u0026#34;error\u0026#34; style=\u0026#34;color: red;\u0026#34;\u0026gt;连接服务器失败，请稍后再试！\u0026#34;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; // 导入话实例化对象 // import axios from \u0026#34;axios\u0026#34; import axios from \u0026#34;axios\u0026#34; import {onMounted, ref} from \u0026#34;vue\u0026#34; export default { setup() { const data = ref(\u0026#39;\u0026#39;) const error = ref(false) function getData() { axios.get(\u0026#39;http://httpbin.org/get\u0026#39;) .then(res =\u0026gt; { data.value = res.data console.log(res) }).catch(res =\u0026gt; { error.value = true console.log(res) }) } // 生命周期钩子 onMounted(() =\u0026gt; { getData() console.log(\u0026#39;挂载完成\u0026#39;) }) return { error, getData, data } } } \u0026lt;/script\u0026gt; POST提交 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;home\u0026#34;\u0026gt; \u0026lt;img alt=\u0026#34;Vue logo\u0026#34; src=\u0026#34;../assets/logo.png\u0026#34;/\u0026gt; \u0026lt;h1\u0026gt;欢迎访问管理后台\u0026lt;/h1\u0026gt; \u0026lt;form action=\u0026#34;#\u0026#34;\u0026gt; 用户名：\u0026lt;input type=\u0026#34;text\u0026#34; v-model=\u0026#34;form.username\u0026#34;/\u0026gt; 密码：\u0026lt;input type=\u0026#34;password\u0026#34; v-model=\u0026#34;form.password\u0026#34;/\u0026gt; \u0026lt;button @click=\u0026#34;loginBtn\u0026#34;\u0026gt;登录\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;p style=\u0026#34;color: red;\u0026#34; v-if=\u0026#34;notice\u0026#34;\u0026gt;用户名和密码不能为空\u0026#34;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; import axios from \u0026#34;axios\u0026#34; import {ref, reactive} from \u0026#34;vue\u0026#34;; export default { name: \u0026#34;Home\u0026#34;, setup() { const form = reactive({ username: \u0026#39;\u0026#39;, password: \u0026#39;\u0026#39; }) const notice = ref(false) function loginBtn() { if (form.username === \u0026#39;\u0026#39; || form.password === \u0026#39;\u0026#39;) { notice.value = true } else { axios.post(\u0026#39;http://httpbin.org/post\u0026#39;, form).then(res =\u0026gt; { console.log(res) }) } } return { form, notice, loginBtn } } } \u0026lt;/script\u0026gt; 自定义实例 自定义实例默默认值认值 有时候服务端接口有多个地址，就会涉及请求的域名不同、配置不同等，这时自定义实例可以很好解决。\n创建src/request/index.js文件，定义实例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import axios from \u0026#34;axios\u0026#34;; const instance = axios.create({ baseURL: \u0026#34;https://httpbin.org\u0026#34;, timeout: 1000, // headers: { \u0026#39;X-Custom-Header\u0026#39;: \u0026#39;foobar\u0026#39; } }); // 拦截器 ，请求拦截 instance.interceptors.request.use( config =\u0026gt; { // 在发送请求之前做些什么 config.headers[\u0026#39;Test-Header\u0026#39;] = \u0026#39;123456\u0026#39; return config; }, error =\u0026gt; { // 对请求错误做些什么 return Promise.reject(error); } ); // 拦截器 ，响应拦截 instance.interceptors.response.use( response =\u0026gt; { // 对响应数据做点什么 return response; }, error =\u0026gt; { // 对响应错误做点什么 return Promise.reject(error); } ); export default instance; 1 2 3 4 5 6 拦截器可以拦截每一次请求和响应，然后进行相应的处理。 请求拦截应用场景： 发起请求前添加header 响应拦截应用场景： 统一处理API响应状态码200或非200的提示消息 统一处理catch异常提示信息 Vue路由路由：Vue Router Vue Router使用 Vue Router 是 Vue.js (opens new window)官方的路由管理器。\n安装\n1 npm install vue-router 创建src/router/index.js文件及目录，之后前端的路由都将维护在此文件中\n使用流程\n开发页面（组件） 定义路由，src/router/index.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import { createRouter, createWebHistory } from \u0026#34;vue-router\u0026#34;; // 引入组件，一开是就会引入，即使不进入页面，也会加载组件 import Home from \u0026#34;@/views/Home.vue\u0026#34;; const routes = [ { path: \u0026#34;/home\u0026#34;, name: \u0026#34;Home\u0026#34;, component: Home, }, { path: \u0026#34;/about\u0026#34;, name: \u0026#34;About\u0026#34;, // 引入组件，懒加载，只有在打开页面的时候才会加载 component: () =\u0026gt; import(\u0026#34;@/views/About.vue\u0026#34;), }, { path: \u0026#34;/login\u0026#34;, name: \u0026#34;Login\u0026#34;, // 引入组件，懒加载，只有在打开页面的时候才会加载 component: () =\u0026gt; import(\u0026#34;@/views/Login.vue\u0026#34;), }, ]; // 创建一个路由实例 const router = createRouter({ // 使用基于hash的路由历史模式 history: createWebHistory(), // 定义路由配置数组 routes: routes, // 配置路由模式 mode: \u0026#34;history\u0026#34;, }); // 导出路由实例以便在其他模块中使用 export default router; 组件使用路由，src/App.vue\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;template\u0026gt; \u0026lt;img alt=\u0026#34;Vue logo\u0026#34; src=\u0026#34;./assets/logo.png\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;!-- 使用router-link组件来导航 --\u0026gt; \u0026lt;!-- 通过传入 to 属性制定链接 --\u0026gt; \u0026lt;!-- router-link 默认会被渲染成一个a标签 --\u0026gt; \u0026lt;router-link to=\u0026#34;/home\u0026#34;\u0026gt;Home\u0026lt;/router-link\u0026gt; \u0026lt;br\u0026gt; \u0026lt;router-link to=\u0026#34;/about\u0026#34;\u0026gt;About\u0026lt;/router-link\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- 路由占位符，路由匹配到的组件都会在这里展示 --\u0026gt; \u0026lt;router-view /\u0026gt; \u0026lt;/template\u0026gt; src/main.js种导入router\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // {} 导入的是组件的方法 import { createApp } from \u0026#39;vue\u0026#39; // 导入组件的别名 import App from \u0026#39;./App.vue\u0026#39; // @ 表示 src 目录 import Test from \u0026#39;@/components/Test.vue\u0026#39; const app = createApp(App) // 注册全局组件 app.component(\u0026#39;Test1\u0026#39;, Test) import router from \u0026#39;./router\u0026#39; app.use(router) app.mount(\u0026#39;#app\u0026#39;) // createApp(App).mount(\u0026#39;#app\u0026#39;) 路由传参 URL传参：一般用于页面跳转，将当前数据传递到新页面，例如详情页 params传参：\n1 2 3 4 配置路由: {path: \u0026#39;/user/:id\u0026#39;, component: about} 传递方式：\u0026lt;router-link :to=\u0026#34;/user/6\u0026#34;\u0026gt;\u0026lt;/router-link\u0026gt; 传递后路径: /user/6 接受参数法：$route.params.id query传参\n1 2 3 4 配置路由: {path: \u0026#39;/user/\u0026#39;, component: about} 传递方式：\u0026lt;router-link :to=\u0026#34;{path: \u0026#39;/about\u0026#39;,query:{id:6}}\u0026#34;\u0026gt;\u0026lt;/router-link\u0026gt; 传递后路径: /user?id=6 接受参数法：$route.query.id 路由守卫 正如其名，vue-router 提供的导航守卫主要用来通过跳转或取消的方式守卫导航。简单来说，就是在路由跳转时候的一些钩子，当从一个页面跳转到另一个页面时，可以在跳转前、中、后做一些事情。\n每个收尾方法接收参数：\ntop： 即将要进入的目标，是一个路由对象 from：当前导航正要离开的路由，也是一个路由对象 next：可选，是一个方法，直接进入下一个路由 你可以使用 router.beforeEach 注册一个全局前置守卫，当一个导航触发时，就会异步执行这个回调函数。\n1 2 3 4 5 6 7 const router = createRouter ({ ..... // 添加全局前置路由守卫 router.befoEach((to,from) \u0026#34;) { // 这里执行具体操作 console.log(to) console.log(from) }) 在网站开发中，使用导航守卫一个普遍需求：登录验证，，即在没有登录的情况下，访问任何页面都跳转到登录页面。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 router.beforeEach((to, from, next) =\u0026gt; { // 添加全局前置导航守卫 // ... console.log(to); console.log(from); if (to.path === \u0026#34;/login\u0026#34;) { return next(); } const token = \u0026#34;\u0026#34; //模拟token，正常是从本地cookie或localstorage中获取 if (token) { return next(); // 如果有token，则正常跳转访问 } else { return next(\u0026#34;/login\u0026#34;); // 如果没有token，跳转到登录页 } }); 完整代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import { createRouter, createWebHistory } from \u0026#34;vue-router\u0026#34;; // 引入组件，一开是就会引入，即使不进入页面，也会加载组件 import Home from \u0026#34;@/views/Home.vue\u0026#34;; const routes = [ { path: \u0026#34;/home\u0026#34;, name: \u0026#34;Home\u0026#34;, component: Home, }, { path: \u0026#34;/about\u0026#34;, name: \u0026#34;About\u0026#34;, // 引入组件，懒加载，只有在打开页面的时候才会加载 component: () =\u0026gt; import(\u0026#34;@/views/About.vue\u0026#34;), }, { path: \u0026#34;/login\u0026#34;, name: \u0026#34;Login\u0026#34;, // 引入组件，懒加载，只有在打开页面的时候才会加载 component: () =\u0026gt; import(\u0026#34;@/views/Login.vue\u0026#34;), }, ]; // 创建一个路由实例 const router = createRouter({ // 使用基于hash的路由历史模式 history: createWebHistory(), // 定义路由配置数组 routes: routes, // 配置路由模式 mode: \u0026#34;history\u0026#34;, }); router.beforeEach((to, from, next) =\u0026gt; { // 添加全局前置导航守卫 // ... console.log(to); console.log(from); if (to.path === \u0026#34;/login\u0026#34;) { return next(); } const token = \u0026#34;\u0026#34; if (token) { return next(); } else { return next(\u0026#34;/login\u0026#34;); } }); // 导出路由实例以便在其他模块中使用 export default router; Ant Design 使用 Element UI 它是由饿了么前端团队推出的基于 Vue 封装的 UI 组件库，提供PC 端组件，简化了常用组件的封装，降低开发难度。 Ant Design 是一套企业级 UI 设计语言和 React 组件库，提供了一套非常完整的组件化设计规范与组件化编码规范，能大幅提高了部分产品的设计研发效率及质量。 组件库文档：https://antdv.com/components/overview-cn 安装\n1 npm install ant-design-vue 全局注册\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import { createApp } from \u0026#39;vue\u0026#39; import App from \u0026#39;./App.vue\u0026#39; // 引入antd和主题样式 import \u0026#39;ant-design-vue/dist/reset.css\u0026#39;; import Antd from \u0026#39;ant-design-vue\u0026#39; // 全局引入antd图标 import * as Icons from \u0026#39;@ant-design/icons-vue\u0026#39;; const app = createApp(App) // 全局引入antd图标, i是组件名。Icons[i]是具体的组件 for (const i in Icons) { app.component(i, Icons[i]) } import router from \u0026#39;./router\u0026#39; app.use(router) // 全局引入antd app.use(Antd) app.mount(\u0026#39;#app\u0026#39;) // createApp(App).mount(\u0026#39;#app\u0026#39;) 路由页面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import {createRouter,createWebHistory} from \u0026#34;vue-router\u0026#34;; const routes = [ { path:\u0026#34;/btn\u0026#34;, component:( ) =\u0026gt; import(\u0026#34;@/view/test/btn.vue\u0026#34;) } ] const router = createRouter({ history:createWebHistory(), routes }) export default router; 使用\u0026lt; router-view /\u0026gt; ，定义路由占位\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // src/App.vue \u0026lt;template\u0026gt; \u0026lt;!-- 路由占位符 --\u0026gt; \u0026lt;template\u0026gt; \u0026lt;!-- 路由占位符--\u0026gt; \u0026lt;router-view/\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;!--这里是全局的css效果--\u0026gt; \u0026lt;style\u0026gt; #app { /* 100%的窗口宽和高 */ width: 100%; height: 100%; } \u0026lt;/style\u0026gt; 使用一个antd的按钮组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;script setup\u0026gt; \u0026lt;/script\u0026gt; \u0026lt;template\u0026gt; \u0026lt;div style=\u0026#34;margin: 20px;text-align: center;\u0026#34;\u0026gt; \u0026lt;a-space wrap\u0026gt; \u0026lt;a-button type=\u0026#34;primary\u0026#34;\u0026gt;Primary Button\u0026lt;/a-button\u0026gt; \u0026lt;a-button\u0026gt;Default Button\u0026lt;/a-button\u0026gt; \u0026lt;a-button type=\u0026#34;dashed\u0026#34;\u0026gt;Dashed Button\u0026lt;/a-button\u0026gt; \u0026lt;a-button type=\u0026#34;text\u0026#34;\u0026gt;Text Button\u0026lt;/a-button\u0026gt; \u0026lt;a-button type=\u0026#34;link\u0026#34;\u0026gt;Link Button\u0026lt;/a-button\u0026gt; \u0026lt;/a-space\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;style scoped\u0026gt; .ant-btn { margin-right: 8px; margin-bottom: 20px; } \u0026lt;/style\u0026gt; 浏览器打开localhost:8080/btn\n开发与响应流程 ","date":"2025-03-10T15:37:46Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202503101537/","title":"使用Vue CLI从零搭建企业级项目实战（Vue3+全家桶）"},{"content":"问题 近日公司有服务内存泄露导致服务假死，导致服务不可用。第一时间查看异常日志，并未发现什么异常，但是进入pod发现生成java_pid1.hprof文件。平台的线上环境均配置了jvm参数，在发生OOM时会自动dump出堆转储文件，接下来就要从dump文件入手来定位问题了。\n项目背景 在Java应用开发中，内存溢出（Out Of Memory, OOM）是开发者最常遇到的严重问题之一。OOM通常由以下原因引发：\n内存分配不足：JVM堆内存设置过小，无法应对高并发或复杂业务场景。 代码漏洞：如内存泄漏（对象无法被GC回收）、大对象未释放、集合类滥用等。 系统突然的流量增大，原有的可用内存不足以支撑等。 当系统突发OOM时，若仅依赖日志定位问题，往往难以快速找到根源。此时，JProfiler作为一款强大的JVM性能分析工具，能够通过内存快照（Dump文件）精准定位问题代码，成为解决问题的关键。\n环境 官方网址：https://www.ej-technologies.com/jprofiler/download92\nJDK版本：Java 8+（推荐JDK 11及以上，支持更全面的诊断功能）。 JProfiler版本：12.0+（支持与IDEA插件集成）。 IDE：IntelliJ IDEA（集成JProfiler插件）。 服务器环境：Linux（需配置OOM自动生成Dump文件）。 实战步骤：从OOM到问题定位 配置JVM参数生成Dump文件 Dump文件 是在OOM内存溢出的时候，自动Dump文件转存快照的，配置JVM参数 ,就能够在发生Out of Memory错误的时候，就是把当前堆栈信息转存为快照Dump文件\n1 2 3 4 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/dev/test_service_jvmDump.hprof 生产 java -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/app.hprof -jar app.jar HeapDumpOnOutOfMemoryError：触发OOM时生成快照。 HeapDumpPath：指定快照文件存储路径（需确保目录可写）。 获取并解析Dump文件 当OOM发生时，服务器会生成.hprof文件，下载至本地。 使用JProfiler打开快照： 启动JProfiler，选择 Open Snapshots → Open a Single Snapshot。 加载.hprof文件后，JProfiler会自动解析堆内存结构。 导入成功后可以看到如下界面: Jprofiler使用 分析大对象来源 Jprofile 分析对象,有两个特别重要的信息即 对象的引用信息\nincoming references 入引用, 显示这个对象被谁引用， 找问题出现的代码位置 outcoming references 出引用,显示这个对象引用的其他对象， 找出他有什么，他为什么这么大内存，内存里面存储的是啥 下面我们先预估对象内存大小\n按照Size排序，这个byte[]使用15350次，占用内存1272MB，看着很大 Incomming References 入引用,找引用当前对象的对象\nUse select Objects 分析对象引用\n首先 选中对象，点击右键，Use select Objects 选择 incoming references 找到对象引用的地方 逐个分析， 点击show more\n查看代码，分析问题\nJprofile 大对象寻找法 上面我们通过 incomming references ，找对了对象的引用，下面我们我们采用更为直接的方法\n我们都知道内存过高, 无法回收,肯定是内存中堆积的东西多了, 那么我们看下有哪些大对象占用了内存,如何找到大对象?\nJprofile提供了Biggest Objects的 查找方法\n我们可以看到按钮 Biggets OBjects 直接点击按钮, JProfiler就会显示当前堆栈中有哪些大对象 在上面的基础上进行操作 然后点开大对象进行分析, 同样的 Used Selected Objects 点击Incomming References 查找对象引用信息\n![![在这里插入图片描述](../../image/af4eba6408a6405f86d7a60748234e10.png) 同样的结果, 可以定位到148行代码存在问题 用Graph 查找对象源 先查找 Incomming References 找到对象被谁引用 找到想要追查的对象后， 选择User Selected Objects 然后点击 Show In Graph 定位线程及Class信息\n然后根据 数据源Heap Walker 定位到问题的Class类信息及具体的行信息\nShow Path To GC Root 找到创建该方法的位置 选择Use Selected Objects ，选择Imcoming References， 然后点击 Show Path To GC Root\n从创建该对象的方法开始检查，检查所有用到该对象的地方，直到找到泄漏位置\n选择All root， 展示所有的根节点链路信息 定位问题，查阅代码 总结 通过JProfiler分析Dump文件，开发者可以快速定位内存泄漏或大对象问题。关键在于：\n生成准确的快照：确保JVM参数配置正确。 掌握分析工具的核心功能：如引用链追踪、大对象筛选。 结合代码与业务场景：避免“治标不治本”的临时修复。 ","date":"2025-03-09T10:34:57Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202503091034/","title":"实战复盘：如何用JProfiler在30分钟内解决内存泄漏？"},{"content":"为什么选择Gin框架？ Gin 是一个基于 Go 语言的高性能 Web 框架，具备以下优势：\n轻量高效：底层依赖 net/http，性能接近原生。 简洁优雅：API 设计友好，支持路由分组、中间件链、参数绑定等特性。 生态丰富：内置 JSON 解析、日志记录、错误恢复等实用功能，社区插件生态完善。 无论是构建 RESTful API 还是全栈应用，Gin 都能显著提升开发效率。 安装 要安装Gin软件包，您需要安装Go并首先设置Go工作区。\n首先需要安装Go（需要1.10+版本），然后可以使用下面的Go命令安装Gin。 1 go get -u github.com/gin-gonic/gin 将其导入您的代码中： 1 import \u0026#34;github.com/gin-gonic/gin\u0026#34; 基础示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { // 1.创建 (实例化gin.Engine结构体对象） r := gin.Default() // 2.绑定路由规则，执行的函数 // gin.Context，封装了request和response r.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;Hello, Gin!\u0026#34;) }) // 3.监听端口，默认在8080 // Run(\u0026#34;里面不指定端口号默认为8080\u0026#34;) r.Run(\u0026#34;:8080\u0026#34;) } 运行后访问 http://localhost:8080，即可看到返回的字符串。\nGin工工作作流流程 核心概念 Engine 容器对象，整个框架的基础 Engine.tree 负责存储路由和handle方法的映射，采用类似于字典树的结构 Engine.RouterGroup 其中handlers存储着所有中间件 Context 上下文对象，负责处理 请求回应 ，其中handles的存储处理请求时中间件和处理方法的 请求处理 流程 GIN启动流程 1 Gin初始化----\u0026gt; Use 中间件 ----\u0026gt; 注册Routers路由 ----\u0026gt; RUN()启动 Gin原原理解析 参考资料：http://v5blog.cn/pages/dd7d5a/\ngin.Default()\nDefault()跟New()几乎一模一样, 就是调用了gin内置的Logger(), Recovery()中间件\n1 2 3 4 5 6 7 8 // Default返回一个已经附加了Logger和Recovery中间件的Engine实例 func Default() *Engine { debugPrintWARNINGDefault() engine := New() \u0026#34;# 默认实例 // 注册中间建，中间件的是一个函数，最终只要返回一个 type HandlerFunc func(*Context) 就可以 engine.Use(Logger(), Recovery()) // 默认注册的两个中间件 return engine } engine := New() 初初始化始化\n通过调用 gin.New() 方法来实例化 Engine容器\nengine.Use() 注注册册中间件 中间件\n路由与参数处理 无参路由 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func HelloWorldhandler(ctx *gin.Context){ ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34;, }) } func main() { // 创建一个默认的路由器 router := gin.Default() // gin.Context是一个结构体，包含了请求和响应的细节， 封装request和response router.GET(\u0026#34;/\u0026#34;,HelloWorldhandler) // 路由重定向 router.GET(\u0026#34;/re\u0026#34;, func(c *gin.Context) { c.Redirect(http.StatusMovedPermanently, \u0026#34;https://www.baidu.com\u0026#34;) }) // 启动服务器 router.Run(\u0026#34;:8081\u0026#34;) } 动态路由参数 通过 :param 捕获 URL 中的变量： (可以通过Context的Param方法来获取API参数)\n1 2 3 4 r.GET(\u0026#34;/book/:id\u0026#34;, func(c *gin.Context) { bookID := c.Param(\u0026#34;id\u0026#34;) c.String(http.StatusOK, \u0026#34;书籍ID: %s\u0026#34;, bookID) }) 查询参数 使用 Query 或 DefaultQuery 获取 URL 参数： http://127.0.0.1:8000/user?name=zhangsan\n1 2 3 4 5 r.GET(\u0026#34;/user\u0026#34;, func(c *gin.Context) { name := c.Query(\u0026#34;name\u0026#34;) role := c.DefaultQuery(\u0026#34;role\u0026#34;, \u0026#34;guest\u0026#34;) c.JSON(200, gin.H{\u0026#34;name\u0026#34;: name, \u0026#34;role\u0026#34;: role}) }) ShouldBind参数绑定 通过 ShouldBind 自动解析请求体（支持 JSON、Form 等）：\n我们可以基于请求的 Content-Type 识别请求数据类型并利用反射机制 自动提取请求中 QueryString 、 form表单 、 JSON 、 XML 等参数到结构体中\n1 2 3 4 5 6 7 8 9 10 11 12 13 type LoginForm struct { Username string `form:\u0026#34;username\u0026#34; binding:\u0026#34;required\u0026#34;` Password string `form:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } r.POST(\u0026#34;/login\u0026#34;, func(c *gin.Context) { var form LoginForm if err := c.ShouldBind(\u0026amp;form); err != nil { c.JSON(400, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } c.String(200, \u0026#34;登录成功: %s\u0026#34;, form.Username) }) 完整代码案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) // HelloWorldhandler 无参数路由处理函数 func HelloWorldhandler(ctx *gin.Context){ ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34;, }) } func GetBookDetailHandler(ctx *gin.Context ) { bookId := ctx.Param(\u0026#34;id\u0026#34;) ctx.String(http.StatusOK, fmt.Sprintf(\u0026#34;成功获取书籍详情：%s\u0026#34;, bookId)) } func GetUserDetailHandlers(ctx *gin.Context) { username := ctx.Query(\u0026#34;username\u0026#34;) ctx.String(http.StatusOK, fmt.Sprintf(\u0026#34;成功获取用户详情：%s\u0026#34;, username)) } type Login struct { Username string `form:\u0026#34;username\u0026#34; json:\u0026#34;username\u0026#34; binding:\u0026#34;required\u0026#34;` Password string `form:\u0026#34;password\u0026#34; json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } func ResponseJsonHandler(c *gin.Context) { type Data struct { Msg string `json:\u0026#34;msg:\u0026#34;` Code int `json:\u0026#34;code\u0026#34;` } d := Data{ Msg: \u0026#34;success\u0026#34;, Code: 200, } c.JSON(http.StatusOK, d) } func Loginhandler(c *gin.Context) { var login Login if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;username\u0026#34;: login.Username, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } } func ResponseStringHandle(c *gin.Context) { c.String(http.StatusOK, \u0026#34;Hello World\u0026#34;) } func main() { // 创建一个默认的路由器 router := gin.Default() // gin.Context是一个结构体，包含了请求和响应的细节， 封装request和response // 无参数路由 router.GET(\u0026#34;/\u0026#34;,HelloWorldhandler) // 返回字符串 router.GET(\u0026#34;/string\u0026#34;, ResponseStringHandle) // 返回json router.GET(\u0026#34;/json\u0026#34;, ResponseJsonHandler) // 动态路由参数 router.GET(\u0026#34;/book/:id\u0026#34;, GetBookDetailHandler) // 查询参数 Query 或 DefaultQuery router.GET(\u0026#34;/user/\u0026#34;, GetUserDetailHandlers) // 参数绑定 router.POST(\u0026#34;/login\u0026#34;, Loginhandler) // 路由重定向 router.GET(\u0026#34;/re\u0026#34;, func(c *gin.Context) { c.Redirect(http.StatusMovedPermanently, \u0026#34;https://www.baidu.com\u0026#34;) }) // 启动服务器 router.Run(\u0026#34;:8081\u0026#34;) } 路由分发 为什么需要路由分发？\n我们一个项目有非常多的模块，如果全部写在一块导致代码结构混乱，不利于后续的扩展 按照大的模块，每个模块有自己独立的路由，主路由可以再main.go中进行注册 项目结构 1 2 3 4 5 6 ├── go.mod ├── go.sum ├── main.go └── routers ├── books.go └── users.go main.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import ( \u0026#34;days/routers\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { router := gin.Default() // 全局中间件 router.Use(MiddleWare()) // 加载路由 routers.LoadUsers(router) routers.LoadBooks(router) router.Run(\u0026#34;:8081\u0026#34;) } routers/users.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package routers import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func LoadUsers(e *gin.Engine) { e.GET(\u0026#34;/user\u0026#34;,MiddleWareOne(),UserHandler) } func UserHandler(c *gin.Context) { fmt.Println(\u0026#34;我是用户路由\u0026#34;) time.Sleep(time.Second * 5) c.JSON(http.StatusOK,gin.H{ \u0026#34;message\u0026#34; : \u0026#34;weclome user\u0026#34;, }) } routers/books.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 package routers import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func LoadBooks(e *gin.Engine) { e.GET(\u0026#34;/book\u0026#34;, GetBookHandler) } func GetBookHandler(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Book Router\u0026#34;, }) } 中间件 Gin框架允许开发者在处理请求的过程中，加入用户自己的钩子（Hook）函数。 这个钩子函数就叫中间件，中间件适合处理一些公共的业务逻辑 比如登录认证、权限校验、数据分页、记录日志、耗时统计等 全局中间件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func Logger() gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() c.Next() latency := time.Since(start) fmt.Printf(\u0026#34;请求耗时: %v\\n\u0026#34;, latency) } } func main() { r := gin.Default() r.Use(Logger()) // 全局生效 r.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { /* ... */ }) } 局部中间件 1 2 3 4 5 6 7 8 9 10 11 12 13 func AuthMiddleware() gin.HandlerFunc { return func(c *gin.Context) { token := c.GetHeader(\u0026#34;token\u0026#34;) if token != \u0026#34;SECRET_KEY\u0026#34; { c.AbortWithStatusJSON(401, gin.H{\u0026#34;error\u0026#34;: \u0026#34;身份验证失败\u0026#34;}) } c.Next() } } r.GET(\u0026#34;/profile\u0026#34;, AuthMiddleware(), func(c *gin.Context) { c.JSON(200, gin.H{\u0026#34;data\u0026#34;: \u0026#34;用户信息\u0026#34;}) }) next()方法 在中间件中调用next()方法，会从next()方法调用的地方跳转到Handler函数 Handler函数执行完成，若中间件还有部分代码未执行（中间件中next()之后的代码），则执行该代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main() { r := gin.Default() r.Use(Log(), RequestID()) r.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { fmt.Println(\u0026#34;app running 1\u0026#34;) time.Sleep(time.Second * 5) c.String(http.StatusOK, \u0026#34;hello World!\u0026#34;) }) r.Run() } func Log() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\u0026#34;log start\u0026#34;) c.Next() fmt.Println(\u0026#34;log end\u0026#34;) } } func RequestID() gin.HandlerFunc { return func(c *gin.Context) { fmt.Println(\u0026#34;requestid start\u0026#34;) c.Next() fmt.Println(\u0026#34;requestid end\u0026#34;) } } 实现token认证 http://127.0.0.1:8080/index index首页无需token直接访问 http://127.0.0.1:8080/home home家目录需要对token进行验证，验证通过才可访问 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func AuthMiddleWare() func(c *gin.Context) { return func(c *gin.Context){ // 客户端携带token 有三种方式 1.放在请求头 2.放在请求体 3.放在url // token 验证成功，返回 c.next()才会继续，否则 c.Abort() token := c.Request.Header.Get(\u0026#34;token\u0026#34;) fmt.Println(\u0026#34;获取token:\u0026#34;,token) if token == \u0026#34;\u0026#34; { c.JSON(200, gin.H{ \u0026#34;code\u0026#34;: 401, \u0026#34;msg\u0026#34;: \u0026#34;身份验证不通过\u0026#34;, }) c.Abort() } if token != \u0026#34;123456\u0026#34; { c.JSON(200, gin.H{ \u0026#34;code\u0026#34;: 401, \u0026#34;msg\u0026#34;: \u0026#34;token错误\u0026#34;, }) c.Abort() } } } func main() { router := gin.Default() // 首页无需验证 router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;首页\u0026#34;, }) }) // Home页面需要验证 router.GET(\u0026#34;/home\u0026#34;, AuthMiddleWare(), func(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Home页面\u0026#34;, }) }) router.Run(\u0026#34;:8081\u0026#34;) } ","date":"2025-03-04T11:07:28Z","image":"https://www.ownit.top/title_pic/63.jpg","permalink":"https://www.ownit.top/p/202503041107/","title":"Gin框架从入门到实战：核心用法与最佳实践"},{"content":"背景 从重复劳动到自动化提效 在日常开发运维工作中，日志分析是每个工程师都会遇到的\u0026quot;必修课\u0026quot;。当我们需要定位线上问题、分析访问趋势或验证测试结果时，往往需要面对以下痛点场景：\n在GB级的日志文件中手动grep关键信息 重复编写awk/sed脚本统计访问量 每次分析日志都需要进行相似的筛选、统计操作，重复劳动让人疲惫不堪 人工分析难以快速提取出日志中的关键指标，例如 IP 访问量、错误状态码分布等。 为此，我基于Go语言开发了这款命令行日志分析工具，它具备以下核心价值：\n极简部署（单文件二进制） 实时交互式分析 常用统计场景全覆盖 毫秒级响应速度 测试友好型输出 工具功能概览 这款日志分析工具 ( log-analyzer ) 基于 Go 语言开发，充分利用了 Go 语言在处理文本和并发方面的优势，旨在提供简洁、高效的日志分析能力。它主要具备以下核心功能：\nTop N 统计分析: 快速统计日志中出现频率最高的 IP 地址、HTTP 状态码和请求路径，帮助你快速了解访问概况和潜在热点。 关键词过滤: 支持根据关键词快速筛选出包含特定信息的日志行，例如错误日志、特定用户的访问日志等，方便问题定位。 时间范围过滤: 允许用户指定时间范围，筛选出特定时间段内的日志，例如分析某个时间段内的异常请求或用户行为。 环境准备与技术选型 开发环境要求 Go 1.18+（支持泛型特性） 支持正则表达式的日志格式（如Nginx/Apache通用格式） 内存：100MB+（取决于日志规模） 关键技术栈 组件 用途 优势 bufio.Scanner 大文件流式读取 内存效率高，支持GB级文件处理 regexp 结构化日志解析 高性能正则匹配（RE2引擎） sort.Slice 统计结果排序 灵活的自定义排序实现 time.Parse 多时区时间解析 精确处理全球分布式系统日志 选择Go语言的核心考量：\n编译为单文件二进制，无依赖部署 原生并发模型适合日志处理场景 卓越的性能表现（相比Python/Node.js） 丰富的标准库覆盖常见需求 核心功能实现解析 Nginx日志格式 Nginx配置\n1 2 3 4 5 6 7 log_format main \u0026#39;$remote_addr $host $document_uri [$time_local] \u0026#39; \u0026#39;\u0026#34;$remote_user\u0026#34; \u0026#34;$scheme\u0026#34; \u0026#34;$request\u0026#34; \u0026#34;$status\u0026#34; \u0026#34;$body_bytes_sent\u0026#34; \u0026#39; \u0026#39;\u0026#34;X-Forwarded-For: $proxy_add_x_forwarded_for\u0026#34; \u0026#39; \u0026#39;\u0026#34;Referer: $http_referer\u0026#34; \u0026#34;User_Agent: $http_user_agent\u0026#34; \u0026#39; \u0026#39;\u0026#34;upstream_addr:$upstream_addr\u0026#34; \u0026#34;upstream_cache_status:$upstream_cache_status\u0026#34; \u0026#39; \u0026#39;\u0026#34;upstream_status:$upstream_status\u0026#34; \u0026#34;upstream_response_time:$upstream_response_time\u0026#34;\u0026#39;; access_log logs/access.log main; 日志格式\n1 2 106.224.56.100 track-backend.xxx.com /trace [28/Jan/2025:23:57:01 +0800] \u0026#34;-\u0026#34; \u0026#34;https\u0026#34; \u0026#34;POST /trace HTTP/1.1\u0026#34; \u0026#34;200\u0026#34; \u0026#34;50\u0026#34; \u0026#34;X-Forwarded-For: 106.224.56.100, 106.224.56.100\u0026#34; \u0026#34;Referer: -\u0026#34; \u0026#34;User_Agent: okhttp/4.2.2\u0026#34; \u0026#34;upstream_addr:172.18.199.211:80\u0026#34; \u0026#34;upstream_cache_status:-\u0026#34; \u0026#34;upstream_status:200\u0026#34; \u0026#34;upstream_response_time:0.008\u0026#34; 8.135.0.81 ykdgate.xxx.com /api/v1/contract/query/WLSK/21070321435307266422910/LOAN_CONTRACT [28/Jan/2025:23:57:01 +0800] \u0026#34;-\u0026#34; \u0026#34;https\u0026#34; \u0026#34;GET /api/v1/contract/query/WLSK/21070321435307266422910/LOAN_CONTRACT HTTP/1.1\u0026#34; \u0026#34;200\u0026#34; \u0026#34;120\u0026#34; \u0026#34;X-Forwarded-For: 8.135.0.81, 8.135.0.81\u0026#34; \u0026#34;Referer: -\u0026#34; \u0026#34;User_Agent: Apache-HttpClient/4.5.2 (Java/1.8.0_402)\u0026#34; \u0026#34;upstream_addr:172.18.199.211:80\u0026#34; \u0026#34;upstream_cache_status:-\u0026#34; \u0026#34;upstream_status:200\u0026#34; \u0026#34;upstream_response_time:0.013\u0026#34; 正则表达 https://hiregex.com/\n1 (?P\u0026lt;ip\u0026gt;\\S+) (?P\u0026lt;host\u0026gt;\\S+) (?P\u0026lt;uri\u0026gt;\\S+) \\[(?P\u0026lt;time\u0026gt;.*?)\\] \u0026#34;(?P\u0026lt;user\u0026gt;.*?)\u0026#34; \u0026#34;(?P\u0026lt;scheme\u0026gt;.*?)\u0026#34; \u0026#34;(?P\u0026lt;request\u0026gt;[^\u0026#34;]+)\u0026#34; \u0026#34;(?P\u0026lt;status\u0026gt;\\d+)\u0026#34; 日志解析引擎设计 1 2 var logPattern = regexp.MustCompile( `(?P\u0026lt;ip\u0026gt;\\S+) (?P\u0026lt;host\u0026gt;\\S+) (?P\u0026lt;uri\u0026gt;\\S+) \\[(?P\u0026lt;time\u0026gt;.*?)\\] \u0026#34;(?P\u0026lt;user\u0026gt;.*?)\u0026#34; \u0026#34;(?P\u0026lt;scheme\u0026gt;.*?)\u0026#34; \u0026#34;(?P\u0026lt;request\u0026gt;[^\u0026#34;]+)\u0026#34; \u0026#34;(?P\u0026lt;status\u0026gt;\\d+)\u0026#34;`) 这个正则表达式定义了日志行中各个字段的提取规则，例如 IP 地址、主机名、请求 URI、时间戳、用户、请求协议、请求内容和 HTTP 状态码等。 (?P...) 是 Go 语言正则表达式的命名捕获组，可以将匹配到的内容存储到指定名称的组中，方便后续代码引用。\n统计数据结构 1 2 3 4 5 6 type LogStats struct { IPs map[string]int // IP访问计数器 Status map[string]int // 状态码分布 Paths map[string]int // URI访问排行 Lines []string // 原始日志缓存 } IPs、 Status 和 Paths 字段使用 map[string]int 存储统计数据，键为字符串类型的 IP 地址、状态码或路径，值为出现次数。 Lines 字段使用字符串切片 []string 存储原始日志行，用于后续的关键词和时间范围过滤。\nProcessLine 函数：日志行的“处理器” ProcessLine 函数负责逐行处理日志文件，解析每行日志并提取关键信息，更新 LogStats 结构体中的统计数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // ProcessLine 处理日志行 func (ls *LogStats) ProcessLine(line string) { matches := logPattern.FindStringSubmatch(line) if matches == nil { return // 如果日志行不匹配正则，则忽略 } fields := make(map[string]string) for i, name := range logPattern.SubexpNames() { if i != 0 \u0026amp;\u0026amp; name != \u0026#34;\u0026#34; { fields[name] = matches[i] // 将正则匹配到的内容存储到 fields map 中 } } ls.IPs[fields[\u0026#34;ip\u0026#34;]]++ // 统计 IP 地址 ls.Status[fields[\u0026#34;status\u0026#34;]]++ // 统计 HTTP 状态码 ls.Paths[fields[\u0026#34;uri\u0026#34;]]++ // 统计请求路径 ls.Lines = append(ls.Lines, line) // 存储原始日志行 } 该函数首先使用 logPattern.FindStringSubmatch(line) 尝试匹配日志行，如果匹配失败则直接返回。匹配成功后，将匹配到的内容存储到 fields map 中，并根据字段名更新 LogStats 结构体中的 IPs、 Status 和 Paths 统计数据，同时将原始日志行添加到 Lines 切片中\n交互式设计 1 2 3 4 5 请选择您的操作(输入数字: 1. 输出 Top 10 IPs, Status Codes, and Paths 2. 根据 keyword 过滤日志行 3. 根据 时间 过滤日志行 4. 退出程序 通过简单的菜单驱动，将常用操作封装为原子功能，提升用户体验。\n完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 package main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;regexp\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; ) /** * @Author: 南宫乘风 * @Description: * @File: main.go * @Email: 1794748404@qq.com * @Date: 2025-02-17 14:15 */ // 正则匹配格式 var logPattern = regexp.MustCompile(`(?P\u0026lt;ip\u0026gt;\\S+) (?P\u0026lt;host\u0026gt;\\S+) (?P\u0026lt;uri\u0026gt;\\S+) \\[(?P\u0026lt;time\u0026gt;.*?)\\] \u0026#34;(?P\u0026lt;user\u0026gt;.*?)\u0026#34; \u0026#34;(?P\u0026lt;scheme\u0026gt;.*?)\u0026#34; \u0026#34;(?P\u0026lt;request\u0026gt;[^\u0026#34;]+)\u0026#34; \u0026#34;(?P\u0026lt;status\u0026gt;\\d+)\u0026#34;`) // 时间格式 const timeLayout = \u0026#34;02/Jan/2006:15:04:05 -0700\u0026#34; // 加入时区解析 // LogStats 日志统计结构体 type LogStats struct { IPs map[string]int Status map[string]int Paths map[string]int Lines []string } // ProcessLine 处理日志行 func (ls *LogStats) ProcessLine(line string) { matches := logPattern.FindStringSubmatch(line) if matches == nil { return } fields := make(map[string]string) for i, name := range logPattern.SubexpNames() { if i != 0 \u0026amp;\u0026amp; name != \u0026#34;\u0026#34; { fields[name] = matches[i] } } ls.IPs[fields[\u0026#34;ip\u0026#34;]]++ ls.Status[fields[\u0026#34;status\u0026#34;]]++ ls.Paths[fields[\u0026#34;uri\u0026#34;]]++ ls.Lines = append(ls.Lines, line) } // printTopN 统计数据 func printTopN(data map[string]int, n int) { type kv struct { Key string Value int } var sortedData []kv for k, v := range data { sortedData = append(sortedData, kv{k, v}) } sort.Slice(sortedData, func(i, j int) bool { return sortedData[i].Value \u0026gt; sortedData[j].Value }) for i, item := range sortedData { if i \u0026gt;= n { break } fmt.Printf(\u0026#34;%s: %d\\n\u0026#34;, item.Key, item.Value) } } // PrintTopN 打印统计数据 func (ls *LogStats) PrintTopN(n int) { fmt.Println(\u0026#34;Top IPs:\u0026#34;) printTopN(ls.IPs, n) fmt.Println(\u0026#34;\\nTop Status Codes:\u0026#34;) printTopN(ls.Status, n) fmt.Println(\u0026#34;\\nTop Paths:\u0026#34;) printTopN(ls.Paths, n) } // FilterLines 根据关键字过滤日志 func (ls *LogStats) FilterLines(keyword string) { fmt.Println(\u0026#34;Filtered Lines:\u0026#34;) for _, line := range ls.Lines { if strings.Contains(line, keyword) { fmt.Println(line) } } } // FilterByTime 根据时间过滤日志 func (ls *LogStats) FilterByTime(start, end string) { fmt.Println(\u0026#34;按时间筛选日志:\u0026#34;) startTime, err1 := time.Parse(timeLayout, start) endTime, err2 := time.Parse(timeLayout, end) if err1 != nil || err2 != nil { fmt.Println(\u0026#34;Invalid time format. Use: 02/Jan/2006:15:04:05 -0700\u0026#34;) return } for _, line := range ls.Lines { matches := logPattern.FindStringSubmatch(line) if matches == nil { continue } logTime, err := time.Parse(timeLayout, matches[4]) // 直接解析日志中的完整时间戳 if err != nil { continue } if (logTime.Equal(startTime) || logTime.After(startTime)) \u0026amp;\u0026amp; logTime.Before(endTime) { fmt.Println(line) } } } // NewLogStats 创建新的 LogStats 实例 func NewLogStats() *LogStats { return \u0026amp;LogStats{ IPs: make(map[string]int), Status: make(map[string]int), Paths: make(map[string]int), Lines: []string{}, } } // main 函数 func main() { // 检查参数 if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;Usage: log-analyzer \u0026lt;logfile\u0026gt;\u0026#34;) os.Exit(1) } // 打开文件 file, err := os.Open(os.Args[1]) if err != nil { fmt.Println(\u0026#34;Error opening file:\u0026#34;, err) os.Exit(1) } defer file.Close() // 创建 LogStats 实例 stats := NewLogStats() // 读取文件 scanner := bufio.NewScanner(file) for scanner.Scan() { stats.ProcessLine(scanner.Text()) } if err := scanner.Err(); err != nil { fmt.Println(\u0026#34;Error reading file:\u0026#34;, err) } // 主循环 for { fmt.Println(\u0026#34;\\n请选择您的操作(输入数字:\u0026#34;) fmt.Println(\u0026#34;1. 输出 Top 10 IPs, Status Codes, and Paths\u0026#34;) fmt.Println(\u0026#34;2. 根据 keyword 过滤日志行\u0026#34;) fmt.Println(\u0026#34;3. 根据 时间 过滤日志行\u0026#34;) fmt.Println(\u0026#34;4. 退出程序\u0026#34;) fmt.Print(\u0026#34;请输入您的选择: \u0026#34;) // 读取用户输入 var choice int fmt.Scanln(\u0026amp;choice) // 根据选择执行操作 switch choice { case 1: stats.PrintTopN(10) case 2: fmt.Print(\u0026#34;您输入的 keyword 是: \u0026#34;) var keyword string fmt.Scanln(\u0026amp;keyword) stats.FilterLines(keyword) case 3: reader := bufio.NewReader(os.Stdin) fmt.Print(\u0026#34;Enter start time (format: 29/Jan/2025:12:58:00 +0800): \u0026#34;) startTime, _ := reader.ReadString(\u0026#39;\\n\u0026#39;) startTime = strings.TrimSpace(startTime) // 去除换行符 fmt.Print(\u0026#34;Enter end time (format: 29/Jan/2025:12:59:00 +0800): \u0026#34;) var endTime string endTime, _ = reader.ReadString(\u0026#39;\\n\u0026#39;) endTime = strings.TrimSpace(endTime) // 去除换行符 stats.FilterByTime(startTime, endTime) case 4: fmt.Println(\u0026#34;退出...\u0026#34;) return default: fmt.Println(\u0026#34;无效的选择, 请重新输入\u0026#34;) } } } 测试 1 2 使用方法 ./二进制文件 日志文件 Top显示\n关键字 时间过滤 总结 log-analyzer 是一个轻量级、实用的日志分析工具，它以简洁的代码实现了核心的日志分析功能，能够有效提升开发者和测试人员的日常工作效率。后续根据公司需求添加更多的功能\n","date":"2025-02-17T17:22:29Z","image":"https://www.ownit.top/title_pic/62.jpg","permalink":"https://www.ownit.top/p/202502171722/","title":"轻量级日志分析利器：Go实战"},{"content":"问题背景 在Kubernetes（k8s）中，Pod的自动扩容（Horizontal Pod Autoscaler, HPA）是一个常见的功能，用于根据负载动态调整Pod的数量。然而，当新Pod加入时，如果没有预热机制，流量会立即涌入新Pod，导致新Pod在启动初期无法处理大量请求，进而引发5xx错误、CPU飙升等问题。特别是在基于JVM的应用程序中，JVM需要一定的时间进行预热（如JIT编译、类加载等），才能达到最佳性能。\n阿里云介绍： 在未启用慢启动预热功能时，每当新目标Pod加入时，请求方都会向该Pod发送一定比例的流量，不支持新Pod的渐进式流量增加。这对于需要一些预热时间来提供全部负载的服务可能是不可取的，并且可能会导致请求超时、数据丢失和用户体验恶化。例如在基于JVM的Web应用程序中，这些应用程序使用水平Pod自动缩放。当服务刚启动时，它会被大量请求淹没，这会导致应用程序预热时持续超时。因此，每次扩展服务时，都会丢失数据或者会导致这部分请求的响应时间增加。预热的基本思想就是让刚启动的机器逐步接入流量。 地址：https://help.aliyun.com/zh/asm/user-guide/use-the-warm-up-feature\n问题分析 是什么： Pod扩容时的流量分配问题：当新Pod加入时，Kubernetes默认会将流量均匀分配到所有Pod（包括新Pod），导致新Pod在启动初期无法处理大量请求。 预热陷阱：新Pod在启动初期需要时间进行预热（如JVM的JIT编译、缓存预热等），如果此时涌入大量请求，会导致请求超时、5xx错误、CPU飙升等问题。 为什么： 流量分配机制：Kubernetes默认的流量分配机制是均匀分配，不支持渐进式流量增加。 预热需求：某些应用程序（如基于JVM的应用）在启动初期需要时间进行预热，才能达到最佳性能。 怎么做： 解决方案：通过引入预热机制，让新Pod在启动初期逐步接入流量，避免一次性涌入大量请求 解决方案 方案一：使用Istio的LoadBalancerSettings进行流量预热 慢启动模式又称渐进式流量增加，用户可以为服务配置一个时间段，每当一个服务实例启动时，请求方会向该实例发送一部分请求负载，并在配置的时间段内逐步增加请求量。当慢启动窗口持续时间到达，就会退出慢启动模式。\n在慢启动模式下，添加新的目标服务Pod时，避免新增Pod被大量请求击垮，这些新目标服务可以根据指定的加速期在接受其均衡策略的请求之前进行预热。\n慢启动对于依赖缓存并且需要预热期才能以最佳性能响应请求的应用程序非常有用。只需在服务对应的DestinationRule下的配置trafficPolicy/loadBalancer即可，需要注意的是：\nloadBalancer：表示负载均衡器信息。类型限定于ROUND_ROBIN和LEAST_REQUEST负载均衡器。\nwarmupDurationSecs：表示Service的预热持续时间。如果设置，则新创建的服务端点在此窗口期间从其创建时间开始保持预热模式，并且Istio逐渐增加该端点的流量，而不是发送成比例的流量。\n慢启动要求应用在当前可用区的副本数不为0。例如：\n数据面集群只有一个可用区A，可用区A中当前有一个副本，启动第二个时，慢启动会生效。\n数据面集群有两个可用区A、B，当前应用只有一个副本，且位于可用区A，如果启动的第二个副本位于可用区B（一些调度器会默认采用跨可用区分布的策略），则不会触发慢启动。此时，如果再启动第三个副本，慢启动会生效。\nIstio 提供了LoadBalancerSettings配置，可以通过warmupDurationSecs参数实现流量预热。该参数允许新Pod在启动后的一段时间内逐步增加流量。 地址：https://istio.io/latest/docs/reference/config/networking/destination-rule/#LoadBalancerSettings 步骤：\n在Istio的DestinationRule中配置LoadBalancerSettings。 设置warmupDurationSecs参数，定义预热时间。 代码示例：\n1 2 3 4 5 6 7 8 9 10 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: mocka spec: host: mocka trafficPolicy: loadBalancer: simple: ROUND_ROBIN warmupDurationSecs: 300 # 预热时间为300秒（5分钟） 优点：\n简单易用，直接通过Istio配置实现流量预热。 无需修改应用程序代码。 缺点：\n需要部署Istio，增加了系统复杂性。 方案二：使用阿里云的JVM预热方案（jwarmup） 参考文档：https://baijiahao.baidu.com/s?id=1752631259548476891\u0026amp;wfr=spider\u0026amp;for=pc\n阿里云提供了jwarmup工具，可以将JVM的JIT编译信息保存到文件中，下次发布时自动加载，从而加速JVM预热过程。\nJwarmUp的基本原理：根据前一次程序运行的情况，记录热点代码以及类加载顺序等信息。在应用下一次启动的时候积极主动地对相关类进行加载，并积极编译相关代码，进而使得应用尽快使用上C2编译优化的指令。从而在流量进来之前，提前完成类的加载、初始化和方法编译, 跳过解释阶段, 直接执行编译好的native code, 避免一面解释执行一面后台编译带来的CPU与load飙高, rt超时等问题。\n上方为JWarmup的流程图，它将应用程序的发布分成了两个阶段，分别是Recording和Replaying。在Recording阶段，JVM会接受线上的请求，同时记录JVM即时编译器它所编译方法的信息，并且将这些信息都输出到一个文件之中。 等到第二次再去启动的时候，JVM就可以去读取刚刚所记录的这些方法编译的信息，同时会主动的触发即时编译器编译刚刚记录的热点方法，使得在用户请求到来之前，就把热点方法编译成为性能较高的Native Code，避免了在用户请求大量进入的时候做编译，这样就能够进一步提高应用程序的性能，节约CPU使用率。\n步骤：\n在应用程序中集成jwarmup工具。 在发布前运行jwarmup，生成JIT编译信息文件。 在下次发布时，自动加载JIT编译信息文件。 代码示例：\n记录编译信息阶段 1 -XX:+CompilationWarmUpRecording -XX:CompilationWarmUpLogfile=jwarmup.log -XX:CompilationWarmUpRecordTime=300 记录模式、记录存储的jwarmup.log，在5分钟后生成profiling data\n使用编译信息阶段 1 -XX:+CompilationWarmUp -XX:CompilationWarmUpLogfile=jwarmup.log -XX:CompilationWarmUpDeoptTime=0 JWarmUp会在指定时间退优化warmup编译的方法，设置CompilationWarmUpDeoptTime为0可以取消这个定时。\n1、recording记录下来的日志，是怎么分发到其他线上机的？\n答：在应用启动的脚本文件进行控制：\n预热节点，会将记录下来的编译信息上传到远程服务器oss上， 发布节点，在启动时从远处机器主动pull下来预热节点上传的编译信息。 2、是怎么制定一台机器做recording的呢？是访问某个url还是判断beta机器？\n答：是通过访问oss做了一个类似于“文件锁”的东西，先拿到锁的beta机器做为预热节点，其余机器为发布节点。想要达到预热的效果请确保：\n发布的机器的参数中有 -XX:+CompilationWarmUp 每次beta发布后，记得检查下编译信息文件是否已经上传 beta发布的那台机器必须是有流量的，Recording时间不要太短，尽量多编译一些方法。 如果不保证上述两点的话，便无法完成预热发布，即没有充分利用beta的编译信息，仍然走正常发布的流程\n方案三：手动预热接口 在Pod启动后，手动调用所有接口进行预热，然后再接入流量。\n步骤：\n在Pod启动后，编写脚本调用所有接口。 确保所有接口都被调用后，再接入流量。 代码示例：\n1 2 3 4 # 预热脚本示例 curl http://my-service/api/endpoint1 curl http://my-service/api/endpoint2 curl http://my-service/api/endpoint3 优点：\n简单直接，适用于所有类型的应用程序。 无需额外工具或配置。 缺点：\n需要手动编写和维护预热脚本。 无法动态调整预热时间。 方案四：使用Kubernetes的Readiness Probe 通过配置Kubernetes的Readiness Probe，确保Pod在完全预热后再接入流量。\n步骤：\n在Pod的Readiness Probe中配置预热检查。 确保Pod在预热完成后再标记为Ready。 代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: my-image readinessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 60 # 延迟60秒开始检查 periodSeconds: 10 # 每10秒检查一次 优点：\n简单易用，直接通过Kubernetes配置实现。 无需修改应用程序代码。 缺点：\n无法精确控制预热时间。 需要应用程序提供健康检查接口。 ","date":"2025-02-16T19:50:18Z","image":"https://www.ownit.top/title_pic/59.jpg","permalink":"https://www.ownit.top/p/202502161950/","title":"Kubernetes Pod扩容预热陷阱：如何避免5xx错误和CPU飙升？"},{"content":"背景 近期，随着机房迁移的进行，为了提升电力保障并确保设备的持续运行，我们在原有基础上新增了多台山特UPS电源作为备用电源。这些UPS电源的主要作用是在电力中断时为机房设备提供足够的电力支持，确保设备有充足的时间进行正常关机，避免由于突然断电带来的数据丢失或硬件损坏。 然而，UPS电源本身并未进行有效的监控，这使得我们无法实时掌握UPS的工作状态、电池电量等关键信息，从而错失可能出现故障的早期预警。因此，为了提高机房电力管理的智能化水平，确保UPS电源的稳定性与可靠性，我们决定对这些UPS电源进行全面的监控。\n目的 通过构建一个完整的UPS监控方案，不仅能及时发现UPS的故障或异常状态，还能够在电池电量不足等紧急情况下，提前做出预警，避免对机房设备造成不必要的影响。\n监控目标 监控UPS状态：实时获取UPS的运行状态，如是否正常、是否有故障等。 监控UPS电池容量：确保电池容量健康，避免突然断电。 监控UPS输入电压：确保UPS接收到的电压符合标准。 提高运维效率：通过自动化监控，减少人工检查和干预。 操作步骤 1. 配置山特UPS客户端 为了使Zabbix能够监控山特UPS电源，我们需要从UPS系统中获取相关数据。假设山特UPS的IP为192.168.81.11，并且它支持通过HTTP接口输出状态数据（比如通过REST API获取JSON格式的数据）。接下来，我们将编写一个Shell脚本，通过调用UPS的API接口，提取数据并交给Zabbix监控。\n关于监控软件的安装：\n下载软件：https://www.santak.com.cn/page/santak-downloads.html 软件名：Winpower_setup_LinuxAMD64.tar.gz\n连接COM线并转为USB插入到负责监控的服务器，当前启动监控软件的服务器为192.168.81.11 安装：\n1 2 3 4 5 6 tar zxf Winpower_setup_LinuxAMD64.tar.gz cd Winpower_setup_LinuxAMD64/LinuxAMD64/ ./setup_console.bin # 一路回车即可 # 启动agent cd /opt/MonitorSoftware/ ./agent start 配置：\n1 2 # 需要安装图形化界面 startx 1 2 3 # 启动管理软件 cd /opt/MonitorSoftware/ ./monitor 配置USB接口：系统-\u0026gt;成为系统管理员(默认没有密码，直接确认即可)-\u0026gt;通讯口设定-\u0026gt;输入/dev/ttyUSB1增加→输入/dev/ttyUSB2增加→确定\n扫描设备：系统→自动搜索设备，完成后即可在页面显示对应的UPS状态信息 配置https端口：远程监控-\u0026gt;Web服务器控制-\u0026gt;停止-\u0026gt;远程监控-\u0026gt;Web服务器控制→修改端口为10086-\u0026gt;开始-\u0026gt;确定\n打开网页 2. 测试数据 访问\n1 2 0 代表ups的位置，假如有三台，则是：0 1 2 地址：https://192.168.96.200:8888/0/json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 { \u0026#34;key\u0026#34;: \u0026#34;COM3/C6K\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;\u0026#34;, -\u0026#34;device\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;COM3/C6K\u0026#34;, \u0026#34;id\u0026#34;: 0, \u0026#34;protocol\u0026#34;: 18, \u0026#34;portIndex\u0026#34;: 1, \u0026#34;ip\u0026#34;: null, \u0026#34;status\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;upsIndex\u0026#34;: 0, \u0026#34;statusIcon\u0026#34;: \u0026#34;online\u0026#34;, \u0026#34;hasWarn\u0026#34;: false }, \u0026#34;status\u0026#34;: \u0026#34;正常\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;C6K\u0026#34;, \u0026#34;loadPercentMax\u0026#34;: 16, \u0026#34;loadSegment2State\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;loadSegment1State\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;redundantNumber\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;inVolt\u0026#34;: \u0026#34;231.6V\u0026#34;, \u0026#34;bypassFreq\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;workMode\u0026#34;: 3, \u0026#34;loadPercent\u0026#34;: \u0026#34;16%\u0026#34;, \u0026#34;lsCounter\u0026#34;: -1, \u0026#34;extStatus\u0026#34;: 0, \u0026#34;oidType\u0026#34;: 0, \u0026#34;outFreq\u0026#34;: \u0026#34;50.0Hz\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;noModule\u0026#34;: false, \u0026#34;batCapacity\u0026#34;: \u0026#34;100%\u0026#34;, \u0026#34;batTimeRemain\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bypassVolt\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;inFreq\u0026#34;: \u0026#34;50.0Hz\u0026#34;, \u0026#34;iStatus\u0026#34;: 0, \u0026#34;outVolt\u0026#34;: \u0026#34;219.5V\u0026#34;, \u0026#34;abmState\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;emdAlarm1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cfgBatNumber\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cfgKVA\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;emdAlarm2\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;batTemp\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;emdHumidity\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;emdTemp\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastEvent1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;statusColor\u0026#34;: 0, \u0026#34;supportTest\u0026#34;: true, \u0026#34;upsTemp\u0026#34;: \u0026#34;27.2C\u0026#34;, \u0026#34;lastEvent2\u0026#34;: \u0026#34;2025/01/15 17:34:54 Agent启动\u0026#34;, \u0026#34;batV\u0026#34;: \u0026#34;201.9V\u0026#34;, \u0026#34;outVA\u0026#34;: \u0026#34;0.9KVA\u0026#34;, \u0026#34;ls1\u0026#34;: -1, \u0026#34;outW\u0026#34;: \u0026#34;0.6KW\u0026#34;, \u0026#34;ls2\u0026#34;: -1, \u0026#34;outA\u0026#34;: \u0026#34;\u0026#34; } 3. 安装Zabbix Agent并配置 在监控服务器上，我们首先需要安装Zabbix Agent，Zabbix Agent是用来采集数据并将其传递给Zabbix Server的工具。假设监控服务器IP为192.168.82.12，我们将在此服务器上进行安装并配置Zabbix Agent。\n安装Zabbix Agent 如果您使用的是基于Debian/Ubuntu的操作系统，可以使用以下命令来安装Zabbix Agent：\n1 2 sudo apt update sudo apt install zabbix-agent 对于RedHat/CentOS系统：\n1 sudo yum install zabbix-agent 安装完成后，启动Zabbix Agent：\n1 2 sudo systemctl start zabbix-agent sudo systemctl enable zabbix-agent 编写UPS监控脚本 在Zabbix Agent所在的服务器上，创建一个新的监控脚本。我们将在/etc/zabbix/zabbix_agentd.d/script/ups-monitor/ups.sh路径下创建脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash num=$2 if [ \u0026#34;$num\u0026#34; = \u0026#34;0\u0026#34; ];then data=$(curl --connect-timeout 3 -k -s https://192.168.81.11:10086/1/json -w \u0026#34;|%{http_code}\u0026#34;) elif [ \u0026#34;$num\u0026#34; = \u0026#34;1\u0026#34; ];then data=$(curl --connect-timeout 3 -k -s https://192.168.81.11:10086/0/json -w \u0026#34;|%{http_code}\u0026#34;) elif [ \u0026#34;$num\u0026#34; = \u0026#34;2\u0026#34; ];then data=$(curl --connect-timeout 3 -k -s https://192.168.81.10:10086/0/json -w \u0026#34;|%{http_code}\u0026#34;) fi case $1 in status) echo \u0026#34;$data\u0026#34;|awk -F\u0026#39;|\u0026#39; \u0026#39;{print $1}\u0026#39;|jq \u0026#39;.[\u0026#34;status\u0026#34;]\u0026#39;|sed \u0026#39;s/\u0026#34;//g\u0026#39; ;; batCapacity) echo \u0026#34;$data\u0026#34;|awk -F\u0026#39;|\u0026#39; \u0026#39;{print $1}\u0026#39;|jq \u0026#39;.[\u0026#34;batCapacity\u0026#34;]\u0026#39;|sed \u0026#39;s/\u0026#34;//g;s/%//g\u0026#39; ;; inVolt) echo \u0026#34;$data\u0026#34;|awk -F\u0026#39;|\u0026#39; \u0026#39;{print $1}\u0026#39;|jq \u0026#39;.[\u0026#34;inVolt\u0026#34;]\u0026#39;|sed \u0026#39;s/\u0026#34;//g;s/V//g\u0026#39; ;; apiStatus) echo \u0026#34;$data\u0026#34;|awk -F\u0026#39;|\u0026#39; \u0026#39;{print $2}\u0026#39; ;; *) esac 此脚本根据不同的参数(status, batCapacity, inVolt)来获取UPS设备的不同数据项。它通过curl命令访问UPS的API接口，并使用jq解析JSON格式的数据。\n配置Zabbix Agent 在Zabbix Agent的配置文件中（通常位于/etc/zabbix/zabbix_agentd.conf），我们需要添加自定义监控项，使Zabbix能够调用上面创建的脚本来获取UPS相关的数据。\n在zabbix_agentd.conf文件中，添加以下行：\n自定义监控项：192.168.82.12:/etc/zabbix/zabbix_agentd.d/userparameter_ups.conf\n1 2 3 4 UserParameter=ups.status[*],/usr/bin/bash /etc/zabbix/zabbix_agentd.d/script/ups-monitor/ups.sh status $1 UserParameter=ups.batCapacity[*],/usr/bin/bash /etc/zabbix/zabbix_agentd.d/script/ups-monitor/ups.sh batCapacity $1 UserParameter=ups.inVolt[*],/usr/bin/bash /etc/zabbix/zabbix_agentd.d/script/ups-monitor/ups.sh inVolt $1 UserParameter=ups.apiStatus[*],/usr/bin/bash /etc/zabbix/zabbix_agentd.d/script/ups-monitor/ups.sh apiStatus $1 保存配置文件并重新启动Zabbix Agent：\n1 sudo systemctl restart zabbix-agent 3. 配置Zabbix Server 接下来，我们需要在Zabbix Server中配置相应的监控项，以便能够收集和显示UPS的相关数据。\n3.1 创建监控项 在Zabbix前端，进入“Configuration” -\u0026gt; “Hosts”，选择您的监控主机，然后点击“Items”选项卡，创建以下监控项：\n监控UPS状态：类型选择“Zabbix agent (active)”，键值输入ups.status。 监控电池容量：类型选择“Zabbix agent (active)”，键值输入ups.batCapacity。 监控输入电压：类型选择“Zabbix agent (active)”，键值输入ups.inVolt。 设置合适的数据收集周期，并保存监控项。 模板文件 Template UPS.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;zabbix_export\u0026gt; \u0026lt;version\u0026gt;4.0\u0026lt;/version\u0026gt; \u0026lt;date\u0026gt;2022-03-31T10:04:22Z\u0026lt;/date\u0026gt; \u0026lt;groups\u0026gt; \u0026lt;group\u0026gt; \u0026lt;name\u0026gt;FJF Dev VM UPS\u0026lt;/name\u0026gt; \u0026lt;/group\u0026gt; \u0026lt;/groups\u0026gt; \u0026lt;templates\u0026gt; \u0026lt;template\u0026gt; \u0026lt;template\u0026gt;Template UPS\u0026lt;/template\u0026gt; \u0026lt;name\u0026gt;Template UPS\u0026lt;/name\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;groups\u0026gt; \u0026lt;group\u0026gt; \u0026lt;name\u0026gt;FJF Dev VM UPS\u0026lt;/name\u0026gt; \u0026lt;/group\u0026gt; \u0026lt;/groups\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;items\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[0]接口状态\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.apiStatus[0]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;3\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units/\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[1]接口状态\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.apiStatus[1]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;3\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units/\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[2]接口状态\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.apiStatus[2]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;3\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units/\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[0]电池剩余电量\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.batCapacity[0]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;3\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units\u0026gt;%\u0026lt;/units\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[1]电池剩余电量\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.batCapacity[1]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;3\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units\u0026gt;%\u0026lt;/units\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[2]电池剩余电量\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.batCapacity[2]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;3\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units\u0026gt;%\u0026lt;/units\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[0]电源输入电压\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.inVolt[0]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;0\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units\u0026gt;V\u0026lt;/units\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[1]电源输入电压\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.inVolt[1]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;0\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units\u0026gt;V\u0026lt;/units\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[2]电源输入电压\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.inVolt[2]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;365d\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;0\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units\u0026gt;V\u0026lt;/units\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[0]电源状态\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.status[0]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;0\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;4\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units/\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[1]电源状态\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.status[1]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;0\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;4\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units/\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;name\u0026gt;UPS[2]电源状态\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;snmp_community/\u0026gt; \u0026lt;snmp_oid/\u0026gt; \u0026lt;key\u0026gt;ups.status[2]\u0026lt;/key\u0026gt; \u0026lt;delay\u0026gt;30s\u0026lt;/delay\u0026gt; \u0026lt;history\u0026gt;90d\u0026lt;/history\u0026gt; \u0026lt;trends\u0026gt;0\u0026lt;/trends\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;value_type\u0026gt;4\u0026lt;/value_type\u0026gt; \u0026lt;allowed_hosts/\u0026gt; \u0026lt;units/\u0026gt; \u0026lt;snmpv3_contextname/\u0026gt; \u0026lt;snmpv3_securityname/\u0026gt; \u0026lt;snmpv3_securitylevel\u0026gt;0\u0026lt;/snmpv3_securitylevel\u0026gt; \u0026lt;snmpv3_authprotocol\u0026gt;0\u0026lt;/snmpv3_authprotocol\u0026gt; \u0026lt;snmpv3_authpassphrase/\u0026gt; \u0026lt;snmpv3_privprotocol\u0026gt;0\u0026lt;/snmpv3_privprotocol\u0026gt; \u0026lt;snmpv3_privpassphrase/\u0026gt; \u0026lt;params/\u0026gt; \u0026lt;ipmi_sensor/\u0026gt; \u0026lt;authtype\u0026gt;0\u0026lt;/authtype\u0026gt; \u0026lt;username/\u0026gt; \u0026lt;password/\u0026gt; \u0026lt;publickey/\u0026gt; \u0026lt;privatekey/\u0026gt; \u0026lt;port/\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;inventory_link\u0026gt;0\u0026lt;/inventory_link\u0026gt; \u0026lt;applications\u0026gt; \u0026lt;application\u0026gt; \u0026lt;name\u0026gt;UPS\u0026lt;/name\u0026gt; \u0026lt;/application\u0026gt; \u0026lt;/applications\u0026gt; \u0026lt;valuemap/\u0026gt; \u0026lt;logtimefmt/\u0026gt; \u0026lt;preprocessing/\u0026gt; \u0026lt;jmx_endpoint/\u0026gt; \u0026lt;timeout\u0026gt;3s\u0026lt;/timeout\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;query_fields/\u0026gt; \u0026lt;posts/\u0026gt; \u0026lt;status_codes\u0026gt;200\u0026lt;/status_codes\u0026gt; \u0026lt;follow_redirects\u0026gt;1\u0026lt;/follow_redirects\u0026gt; \u0026lt;post_type\u0026gt;0\u0026lt;/post_type\u0026gt; \u0026lt;http_proxy/\u0026gt; \u0026lt;headers/\u0026gt; \u0026lt;retrieve_mode\u0026gt;0\u0026lt;/retrieve_mode\u0026gt; \u0026lt;request_method\u0026gt;0\u0026lt;/request_method\u0026gt; \u0026lt;output_format\u0026gt;0\u0026lt;/output_format\u0026gt; \u0026lt;allow_traps\u0026gt;0\u0026lt;/allow_traps\u0026gt; \u0026lt;ssl_cert_file/\u0026gt; \u0026lt;ssl_key_file/\u0026gt; \u0026lt;ssl_key_password/\u0026gt; \u0026lt;verify_peer\u0026gt;0\u0026lt;/verify_peer\u0026gt; \u0026lt;verify_host\u0026gt;0\u0026lt;/verify_host\u0026gt; \u0026lt;master_item/\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;/items\u0026gt; \u0026lt;discovery_rules/\u0026gt; \u0026lt;httptests/\u0026gt; \u0026lt;macros/\u0026gt; \u0026lt;templates/\u0026gt; \u0026lt;screens/\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/templates\u0026gt; \u0026lt;triggers\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.apiStatus[0].last()}\u0026amp;lt;\u0026amp;gt;200\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.apiStatus[0].last()}=200\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS-API[0]接口异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.apiStatus[1].last()}\u0026amp;lt;\u0026amp;gt;200\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.apiStatus[1].last()}=200\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS-API[1]接口异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.apiStatus[2].last()}\u0026amp;lt;\u0026amp;gt;200\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.apiStatus[2].last()}=200\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS-API[2]接口异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.status[0].str(零火线接反)}=0 and {Template UPS:ups.status[0].str(正常)}=0\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.status[0].str(零火线接反)}=1 or {Template UPS:ups.status[0].str(正常)}=1\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[0]状态异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.batCapacity[0].last()}\u0026amp;lt;90\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.batCapacity[0].last()}\u0026amp;gt;90\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[0]电池剩余电量小于90%\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.inVolt[0].avg(#3)}\u0026amp;lt;200\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.inVolt[0].avg(#3)}\u0026amp;gt;200\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[0]输入电压异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.status[1].str(正常)}=0\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.status[1].str(正常)}=1\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[1]状态异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.batCapacity[1].last()}\u0026amp;lt;90\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.batCapacity[1].last()}\u0026amp;gt;90\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[1]电池剩余电量小于90%\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.inVolt[1].avg(#3)}\u0026amp;lt;200\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.inVolt[1].avg(#3)}\u0026amp;gt;200\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[1]输入电压异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.status[2].str(正常)}=0\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.status[2].str(正常)}=1\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[2]状态异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.batCapacity[2].last()}\u0026amp;lt;90\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.batCapacity[2].last()}\u0026amp;gt;90\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[2]电池剩余电量小于90%\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;trigger\u0026gt; \u0026lt;expression\u0026gt;{Template UPS:ups.inVolt[2].avg(#3)}\u0026amp;lt;200\u0026lt;/expression\u0026gt; \u0026lt;recovery_mode\u0026gt;1\u0026lt;/recovery_mode\u0026gt; \u0026lt;recovery_expression\u0026gt;{Template UPS:ups.inVolt[2].avg(#3)}\u0026amp;gt;200\u0026lt;/recovery_expression\u0026gt; \u0026lt;name\u0026gt;UPS[2]输入电压异常\u0026lt;/name\u0026gt; \u0026lt;correlation_mode\u0026gt;0\u0026lt;/correlation_mode\u0026gt; \u0026lt;correlation_tag/\u0026gt; \u0026lt;url/\u0026gt; \u0026lt;status\u0026gt;0\u0026lt;/status\u0026gt; \u0026lt;priority\u0026gt;4\u0026lt;/priority\u0026gt; \u0026lt;description/\u0026gt; \u0026lt;type\u0026gt;0\u0026lt;/type\u0026gt; \u0026lt;manual_close\u0026gt;0\u0026lt;/manual_close\u0026gt; \u0026lt;dependencies/\u0026gt; \u0026lt;tags/\u0026gt; \u0026lt;/trigger\u0026gt; \u0026lt;/triggers\u0026gt; \u0026lt;/zabbix_export\u0026gt; 监控项： 3.2 配置触发器 为了及时发现UPS故障，您可以设置触发器。例如，如果UPS的状态变为“故障”或电池容量低于某个阈值时，触发告警。根据您的需求，可以设置以下触发器：\nUPS状态故障触发器：当ups.status的值为“Fault”时，触发告警。 电池容量低触发器：当ups.batCapacity小于某个阈值（例如20%）时，触发告警。 触发器： 4. 验证与测试 完成上述配置后，Zabbix Server将开始定期从Zabbix Agent中获取UPS的状态数据。您可以通过Zabbix前端查看UPS的状态、剩余电池容量和输入电压等信息，并根据设置的触发器在出现异常时收到告警。 ","date":"2025-01-20T16:28:12Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/202501201628/","title":"Zabbix监控山特UPS电源：实现高效监控与告警"},{"content":"1、问题 我使用Flask和Flask-SocketIO 来做 Websocket 链接。前期正常使用，但是后期布置修改什么导致Websocket连接失败。排查需求，才发现初始化不正常导致。\nSocketIO 和 Flask 应用的初始化顺序和引用循环的问题\n2、环境 1 2 3 python-engineio==4.11.1 python-socketio==5.12.0 Flask-SocketIO==5.3.6 3、正常初始化【单文件】 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 from flask import Flask, render_template from flask_socketio import SocketIO app = Flask(__name__) socketio = SocketIO(app) # 初始化 SocketIO # 默认路由，用于渲染 HTML 页面 @app.route(\u0026#39;/\u0026#39;) def index(): return render_template(\u0026#39;index.html\u0026#39;) # 出现消息后,率先执行此处 @socketio.on(\u0026#34;message\u0026#34;, namespace=\u0026#34;/ws\u0026#34;) def socket(message): print(f\u0026#34;接收到消息: {message}\u0026#34;) for i in range(1, 10): socketio.sleep(1) print(f\u0026#34;发送消息: {i}\u0026#34;) socketio.emit(\u0026#34;response\u0026#34;, # 绑定通信 {\u0026#34;data\u0026#34;: i}, # 返回socket数据 namespace=\u0026#34;/ws\u0026#34;) # 当websocket连接成功时,自动触发connect默认方法 @socketio.on(\u0026#34;connect\u0026#34;, namespace=\u0026#34;/ws\u0026#34;) def connect(): print(\u0026#34;链接建立成功..\u0026#34;) # 当websocket连接失败时,自动触发disconnect默认方法 @socketio.on(\u0026#34;disconnect\u0026#34;, namespace=\u0026#34;/ws\u0026#34;) def disconnect(): print(\u0026#34;链接建立失败..\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: socketio.run(app, debug=True, host=\u0026#39;0.0.0.0\u0026#39;, port=5000,allow_unsafe_werkzeug=True) 4、多文件初始化【两个文件】 主要是 manage.py 和 init.py 两个文件。 app/init.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from flask import Flask from flask_socketio import SocketIO from flasgger import Swagger from flask_cors import CORS from flask_migrate import Migrate from app.config import config from app.extension import db # 创建一个全局的 socketio 对象，但不立即初始化 socketio = SocketIO() # 其他 swagger 配置保持不变... def create_app(DevelopmentConfig=None): if DevelopmentConfig is None: DevelopmentConfig = \u0026#39;development\u0026#39; app = Flask(__name__) app.config[\u0026#39;SECRET_KEY\u0026#39;] = \u0026#39;123456\u0026#39; # 加载配置项 app.config.from_object(config.get(DevelopmentConfig)) from app.api import config_blueprint # 注册蓝图 config_blueprint(app) # 数据库配置初始化 config_extensions(app) # 数据库迁移相关代码... from app.api.models.EsModels import sysEsLogs,sysCretitLogs from app.api.models.NetModels import sysNetLogs from app.api.models.UpsModels import sysUpsLogs migrate = Migrate(app, db) # Swagger初始化 Swagger(app, config=swagger_config, template=swagger_template) # CORS配置 CORS(app, resources={r\u0026#39;/*\u0026#39;: {\u0026#39;origins\u0026#39;: \u0026#39;*\u0026#39;}}, supports_credentials=True) # 初始化 SocketIO socketio.init_app(app, cors_allowed_origins=\u0026#34;*\u0026#34;) return app socket_events.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from app import socketio # Socket.IO 事件处理 @socketio.on(\u0026#34;message\u0026#34;, namespace=\u0026#34;/ws\u0026#34;) def socket(message): print(f\u0026#34;接收到消息: {message}\u0026#34;) for i in range(1, 10): socketio.sleep(1) print(f\u0026#34;发送消息: {i}\u0026#34;) socketio.emit(\u0026#34;response\u0026#34;, {\u0026#34;data\u0026#34;: i}, namespace=\u0026#34;/ws\u0026#34;) @socketio.on(\u0026#34;connect\u0026#34;, namespace=\u0026#34;/ws\u0026#34;) def connect(): print(\u0026#34;链接建立成功..\u0026#34;) @socketio.on(\u0026#34;disconnect\u0026#34;, namespace=\u0026#34;/ws\u0026#34;) def disconnect(): print(\u0026#34;链接建立失败..\u0026#34;) manage.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # -*- coding: utf-8 -*- # @Time : 2024/5/2 12:01 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : manage.py # @Software: PyCharm import atexit import os import sys from apscheduler.schedulers.background import BackgroundScheduler from app import create_app, socketio from app.common.util.LogHandler import log from app.crontab.NetCronTab import NetworkMonitor from app.crontab.UpsCronTab import UPSMonitor from app.socket_events import * # 默认为开发环境，按需求修改 production development config_name = \u0026#39;development\u0026#39; app = create_app(config_name) # 解决中文乱码 # app.json.ensure_ascii = False # from skywalking import agent, config # config.init(agent_collector_backend_services=\u0026#39;192.168.82.105:11800\u0026#39;, agent_name=\u0026#39;python-flask@devTenant\u0026#39;,agent_instance_name=\u0026#34;python-flask\u0026#34;) # agent.start() # 数据库迁移 def start_scheduler(): # 创建一个后台调度器 scheduler = BackgroundScheduler(timezone=\u0026#34;Asia/Shanghai\u0026#34;) from app.crontab.EsCronTab import EsIndexCron scheduler.add_job(func=EsIndexCron, trigger=\u0026#39;interval\u0026#39;, minutes=1) log.info(\u0026#34;EsIndexCron 定时任务启动成功\u0026#34;) # scheduler.add_job(func=send_alert, trigger=\u0026#34;interval\u0026#34;, seconds=20) # 启动调度器 scheduler.start() atexit.register(lambda: scheduler.shutdown()) if __name__ == \u0026#39;__main__\u0026#39;: # 获取当前文件的绝对路径 current_file = os.path.abspath(__file__) base_dir = os.path.dirname(current_file) # 将项目目录添加到 sys.path if base_dir not in sys.path: sys.path.append(base_dir) if not app.debug or app.testing: start_scheduler() log.info(\u0026#34;定时任务启动成功\u0026#34;) socketio.run(app, debug=True, use_reloader=True, host=\u0026#39;0.0.0.0\u0026#39;, port=5001) # use_reloader=False, threaded=True, 在postman输入地址和监听事件\n","date":"2024-12-30T18:02:10Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/202412301802/","title":"Flask 与 SocketIO 正确初始化及最佳实践调试"},{"content":"项目简介 本次基于版本3 开源\n版本3开源地址：https://github.com/nangongchengfeng/CsdnBlogBoard.git 版本1开源地址：https://github.com/nangongchengfeng/CSDash.git\n这是一个基于 Python 的 CSDN 博客数据可视化看板项目，通过爬虫采集 CSDN 博客数据，并以现代化的可视化界面展示博主的各项数据指标。该项目采用前后端分离架构，集成了数据采集、数据存储、API 服务和数据可视化等多个功能模块。 版本一 版本二 版本三 技术栈 后端技术 Python Flask: 作为 Web 框架，提供 RESTful API 服务 SQLAlchemy: ORM 框架，用于数据库操作 BeautifulSoup4: 网页解析，用于爬虫数据提取 Requests: HTTP 请求库，用于数据爬取 前端技术 ECharts: 强大的数据可视化图表库 Axios: HTTP 客户端，用于前后端数据交互 现代 CSS: Flexbox 布局、CSS 变量、响应式设计 核心功能模块 1. 数据采集模块 数据采集模块通过spider.py实现，是项目的核心组件之一。该模块采用了多种技术和策略来确保数据采集的准确性、稳定性和实时性。\n1.1 核心技术栈 Requests: 处理 HTTP 请求，支持自定义请求头和超时设置 BeautifulSoup4: 使用 lxml 解析器进行高效的 HTML 解析 正则表达式(re): 用于精确提取数字和特定格式的数据 SQLAlchemy: 实现数据持久化和 ORM 映射 随机延时策略: 使用 random.uniform() 避免频繁请求 1.2 数据采集功能 1.2.1 博主基础信息采集 采集内容：\n用户名和头像 文章数量统计 粉丝数和访问量 点赞和评论数 等级和积分信息 博主排名数据 技术实现：\n1 2 3 # 示例：用户信息提取 user_info = soup.find(\u0026#39;div\u0026#39;, class_=\u0026#39;user-info d-flex flex-column profile-intro-name-box\u0026#39;) author_name = user_info.find(\u0026#39;a\u0026#39;).get_text(strip=True) 1.2.2 文章分类信息采集 采集内容：\n专栏名称和链接 专栏文章数量 订阅人数统计 阅读量和收藏数 专栏唯一标识 技术实现：\n1 2 3 4 5 # 示例：分类信息提取 spans = soup.find_all(\u0026#39;a\u0026#39;, attrs={\u0026#39;class\u0026#39;: \u0026#39;special-column-name\u0026#39;}) for span in spans: href = span.get(\u0026#39;href\u0026#39;) blog_column = span.text.strip() 1.3 错误处理机制 多级异常处理\n请求异常捕获 数据解析异常处理 数据库操作异常处理 重试机制\n1 2 3 4 5 6 7 8 max_retries = 3 retry_count = 0 while retry_count \u0026lt; max_retries: try: # 请求逻辑 except Exception: retry_count += 1 time.sleep(5) # 失败后等待 1.4 数据更新策略 增量更新机制\n检查数据是否存在 根据情况执行更新或插入 保持数据时效性 数据一致性保证\n1 2 3 4 5 6 with app.app_context(): existing_info = Info.query.filter_by(author_name=author_name).first() if existing_info: # 更新现有记录 else: # 插入新记录 1.5 性能优化策略 请求优化\n自定义请求头 连接超时设置 随机延时控制 解析优化\n使用 lxml 解析器提升性能 精确的选择器定位 数据预处理和清洗 存储优化\n批量数据处理 事务管理 会话复用 1.6 反爬虫策略应对 请求头模拟\n1 2 3 4 headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (MSIE 10.0; Windows NT 6.1; Trident/5.0)\u0026#39;, \u0026#39;referer\u0026#39;: \u0026#39;https://passport.csdn.net/login\u0026#39; } 访问频率控制\n随机延时间隔 请求限速 IP 代理支持（预留） 2. 数据存储模块 数据存储模块采用 SQLAlchemy ORM 框架进行数据建模，实现了高效的数据持久化和查询操作。该模块设计了三个核心数据模型，每个模型都针对特定的数据场景进行了优化。\n2.1 数据模型设计 2.1.1 博主信息模型 (Info) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Info(db.Model): __tablename__ = \u0026#39;info\u0026#39; id = db.Column(db.Integer, primary_key=True, autoincrement=True) date = db.Column(db.Text) # 数据更新时间 head_img = db.Column(db.Text) # 博主头像URL author_name = db.Column(db.Text) # 博主用户名 article_num = db.Column(db.Text) # 文章总数 fans_num = db.Column(db.Text) # 粉丝数量 like_num = db.Column(db.Text) # 获赞数量 comment_num = db.Column(db.Text) # 评论数量 level = db.Column(db.Text) # 博主等级 visit_num = db.Column(db.Text) # 访问量 score = db.Column(db.Text) # 积分 rank = db.Column(db.Text) # 排名 特点说明：\n使用自增主键确保记录唯一性 采用 Text 类型存储可变长度文本 包含完整的博主数据画像 支持时间序列分析（通过 date 字段） 2.1.2 文章分类模型 (Categorize) 1 2 3 4 5 6 7 8 9 10 11 12 class Categorize(db.Model): __tablename__ = \u0026#39;categorize\u0026#39; id = db.Column(db.Integer, primary_key=True, autoincrement=True) href = db.Column(db.Text) # 分类链接 categorize = db.Column(db.Text) # 分类名称 categorize_id = db.Column(db.BigInteger) # 分类ID column_num = db.Column(db.BigInteger) # 专栏数量 num_span = db.Column(db.BigInteger) # 订阅数量 article_num = db.Column(db.BigInteger) # 文章数量 read_num = db.Column(db.BigInteger) # 阅读量 collect_num = db.Column(db.BigInteger) # 收藏数 特点说明：\n使用 BigInteger 类型存储大数值数据 支持分类数据的统计分析 包含完整的分类元数据 可追踪分类的受欢迎程度 2.1.3 文章详情模型 (Article) 1 2 3 4 5 6 7 8 9 10 class Article(db.Model): __tablename__ = \u0026#39;article\u0026#39; id = db.Column(db.Integer, primary_key=True, autoincrement=True) url = db.Column(db.Text) # 文章链接 title = db.Column(db.Text) # 文章标题 date = db.Column(db.Text) # 发布日期 read_num = db.Column(db.BigInteger) # 阅读数 comment_num = db.Column(db.BigInteger) # 评论数 type = db.Column(db.Text) # 文章类型 特点说明：\n记录文章的基本信息和统计数据 支持文章时间序列分析 可追踪章的受欢迎程度 便于文章分类统计 2.2 数据库优化策略 字段类型优化\n使用 BigInteger 存储大数值，避免数值溢出 采用 Text 类型存储变长文本，节省存储空间 主键使用自增 Integer，提高插入性能 查询性能优化\n可以根据需要添加索引 支持复杂的聚合查询 优化的数据结构设计 数据一致性保证\n主键约束确保记录唯一性 适当的字段类型确保数据完整性 支持事务操作 2.3 数据操作示例 数据插入 1 2 3 4 5 6 7 new_info = Info( author_name=\u0026#34;博主名称\u0026#34;, article_num=\u0026#34;100\u0026#34;, fans_num=\u0026#34;1000\u0026#34; ) db.session.add(new_info) db.session.commit() 数据查询 1 2 3 4 5 6 7 8 # 获取博主信息 author_info = Info.query.filter_by(author_name=\u0026#34;博主名称\u0026#34;).first() # 获取分类统计 categories = Categorize.query.order_by(Categorize.read_num.desc()).all() # 获取热门文章 hot_articles = Article.query.order_by(Article.read_num.desc()).limit(10).all() 数据更新 1 2 3 info = Info.query.filter_by(author_name=\u0026#34;博主名称\u0026#34;).first() info.fans_num = str(int(info.fans_num) + 1) db.session.commit() 3. 可视化展示模块 前端采用现代化的可视化方案，基于 ECharts 实现了丰富的数据可视化功能，并通过 Axios 实现了与后端的数据交互。\n3.1 技术架构 核心技术\nECharts 5.5.1：数据可视化库 Axios：HTTP 客户端 CSS3：现代布局和动画 Flexbox：响应式布局 设计规范\n1 2 3 4 5 6 7 8 9 :root { --bg-primary: #f7f8fa; --bg-secondary: #ffffff; --text-primary: #333333; --text-secondary: #555555; --accent-color: #0066cc; --border-radius: 12px; --shadow: 0 4px 6px rgba(0, 0, 0, 0.05); } 3.2 图表组件实现 3.2.1 柱状图（季度数据分析） 数据流程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 async function updateBarChart() { // 1. 获取数据 const response = await axios.get(\u0026#34;/api/quarter\u0026#34;); // 2. 数据转换 const dimensions = [ ...Object.keys(chartData[0]).filter((key) =\u0026gt; key !== \u0026#34;category\u0026#34;), ]; const source = chartData.map((item) =\u0026gt; ({ product: item.category, ...item, })); // 3. 图表配置 const option = { legend: {}, tooltip: {}, dataset: { dimensions: dimensions, source: source, }, xAxis: { type: \u0026#34;category\u0026#34; }, yAxis: {}, series: dimensions.slice(1).map((dim) =\u0026gt; ({ type: \u0026#34;bar\u0026#34;, name: dim, })), }; } 交互特性\n动态数据加载 点击事件响应 自适应布局 3.2.2 饼图（分类占比分析） 实现细节\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 async function updatePieChart() { // 1. 数据获取 const response = await axios.get(\u0026#34;/api/categorize\u0026#34;); // 2. 图表配置 const option = { tooltip: { trigger: \u0026#34;item\u0026#34;, formatter: \u0026#34;{a} \u0026lt;br/\u0026gt;{b}: {c} ({d}%)\u0026#34;, }, series: [ { name: \u0026#34;分类统计\u0026#34;, type: \u0026#34;pie\u0026#34;, radius: [\u0026#34;30%\u0026#34;, \u0026#34;70%\u0026#34;], data: chartData, label: { show: true, position: \u0026#34;outside\u0026#34;, formatter: \u0026#34;{b}: {d}%\u0026#34;, }, }, ], }; } 视觉优化\n内外半径设计 标签自动布局 悬停动画效果 3.2.3 混合图表（阅读量分析） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 async function updateMixChart() { const option = { color: [\u0026#34;#3E82F7\u0026#34;, \u0026#34;#F86C6B\u0026#34;], tooltip: { trigger: \u0026#34;axis\u0026#34;, axisPointer: { type: \u0026#34;shadow\u0026#34;, }, }, legend: { data: [\u0026#34;文章数\u0026#34;, \u0026#34;阅读量\u0026#34;], }, grid: { top: \u0026#34;10%\u0026#34;, bottom: \u0026#34;25%\u0026#34;, right: \u0026#34;10%\u0026#34;, }, }; } 3.3 响应式布局实现 布局结构\n1 2 3 4 5 6 7 \u0026lt;div class=\u0026#34;dashboard-container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;top-section\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;mac-header\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;stats-grid\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- 图表容器 --\u0026gt; \u0026lt;/div\u0026gt; 样式优化\n1 2 3 4 5 6 7 .dashboard-container { display: flex; flex-direction: column; height: 100vh; padding: 15px; gap: 15px; } 3.4 数据更新机制 初始化流程\n1 2 3 4 5 6 7 function initCharts() { updateBarChart(); updatePieChart(); updateMixChart(); updateHeatmap(); updateArticleList(); } 数据刷新策略\n页面加载时初始化 用户交互触发更新 定时自动刷新 3.5 交互设计 图表联动\n点击饼图更新文章列表 柱状图分类筛选 数据钻取功能 视觉反馈\n1 2 3 4 5 .stat-card:hover, .chart-card:hover { transform: translateY(-5px); box-shadow: 0 6px 12px rgba(0, 0, 0, 0.08); } 3.6 性能优化 加载优化\n异步数据加载 图表按需渲染 防抖和节流处理 渲染优化\n合理的图表配置 数据预处理 动画性能调优 4. 后端实现逻辑流程 4.1 核心技术栈 Flask: Web 框架 Blueprint: 路由模块化 Cache: 文件系统缓存 SQLAlchemy: ORM 数据库操作 PyMySQL: MySQL 数据库驱动 4.2 系统架构 4.3 核心功能实现 4.3.1 应用初始化 1 2 3 4 5 6 7 8 9 app = Flask(__name__) # 配置缓存 cache.init_app(app, config={ \u0026#39;CACHE_TYPE\u0026#39;: \u0026#39;filesystem\u0026#39;, \u0026#39;CACHE_DIR\u0026#39;: \u0026#39;cache-directory\u0026#39; }) # 数据库配置 app.config[\u0026#39;SQLALCHEMY_DATABASE_URI\u0026#39;] = \u0026#39;mysql+pymysql://user:pass@host:port/db\u0026#39; db.init_app(app) 4.3.2 路由注册机制 1 2 3 4 5 6 7 8 9 DEFAULT_BLUEPRINT = [ (cs, \u0026#39;/\u0026#39;), # CSDN API蓝图 ] url_path_prefix = \u0026#34;/api\u0026#34; def config_blueprint(app): for blueprint, url_prefix in DEFAULT_BLUEPRINT: url_prefix = url_path_prefix + url_prefix app.register_blueprint(blueprint, url_prefix=url_prefix) 4.4 API 接口设计 4.4.1 数据统计接口 季度数据统计 1 2 3 4 5 6 7 8 9 10 11 @cs.route(\u0026#39;/quarter\u0026#39;) @cache.cached(timeout=60) def GetQuarter(): \u0026#34;\u0026#34;\u0026#34;获取每年每季度博客数量\u0026#34;\u0026#34;\u0026#34; year_quarter_count = defaultdict(lambda: defaultdict(int)) data = GetArticle() for article in data: year = article[\u0026#34;year\u0026#34;] quarter = article[\u0026#34;quarter\u0026#34;] year_quarter_count[year][quarter] += 1 return Result.success(result) 分类数据统计 1 2 3 4 5 6 7 8 9 10 @cs.route(\u0026#39;/categorize\u0026#39;) @cache.cached(timeout=60) def Pie(): \u0026#34;\u0026#34;\u0026#34;获取文章分类统计\u0026#34;\u0026#34;\u0026#34; categorize_data = Categorize.query.all() pie_data = [ {\u0026#34;value\u0026#34;: item.article_num, \u0026#34;name\u0026#34;: item.categorize} for item in categorize_data ] return Result.success(pie_data) 4.4.2 数据处理流程 4.5 性能优化策略 缓存优化\n使用文件系统缓存 60 秒缓存过期时间 针对高频访问接口启用缓存 数据处理优化\n使用 defaultdict 优化数据聚合 批量数据查询 查询 据预处理和转换 查询优化\nORM 延迟加载 查询结果缓存 合理的数据索引 4.6 数据流转流程 4.7 错误处理机制 全局异常处理\n1 2 3 4 5 try: # 业务逻辑 except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return Result.error(str(e)) 数据验证\n1 2 if not data or not data.labels: return Result.error(\u0026#34;Invalid data format\u0026#34;) 结果封装\n1 2 3 4 5 6 7 8 class Result: @staticmethod def success(data): return jsonify({\u0026#34;code\u0026#34;: 200, \u0026#34;data\u0026#34;: data}) @staticmethod def error(msg): return jsonify({\u0026#34;code\u0026#34;: 500, \u0026#34;msg\u0026#34;: msg}) 技术总结 1. 技术栈全景图 ","date":"2024-12-19T21:38:38Z","image":"https://www.ownit.top/title_pic/04.jpg","permalink":"https://www.ownit.top/p/202412192138/","title":"CSDN数据大屏可视化【开源】"},{"content":"简介 随着企业IT基础设施的不断扩大，Linux服务器的数量也日益增多，传统的单机日志管理方式已无法满足对日志数据集中管理、审计和分析的需求。尤其是在大型集群环境中，如何高效地收集、存储和分析日志成为了一项重要的技术挑战。\n背景 在实现对大型Linux集群的日志集中管理与审计，特别是针对rsyslog日志的收集、操作命令日志以及登录日志的审计。通过部署集中的rsyslog服务端，能够统一收集1300多台Linux服务器的系统日志，确保日志数据的集中化管理。使用Filebeat作为日志收集工具，将日志数据推送至Logstash进行清洗和转换，最终存储到Elasticsearch中，并通过Kibana实现实时数据可视化展示。这种方式不仅简化了日志管理流程，还提高了系统的监控效率和安全性。\n需求 统一管理 1300 台服务器的Linux系统日志，能够及时发现问题和告警。 解决方案 集中管理：通过统一的服务端收集所有Linux服务器的日志数据，减少单独配置每台服务器的工作量。 日志审计：对操作命令日志、登录日志等进行审计，确保系统行为的可追溯性。 数据清洗与分析：通过日志清洗与格式转换，确保日志数据的标准化，便于后续的分析和可视化展示。 实时展示：利用Kibana将清洗后的数据实时可视化，帮助运维人员快速发现潜在问题。 整体架构 为实现这些目标，我们设计了以下的系统架构：\nrsyslog：作为日志收集的核心组件，它负责将来自Linux系统的各种日志（包括/var/log/messages等）统一推送到中心化的日志服务端。 Filebeat：作为轻量级的日志收集器，部署在各个Linux节点上，负责将日志文件传输到Logstash。 Logstash：对收集到的日志进行清洗、解析和转换，确保数据符合预定格式，便于存入Elasticsearch。 Elasticsearch：存储经过清洗和转换的日志数据，提供强大的全文搜索和数据查询功能。 Kibana：通过Kibana仪表盘实时展示存储在Elasticsearch中的日志数据，帮助运维人员进行数据分析和可视化展示。 rsyslog汇总 rsyslog服务端 首先配置rsyslog服务器，可以统一收集集群内部的日志。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 加载本地系统日志模块（例如通过 logger 命令发送的日志） $ModLoad imuxsock # 加载内核日志模块（之前由 rklogd 处理） $ModLoad imklog # 加载 UDP 模块，支持通过 UDP 协议接收日志 $ModLoad imudp # 配置 UDP 服务器在 514 端口接收日志 $UDPServerRun 514 # 加载 TCP 模块，支持通过 TCP 协议接收日志 $ModLoad imtcp # 配置 TCP 服务器在 514 端口接收日志 $InputTCPServerRun 514 # 定义一个自定义的日志格式模板 \u0026#39;myFormat\u0026#39; $template myFormat,\u0026#34;%timestamp:::date-rfc3339% %fromhost-ip% %HOSTNAME% [%programname%] %syslogseverity-text%:%msg%\\n\u0026#34; # 设置默认的文件格式为传统的 rsyslog 格式 $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # 加载 /etc/rsyslog.d/ 目录下的所有配置文件 $IncludeConfig /etc/rsyslog.d/*.conf # 配置收集 info 级别的日志，排除 mail、authpriv、cron 类别的日志，输出到 /var/log/messages 文件，并使用 myFormat 格式 *.info;mail.none;authpriv.none;cron.none /var/log/messages;myFormat # 收集所有 authpriv 类别的日志（通常是认证相关的日志），输出到 /var/log/secure 文件，并使用 myFormat 格式 authpriv.* /var/log/secure;myFormat # 收集所有 mail 类别的日志，输出到 /var/log/maillog 文件，使用异步写入（- 表示异步） mail.* -/var/log/maillog # 收集所有 cron 类别的日志（定时任务日志），输出到 /var/log/cron 文件 cron.* /var/log/cron # 收集所有紧急级别（emerg）的日志，将其通过系统消息发送 *.emerg :omusrmsg:* # 收集 uucp 和 news 类别的严重级别（crit）日志，输出到 /var/log/spooler 文件 uucp,news.crit /var/log/spooler # 收集所有 local7 类别的日志，输出到 /var/log/boot.log 文件 local7.* /var/log/boot.log 重启服务端\n1 systemctl restart rsyslog 修改前： 修改后： rsyslog客户端 所有集群的客户端配置最后一行IP，就可以把数据汇总在一切\n1 2 3 [root@zabbix ~]# cat /etc/rsyslog.conf |tail -n 2 #authpriv.* @10.10.10.17 *.* @@192.168.102.20 # rsyslog 服务端的IP 重启\n1 systemctl restart rsyslog.service 日志已经打印到rsyslog 服务端的 /var/log/messages /var/log/secure 等文件\nfilebeaet收集 在rsyslog 服务端的安装filebeaet，并且使用如下配置启动\n1 2 3 4 5 6 7 8 9 10 11 12 filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false filebeat.inputs: - type: log enabled: true tail_files: true paths: - /var/log/messages output.logstash: hosts: [\u0026#34;192.168.1.100:5514\u0026#34;] # 把日志发送到logstatsh中 logstatsh配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [root@game logstash]# cat config/rsyslog.conf input { beats { port =\u0026gt; 5514 type =\u0026gt; syslog } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{TIMESTAMP_ISO8601:time} %{IP:client_ip} %{HOSTNAME:host_name} \\[%{DATA:type}\\] %{GREEDYDATA:info}\u0026#34; } overwrite =\u0026gt; [\u0026#34;message\u0026#34;] } mutate { split =\u0026gt; [\u0026#34;type\u0026#34;,\u0026#34;,\u0026#34;] } mutate{ add_field =\u0026gt; { \u0026#34;types\u0026#34; =\u0026gt; \u0026#34;%{[type][1]}\u0026#34; } } mutate{remove_field =\u0026gt; [ \u0026#34;tags\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;host\u0026#34;,\u0026#34;log\u0026#34;,\u0026#34;ecs\u0026#34;,\u0026#34;type\u0026#34; ]} date { match =\u0026gt; [\u0026#34;time\u0026#34;, \u0026#34;yyyy-MM-dd HH:mm:ss,SSS\u0026#34;, \u0026#34;UNIX\u0026#34;] target =\u0026gt; \u0026#34;@timestamp\u0026#34; locale =\u0026gt; \u0026#34;cn\u0026#34; } } output { stdout { codec=\u0026gt; rubydebug } elasticsearch { hosts =\u0026gt; [\u0026#34;127.0.0.1:9200\u0026#34;] index =\u0026gt; \u0026#34;message\u0026#34; } } 数据格式\n1 2024-12-03T18:35:30+08:00 192.168.102.30 master01 [kubelet] info: E1203 18:35:30.827608 1065 summary_sys_containers.go:83] \u0026#34;Failed to get system container stats\u0026#34; err=\u0026#34;failed to get cgroup stats for \\\u0026#34;/system.slice/docker.service\\\u0026#34;: failed to get container info for \\\u0026#34;/system.slice/docker.service\\\u0026#34;: unknown container \\\u0026#34;/system.slice/docker.service\\\u0026#34;\u0026#34; containerName=\u0026#34;/system.slice/docker.service\u0026#34; GROK解析\n1 %{TIMESTAMP_ISO8601:time} %{IP:client_ip} %{HOSTNAME:host_name} \\[%{DATA:type}\\] %{GREEDYDATA:info} 输出格式\n1 2 3 4 5 6 7 { \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.102.30\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2024-12-03T18:35:30+08:00\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;kubelet\u0026#34;, \u0026#34;host_name\u0026#34;: \u0026#34;master01\u0026#34;, \u0026#34;info\u0026#34;: \u0026#34;info: E1203 18:35:30.827608 1065 summary_sys_containers.go:83] \\\u0026#34;Failed to get system container stats\\\u0026#34; err=\\\u0026#34;failed to get cgroup stats for \\\\\\\u0026#34;/system.slice/docker.service\\\\\\\u0026#34;: failed to get container info for \\\\\\\u0026#34;/system.slice/docker.service\\\\\\\u0026#34;: unknown container \\\\\\\u0026#34;/system.slice/docker.service\\\\\\\u0026#34;\\\u0026#34; containerName=\\\u0026#34;/system.slice/docker.service\\\u0026#34;\\r\u0026#34; } 操作系统命令审计 Linux一般都是终端执行命令，我们可以让命令写到message日志上，在通过过滤，获取操作命令记录。\nLinux终端配置 1 2 3 4 5 6 7 8 [root@zabbix ~]# cat /etc/profile | tail -n 2 unset MAILCHECK export PROMPT_COMMAND=\u0026#39;{ msg=$(history 1 | { read x y; echo $y; });logger -p local2.info \u0026#34;euid=$(whoami)\u0026#34; $(who am i) `pwd` \u0026#34;$msg\u0026#34;; }\u0026#39; 日志格式 Oct 11 17:32:41 zabbix root: euid=root root pts/0 2021-10-11 15:13 (10.10.10.3) /root cat /etc/profile Oct 11 17:32:47 zabbix root: euid=root root pts/0 2021-10-11 15:13 (10.10.10.3) /root cat /etc/profile | tail -n 2 rsyslog服务端展示 filebeat配置 1 2 3 4 5 6 7 8 [root@logserver01 filebeat]# cat system_messages.yml #=========================== Filebeat inputs ============================= filebeat.inputs: - type: log enabled: true tail_files: true paths: - /var/log/messages logstatsh配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 [root@logserver01 config]# cat system_userlog_FromKafkaInES.conf input{ beats { host =\u0026gt; \u0026#39;172.17.9.200\u0026#39; port =\u0026gt; 5046 } #\tkafka{ #\tbootstrap_servers =\u0026gt; [\u0026#34;172.17.8.232:6667\u0026#34;] #\ttopics =\u0026gt; [\u0026#34;sys_os_exe\u0026#34;] #\tcodec =\u0026gt; \u0026#34;json\u0026#34; #\tgroup_id =\u0026gt; \u0026#34;ELK_SYSTEM_EXE_GROUP\u0026#34; #\tconsumer_threads =\u0026gt; 3 #\tclient_id =\u0026gt; \u0026#34;logstash\u0026#34; #\tdecorate_events =\u0026gt; false #\tauto_offset_reset =\u0026gt; \u0026#34;earliest\u0026#34; #\trequest_timeout_ms =\u0026gt; \u0026#34;300000\u0026#34; #\tsession_timeout_ms =\u0026gt; \u0026#34;20000\u0026#34; #\tmax_poll_interval_ms =\u0026gt; \u0026#34;600000\u0026#34; #\t} } filter{ if ([message] =~ \u0026#34;euid\u0026#34;){ grok{ match =\u0026gt; {\u0026#34;message\u0026#34; =\u0026gt; \u0026#39;^(?\u0026lt;exetime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s)(?\u0026lt;deshost\u0026gt;[^ ]+)(?:\\s)(?\u0026lt;name\u0026gt;[^ ]+)(?:\\s\\[)(?\u0026lt;loginuser\u0026gt;[^ |\\]]*)(?:\\]\\s[^ ]+\\seuid=)(?\u0026lt;exeuser\u0026gt;[^ ]+)(?:\\s+)(?\u0026lt;userinfo\u0026gt;[^\\(]+)(?:\\s\\()(?\u0026lt;srchost\u0026gt;[^\\)]+)(?:\\)\\s)(?\u0026lt;exepath\u0026gt;[^ ]+)(\\s+)(?\u0026lt;exeinfo\u0026gt;.*)\u0026#39;} } if \u0026#34;_grokparsefailure\u0026#34; in [tags] { drop { } } mutate{ add_field =\u0026gt; [\u0026#34;tmp_exeinfo\u0026#34;,\u0026#34;%{exeinfo}\u0026#34;] } mutate{ split =\u0026gt; [\u0026#34;exetime\u0026#34;,\u0026#34;-\u0026#34;] split =\u0026gt; [\u0026#34;tmp_exeinfo\u0026#34;,\u0026#34; \u0026#34;] } mutate{ add_field =\u0026gt; [\u0026#34;indextime\u0026#34;,\u0026#34;%{[exetime][0]}%{[exetime][1]}\u0026#34;] add_field =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;%{[exetime][0]}-%{[exetime][1]}-%{[exetime][2]} %{hhmmss}\u0026#34;] add_field =\u0026gt; [\u0026#34;cmd\u0026#34;,\u0026#34;%{[tmp_exeinfo][0]}\u0026#34;] } #Retention log insertion time to ES.............. ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;inserttime\u0026#39;, event.get(\u0026#39;@timestamp\u0026#39;).time.to_i)\u0026#34; } #replace InsertTime with evtTime \u0026#34;yyyy-MM-dd HH:mm:ss eg:2020-06-29 09:24:29\u0026#34; date{ match =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] #kibana use this time.................... target =\u0026gt; \u0026#34;@timestamp\u0026#34; } mutate{replace =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;%{evtTime} +0800\u0026#34;]} date{ match =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss +0800\u0026#34;] timezone =\u0026gt;\u0026#34;UTC\u0026#34; #log event time timestamp................ target =\u0026gt; \u0026#34;logtimestamp\u0026#34; } #log event time long string...................... ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;longtime\u0026#39;, event.get(\u0026#39;logtimestamp\u0026#39;).time.to_i)\u0026#34; } mutate{remove_field =\u0026gt; [ \u0026#34;tmp_exeinfo\u0026#34;,\u0026#34;evtTime\u0026#34;,\u0026#34;host\u0026#34;,\u0026#34;ecs\u0026#34;,\u0026#34;log\u0026#34;,\u0026#34;hhmmss\u0026#34;,\u0026#34;input\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;exetime\u0026#34; ]} } else{ drop{} } } output{ stdout{codec =\u0026gt; rubydebug} if [indextime] !~ \u0026#34;index\u0026#34;{ elasticsearch{ hosts =\u0026gt; [\u0026#34;http://172.17.9.176:9200\u0026#34;] #hosts =\u0026gt; \u0026#34;172.17.9.176\u0026#34; index =\u0026gt; \u0026#34;sys_os_userlog_%{[indextime]}\u0026#34; user =\u0026gt; \u0026#34;*********\u0026#34; password =\u0026gt; \u0026#34;*********\u0026#34; } } } 数据格式\n1 2024-12-03T18:39:05+08:00 192.168.102.30 master01 [root] info: euid=root root pts/0 2024-12-03 18:21 (192.168.96.19) /root [2024-12-03 18:39:05]ip a GROK解析\n1 ^(?\u0026lt;exetime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s)(?\u0026lt;deshost\u0026gt;[^ ]+)(?:\\s)(?\u0026lt;name\u0026gt;[^ ]+)(?:\\s\\[)(?\u0026lt;loginuser\u0026gt;[^ |\\]]*)(?:\\]\\s[^ ]+\\seuid=)(?\u0026lt;exeuser\u0026gt;[^ ]+)(?:\\s+)(?\u0026lt;userinfo\u0026gt;[^\\(]+)(?:\\s\\()(?\u0026lt;srchost\u0026gt;[^\\)]+)(?:\\)\\s)(?\u0026lt;exepath\u0026gt;[^ ]+)(\\s+)(?\u0026lt;exeinfo\u0026gt;.*) 输出格式\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;loginuser\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;hhmmss\u0026#34;: \u0026#34;18:39:05\u0026#34;, \u0026#34;exepath\u0026#34;: \u0026#34;/root\u0026#34;, \u0026#34;deshost\u0026#34;: \u0026#34;192.168.102.30\u0026#34;, \u0026#34;srchost\u0026#34;: \u0026#34;192.168.96.19\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;master01\u0026#34;, \u0026#34;exeinfo\u0026#34;: \u0026#34;[2024-12-03 18:39:05]ip a\\r\u0026#34;, \u0026#34;exeuser\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;userinfo\u0026#34;: \u0026#34;root pts/0 2024-12-03 18:21\u0026#34;, \u0026#34;exetime\u0026#34;: \u0026#34;2024-12-03\u0026#34; } 系统用户登录审计 filebeat配置 1 2 3 4 5 6 7 8 9 10 11 [root@logserver01 filebeat]# cat system_secure.yml #=========================== Filebeat inputs ============================= filebeat.inputs: - type: log enabled: true tail_files: true paths: - /var/log/secure #=========================== Filebeat outppp_id: messuts ============================= output.logstash: hosts: [\u0026#34;172.17.9.200:5045\u0026#34;] logstatsh配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 [root@logserver01 config]# cat system_login_FromKafkaInES.conf input{ beats { host =\u0026gt; \u0026#39;172.17.9.200\u0026#39; port =\u0026gt; 5045 } # #\tkafka{ #\tbootstrap_servers =\u0026gt; [\u0026#34;172.17.8.232:6667\u0026#34;] #\ttopics =\u0026gt; [\u0026#34;sys_os_login\u0026#34;] #\tcodec =\u0026gt; \u0026#34;json\u0026#34; #\tgroup_id =\u0026gt; \u0026#34;ELK_SYSTEM_LOGIN_GROUP\u0026#34; #\tconsumer_threads =\u0026gt; 3 #\tclient_id =\u0026gt; \u0026#34;logstash\u0026#34; #\tdecorate_events =\u0026gt; false #\tauto_offset_reset =\u0026gt; \u0026#34;earliest\u0026#34; #\trequest_timeout_ms =\u0026gt; \u0026#34;300000\u0026#34; #\tsession_timeout_ms =\u0026gt; \u0026#34;20000\u0026#34; #\tmax_poll_interval_ms =\u0026gt; \u0026#34;600000\u0026#34; #\t} } filter{ #login successed log if ([message] =~ \u0026#34;Accepted\u0026#34;){ grok{ match =\u0026gt; {\u0026#34;message\u0026#34; =\u0026gt; \u0026#39;^(?\u0026lt;atime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s+)(?\u0026lt;deshost\u0026gt;\\d+\\.\\d+\\.\\d+\\.\\d+)(?:\\s)(?\u0026lt;name\u0026gt;[^ ]+)(?:[\\S\\s]*Failed\\spassword\\sfor[\\sinvalid\\suser]*\\s)(?\u0026lt;loginuser\u0026gt;[^ ]+)(?:\\sfrom\\s)(?\u0026lt;srchost\u0026gt;[\\d.]+)(?:\\s\\w+\\s\\d+\\s)(?\u0026lt;loginmode\u0026gt;\\w*)\u0026#39;} } if \u0026#34;_grokparsefailure\u0026#34; in [tags] { drop { } } mutate{ add_field =\u0026gt; [\u0026#34;type\u0026#34;,\u0026#34;systemlogin\u0026#34;] split =\u0026gt; [\u0026#34;atime\u0026#34;,\u0026#34;-\u0026#34;] } mutate{ add_field =\u0026gt; [\u0026#34;indextime\u0026#34;,\u0026#34;%{[atime][0]}%{[atime][1]}\u0026#34;] add_field =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;%{[atime][0]}-%{[atime][1]}-%{[atime][2]} %{hhmmss}\u0026#34;] } #Retention log insertion time to ES.............. ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;inserttime\u0026#39;, event.get(\u0026#39;@timestamp\u0026#39;).time.to_i)\u0026#34; } #replace InsertTime with evtTime \u0026#34;yyyy-MM-dd HH:mm:ss eg:2020-06-29 09:24:29\u0026#34; date{ match =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] #kibana use this time.................... target =\u0026gt; \u0026#34;@timestamp\u0026#34; } mutate{replace =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;%{evtTime} +0800\u0026#34;]} date{ match =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss +0800\u0026#34;] timezone =\u0026gt;\u0026#34;UTC\u0026#34; #log event time timestamp................ target =\u0026gt; \u0026#34;logtimestamp\u0026#34; } #log event time long string...................... ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;longtime\u0026#39;, event.get(\u0026#39;logtimestamp\u0026#39;).time.to_i)\u0026#34; } mutate{remove_field =\u0026gt; [ \u0026#34;evtTime\u0026#34;,\u0026#34;host\u0026#34;,\u0026#34;ecs\u0026#34;,\u0026#34;log\u0026#34;,\u0026#34;hhmmss\u0026#34;,\u0026#34;input\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;atime\u0026#34; ]} } #login failed log else if ([message] =~ \u0026#34;Failed password for\u0026#34;){ grok{ match =\u0026gt; {\u0026#34;message\u0026#34; =\u0026gt; \u0026#39;^(?\u0026lt;atime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s+)(?\u0026lt;deshost\u0026gt;\\d+\\.\\d+\\.\\d+\\.\\d+)(?:[\\S\\s]*Failed\\spassword\\sfor[\\sinvalid\\suser]*\\s)(?\u0026lt;loginuser\u0026gt;[^ ]+)(?:\\sfrom\\s)(?\u0026lt;srchost\u0026gt;[\\d.]+)(?:\\s\\w+\\s\\d+\\s)(?\u0026lt;loginmode\u0026gt;\\w*)\u0026#39;} } if \u0026#34;_grokparsefailure\u0026#34; in [tags] { drop { } } mutate{ add_field =\u0026gt; [\u0026#34;type\u0026#34;,\u0026#34;systemloginfailed\u0026#34;] split =\u0026gt; [\u0026#34;atime\u0026#34;,\u0026#34;-\u0026#34;] } mutate{ add_field =\u0026gt; [\u0026#34;indextime\u0026#34;,\u0026#34;%{[atime][0]}%{[atime][1]}\u0026#34;] add_field =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;%{[atime][0]}-%{[atime][1]}-%{[atime][2]} %{hhmmss}\u0026#34;] } #Retention log insertion time to ES.............. ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;inserttime\u0026#39;, event.get(\u0026#39;@timestamp\u0026#39;).time.to_i)\u0026#34; } #replace InsertTime with evtTime \u0026#34;yyyy-MM-dd HH:mm:ss eg:2020-06-29 09:24:29\u0026#34; date{ match =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] #kibana use this time.................... target =\u0026gt; \u0026#34;@timestamp\u0026#34; } mutate{replace =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;%{evtTime} +0800\u0026#34;]} date{ match =\u0026gt; [\u0026#34;evtTime\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss +0800\u0026#34;] timezone =\u0026gt;\u0026#34;UTC\u0026#34; #log event time timestamp................ target =\u0026gt; \u0026#34;logtimestamp\u0026#34; } #log event time long string...................... ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;longtime\u0026#39;, event.get(\u0026#39;logtimestamp\u0026#39;).time.to_i)\u0026#34; } mutate{remove_field =\u0026gt; [ \u0026#34;evtTime\u0026#34;,\u0026#34;host\u0026#34;,\u0026#34;ecs\u0026#34;,\u0026#34;log\u0026#34;,\u0026#34;hhmmss\u0026#34;,\u0026#34;input\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;atime\u0026#34; ]} } #other log else{ drop{} } } output{ if [type] == \u0026#34;systemlogin\u0026#34;{ if [indextime] !~ \u0026#34;index\u0026#34;{ stdout{codec =\u0026gt; rubydebug} elasticsearch{ hosts =\u0026gt; \u0026#34;172.17.9.176\u0026#34; index =\u0026gt; \u0026#34;sys_os_systemlogin_%{[indextime]}\u0026#34; user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;f5OPbv6sqfstmc+\u0026#34; } } } else if [type] == \u0026#34;systemloginfailed\u0026#34;{ if [indextime] !~ \u0026#34;index\u0026#34;{ stdout{codec =\u0026gt; rubydebug} elasticsearch{ hosts =\u0026gt; \u0026#34;172.17.9.176\u0026#34; index =\u0026gt; \u0026#34;sys_os_systemloginfailed_%{[indextime]}\u0026#34; user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;xxxxxxx+\u0026#34; } } } } 成功日志解析\n日志格式\n1 2024-12-03T18:45:09+08:00 192.168.102.42 node03 [sshd] info: Accepted password for root from 192.168.96.19 port 14347 ssh2 GROK语法\n1 %{TIMESTAMP_ISO8601:atime} %{IP:deshost} %{HOSTNAME:name} \\[%{WORD:type}\\] %{DATA:loglevel}: Accepted password for %{WORD:loginuser} from %{IP:srchost} port %{NUMBER:port} %{WORD:loginmode} 日志格式\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;loginuser\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;atime\u0026#34;: \u0026#34;2024-12-03T18:45:09+08:00\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;sshd\u0026#34;, \u0026#34;deshost\u0026#34;: \u0026#34;192.168.102.42\u0026#34;, \u0026#34;srchost\u0026#34;: \u0026#34;192.168.96.19\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;14347\u0026#34;, \u0026#34;loglevel\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;node03\u0026#34;, \u0026#34;loginmode\u0026#34;: \u0026#34;ssh2\u0026#34; } 失败日志格式\n日志格式\n1 2024-12-03T19:01:57+08:00 192.168.102.42 node03 [sshd] info: Failed password for invalid user 123 from 192.168.96.19 port 12103 ssh2 GROK语法\n1 ^(?\u0026lt;atime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s+)(?\u0026lt;deshost\u0026gt;\\d+\\.\\d+\\.\\d+\\.\\d+)(?:[\\S\\s]*Failed\\spassword\\sfor[\\sinvalid\\suser]*\\s)(?\u0026lt;loginuser\u0026gt;[^ ]+)(?:\\sfrom\\s)(?\u0026lt;srchost\u0026gt;[\\d.]+)(?:\\s\\w+\\s\\d+\\s)(?\u0026lt;loginmode\u0026gt;\\w*) 日志输出\n1 2 3 4 5 6 7 8 { \u0026#34;loginuser\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;atime\u0026#34;: \u0026#34;2024-12-03\u0026#34;, \u0026#34;hhmmss\u0026#34;: \u0026#34;19:01:57\u0026#34;, \u0026#34;deshost\u0026#34;: \u0026#34;192.168.102.42\u0026#34;, \u0026#34;srchost\u0026#34;: \u0026#34;192.168.96.19\u0026#34;, \u0026#34;loginmode\u0026#34;: \u0026#34;ssh2\u0026#34; } ","date":"2024-12-19T15:27:19Z","image":"https://www.ownit.top/title_pic/47.jpg","permalink":"https://www.ownit.top/p/202412191527/","title":"集中管理与实时审计：构建Linux集群（1300台服务器）日志平台的最佳实践"},{"content":"在现代 IT 系统中，监控体系是确保高可用性、高性能和稳定性的核心工具。一个完善的监控体系能够及时发现系统问题、分析问题根源并快速采取应对措施，避免故障进一步扩散。本文将从基础设施层、中间件层、容器与编排层、应用与服务层逐步展开，全面介绍如何构建生产环境的监控体系。 1. 基础设施层监控 基础设施是支撑整个 IT 系统运行的根基，对其进行有效的监控，可以及时发现并解决问题，确保整个系统的稳定性和可靠性。\n关键监控指标： CPU： 监控 CPU 使用率，及时发现过高使用情况。 分析可能的原因，如无效循环、死锁等。 工具建议：使用 Prometheus + Node Exporter。 内存： 监控内存使用情况，避免内存泄露导致服务崩溃。 设置阈值告警，提前预警。 网络： 监控网络流量和连接状态，确保网络通畅。 及时发现并处理网络拥堵或攻击事件。 工具建议：使用 cAdvisor、Ntop。 硬盘： 监控硬盘使用率和 I/O 性能，避免磁盘空间不足或 I/O 瓶颈。 工具建议：Prometheus Disk Exporter。 2. 中间件层监控 中间件是应用与底层基础设施之间的桥梁，其性能直接影响上层应用的响应速度和稳定性。\n常见中间件的监控策略： Nginx： 监控请求处理时间、并发连接数、5xx 错误率等指标。 工具建议：Nginx 模块 + Prometheus。 MySQL： 监控数据库响应时间、查询效率、连接数。 设置慢查询日志分析性能瓶颈。 工具建议：Percona Monitoring Plugins 或 Prometheus MySQL Exporter。 RabbitMQ： 监控消息队列长度、处理速度、消费者状态。 工具建议：RabbitMQ 管理插件。 Consul： 监控服务发现与配置的健康状态。 工具建议：Consul 内置监控 API + Prometheus。 Kafka + Zookeeper： 监控 Kafka 消息流量、延迟和消费者组状态。 监控 Zookeeper 的节点状态。 工具建议：Kafka Exporter + Zookeeper Exporter。 3. 容器与编排层监控 容器化和自动化编排是现代云原生应用的标配，对其进行监控可以确保服务的灵活性和可扩展性。\n容器与编排层监控的重点： Kubernetes 集群： 监控集群的资源使用情况、节点健康状态和服务部署状态。 工具建议：kube-state-metrics + Prometheus。 Kubernetes 事件监控： 监控事件日志，及时响应 Pod 的异常状态和调度失败。 Docker 容器监控： 监控容器的运行状态、资源使用情况，确保容器的稳定运行。 工具建议：cAdvisor、Prometheus Docker Exporter。 4. 应用与服务层监控 应用与服务层是与用户直接交互的层面，其性能和稳定性直接影响用户体验。\n监控关键点： 服务应用进程： 监控应用进程的健康状态，包括内存泄露、死锁等问题。 业务链路追踪： 使用分布式链路追踪工具（如 Pinpoint、SkyWalking 或阿里云 ARMS）追踪服务调用链路。 分析服务间调用的延迟，优化性能。 业务日志监控： 使用 Elasticsearch、Logstash 和 Kibana (ELK Stack) 分析业务日志。 在资源有限（如磁盘空间 200G）时，可结合阿里云 SLS。 业务接口响应时间监控： 监控接口的响应时间，确保快速响应用户请求。 工具建议：SkyWalking 或 Prometheus。 调用失败次数监控： 监控服务调用失败次数，分析失败原因并快速修复。 5. 告警平台建设 告警策略： 多渠道通知： 集成钉钉、邮件、电话、短信、微信等多种通知方式。 工具选择： 开源自建：Alertmanager、PrometheusAlert。 商业方案：阿里云告警平台。 关键配置： 定义告警规则（如 CPU 使用率超 90%、接口响应时间超过 1 秒）。 配置分级告警策略，根据问题严重性选择通知方式。 6. 监控可视化建设 可视化的重要性： 监控可视化是监控体系中的重要组成部分，它可以将复杂的数据以图形化的方式直观展示，帮助运维和开发人员快速理解系统状态。\n工具选择： Grafana： 支持多种数据源（如 Prometheus）。 提供丰富的图表类型（折线图、柱状图、饼图等）。 Nightingale： 汇总各个平台的监控数据，集中展示。 总结 一个完善的生产监控体系需要涵盖基础设施、中间件、容器与编排、应用与服务等多个层面，并辅以告警和可视化工具来提升监控效果。通过合理的监控部署和持续优化，能够显著提升系统的可靠性、性能和运维效率，最终为业务保驾护航。\n","date":"2024-12-18T22:18:28Z","image":"https://www.ownit.top/title_pic/16.jpg","permalink":"https://www.ownit.top/p/202412182218/","title":"构建全面的生产监控体系：从基础设施到业务服务"},{"content":"简介 在构建 Web 应用时，处理和传输大量数据是不可避免的。对于需要高效、可扩展的消息处理和异步任务执行的场景，使用 RabbitMQ（一种流行的消息队列中间件）与 Flask（一个轻量级的 Python Web 框架）结合，能够大大提升应用的性能和可靠性。本文将带你通过一个基于 Flask 和 RabbitMQ 的实际应用案例，深入了解如何构建一个高效的消息队列系统，完成从生成假数据到消费数据的全过程。\n背景 我们要开发一个移动可视化平台监控系统，并且这些信息需要被实时分析或存储。面对这样的需求，直接将所有逻辑放在单个应用中可能会导致性能瓶颈。因此，我们考虑采用微服务架构，通过分离数据生成与处理逻辑来提高系统的可扩展性和响应速度。\n环境介绍 为了实现这个项目，我们需要以下环境：\nPython：一个强大的编程语言，适合快速开发。 Flask：一个轻量级的Web应用框架。 Pika：Python的RabbitMQ客户端库。 Faker：一个生成伪数据的Python库，用于生成测试数据 RabbitMQ：消息队列服务，用来存储 生产者产生的数据 技术选型 Flask：轻量级Web框架，非常适合快速开发小型到中等规模的应用。 RabbitMQ：一个广泛使用的开源消息代理软件（也称为消息中间件），用于实现应用程序之间的通信。 系统架构概览 生产者：负责生成模拟用户数据并将其发送至RabbitMQ。 消费者：从RabbitMQ接收数据后执行特定任务，如数据分析或存储。 Flask应用：提供REST API接口给外部调用，同时启动消费者线程监听RabbitMQ中的消息。 搭建RabbitMQ服务 我们使用docker来搭建服务，如果win可以直接跑程序，相关流程请自行查询\n临时使用（停止会自动删除服务）\n1 docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 docker.cloudimages.asia/rabbitmq:4.0-management 长久使用\n1 docker run -it -d --name rabbitmq -p 5672:5672 -p 15672:15672 docker.cloudimages.asia/rabbitmq:4.0-management 1 2 [root@prometheus-server ~]# docker ps | grep 9b3a9355fa4a 9b3a9355fa4a docker.cloudimages.asia/rabbitmq:4.0-management \u0026#34;docker-entrypoint.s…\u0026#34; 21 seconds ago Up 19 seconds 4369/tcp, 5671/tcp, 0.0.0.0:5672-\u0026gt;5672/tcp, :::5672-\u0026gt;5672/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:15672-\u0026gt;15672/tcp, :::15672-\u0026gt;15672/tcp rabbitmq 访问页面地址：http://192.168.82.105:15672/ 使用 RabbitMQ 的管理界面。 访问账号和密码： guest | guest\n队列页面 生产者：将数据发送到 RabbitMQ 队列 生产者的任务是生成一些假数据，并将这些数据发送到 RabbitMQ 队列中。我们使用 Faker 库生成数据，并通过 RabbitMQ 的 basic_publish 方法发送消息。\n确保你的环境中安装了Python。然后，使用pip安装Flask、Pika和Faker：\n1 pip install flask pika faker 生产者代码 生产者部分主要负责生成随机数据并通过RabbitMQ发送出去。这里我们使用Faker库来生成看起来真实的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # -*- coding: utf-8 -*- # @Time : 2024/11/24 10:20 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : test.py # @Software: PyCharm from faker import Faker import pika import json import time # 初始化 Faker 实例 fake = Faker() # 配置 RabbitMQ 连接 connection = pika.BlockingConnection(pika.ConnectionParameters(host=\u0026#39;192.168.82.105\u0026#39;,heartbeat=60)) channel = connection.channel() # 声明一个队列 queue_name = \u0026#39;ownit_queue\u0026#39; channel.queue_declare(queue=queue_name) # 生成并发送假数据 def generate_fake_data(): return { \u0026#34;name\u0026#34;: fake.name(), \u0026#34;address\u0026#34;: fake.address(), \u0026#34;email\u0026#34;: fake.email(), \u0026#34;phone\u0026#34;: fake.phone_number(), \u0026#34;company\u0026#34;: fake.company(), \u0026#34;date\u0026#34;: fake.date_this_year().isoformat(), \u0026#34;text\u0026#34;: fake.text(max_nb_chars=200), } try: for _ in range(10000): # 生成 1000 条假数据 fake_data = generate_fake_data() channel.basic_publish( exchange=\u0026#39;\u0026#39;, routing_key=queue_name, body=json.dumps(fake_data) # 将数据序列化为 JSON 格式 ) # time.sleep(0.1) print(f\u0026#34;Sent: {fake_data}\u0026#34;) finally: connection.close() 执行插入10000条数据 数据持久化 重启docker 服务。让mq重启\n1 docker restart 9b3a9355fa4a 因为重启，队列没有持久化，导致数据丢失 为了确保消息不会丢失，可以配置 RabbitMQ 的队列为持久化队列，即使 RabbitMQ 宕机或重启，队列中的消息也能被恢复。\n1 2 3 4 5 channel.queue_declare(queue=\u0026#39;ownit_queue\u0026#39;, durable=True) properties=pika.BasicProperties(delivery_mode=2) 咦，还是没有数据？为什么？ 因为使用Dokcer启动没有持久数据，重启会丢失数据，就算我们mq做持久化也不起作用。 1 docker stop 9b3a9355fa4a \u0026amp;\u0026amp; docker rm 9b3a9355fa4a 持久化命令\n1 2 3 4 5 6 docker run -it -d \\ --name rabbitmq \\ -p 5672:5672 \\ -p 15672:15672 \\ -v rabbitmq_data:/var/lib/rabbitmq/mnesia \\ docker.cloudimages.asia/rabbitmq:4.0-management 解释： -it -d：以交互模式启动并在后台运行容器。 --name rabbitmq：给容器指定一个名字 rabbitmq。 -p 5672:5672：映射 RabbitMQ 的默认 AMQP 协议端口（5672）到宿主机。 -p 15672:15672：映射 RabbitMQ 的管理界面端口（15672）到宿主机。 -v rabbitmq_data:/var/lib/rabbitmq/mnesia：将宿主机的 Docker 卷 rabbitmq_data 持久化到容器内的 /var/lib/rabbitmq/mnesia 目录，这是 RabbitMQ 默认存储队列和消息数据的地方。 使用宿主机目录 1 2 3 4 5 6 docker run -it -d \\ --name rabbitmq \\ -p 5672:5672 \\ -p 15672:15672 \\ -v /root/rabbitmq:/var/lib/rabbitmq \\ docker.cloudimages.asia/rabbitmq:4.0-management 1 2 [root@prometheus-server ~]# docker restart 9e200cf168c3 9e200cf168c3 关键点解析 Faker 库：用于生成虚拟数据。每次调用 generate_fake_data 函数时都会生成不同的姓名、地址、邮箱等信息。 RabbitMQ 连接：我们使用 pika 库与 RabbitMQ 进行连接，并声明了一个队列 ownit_queue，用于存储消息。 数据发布：使用 channel.basic_publish() 方法将消息发布到指定的队列中，消息体使用 json.dumps() 序列化为 JSON 格式。 持久化完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # -*- coding: utf-8 -*- # @Time : 2024/11/24 10:20 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : test.py # @Software: PyCharm from faker import Faker import pika import json import time # 初始化 Faker 实例 fake = Faker() # 配置 RabbitMQ 连接 connection = pika.BlockingConnection(pika.ConnectionParameters(host=\u0026#39;192.168.82.105\u0026#39;,heartbeat=60)) channel = connection.channel() # 声明一个队列 queue_name = \u0026#39;ownit_queue\u0026#39; channel.queue_declare(queue=queue_name, durable=True) # 生成并发送假数据 def generate_fake_data(): return { \u0026#34;name\u0026#34;: fake.name(), \u0026#34;address\u0026#34;: fake.address(), \u0026#34;email\u0026#34;: fake.email(), \u0026#34;phone\u0026#34;: fake.phone_number(), \u0026#34;company\u0026#34;: fake.company(), \u0026#34;date\u0026#34;: fake.date_this_year().isoformat(), \u0026#34;text\u0026#34;: fake.text(max_nb_chars=200), } try: for _ in range(1000): # 生成 1000 条假数据 fake_data = generate_fake_data() channel.basic_publish( exchange=\u0026#39;\u0026#39;, routing_key=queue_name, body=json.dumps(fake_data), # 将数据序列化为 JSON 格式 properties=pika.BasicProperties(delivery_mode=2) ) # time.sleep(0.1) print(f\u0026#34;Sent: {fake_data}\u0026#34;) finally: connection.close() 消费者：从 RabbitMQ 中获取数据 消费者部分由Flask应用托管，它不仅提供了API接口，还启动了一个后台线程持续监听RabbitMQ上的消息。 消费者的任务是从队列中读取消息，并进行处理。在这个例子中，我们将模拟一个简单的消息消费过程，打印接收到的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import json import threading import time import pika from flask import Flask, request app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def send_order(): return \u0026#39;Hello, World! MQ\u0026#39; # 消费者函数 def consume(): # 创建与 RabbitMQ 服务器的连接 connection = pika.BlockingConnection(pika.ConnectionParameters(\u0026#39;192.168.82.105\u0026#39;)) # 从连接中创建一个通道 channel = connection.channel() # 声明一个名为 order_queue 的队列，如果队列不存在则创建它 channel.queue_declare(queue=\u0026#39;ownit_queue\u0026#39;,durable=True) # 定义一个回调函数，用于处理接收到的消息 def callback(ch, method, properties, body): # 打印接收到的消息体 print(f\u0026#34;数据接受: {body}\u0026#34;) time.sleep(1) # 配置通道以消费来自 order_queue 的消息，指定回调函数处理消息，并设置自动确认消息 channel.basic_consume(queue=\u0026#39;ownit_queue\u0026#39;, on_message_callback=callback, auto_ack=True) # 打印消息表示程序正在等待接收消息，并提示用户按 CTRL+C 退出 print(\u0026#39;Waiting for messages. To exit press CTRL+C\u0026#39;) # 开始一个循环以持续接收消息 channel.start_consuming() # 启动消费者线程 def run_consumer(): thread = threading.Thread(target=consume, daemon=True) # 设置守护线程 thread.start() print(\u0026#34;Consumer thread started.\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: run_consumer() app.run(debug=True) 关键点解析 consume() 函数：通过 pika 连接 RabbitMQ，声明 ownit_queue 队列，并通过回调函数 callback 处理接收到的消息。 线程化消费：为了使 Flask 应用能够正常处理 Web 请求，同时也能处理消息队列中的消息，我们将消息消费部分放在一个单独的线程中运行。 auto_ack=True：自动确认消息，表示一旦消费者接收到消息后会自动从队列中删除该消息。 持续发送数据 每秒接收1条数据 没消费的数据一直在MQ中 总结 结合 Flask 和 RabbitMQ 构建一个高效的消息队列系统，从假数据的生成到数据的消费处理，整个过程都得到了详细展示。在实现过程中，我们主要涵盖了以下内容：\nRabbitMQ 的安装与配置：了解了如何通过 RabbitMQ 管理消息队列，以及如何与 Python 进行交互。 生产者的实现：使用 Faker 库生成假数据，并将其发布到 RabbitMQ 队列中。通过 pika 库与 RabbitMQ 进行连接，确保数据能够被成功发送到队列中。 消费者的实现：通过 Flask 启动一个独立的消费者线程，从 RabbitMQ 队列中获取数据并进行处理。我们还讨论了如何在 Flask 应用中嵌入多线程操作，保证 Web 应用的响应性和消息处理的高效性。 消息确认和持久化：我们讨论了消息的持久化和确认机制，这对于生产环境中的高可用性和数据安全性至关重要。 ","date":"2024-11-27T14:27:58Z","image":"https://www.ownit.top/title_pic/03.jpg","permalink":"https://www.ownit.top/p/202411271427/","title":"基于 Flask 和 RabbitMQ 构建高效消息队列系统：从数据生成到消费"},{"content":"简介 请参考下方，学习入门操作\n基于 Flask 和 Socket.IO 的 WebSocket 实时数据更新实现\n在当今数字化时代，实时性是衡量互联网应用的重要指标之一。无论是股票交易、在线游戏，还是实时监控大屏，WebSocket 已成为实现高效、双向实时通信的最佳选择之一。本文将通过一个基于 WebSocket 实现的实时数据大屏案例，深入探讨 WebSocket 的高级用法和优化技巧。\nWebSocket 的典型应用场景 实时数据监控：如运营监控大屏、设备状态监控等。 在线协作：如 Google Docs 的多人编辑。 实时聊天：如即时通讯工具。 实时通知：如电商的价格变动提醒。 场景分析：实时数据监控大屏 本案例的目标是实现一个实时数据监控大屏，通过 WebSocket 技术，将实时更新的数据动态展示在用户界面中。\n需求分析 实现不同房间的数据订阅（如销售数据和访问数据）。 支持多客户端实时接收服务器推送的最新数据。 动态更新界面，提供流畅的用户体验。 技术选型 前端：HTML、CSS、JavaScript 使用 Socket.IO 客户端库。 后端：基于 Flask 和 Flask-SocketIO 实现 WebSocket 服务。 实时数据生成：使用 Python 的 random 模块模拟实时数据。 后端实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 from flask import Flask, render_template, request from flask_socketio import SocketIO, emit, join_room, leave_room import random import time from threading import Thread app = Flask(__name__) app.config[\u0026#39;SECRET_KEY\u0026#39;] = \u0026#39;secret!\u0026#39; socketio = SocketIO(app) # 存储客户端订阅的房间信息 client_rooms = {} # 存储数据生成器线程 data_threads = {} def generate_sales_data(room): \u0026#34;\u0026#34;\u0026#34;生成销售相关数据\u0026#34;\u0026#34;\u0026#34; while room in data_threads and data_threads[room][\u0026#39;active\u0026#39;]: data = { \u0026#39;sales\u0026#39;: random.randint(1000, 5000), \u0026#39;orders\u0026#39;: random.randint(50, 200), \u0026#39;timestamp\u0026#39;: time.strftime(\u0026#39;%H:%M:%S\u0026#39;) } socketio.emit(\u0026#39;update_data\u0026#39;, data, room=room) time.sleep(2) def generate_visitor_data(room): \u0026#34;\u0026#34;\u0026#34;生成访问量相关数据\u0026#34;\u0026#34;\u0026#34; while room in data_threads and data_threads[room][\u0026#39;active\u0026#39;]: data = { \u0026#39;visitors\u0026#39;: random.randint(100, 1000), \u0026#39;active_users\u0026#39;: random.randint(50, 300), \u0026#39;timestamp\u0026#39;: time.strftime(\u0026#39;%H:%M:%S\u0026#39;) } socketio.emit(\u0026#39;update_data\u0026#39;, data, room=room) time.sleep(3) @app.route(\u0026#39;/\u0026#39;) def index(): return render_template(\u0026#39;index.html\u0026#39;) @socketio.on(\u0026#39;join\u0026#39;) def on_join(data): \u0026#34;\u0026#34;\u0026#34;处理客户端加入房间请求\u0026#34;\u0026#34;\u0026#34; room = data.get(\u0026#39;room\u0026#39;) if not room: return # 获取客户端ID client_id = request.sid # 将客户端加入房间 join_room(room) client_rooms[client_id] = room print(f\u0026#39;Client {client_id} joined room: {room}\u0026#39;) # 如果房间没有数据生成器线程，创建一个 if room not in data_threads: data_threads[room] = { \u0026#39;active\u0026#39;: True, \u0026#39;thread\u0026#39;: Thread( target=generate_sales_data if room == \u0026#39;sales\u0026#39; else generate_visitor_data, args=(room,), daemon=True ) } data_threads[room][\u0026#39;thread\u0026#39;].start() @socketio.on(\u0026#39;leave\u0026#39;) def on_leave(data): \u0026#34;\u0026#34;\u0026#34;处理客户端离开房间请求\u0026#34;\u0026#34;\u0026#34; room = data.get(\u0026#39;room\u0026#39;) if not room: return client_id = request.sid leave_room(room) if client_id in client_rooms: del client_rooms[client_id] print(f\u0026#39;Client {client_id} left room: {room}\u0026#39;) @socketio.on(\u0026#39;connect\u0026#39;) def handle_connect(): print(f\u0026#39;Client connected: {request.sid}\u0026#39;) @socketio.on(\u0026#39;disconnect\u0026#39;) def handle_disconnect(): client_id = request.sid if client_id in client_rooms: room = client_rooms[client_id] leave_room(room) del client_rooms[client_id] # 检查房间是否还有其他客户端 if not client_rooms.values().__contains__(room): # 如果没有，停止数据生成器 if room in data_threads: data_threads[room][\u0026#39;active\u0026#39;] = False data_threads[room][\u0026#39;thread\u0026#39;].join(timeout=1) del data_threads[room] print(f\u0026#39;Client disconnected: {client_id}\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: socketio.run(app, debug=True, host=\u0026#39;0.0.0.0\u0026#39;, port=5000) 数据生成与推送 后端的核心逻辑是数据生成与推送：\n数据生成：通过 generate_sales_data 和 generate_visitor_data 函数生成随机数据，并定时推送到客户端。 房间管理：通过 join_room 和 leave_room 方法管理客户端的房间订阅。 线程管理：使用线程来生成数据，并在客户端离开房间时停止线程。 HTML 结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;实时数据大屏\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; padding: 20px; background-color: #1a1a1a; color: #fff; font-family: Arial, sans-serif; } .controls { text-align: center; margin-bottom: 30px; } .btn { background-color: #4CAF50; border: none; color: white; padding: 10px 20px; margin: 0 10px; border-radius: 5px; cursor: pointer; transition: background-color 0.3s; } .btn:hover { background-color: #45a049; } .btn.active { background-color: #2E7D32; } .dashboard { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; max-width: 1200px; margin: 0 auto; } .card { background-color: #2a2a2a; border-radius: 10px; padding: 20px; text-align: center; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); } .card h2 { margin: 0 0 10px 0; color: #4CAF50; } .value { font-size: 2.5em; font-weight: bold; margin: 10px 0; } .timestamp { text-align: right; color: #888; margin-top: 20px; } @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.05); } 100% { transform: scale(1); } } .update { animation: pulse 0.5s ease-in-out; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 style=\u0026#34;text-align: center; margin-bottom: 40px;\u0026#34;\u0026gt;实时数据监控\u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;controls\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;btn\u0026#34; onclick=\u0026#34;toggleRoom(\u0026#39;sales\u0026#39;)\u0026#34; id=\u0026#34;salesBtn\u0026#34;\u0026gt;销售数据\u0026lt;/button\u0026gt; \u0026lt;button class=\u0026#34;btn\u0026#34; onclick=\u0026#34;toggleRoom(\u0026#39;visitors\u0026#39;)\u0026#34; id=\u0026#34;visitorsBtn\u0026#34;\u0026gt;访问数据\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;dashboard\u0026#34;\u0026gt; \u0026lt;!-- 销售数据卡片 --\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34; id=\u0026#34;salesCard\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;销售额\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;sales\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;实时销售金额 (元)\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34; id=\u0026#34;ordersCard\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;订单数\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;orders\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;实时订单统计\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- 访问数据卡片 --\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34; id=\u0026#34;visitorsCard\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;访问量\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;visitors\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;当前访问人数\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34; id=\u0026#34;activeUsersCard\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;活跃用户\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;active_users\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;实时活跃用户数\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;timestamp\u0026#34; id=\u0026#34;timestamp\u0026#34;\u0026gt;最后更新时间: --:--:--\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; const socket = io(); let currentRooms = new Set(); // 更新数据的函数 function updateValue(elementId, value) { const element = document.getElementById(elementId); if (element) { element.textContent = value; element.classList.remove(\u0026#39;update\u0026#39;); void element.offsetWidth; // 触发重绘 element.classList.add(\u0026#39;update\u0026#39;); } } // 切换房间 function toggleRoom(room) { const btn = document.getElementById(room + \u0026#39;Btn\u0026#39;); if (currentRooms.has(room)) { // 离开房间 socket.emit(\u0026#39;leave\u0026#39;, { room: room }); currentRooms.delete(room); btn.classList.remove(\u0026#39;active\u0026#39;); // 隐藏相关卡片 if (room === \u0026#39;sales\u0026#39;) { document.getElementById(\u0026#39;salesCard\u0026#39;).style.display = \u0026#39;none\u0026#39;; document.getElementById(\u0026#39;ordersCard\u0026#39;).style.display = \u0026#39;none\u0026#39;; } else { document.getElementById(\u0026#39;visitorsCard\u0026#39;).style.display = \u0026#39;none\u0026#39;; document.getElementById(\u0026#39;activeUsersCard\u0026#39;).style.display = \u0026#39;none\u0026#39;; } } else { // 加入房间 socket.emit(\u0026#39;join\u0026#39;, { room: room }); currentRooms.add(room); btn.classList.add(\u0026#39;active\u0026#39;); // 显示相关卡片 if (room === \u0026#39;sales\u0026#39;) { document.getElementById(\u0026#39;salesCard\u0026#39;).style.display = \u0026#39;block\u0026#39;; document.getElementById(\u0026#39;ordersCard\u0026#39;).style.display = \u0026#39;block\u0026#39;; } else { document.getElementById(\u0026#39;visitorsCard\u0026#39;).style.display = \u0026#39;block\u0026#39;; document.getElementById(\u0026#39;activeUsersCard\u0026#39;).style.display = \u0026#39;block\u0026#39;; } } } // 监听数据更新事件 socket.on(\u0026#39;update_data\u0026#39;, function(data) { // 更新所有收到的数据 Object.keys(data).forEach(key =\u0026gt; { if (key !== \u0026#39;timestamp\u0026#39;) { updateValue(key, data[key]); } }); document.getElementById(\u0026#39;timestamp\u0026#39;).textContent = \u0026#39;最后更新时间: \u0026#39; + data.timestamp; }); // 连接时自动加入销售数据房间 socket.on(\u0026#39;connect\u0026#39;, function() { toggleRoom(\u0026#39;sales\u0026#39;); }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; JavaScript 逻辑 在前端代码中，我们使用了 Socket.IO 客户端库来与服务器进行 WebSocket 通信。主要逻辑如下：\n连接服务器：通过 io() 方法连接到服务器。 切换房间：用户点击按钮时，通过 toggleRoom 函数切换不同的数据房间。 更新数据：监听 update_data 事件，更新页面上的数据。 WebSocket 的高级实践与优化 在实际应用中，可能需要管理多个房间，每个房间对应不同的数据类型或用户组。通过 join_room 和 leave_room 方法，可以轻松实现多房间管理。\n数据压缩与优化 对于大规模数据传输，可以考虑使用数据压缩技术来减少带宽占用。例如，使用 gzip 或 brotli 压缩数据包，或者在前端进行数据解压缩。\n断线重连与心跳机制 WebSocket 连接可能会因为网络问题而断开。为了保证连接的稳定性，可以实现断线重连机制和心跳包检测。通过定时发送心跳包，可以及时检测连接状态，并在断线时自动重连。\n安全性与权限控制 在生产环境中，安全性是一个不可忽视的问题。可以通过以下方式增强 WebSocket 连接的安全性：\n使用 HTTPS：确保 WebSocket 连接通过加密的 HTTPS 协议进行。 身份验证：在连接建立时进行身份验证，确保只有授权用户才能访问数据。 权限控制：根据用户角色控制其访问的房间和数据类型。 扩展与定制 WebSocket 的应用场景非常广泛，可以根据具体需求进行扩展和定制。例如，结合 WebRTC 实现实时音视频通信，或者结合 WebGL 实现实时3D数据可视化。\n","date":"2024-11-25T11:39:51Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/202411251139/","title":"深入浅出 WebSocket：构建实时数据大屏的高级实践"},{"content":"简介 随着现代化应用的快速发展，实时数据交互已经成为许多 Web 应用的核心需求，比如实时消息推送、数据监控大屏等。本文将基于一个完整的 WebSocket 实现示例，带你一步步理解如何使用 Flask 和 Socket.IO 构建实时数据更新的 Web 应用。 将实现一个实时数据监控大屏，数据会通过后台自动生成，并以 WebSocket 的方式实时推送到前端页面进行展示\n背景 为什么选择 WebSocket？ 传统的 HTTP 协议是无状态的，服务端和客户端之间的交互通常是基于请求-响应模式。如果需要实时更新数据，比如展示销售额、访问量等统计信息，那么通常需要客户端定时向服务端发起轮询请求。但这种方式有明显的缺点：\n效率低下：轮询会消耗大量的资源，因为大多数情况下数据并未改变，但轮询仍然会占用带宽和计算资源。 延迟较高：无法达到毫秒级的实时性，特别是在高频场景中，数据更新的延迟可能导致用户体验下降。 相比之下，WebSocket 是一种持久化的双向通信协议，可以在客户端和服务端之间建立全双工连接，能够更高效地实现实时数据的双向传输。这使得 WebSocket 成为实时应用的首选技术。\n项目实现的核心功能 服务端：\n使用 Flask 框架和 Flask-SocketIO 实现 WebSocket 协议。 后台自动生成模拟数据（如销售额、访问量、订单数等）。 将生成的数据通过 WebSocket 实时推送到客户端。 客户端：\n通过 Socket.IO 客户端接收服务端推送的数据。 动态更新页面内容并展示实时变化。 提供美观的前端界面，模拟数据监控大屏的效果。 环境准备 在开始之前，请确保你的开发环境中安装了以下工具：\nPython 3.x Flask Flask-SocketIO 你可以通过以下命令安装相关依赖库：\n1 pip install flask flask-socketio 服务端实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from flask import Flask, render_template from flask_socketio import SocketIO, emit import random import time from threading import Thread # 创建 Flask 应用 app = Flask(__name__) app.config[\u0026#39;SECRET_KEY\u0026#39;] = \u0026#39;secret!\u0026#39; socketio = SocketIO(app) # 模拟数据生成函数 def generate_random_data(): \u0026#34;\u0026#34;\u0026#34;后台线程：生成模拟数据并推送到客户端\u0026#34;\u0026#34;\u0026#34; while True: # 生成随机数据 data = { \u0026#39;sales\u0026#39;: random.randint(1000, 5000), # 销售额 \u0026#39;visitors\u0026#39;: random.randint(100, 1000), # 访问量 \u0026#39;orders\u0026#39;: random.randint(50, 200), # 订单数 \u0026#39;timestamp\u0026#39;: time.strftime(\u0026#39;%H:%M:%S\u0026#39;) # 当前时间 } # 推送数据到客户端 socketio.emit(\u0026#39;update_data\u0026#39;, data) time.sleep(2) # 每 2 秒生成一次数据 # 首页路由 @app.route(\u0026#39;/\u0026#39;) def index(): return render_template(\u0026#39;index.html\u0026#39;) # WebSocket 事件：客户端连接 @socketio.on(\u0026#39;connect\u0026#39;) def handle_connect(): print(\u0026#39;Client connected\u0026#39;) emit(\u0026#39;server_response\u0026#39;, {\u0026#39;data\u0026#39;: \u0026#39;Connected to server!\u0026#39;}) # WebSocket 事件：客户端断开连接 @socketio.on(\u0026#39;disconnect\u0026#39;) def handle_disconnect(): print(\u0026#39;Client disconnected\u0026#39;) # WebSocket 事件：接收客户端消息 @socketio.on(\u0026#39;client_message\u0026#39;) def handle_message(message): print(\u0026#39;Received message:\u0026#39;, message) # 将客户端消息广播给所有连接的客户端 emit(\u0026#39;server_response\u0026#39;, {\u0026#39;data\u0026#39;: message[\u0026#39;data\u0026#39;]}, broadcast=True) # 主函数：启动应用 if __name__ == \u0026#39;__main__\u0026#39;: # 启动后台线程 Thread(target=generate_random_data, daemon=True).start() # 启动 Flask-SocketIO 服务 socketio.run(app, debug=True, host=\u0026#39;0.0.0.0\u0026#39;, port=5000) 服务端代码解析 Flask-SocketIO 的使用： Flask-SocketIO 是基于 Flask 的扩展库，支持 WebSocket 协议，可以轻松实现实时通信。\n1 socketio = SocketIO(app) 后台数据生成线程： 使用 Python 的 Thread 模块在后台不断生成随机数据并通过 socketio.emit 方法推送到客户端。\n1 Thread(target=generate_random_data, daemon=True).start() WebSocket 事件监听：\n@socketio.on('connect')：当客户端连接时触发。 @socketio.on('disconnect')：当客户端断开连接时触发。 @socketio.on('client_message')：监听客户端发送的消息，并将其广播给所有连接的客户端。 前端实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;实时数据大屏\u0026lt;/title\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; padding: 20px; background-color: #1a1a1a; color: #fff; font-family: Arial, sans-serif; } .dashboard { display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; max-width: 1200px; margin: 0 auto; } .card { background-color: #2a2a2a; border-radius: 10px; padding: 20px; text-align: center; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); } .card h2 { margin: 0 0 10px 0; color: #4CAF50; } .value { font-size: 2.5em; font-weight: bold; margin: 10px 0; } .timestamp { text-align: right; color: #888; margin-top: 20px; } @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.05); } 100% { transform: scale(1); } } .update { animation: pulse 0.5s ease-in-out; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 style=\u0026#34;text-align: center; margin-bottom: 40px;\u0026#34;\u0026gt;实时数据监控\u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;dashboard\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;销售额\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;sales\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;实时销售金额 (元)\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;访问量\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;visitors\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;当前访问人数\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;订单数\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;orders\u0026#34; class=\u0026#34;value\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;实时订单统计\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;timestamp\u0026#34; id=\u0026#34;timestamp\u0026#34;\u0026gt;最后更新时间: --:--:--\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; const socket = io(); // 更新数据的函数 function updateValue(elementId, value) { const element = document.getElementById(elementId); element.textContent = value; element.classList.remove(\u0026#39;update\u0026#39;); void element.offsetWidth; // 触发重绘 element.classList.add(\u0026#39;update\u0026#39;); } // 监听数据更新事件 socket.on(\u0026#39;update_data\u0026#39;, function(data) { updateValue(\u0026#39;sales\u0026#39;, data.sales); updateValue(\u0026#39;visitors\u0026#39;, data.visitors); updateValue(\u0026#39;orders\u0026#39;, data.orders); document.getElementById(\u0026#39;timestamp\u0026#39;).textContent = \u0026#39;最后更新时间: \u0026#39; + data.timestamp; }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 前端代码解析 引入 Socket.IO 客户端库： 引入 Socket.IO 客户端库，用于与服务端建立 WebSocket 连接。\n1 \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 动态更新数据的实现： 前端 JavaScript 代码通过监听 update_data 事件来获取服务端推送的数据，然后更新对应的 HTML 元素内容，并通过 CSS 动画增加视觉效果。\n1 2 3 4 5 6 socket.on(\u0026#39;update_data\u0026#39;, function(data) { updateValue(\u0026#39;sales\u0026#39;, data.sales); updateValue(\u0026#39;visitors\u0026#39;, data.visitors); updateValue(\u0026#39;orders\u0026#39;, data.orders); document.getElementById(\u0026#39;timestamp\u0026#39;).textContent = \u0026#39;最后更新时间: \u0026#39; + data.timestamp; }); 样式和布局： 使用 CSS Grid 布局创建整洁的数据卡片界面，每个卡片显示一种数据类型。卡片使用动画效果突出数据更新。\n","date":"2024-11-25T11:05:13Z","image":"https://www.ownit.top/title_pic/51.jpg","permalink":"https://www.ownit.top/p/202411251105/","title":"基于 Flask 和 Socket.IO 的 WebSocket 实时数据更新实现"},{"content":"OpenStack介绍 OpenStack是一个开源的云计算管理平台，旨在为公共及私有云的建设与管理提供软件。它由几个主要的组件组合起来完成一些具体的工作，包括计算、存储和网络资源的管理。OpenStack支持所有类型的云环境，可以简单地实施并大规模扩展，提供了一个丰富、标准化、统一的云计算管理平台。\n在私有云的部署中，OpenStack提供了强大的灵活性和可扩展性，允许企业根据自己的需求来定制和优化云环境。例如，通过OpenStack，企业可以规划并管理大量虚拟机，提供所需的对象及块存储资源，并实现网络资源的高扩展和自动化管理\n组件 说明 Nova Compute HyperV管理(libvirt,qumu\u0026hellip;) Neutron 网络和地址管理(原名Quantum) Swift 对象存储(Object) Cinder 块存储管理(Cinder):主机指虚拟机的存储管理 Keystone 身份认证授权(Identity) Glance 镜象管理(Image)，支持本地存储、NFS、Swift、sheepdog和Ceph Horizon UI界面 (Dashboard) Ceilometer 监控计量(Metering) Heat 软件部署(编配Orchestration) Lbass 负载均衡，后端可以是各种商业和开源产品，如：F5、Nginx、Haproxy、LVS oslo 把所有组件需要用的相同的东西集中起来 Moniker DNS:每个虚拟机，都会自动有一个dns记录 marconi 消息队列扩展(Queue Service) Trove Database Service Ironic 裸机部署(Bare Metal) Savannah Data Processing 公司需求 因为公司业务拓展，需要有相关的设备建设对应的环境，我们采购多台服务器，使用raid5在组成存储。在这基础上使用OpenStack搭建私有云，来运行相关的业务和虚拟机。\n机房检测环境通常需要一个稳定、安全、可控制的私有云环境。在这样的环境中，OpenStack能够提供以下优势：\n自主可控：企业可以根据自己的需求定制和优化云环境，不受外部云服务提供商的限制。 安全性：私有云部署在企业内部，可以提供更高的安全和隐私级别，确保敏感数据不被第三方访问。 灵活性：OpenStack的开源特性使得企业可以灵活地选择硬件和软件，以及如何部署和管理这些资源。 成本效益：虽然初期建设成本可能较高，但随着业务量的增加，私有云的平均运营成本可能会低于公有云。 准备 OS：Centos 7 1804 Minimal CPU：4c MEM：8G xvda：50G 系统盘 xvdb：100G 数据盘 Master IP：192.168.81.40\n目标： 1.安装openstack Ocata 2.参考文档 https://docs.openstack.org/ocata/install-guide-rdo/\n初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #初始化配置 1.配置静态IP 2.关闭 selinux sed -i \u0026#34;s/SELINUX=.*/SELINUX=disabled/g\u0026#34; /etc/selinux/config 3.关闭 NetworkManger systemctl stop NetworkManger systemctl disable NetworkManager 4.关闭 firewalld systemctl stop firewalld systemctl disable firewalld 5.配置主机名 hostnamectl set-hostname controller.openstack.fjf 6.重启 reboot 网络时间协议（NTP） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1.编辑 /etc/chrony.conf server ntp1.aliyun.com iburst server ntp2.aliyun.com iburst server ntp3.aliyun.com iburst server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst driftfile /var/lib/chrony/drift makestep 1.0 3 rtcsync allow 192.168.0.0/16 logdir /var/log/chrony 2.重启服务 systemctl restart chronyd 安装OpenStack软件包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 1.准备密码 ADMIN_PASS=Pass@w0rd CINDER_DBPASS=Pass@w0rd CINDER_PASS=Pass@w0rd DASH_DBPASS=Pass@w0rd DEMO_PASS=Pass@w0rd GLANCE_DBPASS=Pass@w0rd GLANCE_PASS=Pass@w0rd KEYSTONE_DBPASS=Pass@w0rd METADATA_SECRET=Pass@w0rd NEUTRON_DBPASS=Pass@w0rd NEUTRON_PASS=Pass@w0rd NOVA_DBPASS=Pass@w0rd NOVA_PASS=Pass@w0rd PLACEMENT_PASS=Pass@w0rd RABBIT_PASS=Pass@w0rd 2.配置hosts echo \u0026#34;192.168.81.40 controller.openstack.fjf\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;192.168.81.40 controller\u0026#34; \u0026gt;\u0026gt; /etc/hosts 3.安装openstack yum 源，官方已经移除了ocata的yum源，所以用本地同步好的 mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/* /etc/yum.repos.d/bak/ for i in CentOS-Base.repo CentOS-OpenStack-Ocata.repo epel.repo;do curl -s -o /etc/yum.repos.d/$i http://source.fjf.com/bdata/centos7/$i done 4.重建yum缓存 yum clean all yum makecache 5.升级软件 yum upgrade -y 6.内核可能会有更新，重启一次系统 reboot 7.安装openstack client yum install python-openstackclient -y 创建目录 1 mkdir -p /var/lib/nova 分区并格式化磁盘 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [root@controller nova]# fdisk /dev/xvdb Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0x3974d932. Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): 1 First sector (2048-209715199, default 2048): Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-209715199, default 209715199): Using default value 209715199 Partition 1 of type Linux and of size 100 GiB is set Command (m for help): p Disk /dev/xvdb: 107.4 GB, 107374182400 bytes, 209715200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x3974d932 Device Boot Start End Blocks Id System /dev/xvdb1 2048 209715199 104856576 83 Linux Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. [root@controller lib]# mkfs.xfs /dev/xvdb1 挂载至/var/lib/nova 1 2 3 mount /dev/xvdb1 /var/lib/nova 编辑/etc/fstab,添加一行 /dev/xvdb1 /var/lib/nova xfs defaults 0 0 部署基础组件 安装数据库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 1.安装 yum install mariadb mariadb-server python2-PyMySQL -y 2.配置 cat \u0026gt; /etc/my.cnf.d/openstack.cnf \u0026lt;\u0026lt;EOF [mysqld] bind-address = 192.168.81.40 default-storage-engine = innodb innodb_file_per_table = on max_connections = 4096 collation-server = utf8_general_ci character-set-server = utf8 EOF 3.优化参数 在文件 /usr/lib/systemd/system/mariadb.service 的[Service]字段添加下列参数，否则会造成虚拟机创建失败的情况 LimitNOFILE=10000 LimitNPROC=10000 4.启动 systemctl enable mariadb.service systemctl start mariadb.service 5.设置密码 [root@master ~]# mysql_secure_installation NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MariaDB to secure it, we\u0026#39;ll need the current password for the root user. If you\u0026#39;ve just installed MariaDB, and you haven\u0026#39;t set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): 默认没密码，直接回车 OK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MariaDB root user without the proper authorisation. Set root password? [Y/n] Y New password: Pass@w0rd Re-enter new password: Pass@w0rd Password updated successfully! Reloading privilege tables.. ... Success! By default, a MariaDB installation has an anonymous user, allowing anyone to log into MariaDB without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] Y ... Success! Normally, root should only be allowed to connect from \u0026#39;localhost\u0026#39;. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] Y #禁止root远程登陆，选择为Y之后只有master节点能使用root账号登陆。如果不想这样，按n ... Success! By default, MariaDB comes with a database named \u0026#39;test\u0026#39; that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] Y - Dropping test database... ... Success! - Removing privileges on test database... ... Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] Y ... Success! Cleaning up... All done! If you\u0026#39;ve completed all of the above steps, your MariaDB installation should now be secure. Thanks for using MariaDB! 6.测试一下 [root@master ~]# mysql -uroot -pPass@w0rd Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 16 Server version: 10.3.20-MariaDB MariaDB Server Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ 3 rows in set (0.001 sec) 安装消息队列服务 1 2 3 4 5 6 7 8 9 10 11 12 13 1.安装rabbitmq yum install rabbitmq-server -y 2.启动 systemctl enable rabbitmq-server.service systemctl start rabbitmq-server.service 3.设置密码 rabbitmqctl add_user openstack Pass@w0rd 4.授权 rabbitmqctl set_permissions openstack \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; 5.开启rabbitmq management方便以后管理 rabbitmq-plugins enable rabbitmq_management 默认密码：guest/guest 后台：192.168.81.40:15672 安装缓存服务 1 2 3 4 5 6 7 8 1.安装memcached yum install memcached python-memcached -y 2.修改配置文件 vim /etc/sysconfig/memcached OPTIONS=\u0026#34;-l 127.0.0.1,::1,controller.openstack.fjf\u0026#34; 3.启动 systemctl enable memcached.service systemctl start memcached.service 安装键值存储服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 1.安装etcd yum install etcd -y 2.配置 cat \u0026gt; /etc/etcd/etcd.conf \u0026lt;\u0026lt;EOF #[Member] ETCD_DATA_DIR=\u0026#34;/var/lib/etcd/default.etcd\u0026#34; ETCD_LISTEN_PEER_URLS=\u0026#34;http://192.168.81.40:2380\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http://192.168.81.40:2379\u0026#34; ETCD_NAME=\u0026#34;controller.openstack.fjf\u0026#34; #[Clustering] ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;http://192.168.81.40:2380\u0026#34; ETCD_ADVERTISE_CLIENT_URLS=\u0026#34;http://192.168.81.40:2379\u0026#34; ETCD_INITIAL_CLUSTER=\u0026#34;controller.openstack.fjf=http://192.168.81.40:2380\u0026#34; ETCD_INITIAL_CLUSTER_TOKEN=\u0026#34;etcd-cluster-01\u0026#34; ETCD_INITIAL_CLUSTER_STATE=\u0026#34;new\u0026#34; EOF 3.启动 systemctl enable etcd systemctl start etcd 系统参数优化 1 2 3 4 5 6 7 8 9 1.修改/etc/security/limits.d/20-nproc.conf * soft nproc 65535 root soft nproc unlimited 2.修改 /etc/security/limits.conf * soft nofile 655350 * hard nofile 655350 * soft nproc 655350 * hard nproc 655350 安装keystone 一. 安装keystone(身份认证服务)\n建库，建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@controller ~]# mysql -uroot -pPass@w0rd Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 11 Server version: 10.1.20-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; CREATE DATABASE keystone; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON keystone.* TO \u0026#39;keystone\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON keystone.* TO \u0026#39;keystone\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) 安装组件\n1 yum install openstack-keystone httpd mod_wsgi -y 编辑/etc/keystone/keystone.conf文件并完成以下操作：\n1 2 3 4 5 6 7 8 9 1.在该[database]部分中，配置数据库访问： [database] # ... connection = mysql+pymysql://keystone:Pass@w0rd@controller.openstack.fjf/keystone 2.在该[token]部分中，配置Fernet令牌提供者 [token] # ... provider = fernet 填充身份服务数据库\n1 su -s /bin/sh -c \u0026#34;keystone-manage db_sync\u0026#34; keystone 初始化Fernet密钥存储库\n1 2 keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone keystone-manage credential_setup --keystone-user keystone --keystone-group keystone 引导身份服务\n1 2 3 4 5 keystone-manage bootstrap --bootstrap-password Pass@w0rd \\ --bootstrap-admin-url http://controller.openstack.fjf:5000/v3/ \\ --bootstrap-internal-url http://controller.openstack.fjf:5000/v3/ \\ --bootstrap-public-url http://controller.openstack.fjf:5000/v3/ \\ --bootstrap-region-id RegionOne 配置HTTP接口\n1 2 vim /etc/httpd/conf/httpd.conf ServerName controller.openstack.fjf 创建链接\n1 ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 启动\n1 2 systemctl enable httpd.service systemctl start httpd.service 创建admin-openrc\n1 2 3 4 5 6 7 8 9 cat \u0026gt; /root/admin-openrc \u0026lt;\u0026lt;EOF export OS_USERNAME=admin export OS_PASSWORD=Pass@w0rd export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://controller.openstack.fjf:5000/v3 export OS_IDENTITY_API_VERSION=3 EOF 加载admin-openrc\n1 2 . admin-openrc 创建service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 openstack project create --domain default \\ --description \u0026#34;Service Project\u0026#34; service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | default | | enabled | True | | id | 2c7d5b03d68746229cb60b9ae382d0ae | | is_domain | False | | name | service | | parent_id | default | | tags | [] | +-------------+----------------------------------+ 验证操作\n1 2 3 4 5 6 7 8 9 10 11 openstack --os-auth-url http://controller.openstack.fjf:5000/v3 \\ --os-project-domain-name Default --os-user-domain-name Default \\ --os-project-name admin --os-username admin token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2021-03-18T12:10:01+0000 | | id | gAAAAABgUzUJjJTsx4Vz1OY_vBBN28FQC3hH-xpqhITyvjSdF4pOykX8tCd0WX59rChxhC7kWZkrhQygcLj9Ari3pmKcZ5u9Ae4RTYJh1NpF8418xbbZCeZgxbVb9SOUgqCe0ftFzbewGmEL-hxhOmr3rd3WO2HYG47H6CR9WmDzSWsIei2NpPw | | project_id | 750d6512fc3043d1b1c6d832465cbf9f | | user_id | 99cb132981a74d339868e54a0a4deb3c | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 安装glance 二、安装glance(镜像服务)\n建库，建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@controller ~]# mysql -uroot -pPass@w0rd Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 3 Server version: 10.1.20-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; CREATE DATABASE glance; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON glance.* TO \u0026#39;glance\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Passw0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON glance.* TO \u0026#39;glance\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Passw0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) 加载admin-openrc\n1 . admin-openrc 创建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@controller ~]# openstack user create --domain default --password-prompt glance User Password:Pass@w0rd Repeat User Password:Pass@w0rd +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | 06d7dd9775a3482886790405f3bfe298 | | name | glance | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 授权\n1 openstack role add --project service --user glance admin 创建服务\n1 2 3 4 5 6 7 8 9 10 [root@controller ~]# openstack service create --name glance --description \u0026#34;OpenStack Image\u0026#34; image +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Image | | enabled | True | | id | 56c2b257f48d4d7aa4768a0f1f2367ca | | name | glance | | type | image | +-------------+----------------------------------+ 创建API接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [root@controller ~]# openstack endpoint create --region RegionOne image public http://controller.openstack.fjf:9292 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 36c0f0dec23f435fb5e3bbd6ac0ce5ff | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 56c2b257f48d4d7aa4768a0f1f2367ca | | service_name | glance | | service_type | image | | url | http://controller.openstack.fjf:9292 | +--------------+--------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne image internal http://controller.openstack.fjf:9292 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 236f70c24afb4049ac2660cbecd0b7a2 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 56c2b257f48d4d7aa4768a0f1f2367ca | | service_name | glance | | service_type | image | | url | http://controller.openstack.fjf:9292 | +--------------+--------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne image admin http://controller.openstack.fjf:9292 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 36f005f69dd74f52b3f5bf0cce3c46d7 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 56c2b257f48d4d7aa4768a0f1f2367ca | | service_name | glance | | service_type | image | | url | http://controller.openstack.fjf:9292 | +--------------+--------------------------------------+ 安装和配置组件\n1 2 yum install openstack-glance -y 编辑/etc/glance/glance-api.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [database] # ... # 注意此处的密码是Passw0rd而不是Pass@w0rd,改组件连接串中不支持\u0026#39;@\u0026#39;符号 connection = mysql+pymysql://glance:Passw0rd@controller.openstack.fjf/glance [keystone_authtoken] # ... www_authenticate_uri = http://controller.openstack.fjf:5000 auth_url = http://controller.openstack.fjf:5000 memcached_servers = controller.openstack.fjf:11211 auth_type = password project_domain_name = Default user_domain_name = Default project_name = service username = glance password = Pass@w0rd [paste_deploy] # ... flavor = keystone [glance_store] # ... stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ 填充图像服务数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [root@controller ~]# su -s /bin/sh -c \u0026#34;glance-manage db_sync\u0026#34; glance # 出现此警告可以忽略 /usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:1336: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade expire_on_commit=expire_on_commit, _conf=conf) INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade -\u0026gt; liberty, liberty initial INFO [alembic.runtime.migration] Running upgrade liberty -\u0026gt; mitaka01, add index on created_at and updated_at columns of \u0026#39;images\u0026#39; table INFO [alembic.runtime.migration] Running upgrade mitaka01 -\u0026gt; mitaka02, update metadef os_nova_server INFO [alembic.runtime.migration] Running upgrade mitaka02 -\u0026gt; ocata_expand01, add visibility to images INFO [alembic.runtime.migration] Running upgrade ocata_expand01 -\u0026gt; pike_expand01, empty expand for symmetry with pike_contract01 INFO [alembic.runtime.migration] Running upgrade pike_expand01 -\u0026gt; queens_expand01 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Upgraded database to: queens_expand01, current revision(s): queens_expand01 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Database migration is up to date. No migration needed. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade mitaka02 -\u0026gt; ocata_contract01, remove is_public from images INFO [alembic.runtime.migration] Running upgrade ocata_contract01 -\u0026gt; pike_contract01, drop glare artifacts tables INFO [alembic.runtime.migration] Running upgrade pike_contract01 -\u0026gt; queens_contract01 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Upgraded database to: queens_contract01, current revision(s): queens_contract01 INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Database is synced successfully. 启动\n1 2 systemctl enable openstack-glance-api.service systemctl start openstack-glance-api.service 安装Compute|控制节点 三、安装Compute(计算服务)\u0026amp;Placement(调度服务)\n建库，建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [root@controller ~]# mysql -uroot -pPass@w0rd Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 11 Server version: 10.1.20-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; CREATE DATABASE nova_api; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; CREATE DATABASE nova; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; CREATE DATABASE nova_cell0; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON nova_api.* TO \u0026#39;nova\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON nova_api.* TO \u0026#39;nova\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON nova.* TO \u0026#39;nova\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON nova.* TO \u0026#39;nova\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO \u0026#39;nova\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON nova_cell0.* TO \u0026#39;nova\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) 加载admin-openrc\n1 . admin-openrc 创建Nova用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@controller ~]# openstack user create --domain default --password-prompt nova User Password:Pass@w0rd Repeat User Password:Pass@w0rd +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | fa90e0ee1be34a7c8d9d77bf2db3ca7e | | name | nova | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 授权Nova\n1 openstack role add --project service --user nova admin 创建计算服务\n1 2 3 4 5 6 7 8 9 10 [root@controller ~]# openstack service create --name nova --description \u0026#34;OpenStack Compute\u0026#34; compute +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Compute | | enabled | True | | id | 0795642e98ce42c8bac111a21625976c | | name | nova | | type | compute | +-------------+----------------------------------+ 创建计算API接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [root@controller ~]# openstack endpoint create --region RegionOne compute public http://controller.openstack.fjf:8774/v2.1 +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | f1569159f0444831b29d4e0732aeca27 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 0795642e98ce42c8bac111a21625976c | | service_name | nova | | service_type | compute | | url | http://controller.openstack.fjf:8774/v2.1 | +--------------+-------------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne compute internal http://controller.openstack.fjf:8774/v2.1 +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 465c36ab7dcc4e849c835ae9ef174eba | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 0795642e98ce42c8bac111a21625976c | | service_name | nova | | service_type | compute | | url | http://controller.openstack.fjf:8774/v2.1 | +--------------+-------------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne compute admin http://controller.openstack.fjf:8774/v2.1 +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 879816f84f6444b4a65bab6f8d620d08 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 0795642e98ce42c8bac111a21625976c | | service_name | nova | | service_type | compute | | url | http://controller.openstack.fjf:8774/v2.1 | +--------------+-------------------------------------------+ 创建placement用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@controller ~]# openstack user create --domain default --password-prompt placement User Password:Pass@w0rd Repeat User Password:Pass@w0rd +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | f0b41802435f4d3dad27c6711bb3de73 | | name | placement | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 授权placement\n1 openstack role add --project service --user placement admin 创建调度器服务\n1 2 3 4 5 6 7 8 9 10 [root@controller ~]# openstack service create --name placement --description \u0026#34;Placement API\u0026#34; placement +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Placement API | | enabled | True | | id | 315aa5dae18c4c6bb2a6105f25a11c98 | | name | placement | | type | placement | +-------------+----------------------------------+ 创建调度器API接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [root@controller ~]# openstack endpoint create --region RegionOne placement public http://controller.openstack.fjf:8778 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | bb6eeb117b2c421498bd5d74ab2304e0 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 7b2281d9a2e1436e8d6b9721774deb01 | | service_name | placement | | service_type | placement | | url | http://controller.openstack.fjf:8778 | +--------------+--------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne placement internal http://controller.openstack.fjf:8778 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | fed2080d301247e4b9cb9e2b2a4bde55 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 7b2281d9a2e1436e8d6b9721774deb01 | | service_name | placement | | service_type | placement | | url | http://controller.openstack.fjf:8778 | +--------------+--------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne placement admin http://controller.openstack.fjf:8778 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 3d255c3a2ebb4d249f063e1392767fd7 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 7b2281d9a2e1436e8d6b9721774deb01 | | service_name | placement | | service_type | placement | | url | http://controller.openstack.fjf:8778 | +--------------+--------------------------------------+ 安装和配置组件\n1 yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api -y 编辑/etc/nova/nova.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 [DEFAULT] # ... enabled_apis = osapi_compute,metadata use_neutron = True firewall_driver = nova.virt.firewall.NoopFirewallDriver my_ip = 192.168.81.40 connection = mysql+pymysql://nova:Pass@w0rd@controller.openstack.fjf/nova_api transport_url = rabbit://openstack:Pass@w0rd@controller.openstack.fjf [api_database] # ... connection = mysql+pymysql://nova:Pass@w0rd@controller.openstack.fjf/nova_api [database] # ... connection = mysql+pymysql://nova:Pass@w0rd@controller.openstack.fjf/nova [api] # ... auth_strategy = keystone [keystone_authtoken] # ... auth_uri = http://controller.openstack.fjf:5000 auth_url = http://controller.openstack.fjf:35357 memcached_servers = controller.openstack.fjf:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = Pass@w0rd [vnc] enabled = true # ... vncserver_listen = $my_ip vncserver_proxyclient_address = $my_ip novncproxy_base_url = http://controller.openstack.fjf:6080/vnc_auto.html [glance] # ... api_servers = http://controller.openstack.fjf:9292 [oslo_concurrency] # ... lock_path = /var/lib/nova/tmp [placement] # ... os_region_name = RegionOne project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller.openstack.fjf:35357/v3 username = placement password = Pass@w0rd 编辑/etc/httpd/conf.d/00-nova-placement-api.conf，尾部添加\n1 2 3 4 5 6 7 8 9 \u0026lt;Directory /usr/bin\u0026gt; \u0026lt;IfVersion \u0026gt;= 2.4\u0026gt; Require all granted \u0026lt;/IfVersion\u0026gt; \u0026lt;IfVersion \u0026lt; 2.4\u0026gt; Order allow,deny Allow from all \u0026lt;/IfVersion\u0026gt; \u0026lt;/Directory\u0026gt; 重启httpd\n1 systemctl restart httpd 初始化数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [root@controller ~]# su -s /bin/sh -c \u0026#34;nova-manage api_db sync\u0026#34; nova [root@controller ~]# su -s /bin/sh -c \u0026#34;nova-manage cell_v2 map_cell0\u0026#34; nova [root@controller ~]# su -s /bin/sh -c \u0026#34;nova-manage cell_v2 create_cell --name=cell1 --verbose\u0026#34; nova 5d590937-4a0e-488e-9766-9db19d151a51 [root@controller ~]# su -s /bin/sh -c \u0026#34;nova-manage db sync\u0026#34; nova # 警告信息可以忽略 /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u\u0026#39;Duplicate index `block_device_mapping_instance_uuid_virtual_name_device_name_idx`. This is deprecated and will be disallowed in a future release.\u0026#39;) result = self._query(query) /usr/lib/python2.7/site-packages/pymysql/cursors.py:170: Warning: (1831, u\u0026#39;Duplicate index `uniq_instances0uuid`. This is deprecated and will be disallowed in a future release.\u0026#39;) result = self._query(query) [root@controller ~]# nova-manage cell_v2 list_cells # 验证数据 +-------+--------------------------------------+--------------------------------------------------+---------------------------------------------------------------+ | Name | UUID | Transport URL | Database Connection | +-------+--------------------------------------+--------------------------------------------------+---------------------------------------------------------------+ | cell0 | 00000000-0000-0000-0000-000000000000 | none:/ | mysql+pymysql://nova:****@controller.openstack.fjf/nova_cell0 | | cell1 | 5d590937-4a0e-488e-9766-9db19d151a51 | rabbit://openstack:****@controller.openstack.fjf | mysql+pymysql://nova:****@controller.openstack.fjf/nova | +-------+--------------------------------------+--------------------------------------------------+---------------------------------------------------------------+ 启动服务\n1 2 3 systemctl enable openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service systemctl start openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service 安装Compute|计算节点 四、安装Compute(计算服务)，为了节省资源可以将计算节点和控制节点安装在一起这样控制节点也可以运行虚拟机\n安装组件\n1 yum install openstack-nova-compute 编辑配置/etc/nova/nova.conf，如果是在控制节点上添加计算服务则不需要修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 [DEFAULT] # ... enabled_apis = osapi_compute,metadata my_ip = 192.168.81.40 use_neutron = True firewall_driver = nova.virt.firewall.NoopFirewallDriver transport_url = rabbit://openstack:Pass@w0rd@controller.openstack.fjf [api] # ... auth_strategy = keystone [keystone_authtoken] # ... auth_uri = http://controller.openstack.fjf:5000 auth_url = http://controller.openstack.fjf:35357 memcached_servers = controller.openstack.fjf:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = Pass@w0rd [vnc] # ... enabled = True vncserver_listen = 0.0.0.0 vncserver_proxyclient_address = $my_ip novncproxy_base_url = http://controller.openstack.fjf:6080/vnc_auto.html [glance] # ... api_servers = http://controller.openstack.fjf:9292 [oslo_concurrency] # ... lock_path = /var/lib/nova/tmp [placement] # ... os_region_name = RegionOne project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller.openstack.fjf:35357/v3 username = placement password = Pass@w0rd 检查是否支持硬件虚拟化\n1 2 3 4 5 6 7 [root@controller ~]# egrep -c \u0026#39;(vmx|svm)\u0026#39; /proc/cpuinfo 8 # 如果结果为0，则需要添加以下配置 编辑 /etc/nova/nova.conf [libvirt] # ... virt_type = qemu 启动服务\n1 2 systemctl enable libvirtd.service openstack-nova-compute.service systemctl start libvirtd.service openstack-nova-compute.service 添加计算节点到元数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . admin-openrc # 扫描计算主机 [root@controller ~]# su -s /bin/sh -c \u0026#34;nova-manage cell_v2 discover_hosts --verbose\u0026#34; nova Found 2 cell mappings. Skipping cell0 since it does not contain hosts. Getting computes from cell \u0026#39;cell1\u0026#39;: 5d590937-4a0e-488e-9766-9db19d151a51 Checking host mapping for compute host \u0026#39;controller.openstack.fjf\u0026#39;: deb6c5fe-24a6-4b1e-af8f-fe448491a565 Creating host mapping for compute host \u0026#39;controller.openstack.fjf\u0026#39;: deb6c5fe-24a6-4b1e-af8f-fe448491a565 Found 1 unmapped computes in cell: 5d590937-4a0e-488e-9766-9db19d151a51 # 查看主机状态 [root@controller ~]# openstack hypervisor list +----+--------------------------+-----------------+---------------+-------+ | ID | Hypervisor Hostname | Hypervisor Type | Host IP | State | +----+--------------------------+-----------------+---------------+-------+ | 1 | controller.openstack.fjf | QEMU | 192.168.81.40 | up | +----+--------------------------+-----------------+---------------+-------+ 注意：添加计算节点后必须在控制节点执行nova-manage cell_v2 discover_hosts才能发现新的计算节点，可以调整/etc/nova/nova.conf中的参数discover_hosts_in_cells_interval = 300，表示为300s自动扫描一次\n安装neutron 五、安装neutron(网络服务)\n建库，建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [root@controller ~]# mysql -uroot -pPass@w0rd Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 3182 Server version: 10.1.20-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; CREATE DATABASE neutron; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON neutron.* TO \u0026#39;neutron\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; GRANT ALL PRIVILEGES ON neutron.* TO \u0026#39;neutron\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Pass@w0rd\u0026#39;; Query OK, 0 rows affected (0.00 sec) 加载admin-openrc\n1 2 . admin-openrc 创建用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@controller ~]# openstack user create --domain default --password-prompt neutron User Password: Repeat User Password: +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | default | | enabled | True | | id | c6ea433a2f83472f8218b60dbdf1827a | | name | neutron | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ 授权\n1 2 openstack role add --project service --user neutron admin 创建服务\n1 2 3 4 5 6 7 8 9 10 [root@controller ~]# openstack service create --name neutron --description \u0026#34;OpenStack Networking\u0026#34; network +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Networking | | enabled | True | | id | 9b2865e3489c4da3b6c250abf2e73e80 | | name | neutron | | type | network | +-------------+----------------------------------+ 创建API接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [root@controller ~]# openstack endpoint create --region RegionOne network public http://controller.openstack.fjf:9696 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | f3a7c91366d84b49aa84841585ee3d36 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 9b2865e3489c4da3b6c250abf2e73e80 | | service_name | neutron | | service_type | network | | url | http://controller.openstack.fjf:9696 | +--------------+--------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne network internal http://controller.openstack.fjf:9696 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | e5bac77508904343b93f12a19294a7dd | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 9b2865e3489c4da3b6c250abf2e73e80 | | service_name | neutron | | service_type | network | | url | http://controller.openstack.fjf:9696 | +--------------+--------------------------------------+ [root@controller ~]# openstack endpoint create --region RegionOne network admin http://controller.openstack.fjf:9696 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 20e22cf74f9b4ac99ee20bc95cfd3433 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 9b2865e3489c4da3b6c250abf2e73e80 | | service_name | neutron | | service_type | network | | url | http://controller.openstack.fjf:9696 | +--------------+--------------------------------------+ 安装网络组件，有两种Provider networks和Self-service networks，Self-service networks提供L3（三层路由）的能力，因为网络组件部署后无法更改，所以使用Self-service networks，其可以涵盖更多的网络类型方便以后网络变化\n1 yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y 编辑配置文件\n编辑/etc/neutron/neutron.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [database] # ... connection = mysql+pymysql://neutron:Pass@w0rd@controller.openstack.fjf/neutron [DEFAULT] # ... core_plugin = ml2 service_plugins = router allow_overlapping_ips = true transport_url = rabbit://openstack:Pass@w0rd@controller.openstack.fjf auth_strategy = keystone notify_nova_on_port_status_changes = true notify_nova_on_port_data_changes = true [keystone_authtoken] # ... auth_uri = http://controller.openstack.fjf:5000 auth_url = http://controller.openstack.fjf:35357 memcached_servers = controller.openstack.fjf:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = Pass@w0rd [nova] # ... auth_url = http://controller.openstack.fjf:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = Pass@w0rd [oslo_concurrency] # ... lock_path = /var/lib/neutron/tmp 编辑/etc/neutron/plugins/ml2/ml2_conf.ini\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [ml2] # ... type_drivers = flat,vlan,vxlan tenant_network_types = vxlan mechanism_drivers = linuxbridge,l2population extension_drivers = port_security [ml2_type_flat] # ... flat_networks = provider [ml2_type_vlan] # ... network_vlan_ranges = physvlan:1:1000 [ml2_type_vxlan] # ... vni_ranges = 1:1000 [securitygroup] # ... enable_ipset = true 编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini\n1 2 3 4 5 6 7 8 9 10 11 12 [linux_bridge] physical_interface_mappings = provider:eth0 [vxlan] enable_vxlan = true local_ip = 192.168.81.40 l2_population = true [securitygroup] # ... enable_security_group = true firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 编辑/etc/neutron/l3_agent.ini\n1 2 3 [DEFAULT] # ... interface_driver = linuxbridge 编辑/etc/neutron/dhcp_agent.ini\n1 2 3 4 5 [DEFAULT] # ... interface_driver = linuxbridge dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true 编辑/etc/neutron/metadata_agent.ini\n1 2 3 4 5 [DEFAULT] # ... nova_metadata_host = controller.openstack.fjf # 共享密钥随机生成一个 metadata_proxy_shared_secret = RpP55i3dSRRoHq8x 编辑/etc/nova/nova.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 [neutron] # ... url = http://controller.openstack.fjf:9696 auth_url = http://controller.openstack.fjf:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = Pass@w0rd service_metadata_proxy = true metadata_proxy_shared_secret = RpP55i3dSRRoHq8x 创建配置链接\n1 ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 初始化数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 [root@controller ~]# su -s /bin/sh -c \u0026#34;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\u0026#34; neutron INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. Running upgrade for neutron ... INFO [alembic.runtime.migration] Context impl MySQLImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running upgrade -\u0026gt; kilo, kilo_initial INFO [alembic.runtime.migration] Running upgrade kilo -\u0026gt; 354db87e3225, nsxv_vdr_metadata.py INFO [alembic.runtime.migration] Running upgrade 354db87e3225 -\u0026gt; 599c6a226151, neutrodb_ipam INFO [alembic.runtime.migration] Running upgrade 599c6a226151 -\u0026gt; 52c5312f6baf, Initial operations in support of address scopes INFO [alembic.runtime.migration] Running upgrade 52c5312f6baf -\u0026gt; 313373c0ffee, Flavor framework INFO [alembic.runtime.migration] Running upgrade 313373c0ffee -\u0026gt; 8675309a5c4f, network_rbac INFO [alembic.runtime.migration] Running upgrade 8675309a5c4f -\u0026gt; 45f955889773, quota_usage INFO [alembic.runtime.migration] Running upgrade 45f955889773 -\u0026gt; 26c371498592, subnetpool hash INFO [alembic.runtime.migration] Running upgrade 26c371498592 -\u0026gt; 1c844d1677f7, add order to dnsnameservers INFO [alembic.runtime.migration] Running upgrade 1c844d1677f7 -\u0026gt; 1b4c6e320f79, address scope support in subnetpool INFO [alembic.runtime.migration] Running upgrade 1b4c6e320f79 -\u0026gt; 48153cb5f051, qos db changes INFO [alembic.runtime.migration] Running upgrade 48153cb5f051 -\u0026gt; 9859ac9c136, quota_reservations INFO [alembic.runtime.migration] Running upgrade 9859ac9c136 -\u0026gt; 34af2b5c5a59, Add dns_name to Port INFO [alembic.runtime.migration] Running upgrade 34af2b5c5a59 -\u0026gt; 59cb5b6cf4d, Add availability zone INFO [alembic.runtime.migration] Running upgrade 59cb5b6cf4d -\u0026gt; 13cfb89f881a, add is_default to subnetpool INFO [alembic.runtime.migration] Running upgrade 13cfb89f881a -\u0026gt; 32e5974ada25, Add standard attribute table INFO [alembic.runtime.migration] Running upgrade 32e5974ada25 -\u0026gt; ec7fcfbf72ee, Add network availability zone INFO [alembic.runtime.migration] Running upgrade ec7fcfbf72ee -\u0026gt; dce3ec7a25c9, Add router availability zone INFO [alembic.runtime.migration] Running upgrade dce3ec7a25c9 -\u0026gt; c3a73f615e4, Add ip_version to AddressScope INFO [alembic.runtime.migration] Running upgrade c3a73f615e4 -\u0026gt; 659bf3d90664, Add tables and attributes to support external DNS integration INFO [alembic.runtime.migration] Running upgrade 659bf3d90664 -\u0026gt; 1df244e556f5, add_unique_ha_router_agent_port_bindings INFO [alembic.runtime.migration] Running upgrade 1df244e556f5 -\u0026gt; 19f26505c74f, Auto Allocated Topology - aka Get-Me-A-Network INFO [alembic.runtime.migration] Running upgrade 19f26505c74f -\u0026gt; 15be73214821, add dynamic routing model data INFO [alembic.runtime.migration] Running upgrade 15be73214821 -\u0026gt; b4caf27aae4, add_bgp_dragent_model_data INFO [alembic.runtime.migration] Running upgrade b4caf27aae4 -\u0026gt; 15e43b934f81, rbac_qos_policy INFO [alembic.runtime.migration] Running upgrade 15e43b934f81 -\u0026gt; 31ed664953e6, Add resource_versions row to agent table INFO [alembic.runtime.migration] Running upgrade 31ed664953e6 -\u0026gt; 2f9e956e7532, tag support INFO [alembic.runtime.migration] Running upgrade 2f9e956e7532 -\u0026gt; 3894bccad37f, add_timestamp_to_base_resources INFO [alembic.runtime.migration] Running upgrade 3894bccad37f -\u0026gt; 0e66c5227a8a, Add desc to standard attr table INFO [alembic.runtime.migration] Running upgrade 0e66c5227a8a -\u0026gt; 45f8dd33480b, qos dscp db addition INFO [alembic.runtime.migration] Running upgrade 45f8dd33480b -\u0026gt; 5abc0278ca73, Add support for VLAN trunking INFO [alembic.runtime.migration] Running upgrade 5abc0278ca73 -\u0026gt; d3435b514502, Add device_id index to Port INFO [alembic.runtime.migration] Running upgrade d3435b514502 -\u0026gt; 30107ab6a3ee, provisioning_blocks.py INFO [alembic.runtime.migration] Running upgrade 30107ab6a3ee -\u0026gt; c415aab1c048, add revisions table INFO [alembic.runtime.migration] Running upgrade c415aab1c048 -\u0026gt; a963b38d82f4, add dns name to portdnses INFO [alembic.runtime.migration] Running upgrade kilo -\u0026gt; 30018084ec99, Initial no-op Liberty contract rule. INFO [alembic.runtime.migration] Running upgrade 30018084ec99 -\u0026gt; 4ffceebfada, network_rbac INFO [alembic.runtime.migration] Running upgrade 4ffceebfada -\u0026gt; 5498d17be016, Drop legacy OVS and LB plugin tables INFO [alembic.runtime.migration] Running upgrade 5498d17be016 -\u0026gt; 2a16083502f3, Metaplugin removal INFO [alembic.runtime.migration] Running upgrade 2a16083502f3 -\u0026gt; 2e5352a0ad4d, Add missing foreign keys INFO [alembic.runtime.migration] Running upgrade 2e5352a0ad4d -\u0026gt; 11926bcfe72d, add geneve ml2 type driver INFO [alembic.runtime.migration] Running upgrade 11926bcfe72d -\u0026gt; 4af11ca47297, Drop cisco monolithic tables INFO [alembic.runtime.migration] Running upgrade 4af11ca47297 -\u0026gt; 1b294093239c, Drop embrane plugin table INFO [alembic.runtime.migration] Running upgrade 1b294093239c -\u0026gt; 8a6d8bdae39, standardattributes migration INFO [alembic.runtime.migration] Running upgrade 8a6d8bdae39 -\u0026gt; 2b4c2465d44b, DVR sheduling refactoring INFO [alembic.runtime.migration] Running upgrade 2b4c2465d44b -\u0026gt; e3278ee65050, Drop NEC plugin tables INFO [alembic.runtime.migration] Running upgrade e3278ee65050 -\u0026gt; c6c112992c9, rbac_qos_policy INFO [alembic.runtime.migration] Running upgrade c6c112992c9 -\u0026gt; 5ffceebfada, network_rbac_external INFO [alembic.runtime.migration] Running upgrade 5ffceebfada -\u0026gt; 4ffceebfcdc, standard_desc INFO [alembic.runtime.migration] Running upgrade 4ffceebfcdc -\u0026gt; 7bbb25278f53, device_owner_ha_replicate_int INFO [alembic.runtime.migration] Running upgrade 7bbb25278f53 -\u0026gt; 89ab9a816d70, Rename ml2_network_segments table INFO [alembic.runtime.migration] Running upgrade a963b38d82f4 -\u0026gt; 3d0e74aa7d37, Add flavor_id to Router INFO [alembic.runtime.migration] Running upgrade 3d0e74aa7d37 -\u0026gt; 030a959ceafa, uniq_routerports0port_id INFO [alembic.runtime.migration] Running upgrade 030a959ceafa -\u0026gt; a5648cfeeadf, Add support for Subnet Service Types INFO [alembic.runtime.migration] Running upgrade a5648cfeeadf -\u0026gt; 0f5bef0f87d4, add_qos_minimum_bandwidth_rules INFO [alembic.runtime.migration] Running upgrade 0f5bef0f87d4 -\u0026gt; 67daae611b6e, add standardattr to qos policies INFO [alembic.runtime.migration] Running upgrade 89ab9a816d70 -\u0026gt; c879c5e1ee90, Add segment_id to subnet INFO [alembic.runtime.migration] Running upgrade c879c5e1ee90 -\u0026gt; 8fd3918ef6f4, Add segment_host_mapping table. INFO [alembic.runtime.migration] Running upgrade 8fd3918ef6f4 -\u0026gt; 4bcd4df1f426, Rename ml2_dvr_port_bindings INFO [alembic.runtime.migration] Running upgrade 4bcd4df1f426 -\u0026gt; b67e765a3524, Remove mtu column from networks. INFO [alembic.runtime.migration] Running upgrade 67daae611b6e -\u0026gt; 6b461a21bcfc, uniq_floatingips0floating_network_id0fixed_port_id0fixed_ip_addr INFO [alembic.runtime.migration] Running upgrade 6b461a21bcfc -\u0026gt; 5cd92597d11d, Add ip_allocation to port INFO [alembic.runtime.migration] Running upgrade 5cd92597d11d -\u0026gt; 929c968efe70, add_pk_version_table INFO [alembic.runtime.migration] Running upgrade 929c968efe70 -\u0026gt; a9c43481023c, extend_pk_with_host_and_add_status_to_ml2_port_binding INFO [alembic.runtime.migration] Running upgrade a9c43481023c -\u0026gt; 804a3c76314c, Add data_plane_status to Port INFO [alembic.runtime.migration] Running upgrade 804a3c76314c -\u0026gt; 2b42d90729da, qos add direction to bw_limit_rule table INFO [alembic.runtime.migration] Running upgrade 2b42d90729da -\u0026gt; 62c781cb6192, add is default to qos policies INFO [alembic.runtime.migration] Running upgrade 62c781cb6192 -\u0026gt; c8c222d42aa9, logging api INFO [alembic.runtime.migration] Running upgrade c8c222d42aa9 -\u0026gt; 349b6fd605a6, Add dns_domain to portdnses INFO [alembic.runtime.migration] Running upgrade 349b6fd605a6 -\u0026gt; 7d32f979895f, add mtu for networks INFO [alembic.runtime.migration] Running upgrade 7d32f979895f -\u0026gt; 594422d373ee, fip qos INFO [alembic.runtime.migration] Running upgrade b67e765a3524 -\u0026gt; a84ccf28f06a, migrate dns name from port INFO [alembic.runtime.migration] Running upgrade a84ccf28f06a -\u0026gt; 7d9d8eeec6ad, rename tenant to project INFO [alembic.runtime.migration] Running upgrade 7d9d8eeec6ad -\u0026gt; a8b517cff8ab, Add routerport bindings for L3 HA INFO [alembic.runtime.migration] Running upgrade a8b517cff8ab -\u0026gt; 3b935b28e7a0, migrate to pluggable ipam INFO [alembic.runtime.migration] Running upgrade 3b935b28e7a0 -\u0026gt; b12a3ef66e62, add standardattr to qos policies INFO [alembic.runtime.migration] Running upgrade b12a3ef66e62 -\u0026gt; 97c25b0d2353, Add Name and Description to the networksegments table INFO [alembic.runtime.migration] Running upgrade 97c25b0d2353 -\u0026gt; 2e0d7a8a1586, Add binding index to RouterL3AgentBinding INFO [alembic.runtime.migration] Running upgrade 2e0d7a8a1586 -\u0026gt; 5c85685d616d, Remove availability ranges. 重启Nova服务\n1 systemctl restart openstack-nova-api.service 启动网络服务\n1 2 systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service systemctl start neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service 安装dashboard 安装Dashboard(WEB管理控制台)\n安装组件\n1 yum install openstack-dashboard -y 编辑配置/etc/openstack-dashboard/local_settings，如果是在控制节点上添加计算服务则不需要修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 OPENSTACK_HOST = \u0026#34;controller.openstack.fjf\u0026#34; ALLOWED_HOSTS = [\u0026#39;controller.openstack.fjf\u0026#39;, \u0026#39;192.168.81.40\u0026#39;,\u0026#39;localhost\u0026#39;] SESSION_ENGINE = \u0026#39;django.contrib.sessions.backends.cache\u0026#39; CACHES = { \u0026#39;default\u0026#39;: { \u0026#39;BACKEND\u0026#39;: \u0026#39;django.core.cache.backends.memcached.MemcachedCache\u0026#39;, \u0026#39;LOCATION\u0026#39;: \u0026#39;controller.openstack.fjf:11211\u0026#39;, } } OPENSTACK_KEYSTONE_URL = \u0026#34;http://%s:5000/v3\u0026#34; % OPENSTACK_HOST OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True OPENSTACK_API_VERSIONS = { \u0026#34;identity\u0026#34;: 3, \u0026#34;image\u0026#34;: 2, \u0026#34;volume\u0026#34;: 2, } OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \u0026#34;Default\u0026#34; OPENSTACK_KEYSTONE_DEFAULT_ROLE = \u0026#34;admin\u0026#34; OPENSTACK_NEUTRON_NETWORK = { ... \u0026#39;enable_router\u0026#39;: False, \u0026#39;enable_quotas\u0026#39;: False, \u0026#39;enable_distributed_router\u0026#39;: False, \u0026#39;enable_ha_router\u0026#39;: False, \u0026#39;enable_lb\u0026#39;: False, \u0026#39;enable_firewall\u0026#39;: False, \u0026#39;enable_vpn\u0026#39;: False, \u0026#39;enable_fip_topology_check\u0026#39;: False, } TIME_ZONE = \u0026#34;Asia/Shanghai\u0026#34; 重启服务\n1 systemctl restart httpd.service memcached.service 访问控制台：http://192.168.81.40/dashboard 账号/密码\n1 2 3 4 5 cat admin-openrc # 这个账号密码就是用来登陆控制台的，如果要求输入域则参考OS_USER_DOMAIN_NAME export OS_USERNAME=admin export OS_PASSWORD=Pass@w0rd export OS_USER_DOMAIN_NAME=Default 下载并导入镜像 导入镜像\n下载镜像\n1 2 3 4 5 6 #测试镜像：cirros，最小的一种镜像，只有16M，方便测试 wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img #Centos7镜像 http://cloud.centos.org/centos/7/images/ #其他类型镜像 https://docs.openstack.org/image-guide/obtain-images.html 加载admin-openrc\n1 . admin-openrc 导入镜像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 openstack image create \u0026#34;cirros\u0026#34; --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2 --container-format bare --public +------------------+------------------------------------------------------+ | Field | Value | +------------------+------------------------------------------------------+ | checksum | 1d3062cd89af34e419f7100277f38b2b | | container_format | bare | | created_at | 2021-03-26T11:09:11Z | | disk_format | qcow2 | | file | /v2/images/b3befb16-19c7-4775-ac0a-1961ff269461/file | | id | b3befb16-19c7-4775-ac0a-1961ff269461 | | min_disk | 0 | | min_ram | 0 | | name | cirros | | owner | 750d6512fc3043d1b1c6d832465cbf9f | | protected | False | | schema | /v2/schemas/image | | size | 16338944 | | status | active | | tags | | | updated_at | 2021-03-26T11:09:11Z | | virtual_size | None | | visibility | public | +------------------+------------------------------------------------------+ 创建网络\u0026amp;虚拟机 创建一个桥接网络，Flat，管理员 -\u0026gt; 网络 -\u0026gt; 网络 -\u0026gt; 创建网络 物理网络名称通过：cat /etc/neutron/plugins/ml2/ml2_conf.ini |grep flat_networks 获取 配置网络信息 创建实例类型，管理员 -\u0026gt; 计算 -\u0026gt; 实例类型 -\u0026gt; 创建实例类型 创建密钥对，项目 -\u0026gt; 计算 -\u0026gt; 密钥对 -\u0026gt; 导入公钥，创建完会自动下载私钥，也可以导入已经存在的密钥对公钥 可以进入控制台 通过SSH连接 ","date":"2024-10-25T16:28:51Z","image":"https://www.ownit.top/title_pic/01.jpg","permalink":"https://www.ownit.top/p/202410251628/","title":"机房私有云OpenStack搭建详细步骤流程"},{"content":"项目背景 OpenStack，作为一个开源的云计算平台，经常被用于构建私有云和公有云服务。然而，随着业务的发展和扩展，企业可能会面临将在OpenStack上运行的虚拟机迁移到其他云服务供应商的需求\n需求 因为运营团队在本地机房有一台 OpenStack中的虚拟机业务，负责邮件发送，跑定时任务等等。但是由于机房迁移，会导致机房服务中断。但由于考虑业务继续跑，需要把这台机器 迁移到阿里云的环境。但是由于依赖和环境众多，差异性太大，需要有个简便的方式完成迁移。\n环境介绍 环境介绍\n源环境：OpenStack 版本（适用于其他版本，但步骤可能略有不同） 目标环境：阿里云ECS 操作系统：以Centos7 为例（其他Linux发行版的操作可能类似） 必要工具：qemu-img（用于转换镜像格式） 准备工作 在开始之前，确保您有以下准备：\n确认您有足够的权限来访问OpenStack环境和阿里云账户。 安装qemu-img工具，这通常可以在Linux发行版的官方仓库中找到。 确保有足够的本地存储空间来保存导出的QCOW2镜像文件。 导出OpenStack虚拟机镜像 首先排查这台机器虚拟机在那台OpenStack宿主机上， 比如我这台OpenStack虚拟机在 compute4.openstack.fjf， 虚拟机id：e7c5b097-e842-4db1-849b-fd4af3cb9380 进入工作目录 进入compute4.openstack.fjf 这台OpenStack 的node节点，在进入下方目录\n1 2 3 cd /var/lib/nova/instaces/$your_instance_id$ 这个是我的 cd /var/lib/nova/instances/e7c5b097-e842-4db1-849b-fd4af3cb9380/ 导出运行的镜像 使用qemu-img工具将OpenStack的QCOW2镜像转换为适合阿里云的格式：\n如果保存，请关闭虚拟机在试试\n1 2 3 4 5 6 7 [dev][root@compute4-192.168.81.14 e7c5b097-e842-4db1-849b-fd4af3cb9380]# qemu-img convert -c -O qcow2 disk test.qcow2 qemu-img: Could not open \u0026#39;disk\u0026#39;: Failed to get shared \u0026#34;write\u0026#34; lock Is another process using the image [disk]? #这个正常的 qemu-img convert -c -O qcow2 disk test.qcow2 #这个命令会创建一个新的压缩的QCOW2镜像文件 传镜像到阿里云OSS 在导入镜像到阿里云ECS之前，您需要先将镜像上传到阿里云的对象存储服务（OSS）。\n登录到阿里云控制台。 创建一个OSS Bucket。 使用OSS的上传功能或者OSS提供的命令行工具ossutil上传您的QCOW2镜像文件。 导入镜像到阿里云ECS 一旦镜像上传到OSS，您可以通过阿里云ECS控制台导入镜像：\n在ECS控制台中，找到“镜像和模板”部分。 选择“导入镜像”。 提供OSS中镜像的URL，以及其他必要的信息。 启动导入任务。 阿里云会处理镜像的导入过程，这可能需要一些时间。 创建ECS实例 导入镜像完成后，您可以使用该镜像创建新的ECS实例：\n在ECS控制台中，选择“实例”。 点击“创建实例”。 在创建向导中，选择您刚刚导入的镜像作为基础。 完成实例的配置，包括选择实例类型、配置网络和安全组等。 启动实例。 总结 将OpenStack环境中的虚拟机镜像成功迁移到阿里云ECS。这个过程不仅增强了云资源的可移植性，而且为企业提供了更多的灵活性和选择权。无论是为了成本优化、性能提升还是遵循合规性要求，这种迁移策略都是现代云基础设施管理不可或缺的一部分。\n","date":"2024-10-23T18:44:38Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/202410231844/","title":"OpenStack将运行的系统导出 QCOW2 镜像并导入阿里云"},{"content":" HAProxy + Keepalived 部署高可用性入口： 部署两台或多台节点运行 HAProxy 作为负载均衡器。 使用 Keepalived 实现 VIP（虚拟 IP），为 HAProxy 提供高可用性。Keepalived 会监控 HAProxy 的状态，如果主节点失效，VIP 会自动漂移到备用节点，确保高可用性。 HAProxy 配置： HAProxy 会作为 Kubernetes 的入口，转发流量到 Kubernetes 集群的 apiserver。 配置 HAProxy 负载均衡规则，分发流量到多个 Kubernetes master 节点的 API server。 Keepalived 配置： Keepalived 会监控 HAProxy 的状态，确保在节点失效时自动切换到备用服务器，保障服务连续性。 配置高优先级的主节点和低优先级的备份节点，当主节点不可用时，VIP 切换到备份节点。 办公网络打通： 办公网络通过 VPN 或路由器配置，将办公网络和 Kubernetes 集群的网络打通。 确保办公网络能够通过 VIP 访问 Kubernetes 集群服务，包括 API server 和集群内的其他服务。 环境准备 环境 系统版本：CentOS 7.5.1804 Kubernetes 版本：v1.23.1 Calico 版本：v3.8.0\n机器 主机名 IP 角色 安装软件 k8s-test-lvs-1.fjf 192.168.83.15/vip 192.168.83.3 高可用\u0026amp;负载均衡 Master keepalive、haproxy k8s-test-lvs-2.fjf 192.168.83.26/vip 192.168.83.3 高可用\u0026amp;负载均衡 Slave keepalive、haproxy k8s-test-master-1.fjf 192.168.83.36 Kubernetes Master Node kubernetes k8s-test-master-2.fjf 192.168.83.54 Kubernetes Master Node kubernetes k8s-test-master-3.fjf 192.168.83.49 Kubernetes Master Node kubernetes k8s-test-node-1.fjf 192.168.83.52 Kubernetes Worker Node kubernetes k8s-test-node-2.fjf 192.168.83.22 Kubernetes Worker Node kubernetes k8s-test-node-3.fjf 192.168.83.37 Kubernetes Worker Node kubernetes 网络规划 名称 网络 备注 节点网络 192.168.83.0/24 容器宿主机所用网络 Pod网络 172.15.0.0/16 容器网络 Svc 网络 172.16.0.0/16 服务网络 系统配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 所有节点关闭Selinux sed -i \u0026#34;/SELINUX/ s/enforcing/disabled/g\u0026#34; /etc/selinux/config # 所有节点关闭防火墙 systemctl disable firewalld # 所有节点配置内核参数 cat \u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt;EOF net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1 vm.swappiness = 0 net.ipv4.neigh.default.gc_stale_time=120 # see details in https://help.aliyun.com/knowledge_detail/39428.html net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 net.ipv4.conf.default.arp_announce = 2 net.ipv4.conf.lo.arp_announce=2 net.ipv4.conf.all.arp_announce=2 # see details in https://help.aliyun.com/knowledge_detail/41334.html net.ipv4.tcp_max_tw_buckets = 5000 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 1024 net.ipv4.tcp_synack_retries = 2 net.ipv4.tcp_fin_timeout = 30 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF 如果是在OpenStack中部署，需要关闭所有主机的端口安全组，如果不关闭，pod网络和svc网络将无法正常通信 在openstack控制节点执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 所有节点都需要操作，可以使用for循环批量操作，下面是单主机移除步骤 # 查看端口ID neutron port-list| grep 192.168.83.15 | ea065e6c-65d9-457b-a68e-282653c890e5 | | 9f83fb35aed1422588096b578cc01341 | fa:16:3e:bd:41:81 | {\u0026#34;subnet_id\u0026#34;: \u0026#34;710ffde5-d820-4a30-afe2-1dfd6f40e288\u0026#34;, \u0026#34;ip_address\u0026#34;: \u0026#34;192.168.83.15\u0026#34;} | # 移除安全组 neutron port-update --no-security-groups ea065e6c-65d9-457b-a68e-282653c890e5 Updated port: ea065e6c-65d9-457b-a68e-282653c890e5 # 关闭端口安全组 neutron port-update ea065e6c-65d9-457b-a68e-282653c890e5 --port-security-enabled=False Updated port: ea065e6c-65d9-457b-a68e-282653c890e5 # 查看状态 neutron port-show fcf46a43-5a72-4a28-8e57-1eb04ae24c42 +-----------------------+--------------------------------------------------------------------------------------+ | Field | Value | +-----------------------+--------------------------------------------------------------------------------------+ | admin_state_up | True | | allowed_address_pairs | | | binding:host_id | compute6.openstack.fjf | | binding:profile | {} | | binding:vif_details | {\u0026#34;port_filter\u0026#34;: true} | | binding:vif_type | bridge | | binding:vnic_type | normal | | created_at | 2021-07-01T10:13:32Z | | description | | | device_id | 0a10a372-95fa-4b95-a036-ee43675f1ff4 | | device_owner | compute:nova | | extra_dhcp_opts | | | fixed_ips | {\u0026#34;subnet_id\u0026#34;: \u0026#34;710ffde5-d820-4a30-afe2-1dfd6f40e288\u0026#34;, \u0026#34;ip_address\u0026#34;: \u0026#34;192.168.83.22\u0026#34;} | | id | fcf46a43-5a72-4a28-8e57-1eb04ae24c42 | | mac_address | fa:16:3e:3f:1d:a8 | | name | | | network_id | 02a8d505-af1e-4da5-af08-ed5ea7600293 | | port_security_enabled | False | # 此项为False则代表端口安全组关闭 | project_id | 9f83fb35aed1422588096b578cc01341 | | revision_number | 10 | | security_groups | | # 此项必须为空 | status | ACTIVE | | tags | | | tenant_id | 9f83fb35aed1422588096b578cc01341 | | updated_at | 2021-07-01T10:13:42Z | +-----------------------+--------------------------------------------------------------------------------------+ 部署haproxy+keepalived [192.168.83.15,192.168.83.26]安装haproxy+keepalived\n1 yum -y install haproxy keepalived [192.168.83.15,192.168.83.26]配置haproxy\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 cat \u0026gt; /etc/haproxy/haproxy.cfg \u0026lt;\u0026lt;EOF global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 5000 defaults mode tcp option redispatch option abortonclose timeout connect 5000s timeout client 50000s timeout server 50000s log 127.0.0.1 local0 balance roundrobin maxconn 50000 listen admin_stats 0.0.0.0:50101 mode http stats uri / stats realm Global\\ statistics stats auth haproxy:password stats hide-version stats admin if TRUE listen KubeApi bind 0.0.0.0:6443 mode tcp server KubeMaster1 192.168.83.36:6443 weight 1 check port 6443 inter 12000 rise 1 fall 3 server KubeMaster2 192.168.83.54:6443 weight 1 check port 6443 inter 12000 rise 1 fall 3 server KubeMaster3 192.168.83.49:6443 weight 1 check port 6443 inter 12000 rise 1 fall 3 EOF [192.168.83.15]配置keepalived\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt;EOF ! Configuration File for keepalived global_defs { router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 52 priority 100 advert_int 1 authentication { auth_type PASS auth_pass test-pass } virtual_ipaddress { 192.168.83.3 } } EOF [192.168.83.26]配置keepalived\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt;EOF ! Configuration File for keepalived global_defs { router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state BACKUP # 与master值不同 interface eth0 virtual_router_id 52 priority 99 # 与master值不同 advert_int 1 authentication { auth_type PASS auth_pass test-pass } virtual_ipaddress { 192.168.83.3 } } EOF [192.168.83.15,192.168.83.26]启动服务\n1 2 3 4 systemctl start haproxy keepalived systemctl enable haproxy keepalived Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service. Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service. 查看管理后台，http://192.168.83.3:50101/，账号密码在haproxy配置文件中stats auth字段。 此时所有后端处于Down的状态属于正常情况，因为kubernetes master节点还没有部署 部署kubernetes master [192.168.83.36,192.168.83.54,192.168.83.49]添加Yum源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 移除可能存在的组件 yum remove docker docker-common docker-selinux docker-engine # 安装一些依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 # 配置Docker CE 源 wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo sudo sed -i \u0026#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+\u0026#39; /etc/yum.repos.d/docker-ce.repo # Kubernetes cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 重建缓存 yum makecache fast [192.168.83.36,192.168.83.54,192.168.83.49]安装\n1 yum -y install docker-ce kubelet-1.23.1 kubeadm-1.23.1 kubectl-1.23.1 [192.168.83.36,192.168.83.54,192.168.83.49]添加docker配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 mkdir /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://xxxxxx.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;oom-score-adjust\u0026#34;: -1000, \u0026#34;live-restore\u0026#34;: true } EOF [192.168.83.36,192.168.83.54,192.168.83.49]启动服务\n1 2 3 4 systemctl start docker kubelet systemctl enable docker kubelet Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service. [192.168.83.36]创建集群初始化配置文件\n1 2 3 4 5 6 7 8 9 10 11 cat \u0026gt; /etc/kubernetes/kubeadm-config.yaml \u0026lt;\u0026lt;EOF apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: stable controlPlaneEndpoint: \u0026#34;192.168.83.3:6443\u0026#34; #VIP，浮动IP，端口固定6443不要修改 kubernetesVersion: v1.15.0 networking: podSubnet: 172.15.0.0/16 serviceSubnet: 172.16.0.0/16 dnsDomain: k8s-test.fjf # 默认值：cluster.local,在使用svc网络是可以使用此后缀,需要dns指定到k8s内部dns才能完成解析 EOF [192.168.83.36,192.168.83.54,192.168.83.49]手动下载镜像，加速集群初始化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 KUBE_VERSION=v1.15.0 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.3.10 CORE_DNS_VERSION=1.3.1 GCR_URL=k8s.gcr.io ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${CORE_DNS_VERSION}) for imageName in ${images[@]} ; do docker pull $ALIYUN_URL/$imageName docker tag $ALIYUN_URL/$imageName $GCR_URL/$imageName docker rmi $ALIYUN_URL/$imageName done [192.168.83.36]初始化集群\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 # --upload-certs 用于上传证书到集群，其他节点就不需要手动复制证书了 # tee kubeadm-init.log 保存屏幕输出信息，后面节点加入需要此信息 kubeadm init --config=/etc/kubernetes/kubeadm-config.yaml --upload-certs | tee kubeadm-init.log # 以下，输出信息 W0705 18:04:22.814808 11579 strict.go:54] error unmarshaling configuration schema.GroupVersionKind{Group:\u0026#34;kubeadm.k8s.io\u0026#34;, Version:\u0026#34;v1beta2\u0026#34;, Kind:\u0026#34;ClusterConfiguration\u0026#34;}: error converting YAML to JSON: yaml: unmarshal errors: line 5: key \u0026#34;kubernetesVersion\u0026#34; already set in map [init] Using Kubernetes version: v1.15.0 [preflight] Running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.7. Latest validated version: 18.09 [WARNING Hostname]: hostname \u0026#34;k8s-test-master-1.fjf\u0026#34; could not be reached [WARNING Hostname]: hostname \u0026#34;k8s-test-master-1.fjf\u0026#34;: lookup k8s-test-master-1.fjf on 192.168.81.10:53: no such host [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-test-master-1.fjf localhost] and IPs [192.168.83.36 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-test-master-1.fjf localhost] and IPs [192.168.83.36 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-test-master-1.fjf kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.k8s-test.fjf] and IPs [172.16.0.1 192.168.83.36 192.168.83.3] [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 18.005364 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.15\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Storing the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [upload-certs] Using certificate key: d3123a4998a8715f1c12830d7d54a63aa790fcf3da0cf4b5e7514cd7ffd6e200 [mark-control-plane] Marking the node k8s-test-master-1.fjf as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node k8s-test-master-1.fjf as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: mpzs9m.oec6ixeesemzbxle [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: # Master加入 kubeadm join 192.168.83.3:6443 --token mpzs9m.oec6ixeesemzbxle \\ --discovery-token-ca-cert-hash sha256:7c05c8001693061902d6f20947fbc60c1b6a12e9ded449e6c59a71e6448fac5d \\ --experimental-control-plane --certificate-key d3123a4998a8715f1c12830d7d54a63aa790fcf3da0cf4b5e7514cd7ffd6e200 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: # Worker节点接入 kubeadm join 192.168.83.3:6443 --token mpzs9m.oec6ixeesemzbxle \\ --discovery-token-ca-cert-hash sha256:7c05c8001693061902d6f20947fbc60c1b6a12e9ded449e6c59a71e6448fac5d [192.168.83.54,192.168.83.49]其他master节点加入集群\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 # 此命令直接复制上一步输出的结果就可以了，注意区分master节点和worker节点的指令是不同的 kubeadm join 192.168.83.3:6443 --token mpzs9m.oec6ixeesemzbxle \\ --discovery-token-ca-cert-hash sha256:7c05c8001693061902d6f20947fbc60c1b6a12e9ded449e6c59a71e6448fac5d \\ --experimental-control-plane --certificate-key d3123a4998a8715f1c12830d7d54a63aa790fcf3da0cf4b5e7514cd7ffd6e200 # 以下，输出信息 Flag --experimental-control-plane has been deprecated, use --control-plane instead [preflight] Running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.7. Latest validated version: 18.09 [WARNING Hostname]: hostname \u0026#34;k8s-test-master-2.fjf\u0026#34; could not be reached [WARNING Hostname]: hostname \u0026#34;k8s-test-master-2.fjf\u0026#34;: lookup k8s-test-master-2.fjf on 192.168.81.10:53: no such host [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [preflight] Running pre-flight checks before initializing the new control plane instance [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [download-certs] Downloading the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-test-master-2.fjf localhost] and IPs [192.168.83.54 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-test-master-2.fjf localhost] and IPs [192.168.83.54 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-test-master-2.fjf kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.k8s-test.fjf] and IPs [172.16.0.1 192.168.83.54 192.168.83.3] [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Valid certificates and keys now exist in \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Using the existing \u0026#34;sa\u0026#34; key [kubeconfig] Generating kubeconfig files [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [check-etcd] Checking that the etcd cluster is healthy [kubelet-start] Downloading configuration for the kubelet from the \u0026#34;kubelet-config-1.15\u0026#34; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... [etcd] Announced new etcd member joining to the existing etcd cluster [etcd] Wrote Static Pod manifest for a local etcd member to \u0026#34;/etc/kubernetes/manifests/etcd.yaml\u0026#34; [etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [mark-control-plane] Marking the node k8s-test-master-2.fjf as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node k8s-test-master-2.fjf as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane (master) label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Run \u0026#39;kubectl get nodes\u0026#39; to see this node join the cluster. [192.168.83.36,192.168.83.54,192.168.83.49]创建集群配置文件\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 检查集群状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kubectl get node NAME STATUS ROLES AGE VERSION k8s-test-master-1.fjf NotReady master 7m19s v1.15.0 k8s-test-master-2.fjf NotReady master 112s v1.15.0 k8s-test-master-3.fjf NotReady master 62s v1.15.0 kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5c98db65d4-6kxg6 0/1 Pending 0 7m30s # Pending状态正常，在安装网络组件后即可恢复 kube-system coredns-5c98db65d4-x6x4b 0/1 Pending 0 7m30s # Pending状态正常，在安装网络组件后即可恢复 kube-system etcd-k8s-test-master-1.fjf 1/1 Running 0 6m37s kube-system etcd-k8s-test-master-2.fjf 1/1 Running 0 2m22s kube-system etcd-k8s-test-master-3.fjf 1/1 Running 0 93s kube-system kube-apiserver-k8s-test-master-1.fjf 1/1 Running 0 6m31s kube-system kube-apiserver-k8s-test-master-2.fjf 1/1 Running 0 2m22s kube-system kube-apiserver-k8s-test-master-3.fjf 1/1 Running 1 91s kube-system kube-controller-manager-k8s-test-master-1.fjf 1/1 Running 1 6m47s kube-system kube-controller-manager-k8s-test-master-2.fjf 1/1 Running 0 2m22s kube-system kube-controller-manager-k8s-test-master-3.fjf 1/1 Running 0 22s kube-system kube-proxy-hrfgq 1/1 Running 0 2m23s kube-system kube-proxy-nlm68 1/1 Running 0 93s kube-system kube-proxy-tt8dg 1/1 Running 0 7m30s kube-system kube-scheduler-k8s-test-master-1.fjf 1/1 Running 1 6m28s kube-system kube-scheduler-k8s-test-master-2.fjf 1/1 Running 0 2m22s kube-system kube-scheduler-k8s-test-master-3.fjf 1/1 Running 0 21s 查看管理后台，http://192.168.83.3:50101/，账号密码在haproxy配置文件中stats auth字段。 所有Master节点状态为UP则为正常，如果不是，检查前面的步骤是否全部完成，并且输出内容一致 部署kubernetes 网络组件 目前主要支持的CNI组件 CNI插件：Flannel、Calico、Weave和Canal(技术上是多个插件的组合) 本次部署，使用Calico组件，可以使用改组件的BGP功能与内网打通Pod和SVC网络\n在Kubernetes集群中部署Calico，在任一Master节点执行即可 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 下载calico部署文件 wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml # 修改CIDR - name: CALICO_IPV4POOL_CIDR value: \u0026#34;172.15.0.0/16\u0026#34; #修改为Pod的网络 # 部署calico kubectl apply -f calico.yaml # 以下为输出 configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created 等待CNI生效\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 watch kubectl get pods --all-namespaces # 所有Pod状态为Running后即可退出 Every 2.0s: kubectl get pods --all-namespaces Mon Jul 5 18:49:49 2021 NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-7fc57b95d4-wsvpm 1/1 Running 0 85s kube-system calico-node-7f99k 0/1 Running 0 85s kube-system calico-node-jssb6 0/1 Running 0 85s kube-system calico-node-qgvp9 1/1 Running 0 85s kube-system coredns-5c98db65d4-6kxg6 1/1 Running 0 44m kube-system coredns-5c98db65d4-x6x4b 1/1 Running 0 44m kube-system etcd-k8s-test-master-1.fjf 1/1 Running 0 43m kube-system etcd-k8s-test-master-2.fjf 1/1 Running 0 39m kube-system etcd-k8s-test-master-3.fjf 1/1 Running 0 38m kube-system kube-apiserver-k8s-test-master-1.fjf 1/1 Running 0 43m kube-system kube-apiserver-k8s-test-master-2.fjf 1/1 Running 0 39m kube-system kube-apiserver-k8s-test-master-3.fjf 1/1 Running 1 38m kube-system kube-controller-manager-k8s-test-master-1.fjf 1/1 Running 1 44m kube-system kube-controller-manager-k8s-test-master-2.fjf 1/1 Running 0 39m kube-system kube-controller-manager-k8s-test-master-3.fjf 1/1 Running 0 37m kube-system kube-proxy-hrfgq 1/1 Running 0 39m kube-system kube-proxy-nlm68 1/1 Running 0 38m kube-system kube-proxy-tt8dg 1/1 Running 0 44m kube-system kube-scheduler-k8s-test-master-1.fjf 1/1 Running 1 43m kube-system kube-scheduler-k8s-test-master-2.fjf 1/1 Running 0 39m kube-system kube-scheduler-k8s-test-master-3.fjf 1/1 Running 0 37m 查看CoreDNS状态是否恢复\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kubectl get pod -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-7fc57b95d4-zxc4d 1/1 Running 0 88s 172.15.139.1 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-24j88 1/1 Running 0 89s 192.168.83.36 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-49vnk 1/1 Running 0 89s 192.168.83.54 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system calico-node-vzjk8 1/1 Running 0 89s 192.168.83.49 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-5c98db65d4-frfx7 1/1 Running 0 16s 172.15.159.129 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 状态已经为Running kube-system coredns-5c98db65d4-tt9w5 1/1 Running 0 27s 172.15.221.1 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 状态已经为Running kube-system etcd-k8s-test-master-1.fjf 1/1 Running 0 48m 192.168.83.36 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-k8s-test-master-2.fjf 1/1 Running 0 44m 192.168.83.54 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-k8s-test-master-3.fjf 1/1 Running 0 43m 192.168.83.49 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-k8s-test-master-1.fjf 1/1 Running 0 48m 192.168.83.36 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-k8s-test-master-2.fjf 1/1 Running 0 44m 192.168.83.54 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-k8s-test-master-3.fjf 1/1 Running 1 43m 192.168.83.49 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-k8s-test-master-1.fjf 1/1 Running 1 48m 192.168.83.36 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-k8s-test-master-2.fjf 1/1 Running 0 44m 192.168.83.54 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-k8s-test-master-3.fjf 1/1 Running 0 42m 192.168.83.49 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-hrfgq 1/1 Running 0 44m 192.168.83.54 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-nlm68 1/1 Running 0 43m 192.168.83.49 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-tt8dg 1/1 Running 0 49m 192.168.83.36 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-k8s-test-master-1.fjf 1/1 Running 1 48m 192.168.83.36 k8s-test-master-1.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-k8s-test-master-2.fjf 1/1 Running 0 44m 192.168.83.54 k8s-test-master-2.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-k8s-test-master-3.fjf 1/1 Running 0 42m 192.168.83.49 k8s-test-master-3.fjf \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 部署kuberntes node [192.168.83.52,192.168.83.22,192.168.83.37]添加Yum源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 移除可能存在的组件 yum remove docker docker-common docker-selinux docker-engine # 安装一些依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 # 配置Docker CE 源 wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo sudo sed -i \u0026#39;s+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+\u0026#39; /etc/yum.repos.d/docker-ce.repo # Kubernetes cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt;EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 重建缓存 yum makecache fast [192.168.83.52,192.168.83.22,192.168.83.37]安装\n1 2 3 yum -y install kubelet-1.23.1 kubeadm-1.23.1 kubectl-1.23.1 yum install -y docker-ce-18.09.9 docker-ce-cli-18.09.9 containerd.io [192.168.83.52,192.168.83.22,192.168.83.37]添加docker配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 mkdir /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://h3klxkkx.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;oom-score-adjust\u0026#34;: -1000, \u0026#34;live-restore\u0026#34;: true } EOF [192.168.83.52,192.168.83.22,192.168.83.37]启动服务\n1 2 3 4 systemctl start docker kubelet systemctl enable docker kubelet Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service. [192.168.83.52,192.168.83.22,192.168.83.37]手动下载镜像，加速集群初始化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 KUBE_VERSION=v1.15.0 KUBE_PAUSE_VERSION=3.1 GCR_URL=k8s.gcr.io ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers images=(kube-proxy:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION}) for imageName in ${images[@]} ; do docker pull $ALIYUN_URL/$imageName docker tag $ALIYUN_URL/$imageName $GCR_URL/$imageName docker rmi $ALIYUN_URL/$imageName done [192.168.83.52,192.168.83.22,192.168.83.37]加入集群\n1 2 # 此命令在初始化Master集群时有记录，如果找不到了，可以参考 http://wiki.fjf.com/pages/viewpage.action?pageId=14682096 kubeadm join 192.168.83.2:6443 --token wiqtis.k4g3jm9z94qykiyl --discovery-token-ca-cert-hash sha256:2771e3f29b2628edac9c5e1433dff5c3275ab870ad14ca7c9e6adaf1eed3179e 在Master节点查看node节点状态\n1 2 3 4 5 6 7 8 kubectl get node NAME STATUS ROLES AGE VERSION k8s-test-master-1.fjf Ready master 73m v1.15.0 k8s-test-master-2.fjf Ready master 68m v1.15.0 k8s-test-master-3.fjf Ready master 67m v1.15.0 k8s-test-node-1.fjf Ready \u0026lt;none\u0026gt; 75s v1.15.0 #状态为Ready即为成功加入集群 k8s-test-node-2.fjf Ready \u0026lt;none\u0026gt; 63s v1.15.0 #状态为Ready即为成功加入集群 k8s-test-node-3.fjf Ready \u0026lt;none\u0026gt; 59s v1.15.0 #状态为Ready即为成功加入集群 使用Calico打通Pod网络 现状 集群内pod\u0026amp;node可以通过pod ip直接进行访问，容器访问虚拟机没有问题，但是虚拟机不能访问容器，尤其是通过consul注册的服务，必须打通网络后才可以互相调用\n目标 打通pod和虚拟机的网络，使虚拟机可以访问pod ip 官方文档：https://docs.projectcalico.org/archive/v3.8/networking/bgp\n[Kubernetes Master节点]安装calico控制命令calicoctl\n1 2 3 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.8.9/calicoctl chmod +x calicoctl mv calicoctl /usr/bin/calicoctl [calicoctl安装节点]添加calico配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mkdir /etc/calico cat \u0026gt; /etc/calico/calicoctl.cfg \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \u0026#34;kubernetes\u0026#34; kubeconfig: \u0026#34;/root/.kube/config\u0026#34; EOF # 测试 calicoctl version Client Version: v3.8.9 Git commit: 0991d2fb Cluster Version: v3.8.9 # 出现此行代表配置正确 Cluster Type: k8s,bgp,kdd # 出现此行代表配置正确 [calicoctl安装节点]配置集群路由反射器，node节点与master节点对等、master节点彼此对等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # 在本环境下将kubernetes master节点作为反射器使用 # 查看节点信息 kubectl get node NAME STATUS ROLES AGE VERSION k8s-test-master-1.fjf Ready master 3d1h v1.15.0 k8s-test-master-2.fjf Ready master 3d1h v1.15.0 k8s-test-master-3.fjf Ready master 3d1h v1.15.0 k8s-test-node-1.fjf Ready \u0026lt;none\u0026gt; 2d23h v1.15.0 k8s-test-node-2.fjf Ready \u0026lt;none\u0026gt; 2d23h v1.15.0 k8s-test-node-3.fjf Ready \u0026lt;none\u0026gt; 2d23h v1.15.0 # 导出Master节点配置 calicoctl get node k8s-test-master-1.fjf --export -o yaml \u0026gt; k8s-test-master-1.yml calicoctl get node k8s-test-master-2.fjf --export -o yaml \u0026gt; k8s-test-master-2.yml calicoctl get node k8s-test-master-3.fjf --export -o yaml \u0026gt; k8s-test-master-3.yml # 在3个Master节点配置中添加以下配置用于标识该节点为反射器 metadata: ...... labels: ...... i-am-a-route-reflector: true ...... spec: bgp: ...... routeReflectorClusterID: 224.0.0.1 # 更新节点配置 calicoctl apply -f k8s-test-master-1.yml calicoctl apply -f k8s-test-master-2.yml calicoctl apply -f k8s-test-master-3.yml # 其他节点与反射器对等 calicoctl apply -f - \u0026lt;\u0026lt;EOF kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: peer-to-rrs spec: nodeSelector: \u0026#34;!has(i-am-a-route-reflector)\u0026#34; peerSelector: has(i-am-a-route-reflector) EOF # 反射器彼此对等 calicoctl apply -f - \u0026lt;\u0026lt;EOF kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: rr-mesh spec: nodeSelector: has(i-am-a-route-reflector) peerSelector: has(i-am-a-route-reflector) EOF [calicoctl安装节点]配置Master节点与核心交换机对等\n1 2 3 4 5 6 7 8 9 10 11 12 calicoctl apply -f - \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: rr-border spec: nodeSelector: has(i-am-a-route-reflector) peerIP: 192.168.83.1 asNumber: 64512 EOF # peerIP: 核心交换机IP # asNumber: 用于和核心交换机对等的ID [192.168.83.1,设备型号：cisco 3650]配置核心交换与Master(反射器)节点对等，这一步需要在对端BGP设备上操作，这里是用核心交换机\n1 2 3 4 5 router bgp 64512 bgp router-id 192.168.83.1 neighbor 192.168.83.36 remote-as 64512 neighbor 192.168.83.49 remote-as 64512 neighbor 192.168.83.54 remote-as 64512 查看BGP 对等状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 calicoctl node status # INFO字段全部为Established 即为正常 Calico process is running. IPv4 BGP status +---------------+---------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +---------------+---------------+-------+----------+-------------+ | 192.168.83.1 | node specific | up | 06:38:55 | Established | | 192.168.83.54 | node specific | up | 06:38:55 | Established | | 192.168.83.22 | node specific | up | 06:38:55 | Established | | 192.168.83.37 | node specific | up | 06:38:55 | Established | | 192.168.83.49 | node specific | up | 06:38:55 | Established | | 192.168.83.52 | node specific | up | 06:38:55 | Established | +---------------+---------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 测试，使用其他网段，如192.168.82.0/24的虚拟机ping 某一个pod ip，能正常通信即代表成功\n1 2 3 4 5 6 7 8 9 10 [dev][root@spring-boot-demo1-192.168.82.85 ~]# ping -c 3 172.15.190.2 PING 172.15.190.2 (172.15.190.2) 56(84) bytes of data. 64 bytes from 172.15.190.2: icmp_seq=1 ttl=62 time=0.677 ms 64 bytes from 172.15.190.2: icmp_seq=2 ttl=62 time=0.543 ms 64 bytes from 172.15.190.2: icmp_seq=3 ttl=62 time=0.549 ms --- 172.15.190.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2000ms rtt min/avg/max/mdev = 0.543/0.589/0.677/0.067 ms [dev][root@spring-boot-demo1-192.168.82.85 ~]# 使用Calico打通Svc网络 现状 一般情况下，Kuberntes集群暴露服务的方式有Ingress、NodePort、HostNetwork，这几种方式用在生产环境下是没有问题的，安全性和稳定性有保障。但是在内部开发环境下，使用起来就有诸多不便，开发希望可以直接访问自己的服务，但是Pod IP又是随机变化的，这个时候我们就可以使用SVC IP 或者SVC Name进行访问\n目标 打通SVC网络，使开发本地可以通过SVC IP 或 SVC Name访问集群服务 官方文档：https://docs.projectcalico.org/archive/v3.8/networking/service-advertisement 注意：前提是已经用BGP打通了Pod网络或已经建立了BGP对等才可以继续进行\n[Kubernetes Master]确定SVC网络信息\n1 2 3 4 5 kubectl cluster-info dump|grep -i \u0026#34;service-cluster-ip-range\u0026#34; # 以下为输出 \u0026#34;--service-cluster-ip-range=172.16.0.0/16\u0026#34;, \u0026#34;--service-cluster-ip-range=172.16.0.0/16\u0026#34;, \u0026#34;--service-cluster-ip-range=172.16.0.0/16\u0026#34;, [Kubernetes Master]启用SVC网络广播\n1 2 3 4 kubectl patch ds -n kube-system calico-node --patch \\ \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;template\u0026#34;: {\u0026#34;spec\u0026#34;: {\u0026#34;containers\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;calico-node\u0026#34;, \u0026#34;env\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;CALICO_ADVERTISE_CLUSTER_IPS\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;172.16.0.0/16\u0026#34;}]}]}}}}\u0026#39; # 以下为输出 daemonset.extensions/calico-node patched 测试，正常情况下启用BGP广播后，3分钟内核心交换即可接收到路由信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 找到集群DNS服务进行测试 kubectl get svc kube-dns -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 172.16.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 3d21h # 找一个Pod IP在集群外进行解析测试，如果可以解析到结果说明SVC网络已经打通 [dev][root@spring-boot-demo1-192.168.82.85 ~]# dig -x 172.15.190.2 @172.16.0.10 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -x 172.15.190.2 @172.16.0.10 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 23212 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;2.190.15.172.in-addr.arpa. IN PTR ;; ANSWER SECTION: 2.190.15.172.in-addr.arpa. 30 IN PTR 172-15-190-2.ingress-nginx.ingress-nginx.svc.k8s-test.fjf. # 可以正常解析到主机记录 ;; Query time: 3 msec ;; SERVER: 172.16.0.10#53(172.16.0.10) ;; WHEN: Fri Jul 09 15:26:55 CST 2021 ;; MSG SIZE rcvd: 150 ","date":"2024-09-09T15:43:46Z","image":"https://www.ownit.top/title_pic/48.jpg","permalink":"https://www.ownit.top/p/202409091543/","title":"Kubernetes部署(haproxy+keepalived)高可用环境和办公网络打通"},{"content":"项目背景 在云计算和容器技术飞速发展的今天，Kubernetes 已经成为容器编排和管理的事实标准。然而，随着业务的不断扩展，如何在云原生环境下保护和迁移 Kubernetes 集群资源，成为了摆在运维人员面前的一大挑战。Velero，作为一款云原生的灾难恢复和迁移工具，以其强大的功能和灵活的使用方式，为 Kubernetes 集群的数据安全提供了坚实的保障。\n环境介绍 本文将基于一个典型的云原生环境，介绍如何使用 Velero 进行 Kubernetes 集群的备份与迁移\nKubernetes：v1.23.1 Velero：1.11 开源地址：https://github.com/vmware-tanzu/velero 官方文档：https://velero.io/docs/v1.11/\n支持的版本列表 组件介绍 Velero组件 Velero 组件一共分两部分，分别是服务端和客户端。\n服务端：运行在你 Kubernetes 的集群中 客户端：是一些运行在本地的命令行的工具，需要已配置好 kubectl 及集群 kubeconfig 的机器上 velero备份流程 Velero 客户端 发送 API 请求至 Kubernetes API Server，创建备份任务。 Kubernetes API Server 接收并处理 Velero 客户端的请求，创建备份任务，同时通过 Watch 机制通知相关的控制器。 Backup 控制器 通过 Watch 机制监听 API Server，获取备份任务后，向 API Server 请求需要备份的数据。 对象存储服务器 Backup 控制器将从 API Server 获取的数据备份至指定的对象存储服务器。 Velero 后端存储 Velero 支持两种用于配置后端存储的自定义资源定义 (CRD)，分别是 BackupStorageLocation 和 VolumeSnapshotLocation。\nBackupStorageLocation BackupStorageLocation 主要用于定义 Kubernetes 集群资源的备份存储位置，针对的是集群对象数据，而非持久化卷 (PVC) 的数据。该存储通常是 S3 兼容的对象存储，如 MinIO、阿里云 OSS 等。\nVolumeSnapshotLocation VolumeSnapshotLocation 主要用于对持久化卷 (PV) 进行快照备份，这要求云提供商提供相应的插件。阿里云已经提供了该插件，用户可以通过 CSI (容器存储接口) 等存储机制来进行 PV 的快照备份。此外，用户还可以使用专门的备份工具 Restic，将 PV 的数据备份到阿里云 OSS（安装 Velero 时可以选择自定义该选项）。\nRestic 是一款 GO 语言开发的数据加密备份工具，顾名思义，可以将本地数据加密后传输到指定的仓库。支持的仓库有 Local、SFTP、Aws S3、Minio、OpenStack Swift、Backblaze B2、Azure BS、Google Cloud storage、Rest Server。\nvelero客户端 在 Github Release 页面下载指定的 velero 二进制客户端安装包，比如这里我们下载我们k8s集群对应的版本为 v1.11.1 版本列表：https://github.com/vmware-tanzu/velero/releases\n安装velero客户端 1 2 3 4 5 6 7 $ wget https://github.com/vmware-tanzu/velero/releases/download/v1.11.1/velero-v1.11.1-linux-amd64.tar.gz $ tar zxf velero-v1.11.1-linux-amd64.tar.gz $ mv velero-v1.11.1-linux-amd64/velero /usr/bin/ $ velero -h # 启用命令补全 $ source \u0026lt;(velero completion bash) $ velero completion bash \u0026gt; /etc/bash_completion.d/velero 安装minio Velero支持很多种存储插件，可查看：Velero Docs - Providers获取插件信息，我们这里使用minio作为S3兼容的对象存储提供程序。您也可以在任意地方部署Minio对象存储，只需要保证K8S集群可以访问到即可。 minio.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 --- apiVersion: v1 kind: Namespace metadata: name: velero --- apiVersion: apps/v1 kind: Deployment metadata: namespace: velero name: minio labels: component: minio spec: strategy: type: Recreate selector: matchLabels: component: minio template: metadata: labels: component: minio spec: volumes: - name: storage emptyDir: {} - name: config emptyDir: {} containers: - name: minio image: minio/minio:latest imagePullPolicy: IfNotPresent args: - server - /storage - --config-dir=/config - --console-address=:9001 env: - name: MINIO_ACCESS_KEY value: \u0026#34;minio\u0026#34; - name: MINIO_SECRET_KEY value: \u0026#34;minio123\u0026#34; ports: - containerPort: 9000 volumeMounts: - name: storage mountPath: \u0026#34;/storage\u0026#34; - name: config mountPath: \u0026#34;/config\u0026#34; --- apiVersion: v1 kind: Service metadata: namespace: velero name: minio labels: component: minio spec: # ClusterIP is recommended for production environments. # Change to NodePort if needed per documentation, # but only if you run Minio in a test/trial environment, for example with Minikube. type: NodePort ports: - name: api port: 9000 targetPort: 9000 nodePort: 32000 - name: console port: 9001 targetPort: 9001 nodePort: 32001 selector: component: minio --- apiVersion: batch/v1 kind: Job metadata: namespace: velero name: minio-setup labels: component: minio spec: template: metadata: name: minio-setup spec: restartPolicy: OnFailure volumes: - name: config emptyDir: {} containers: - name: mc image: minio/mc:latest imagePullPolicy: IfNotPresent command: - /bin/sh - -c - \u0026#34;mc --config-dir=/config config host add velero http://minio:9000 minio minio123 \u0026amp;\u0026amp; mc --config-dir=/config mb -p velero/velero\u0026#34; volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; 创建minio应用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 创建velero命名空间 $ kubectl create namespace velero # 创建minio资源 $ kubectl apply -f minio.yaml # 查看部署状态 [root@bt velero]# kubectl get sts,pod,svc -n velero NAME READY STATUS RESTARTS AGE pod/minio-78f994f86c-4t9lw 1/1 Running 0 27h pod/minio-setup-d7vbl 0/1 Completed 4 27h pod/node-agent-6z5mn 1/1 Running 0 27h pod/node-agent-8kjcm 1/1 Running 0 27h pod/node-agent-k8zbk 1/1 Running 0 27h pod/velero-85dd87c457-mz5jm 1/1 Running 0 27h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/minio NodePort 172.21.25.69 \u0026lt;none\u0026gt; 9000:32000/TCP,9001:32001/TCP 27h # 开放NodePort端口 $ kubectl patch svc minio -n velero -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; $ kubectl patch svc minio -n velero --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;:9000},{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/1/nodePort\u0026#34;, \u0026#34;value\u0026#34;:9001}]\u0026#39; [root@bt velero]# kubectl get svc -n velero NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE minio NodePort 172.21.25.69 \u0026lt;none\u0026gt; 9000:32000/TCP,9001:32001/TCP 27h 通过浏览器访问服务器IP:30282，并使用账号minio密码minio123登入验证。 velero 服务端 我们可以使用 velero 客户端来安装服务端，也可以使用 Helm Chart 来进行安装，比如这里我们用客户端来安装，velero 命令默认读取 kubectl 配置的集群上下文，所以前提是 velero 客户端所在的节点有可访问集群的 kubeconfig 配置。\n创建密钥 首先准备密钥文件，在当前目录建立一个空白文本文件，内容如下所示：\n1 2 3 4 5 $ cat \u0026gt; credentials-velero \u0026lt;\u0026lt;EOF [default] aws_access_key_id = minio aws_secret_access_key = minio123 EOF 安装velero到k8s集群 替换为之前步骤中 minio 的对应 access key id 和 secret access key如果 minio 安装在 kubernetes 集群内时按照如下命令安装 velero 服务端:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ velero install \\ --provider aws \\ --image velero/velero:v1.11.1 \\ --plugins velero/velero-plugin-for-aws:v1.7.1 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-node-agent \\ --use-volume-snapshots=false \\ --namespace velero \\ --backup-location-config region=minio,s3ForcePathStyle=\u0026#34;true\u0026#34;,s3Url=http://minio:9000 \\ --wait # 执行install命令后会创建一系列清单，包括CustomResourceDefinition、Namespace、Deployment等。 # velero install .... --dry-run -o yaml \u0026gt; velero_deploy.yaml 如果为私仓，可以通过--dry-run 导出 YAML 文件调整在应用。 # 可使用如下命令查看运行日志 $ kubectl logs deployment/velero -n velero # 查看velero创建的api对象 $ kubectl api-versions | grep velero velero.io/v1 # 查看备份位置 $ velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero Available 2024-08-16 14:42:38 +0800 CST ReadWrite true #卸载命令 # velero uninstall --namespace velero You are about to uninstall Velero. Are you sure you want to continue (Y/N)? y Waiting for velero namespace \u0026#34;velero\u0026#34; to be deleted ............................................................................................................................................................................................ Velero namespace \u0026#34;velero\u0026#34; deleted Velero uninstalled ⛵ 选项说明：\n--kubeconfig(可选)：指定kubeconfig认证文件，默认使用.kube/config； --provider：定义插件提供方； --image：定义运行velero的镜像，默认与velero客户端一致； --plugins：指定使用aws s3兼容的插件镜像； --bucket：指定对象存储Bucket桶名称； --secret-file：指定对象存储认证文件； --use-node-agent：创建Velero Node Agent守护进程，托管FSB模块； --use-volume-snapshots：是否启使用快照； --namespace：指定部署的namespace名称，默认为velero； --backup-location-config：指定对象存储地址信息； aws插件与velero版本对应关系： 卸载velero 如果您想从集群中完全卸载Velero，则以下命令将删除由velero install创建的所有资源:\n1 2 kubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component=velero 备份与恢复 备份命令：velero create backup NAME [flags] backup选项：\n--exclude-namespaces stringArray : 要从备份中排除的名称空间 --exclude-resources stringArray: 要从备份中排除的资源，如storageclasses.storage.k8s.io --include-cluster-resources optionalBool[=true]: 包含集群资源类型 --include-namespaces stringArray: 要包含在备份中的名称空间(默认\u0026rsquo;*') --include-resources stringArray: 备份中要包括的资源 --labels mapStringString: 给这个备份加上标签 -o, --output string: 指定输出格式，支持\u0026rsquo;table\u0026rsquo;、\u0026lsquo;json\u0026rsquo;和\u0026rsquo;yaml\u0026rsquo;； -l, --selector labelSelector: 对指定标签的资源进行备份 --snapshot-volumes optionalBool[=true]: 对 PV 创建快照 --storage-location string: 指定备份的位置 --ttl duration: 备份数据多久删掉 --volume-snapshot-locations strings: 指定快照的位置，也就是哪一个公有云驱动 使用官方案例创建测试应用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ kubectl apply -f examples/nginx-app/base.yaml namespace/nginx-example created deployment.apps/nginx-deployment created service/my-nginx created # 查看资源清单 $ kubectl get all -n nginx-example NAME READY STATUS RESTARTS AGE pod/nginx-deployment-5c844b66c8-t5l9z 1/1 Running 0 27h pod/nginx-deployment-5c844b66c8-vlkqj 1/1 Running 0 27h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-nginx LoadBalancer 172.21.18.168 192.168.102.51 80:31831/TCP 27h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 2/2 2 2 27h NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5c844b66c8 2 2 2 27h 备份测试应用 1 2 3 $ velero backup create nginx-backup --include-namespaces nginx-example Backup request \u0026#34;nginx-backup\u0026#34; submitted successfully. Run `velero backup describe nginx-backup` or `velero backup logs nginx-backup` for more details. 选项：\n--include-namespaces：指定命名空间 --selector：标签选择器，如app=nginx 查看备份列表 1 2 3 4 5 6 7 8 9 10 $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2024-08-15 11:17:18 +0800 CST 28d default \u0026lt;none\u0026gt; # 查看备份详细信息 $ velero backup describe nginx-backup # 查看备份日志 $ velero backup logs nginx-backup 登入minio控制台查看备份内容 定时备份指南 1 2 3 4 5 6 7 8 # 使用cron表达式备份 $ velero schedule create nginx-daily --schedule=\u0026#34;0 1 * * *\u0026#34; --include-namespaces nginx-example # 使用一些非标准的速记 cron 表达式 $ velero schedule create nginx-daily --schedule=\u0026#34;@daily\u0026#34; --include-namespaces nginx-example # 手动触发定时任务 $ velero backup create --from-schedule nginx-daily 更多cron示例请参考：cron package’s documentation\n恢复 模拟灾难\n1 2 3 4 5 # 删除nginx-example命名空间和资源 $ kubectl delete namespace nginx-example # 检查是否删除 $ kubectl get all -n nginx-example No resources found in nginx-example namespace. 恢复资源\n1 2 3 4 5 6 7 $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2024-04-06 21:47:16 +0800 CST 29d default \u0026lt;none\u0026gt; $ velero restore create --from-backup nginx-backup Restore request \u0026#34;nginx-backup-20240406215611\u0026#34; submitted successfully. Run `velero restore describe nginx-backup-20240406215611` or `velero restore logs nginx-backup-20240406215611` for more details. 检查恢复的资源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20240406215611 nginx-backup Completed 2024-04-06 21:56:11 +0800 CST 2024-04-06 21:56:12 +0800 CST 0 2 2024-04-06 21:56:11 +0800 CST \u0026lt;none\u0026gt; # 查看详细信息 $ velero restore describe nginx-backup-20240406215611 # 检查资源状态 $ kubectl get all -n nginx-example NAME READY STATUS RESTARTS AGE pod/nginx-deployment-5c844b66c8-t5l9z 1/1 Running 0 27h pod/nginx-deployment-5c844b66c8-vlkqj 1/1 Running 0 27h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/my-nginx LoadBalancer 172.21.18.168 192.168.102.51 80:31831/TCP 27h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 2/2 2 2 27h NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5c844b66c8 2 2 2 27h 参考: https://www.cnblogs.com/nf01/p/18118159 https://github.com/vmware-tanzu/velero https://velero.io/docs/v1.11/\n","date":"2024-08-16T15:19:11Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/202408161519/","title":"云原生时代的数据守护者：Velero 备份与迁移实战"},{"content":"背景： 在现代企业中，跨地区的办公室和数据中心之间的互联互通是非常常见的需求。企业可能拥有多个分支机构、数据中心（IDC），这些地点需要通过可靠的网络连接进行数据传输、资源共享和远程管理。传统上，企业可能会选择租用专线来连接不同地点的网络，但这种方式成本较高且实施复杂。为了在保持安全性和可靠性的同时降低成本，越来越多的企业选择使用 VPN（虚拟专用网络）技术，而 OpenVPN 是其中一种广受欢迎的解决方案。\nOpenVPN 是一个开源的 VPN 解决方案，通过创建加密的隧道，确保不同网络之间的通信安全无虞。这种加密隧道不仅能有效保护数据传输，还能让不同地理位置的网络看起来像是在同一个局域网（LAN）内。这对于需要跨地域进行数据共享、资源访问和远程管理的企业来说，是一个非常实用的工具。\n常见应用场景 分支机构与总部的网络互通： 分支机构的数据需要实时传输到总部进行处理。例如，一个公司在北京设有总部，在上海设有分支机构，北京的服务器需要实时接收和处理来自上海的业务数据。 办公室与 IDC 机房的网络互通： IT 运维或技术人员需要从办公室远程管理位于 IDC 机房的服务器。此外，IDC 内的服务器也可能需要访问办公室内网中的资源。例如，运维团队在北京的办公室，而服务器部署在上海的 IDC 机房中，通过 OpenVPN，可以让他们轻松远程管理这些服务器。 不同 IDC 机房之间的网络互通： 两个数据中心之间可能需要同步数据或共享资源。例如，一个企业在北京和上海都拥有数据中心，为了确保数据的高可用性和安全性，他们需要在两个数据中心之间建立一个加密的连接，确保数据能够在两个 IDC 之间安全传输。 我们使用的场景 我们北京和深圳各有分部，但是由于网络原因，导致两边的资源不互通，经常做一些事情效率底下。现在进行网络调整优化，北京可以访问深圳网络，深圳可以访问北京网络，这样网络打通，资源共享方便，有助于提高工作效率和流程办事速度。\n为什么选择 OpenVPN 安全性： OpenVPN 使用 SSL/TLS 协议进行加密，能够提供高水平的安全性，防止数据在传输过程中被窃取或篡改。 成本效益： 与租用专线相比，使用 OpenVPN 可以大幅降低企业的网络连接成本。 灵活性： OpenVPN 支持多种网络架构和配置，能够适应企业的多样化需求。不论是通过互联网连接不同地点，还是在复杂的网络环境下建立多点连接，OpenVPN 都能轻松应对。 跨平台支持： OpenVPN 兼容多种操作系统，包括 Windows、Linux、macOS 以及各种移动设备，确保企业能够在不同的平台上实现无缝连接 北京和深圳两边网络双向互通流程 环境配置 主机 ip 用途 openvpn服务端（深圳） 172.17.10.20 openvpn-server openvpn-server网段机器（深圳办公区域） 172.17.10.21 server同一网段机器 openvpn客户端（北京） 172.17.20.20 openvpn-client openvpn-client网段机器（北京办公区域） 172.17.20.21 client同一网段机器 虚拟IP 10.10.10.0/24 openvpn的虚拟IP 部署流程 安装openvpn、easy-rsa、iptables-server 1 2 yum install epel-* -y yum -y install openvpn easy-rsa iptables-services 生成证书及相关文件 利用easy-rsa生成相关证书文件\nCA根证书\nopenvpn服务器证书\nDiffie-Hellman算法用到的key 复制easy-rsa脚本到/etc/openvpn下面，该脚本是用来生成CA证书和各种key文件\n复制easy-rsa脚本到/etc/openvpn下面，该脚本是用来生成CA证书和各种key文件\n1 cp -r /usr/share/easy-rsa/ /etc/openvpn/ 1 2 3 4 5 6 7 8 9 10 11 12 13 [root@openvpn openvpn]# cd /etc/openvpn/easy-rsa/3.0.8/ [root@openvpn 3.0.8]# ls easyrsa openssl-easyrsa.cnf vars x509-types #新建vars文件，编辑变量，初始化 [root@openvpn 3.0.8]# vim vars export KEY_COUNTRY=\u0026#34;CN\u0026#34; export KEY_PROVINCE=\u0026#34;ShangHai\u0026#34; export KEY_CITY=\u0026#34;ShangHai\u0026#34; export KEY_ORG=\u0026#34;YCZB\u0026#34; export KEY_EMAIL=\u0026#34;heian@heian.com\u0026#34; [root@openvpn 3.0.8]# source ./vars 生成CA根证书 1 2 3 #初始化 pki 相关目录 ./easyrsa init-pki 生成 CA 根证书, 输入 Common Name，名字随便起。\n1 2 ./easyrsa build-ca nopass 这个命令是不生成密码 生成openvpn服务器证书和密钥，第一个参数是证书名称\n1 ./easyrsa build-server-full server nopass 生成Diffie-Hellman算法需要的密钥文件\n1 /easyrsa gen-dh #创建 Diffie-Hellman ，时间比较久一些 生成tls-auth key，为了防止DDOS和TLS攻击，这个属于可选安全配置\n1 openvpn --genkey --secret ta.key openvpn文件整理 1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir /etc/openvpn/server/certs cd /etc/openvpn/server/certs/ # SSL 协商时 Diffie-Hellman 算法需要的 key cp /etc/openvpn/easy-rsa/3/pki/dh.pem ./ # CA 根证书 cp /etc/openvpn/easy-rsa/3/pki/ca.crt ./ # open VPN 服务器证书 cp /etc/openvpn/easy-rsa/3/pki/issued/yczbjt.crt ./server.crt # open VPN 服务器证书 key cp /etc/openvpn/easy-rsa/3/pki/private/yczbjt.key ./server.key # tls-auth key cp /etc/openvpn/easy-rsa/3/ta.key ./ ----------------------------------- 创建openvpn日志目录 1 2 3 4 5 [root@openvpnservice certs]# /etc/openvpn/cdd #创建日志目录 [root@openvpnservice certs]# mkdir -p /var/log/openvpn/ #给予权限 [root@openvpnservice certs]# chown openvpn:openvpn /var/log/openvpn 配置OpenVPN 深圳服务端配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 port 1194 proto udp dev tun ca /etc/openvpn/server/certs/ca.crt cert /etc/openvpn/server/certs/server.crt key /etc/openvpn/server/certs/server.key dh /etc/openvpn/server/certs/dh.pem tls-auth /etc/openvpn/server/certs/ta.key 0 server 10.10.10.0 255.255.255.0 route 172.17.20.0 255.255.255.0 push \u0026#34;route 172.17.10.0 255.255.255.0\u0026#34; ifconfig-pool-persist ipp.txt client-to-client compress lzo keepalive 10 120 client-config-dir /etc/openvpn/ccd cipher AES-256-CBC comp-lzo persist-key persist-tun log /var/log/openvpn/server.log log-append /var/log/openvpn/server.log status /var/log/openvpn/status.log verb 3 explicit-exit-notify 1 script-security 2 client-connect /etc/openvpn/script/connect.sh client-disconnect /etc/openvpn/script/disconnect.sh connect.sh 1 2 3 4 5 6 7 8 9 #!/bin/bash Time=`date +%F` if [ -f /etc/openvpn/log/openvpn_$Time.log ];then touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is login,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log else touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is login,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log fi disconnect.sh 1 2 3 4 5 6 7 8 9 #!/bin/bash Time=`date +%F` if [ -f /etc/openvpn/log/openvpn_$Time.log ];then touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is logout,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log else touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is logout,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log fi 开启路由转发 1 2 echo net.ipv4.ip_forward = 1 \u0026gt;\u0026gt;/etc/sysctl.conf sysctl -p 添加IPtables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@bt script]# cat /etc/sysconfig/iptables # Generated by iptables-save v1.4.21 on Sun Jun 26 10:24:16 2022 *filter :INPUT ACCEPT [1248:200636] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [2012:242515] -A FORWARD -j ACCEPT COMMIT # Completed on Sun Jun 26 10:24:16 2022 # Generated by iptables-save v1.4.21 on Sun Jun 26 10:24:16 2022 *nat :PREROUTING ACCEPT [2:162] :INPUT ACCEPT [1:78] :OUTPUT ACCEPT [271:36649] :POSTROUTING ACCEPT [72:5056] -A POSTROUTING -s 172.17.10.0/24 -j MASQUERADE -A POSTROUTING -s 10.10.10.0/24 -o ens33 -j SNAT --to-source 172.17.10.20 COMMIT # Completed on Sun Jun 26 10:24:16 2022 这是一些iptables规则，用于网络地址转换（NAT）和IP数据包过滤。这些规则可能在Linux操作系统上的iptables防火墙中使用。\n第一个规则的意思是将来自172.17.10.0/24子网中的数据包的源地址改为防火墙的IP地址，以便将数据包传递到互联网，并确保响应数据包可以正确返回。使用MASQUERADE选项的目的是自动根据防火墙的出口IP地址进行地址转换。\n第二个规则的意思是将来自10.10.10.0/24子网的数据包的源IP地址改为172.17.10.20，并在防火墙的网络接口（ens33）上发送。这个规则可能会用于特殊情况，例如需要使用特定的IP地址来绕过某些防火墙或网络限制。\n加载防火墙\n1 iptables-restore \u0026gt; /etc/sysconfig/iptables ccd 1 2 3 [root@bt openvpn]# cat ccd/fujinfu iroute 172.17.20.0 255.255.255.0 配置客户端推送的路由 添加客户端用户配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@openvpnservice ~]# cd /etc/openvpn/client/ [root@openvpnservice client]# touch sample.ovpn [root@openvpnservice client]# vim sample.ovpn client proto udp dev tun remote 49.234.26.34 1194 #openvpn公网映射的ip地址和端口 ca ca.crt cert admin.crt key admin.key tls-auth ta.key 1 remote-cert-tls server persist-tun persist-key comp-lzo verb 3 mute-replay-warnings 创建用户脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 yum install -y zip,unzip [root@bt client]# cat bash_openuser.sh # ! /bin/bash set -e OVPN_USER_KEYS_DIR=/etc/openvpn/client/keys EASY_RSA_VERSION=3 EASY_RSA_DIR=/etc/openvpn/easy-rsa/ PKI_DIR=$EASY_RSA_DIR/$EASY_RSA_VERSION/pki for user in \u0026#34;$@\u0026#34; do if [ -d \u0026#34;$OVPN_USER_KEYS_DIR/$user\u0026#34; ]; then rm -rf $OVPN_USER_KEYS_DIR/$user rm -rf $PKI_DIR/reqs/$user.req sed -i \u0026#39;/\u0026#39;\u0026#34;$user\u0026#34;\u0026#39;/d\u0026#39; $PKI_DIR/index.txt fi cd $EASY_RSA_DIR/$EASY_RSA_VERSION # 生成客户端 ssl 证书文件 ./easyrsa build-client-full $user nopass # 整理下生成的文件 mkdir -p $OVPN_USER_KEYS_DIR/$user cp $PKI_DIR/ca.crt $OVPN_USER_KEYS_DIR/$user/ # CA 根证书 cp $PKI_DIR/issued/$user.crt $OVPN_USER_KEYS_DIR/$user/ # 客户端证书 cp $PKI_DIR/private/$user.key $OVPN_USER_KEYS_DIR/$user/ # 客户端证书密钥 cp /etc/openvpn/client/sample.ovpn $OVPN_USER_KEYS_DIR/$user/$user.ovpn # 客户端配置文件 sed -i \u0026#39;s/admin/\u0026#39;\u0026#34;$user\u0026#34;\u0026#39;/g\u0026#39; $OVPN_USER_KEYS_DIR/$user/$user.ovpn cp /etc/openvpn/server/certs/ta.key $OVPN_USER_KEYS_DIR/$user/ta.key # auth-tls 文件 cd $OVPN_USER_KEYS_DIR zip -r $user.zip $user done exit 0 北京客户端配置 iptables设置 开启转发路由\n1 2 3 4 5 6 7 systemctl stop firewalld systemctl mask firewalld setenforce 0\t# 临时关闭 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config echo net.ipv4.ip_forward = 1 \u0026gt;\u0026gt;/etc/sysctl.conf sysctl -p 根据下图设置IPtables的规则\n1 2 3 4 5 6 7 8 vim /etc/sysconfig/iptables -A POSTROUTING -s 172.17.20.0/24 -j MASQUERADE -A POSTROUTING -s 10.10.10.0/24 -o ens33 -j SNAT --to-source 172.17.20.20 加载防火墙 iptables-restore \u0026gt; /etc/sysconfig/iptables 交换机配置 深圳访问北京的机器 1 route add -net 172.17.20.0/24 gw 172.17.10.20 如果使用的是华为交换机（Huawei），命令则如下：\n1 ip route-static 172.17.20.0 255.255.255.0 172.17.10.20 请根据具体使用的交换机型号和品牌，选择相应的命令格式。\n北京机器访问深圳段的机器 1 route add -net 172.17.10.0/24 gw 172.17.20.20 如果使用的是华为交换机（Huawei），命令则如下：\n1 ip route-static 172.17.10.0 255.255.255.0 172.17.20.20 上面是临时加到机器上，我们应该加到交换机上，这样整个网段都可以访问\n在实际的环境中可以在内网的路由器上做，这样就不需要在主机上配，比较省事.\n参考文档：\nhttps://ownit.top/p/202407290930/\nhttps://www.cnblogs.com/huangweimin/articles/7700892.html\n","date":"2024-07-30T22:22:16Z","image":"https://www.ownit.top/title_pic/11.jpg","permalink":"https://www.ownit.top/p/202407302222/","title":"Openvpn打通北京和深圳两地办公室网络"},{"content":"openvpn介绍 OpenVPN是一个全特性的SSL VPN，支持弹性的客户端认证方法，基于证书、智能卡，和/或用户名及密码凭证等，并且通过应用于VPN虚拟接口的防火墙规则，可以使用特定用户或组的访问控制策略。\nVPN 全称为虚拟私人网络(Virtual Private Network)，常用于连接中、大型企业或团体间私人网络的通讯方法，利用隧道协议（Tunneling Protocol）来达到发送端认证、消息保密与准确性等功能。\n比如多地办公的公司，可以使用 VPN 将不同地区连接在同一内网下；或者在家办公的时候也可以通过 VPN 接入公司内网中。\nVPN 以 CS 架构运行，工作流程如下：\nVPN工作流程\n在外网的用户可以使用 vpn client 连接组织搭建的 vpn server 以建立通信隧道，随后便建立了虚拟的私人网络，处于外网的 worker 和内网中的 server 可以相互通信。\n那么我们可以简单理解 VPN，由 VPN client 捕获用户发出的报文，封装报文后通过物理网络通信链路将报文发给 VPN server，VPN server 接收到报文后进行解包，再将其转发给实际的目标，反之同理； VPN 在逻辑层面构建了虚拟网络。\nopenvpn原理 OpenVpn的技术核心是虚拟网卡，其次是SSL协议实现，由于SSL协议在其它的词条中介绍的比较清楚了，这里重点对虚拟网卡及其在OpenVpn的中的工作机理进行介绍：\n虚拟网卡是使用网络底层编程技术实现的一个驱动软件，安装后在主机上多出现一个网卡，可以像其它网卡一样进行配置。服务程序可以在应用层打开虚拟网卡，如果应用软件（如IE）向虚拟网卡发送数据，则服务程序可以读取到该数据，如果服务程序写合适的数据到虚拟网卡，应用软件也可以接收得到。虚拟网卡在很多的操作系统下都有相应的实现，这也是OpenVpn能够跨平台一个很重要的理由。\n在OpenVpn中，如果用户访问一个远程的虚拟地址（属于虚拟网卡配用的地址系列，区别于真实地址），则操作系统会通过路由机制将数据包（TUN模式）或数据帧（TAP模式）发送到虚拟网卡上，服务程序接收该数据并进行相应的处理后，通过SOCKET从外网上发送出去，远程服务程序通过SOCKET从外网上接收数据，并进行相应的处理后，发送给虚拟网卡，则应用软件可以接收到，完成了一个单向传输的过程，反之亦然。\nOpenVPN应用场景 Peer-to-Peer VPN(点对点连接)，将Internet两台机器（公网地址）使用VPN连接起来，比如上海服务器和北京服务器之间的数据需要相互调用，但是数据又比较敏感，直接通过http公共网络传输，容易被窃取，如果拉一条专线成本又太高。那么我们可以通过VPN使用现有网络，将两台主机逻辑上捆绑在一个虚拟网络中，这样既保证了数据传输安全，同时又节省了成本。\n1.用户通过Internet连接vpn服务通过加密技术进行数据传输；\n2.加密实现方式是客户端公钥加密，服务端私钥解密；\n3.客户端连接vpn后，vpn会给客户端推送一条内网路由以此实现内部主机通信\n私有网段划分 搭建VPN通常是为了将位于不同地理位置的私有网段连接起来。要注意，VPN所连接的两端的私网网段不能冲突。\nIANA (The Internet Assigned Numbers Authority)定义了下面三个IP地址块用于私有网络：\n10.0.0.0 10.255.255.255 (10/8前缀) 172.16.0.0 172.31.255.255 (172.16/12前缀) 192.168.0.0 192.168.255.255 (192.168/16前缀) Openvpn部署 环境介绍\n主机 ip 网段 openvpn-server(vpn-192.168.83.44 ) 192.168.83.10（物理ip）/192.168.255.1(虚拟ip) 192.168.0.0/16 openvpn-client01\u0026ndash;（主节点） 10.134.18.10（物理ip）/192.168.255.10(虚拟ip) 10.0.0.0/8 openvpn-client02\u0026ndash;（备节点） 10.128.19.10（物理ip）/192.168.255.6 10.0.0.0/8 server虚拟ip 192.168.255.1/24 192.168.255.0/16 安装系统：CentOS Linux release 7.5.1804 (Core)\n启用EPEL的yum源，安装OpenVPN：\n1 2 3 [root@gw ~]# yum install epel-release [root@gw ~]# yum install openvpn OpenVPN程序应该在客户端和服务端机器都安装，它是同一个程序包提供客户端或服务端的功能，取决于你的配置 服务端 生成所有证书\u0026amp;密钥原理 搭建OpenVPN，需要设置你自己的CA(Certificate Authority)并且为OpenVPN服务器和多个客户端分别生成证书和密钥。\n构建一个OpenVPN 2.x配置的第一个步骤是建立一个PKI(public key infrastructure)。该PKI包含：\n服务器和每一个客户端都要有一个单独的证书(也叫做公钥)和私钥。\n一个master CA (Certificate Authority)证书和密钥，用于为每一个服务器和客户端证书签名。\nOpenVPN支持基于证书的双向认证，这意味着，在建立相互信任之前，客户端必须认证服务器证书，服务端也必须认证客户端证书。服务器和客户端在认证彼此时，会首先验证对方呈现的证书是否由master CA签名，然后测试证书报头中的信息，比如证书common name或证书类型(客户端或服务器)。\n注意：服务器和客户端时钟需要基本同步，否则证书可能不会正常工作。\n生成CA的证书\u0026amp;密钥 生成一个master CA证书和密钥，一个服务器证书和密钥，和为3个不同的客户端生成证书和密钥。\n对于PKI管理，我们会使用easy-rsa 2，这是与OpenVPN 2.2.x和之前的版本绑定的一个脚本集合。如果你使用的是OpenVPN 2.3.x，你需要单独从easy-rsa-old项目页面下载easy-rsa 2。而在*NIX平台上，你应该使用easy-rsa 3，参考它的文档以了解详细信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 [root@gw ~]# yum install easy-rsa 将整个easy-rsa的目录拷贝到另一个目录下，比如/etc/openvpn，这样未来升级OpenVPN包时不会覆盖你的修改。 [root@gw ~]# cp -ar /usr/share/easy-rsa /etc/openvpn 生成变量配置文件： [root@gw ~]# cp /usr/share/doc/easy-rsa-3.0.3/vars.example /etc/openvpn/easy-rsa/3.0.3/vars 可以在vars中设置证书有效期 [root@gw ~]# vim vars #配置一下相关的信息 export KEY_COUNTRY=\u0026#34;CN\u0026#34; export KEY_PROVINCE=\u0026#34;ShangHai\u0026#34; export KEY_CITY=\u0026#34;ShangHai\u0026#34; export KEY_ORG=\u0026#34;YCZB\u0026#34; export KEY_EMAIL=\u0026#34;heian@heian.com\u0026#34; set_var EASYRSA_CA_EXPIRE 3650 // CA证书的默认有效期 set_var EASYRSA_CERT_EXPIRE 3650 // CA签发的证书的默认有效期 set_var EASYRSA_CRL_DAYS 3650 // 貌似如果CRL文件在该时间内都未修改过的话，所有客户端都将无法连接 进入easy-rsa的目录，初始化环境： cd /etc/openvpn/easy-rsa/3.0.3/ source ./vars ./easyrsa init-pki #初始化一个新的PKI，其实就是移除并重新创建pki目录结构 创建CA证书和密钥对： ./easyrsa build-ca nopass # build-ca命令创建CA证书和密钥对。nopass选项表示不对CA密钥进行加密。nopass 配置无密码。 #生产环境建议设置密码。在生成证书、密钥的时候，会要求输入该CA密钥的密码。 现在，已经生成了CA的证书文件pki/ca.crt和私钥文件pki/private/ca.key。 Generating a 2048 bit RSA private key .................................+++ ...............+++ writing new private key to \u0026#39;/etc/openvpn/easy-rsa/3.0.3/pki/private/ca.key.lhgETNINjO\u0026#39; ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]:Guowei_CA #输入用于辨识的一个名称 CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/easy-rsa/3.0.3/pki/ca.crt 现在，已经生成了CA的证书文件pki/ca.crt和私钥文件pki/private/ca.key。\n生成服务端的证书\u0026amp;密钥 生成OpenVPN服务端的证书和密钥对：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ./easyrsa build-server-full server nopass # build-server-full命令生成服务端的证书和密钥对。 # server参数是生成证书时用的Common Name，建议就使用server。nopass选项表示不对密钥进行加密。 Generating a 2048 bit RSA private key ..+++ ................+++ writing new private key to \u0026#39;/etc/openvpn/easy-rsa/3.0.3/pki/private/server.key.gYkfUBDAFl\u0026#39; ----- Using configuration from ./openssl-1.0.cnf Check that the request matches the signature Signature ok The Subject\u0026#39;s Distinguished Name is as follows commonName :ASN.1 12:\u0026#39;server\u0026#39; Certificate is to be certified until Aug 12 01:22:27 2028 GMT (3650 days) Write out database with 1 new entries Data Base Updated 现在，已经生成了服务端的证书文件pki/issued/server.crt和私钥文件pki/private/server.key。并且，生成的服务端证书已经是经过先前的CA密钥签名的了。\n生成客户端的证书\u0026amp;密钥 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ./easyrsa build-client-full clientname nopass # build-client-full命令生成客户端的证书和密钥对。 # client1参数是生成证书时用的Common Name。nopass选项表示不对密钥进行加密。 Generating a 2048 bit RSA private key ......................................+++ ............................................................................+++ writing new private key to \u0026#39;/etc/openvpn/easy-rsa/3.0.3/pki/private/client1.key.wQ7i2qbP93\u0026#39; ----- Using configuration from ./openssl-1.0.cnf Check that the request matches the signature Signature ok The Subject\u0026#39;s Distinguished Name is as follows commonName :ASN.1 12:\u0026#39;client1\u0026#39; Certificate is to be certified until Aug 12 02:08:02 2028 GMT (3650 days) Write out database with 1 new entries Data Base Updated 现在，已经生成了服务端的证书文件pki/issued/client1.crt和私钥文件pki/private/client1.key。并且，生成的客户端证书已经是经过先前的CA密钥签名的了。\n类似的，我们还可以为client2、client3等分别生成证书和密钥对。总是为每一个客户端使用一个唯一的Common Name。\n生成Diffie Hellman参数 我们需要为OpenVPN服务端生成Diffie Hellman参数：\n1 2 3 4 5 ./easyrsa gen-dh Generating DH parameters, 2048 bit long safe prime, generator 2 This is going to take a long time ......................................................................+.......+...........................+......................................+................................................+....+.............................................................................................................................................................................. DH parameters of size 2048 created at /etc/openvpn/easy-rsa/3.0.3/pki/dh.pem 现在，已经生成了Diffie Hellman参数文件pki/dh.pem。\n生成tls-auth key 生成tls-auth key，为了防止DDOS和TLS攻击，这个属于可选安全配置\n1 openvpn --genkey --secret ta.key 服务端和创建配置文件 OpenVPN软件包自带了一些示例配置文件，因为我们是使用rpm包安装的，可以在/usr/share/doc/openvpn-2.4.6/sample/sample-config-files/目录下找到：\n服务器和客户端的示例配置文件分别是server.conf和client.conf\n证书目录优化 1 2 3 4 5 6 # mkdir /etc/openvpn/server/certs \u0026amp;\u0026amp; cd /etc/openvpn/server/certs # cp /etc/openvpn/easy-rsa/pki/dh.pem ./ # cp /etc/openvpn/easy-rsa/pki/ca.crt ./ # cp /etc/openvpn/easy-rsa/pki/issued/server.crt ./ # cp /etc/openvpn/easy-rsa/pki/private/server.key ./ # cp /etc/openvpn/easy-rsa/ta.key ./ Server服务器配置文件 OpenVPN软件包自带的服务器示例配置文件很适合用来生成真正使用的服务器配置文件。它会创建一个VPN，使用一个虚拟的TUN网络接口(用于路由)，会在UDP端口1194(OpenVPN的官方端口号)监听客户端连接，并分发10.8.0.0/24子网中的虚拟地址给连接进来的客户端。\n在你使用该配置文件之前，你应该修改该配置文件中的ca、cert、key和dh选项，让它们指向各自的文件(我们先前生成的)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #启动的端口 port 1194 #使用的tcp协议 proto tcp ## \u0026#34;dev tun\u0026#34;将会创建一个路由IP隧道 dev tun # 生成的ca证书 ca /etc/openvpn/server/certs/ca.crt #服务端的证书 cert /etc/openvpn/server/certs/server.crt #服务端的密钥 key /etc/openvpn/server/certs/server.key #Diffie-Hellman密钥交换 文件 dh /etc/openvpn/server/certs/dh.pem #openvpn server的虚拟ip网段 server 192.168.255.0 255.255.255.0 #route控制了从内核到OpenVPN服务器(通过TUN接口)的路由 ### 具体下一跳是哪里，由 client-config 里的 iroute 指定 route 10.0.0.0 255.0.0.0 # 在此文件中维护客户端与虚拟IP地址之间的关联记录 # 如果OpenVPN重启，重新连接的客户端可以被分配到先前分配的虚拟IP地址 ifconfig-pool-persist /etc/openvpn/server/ipp.txt #下面是定义服务器读取特殊客户端配置文件的目录为ccd； client-config-dir /etc/openvpn/ccd #允许客户端子网互通 client-to-client # keepalive指令将导致类似于ping命令的消息被来回发送，以便于服务器端和客户端知道对方何时被关闭 # 每10秒钟ping一次，如果120秒内都没有收到对方的回复，则表示远程连接已经关闭 keepalive 10 120 # 服务器和每个客户端都需要拥有该密钥的一个拷贝 # 第二个参数在服务器端应该为\u0026#39;0\u0026#39;，在客户端应该为\u0026#39;1\u0026#39; tls-auth /etc/openvpn/server/certs/ta.key 0 # 选择一个密码加密算法，该配置项也必须复制到每个客户端配置文件中 cipher AES-256-CBC # 持久化选项可以尽量避免访问那些在重启之后由于用户权限降低而无法访问的某些资源 persist-key persist-tun #日志文件 status /var/log/openvpn-status.log # 为日志文件设置适当的冗余级别(0~9) # 冗余级别越高，输出的信息越详细 # # 0 表示静默运行，只记录致命错误 # 4 表示合理的常规用法 # 5和6 可以帮助调试连接错误 # 9 表示极度冗余，输出非常详细的日志信息 verb 3 # 通知客户端，当服务器重新启动时，可以自动重新连接 # 只能是UDP协议使用，TCP使用的话不能启动服务 # 测试需要push到客户端才有效果 #explicit-exit-notify 1 explicit-exit-notify 1 script-security 2 client-connect /etc/openvpn/script/connect.sh client-disconnect /etc/openvpn/script/disconnect.sh connect.sh\n1 2 3 4 5 6 7 8 9 #!/bin/bash Time=`date +%F` if [ -f /etc/openvpn/log/openvpn_$Time.log ];then touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is login,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log else touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is login,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log fi disconnect.sh\n1 2 3 4 5 6 7 8 9 #!/bin/bash Time=`date +%F` if [ -f /etc/openvpn/log/openvpn_$Time.log ];then touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is logout,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log else touch /etc/openvpn/log/openvpn_$Time.log echo \u0026#34;`date \u0026#39;+%F %H:%M:%S\u0026#39;` User $common_name trust_ip $trusted_ip is logout,Remote_ip is $ifconfig_pool_remote_ip, Mask is $route_netmask_1\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/log/openvpn_$Time.log fi 开启forward转发 1 2 3 4 systemctl stop firewalld systemctl mask firewalld setenforce 0\t# 临时关闭 sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config\t1 2 3 echo net.ipv4.ip_forward = 1 \u0026gt;\u0026gt;/etc/sysctl.conf sysctl -p 配置iptables规则 1 2 3 systemctl enable iptables systemctl start iptables iptables -F # 清理所有防火墙规则 将 openvpn 的网络流量转发到公网：snat 规则\n1 2 3 4 # 如下网段记得与server.conf 当中定义的网段保持一致 iptables -t nat -A POSTROUTING -s 10.0.0.0/8 -o eth0 -j MASQUERADE iptables -L -t nat iptables-save \u0026gt; /etc/sysconfig/iptables # iptables 规则持久化保存 1 2 3 4 5 把iptables规则备份到/etc/sysconfig/iptables文件中 iptables-save \u0026gt; /etc/sysconfig/iptables 恢复刚才备份的规则 iptables-restore \u0026lt; /etc/sysconfig/iptables 我测试环境的iptables配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [dev][root@foxconn-vpn-192.168.83.44 backup_openvpn]# cat /etc/sysconfig/iptables # Generated by iptables-save v1.4.21 on Thu May 6 14:02:02 2021 *nat :PREROUTING ACCEPT [4743102:251117700] :INPUT ACCEPT [2104299:126313633] :OUTPUT ACCEPT [54995:3991628] :POSTROUTING ACCEPT [25003:2044398] :l - [0:0] #表示走 10.0.0.0/8数据的源IP都转换tun0这个接口的IP然后转发出去。 -A POSTROUTING -d 10.0.0.0/8 -o tun0 -j MASQUERADE #表示走 192.168.0.0/16数据的源IP都转换eth0这个接口的IP然后转发出去。 -A POSTROUTING -d 192.168.0.0/16 -o eth0 -j MASQUERADE COMMIT # Completed on Thu May 6 14:02:02 2021 # Generated by iptables-save v1.4.21 on Thu May 6 14:02:02 2021 *filter :INPUT ACCEPT [431:383193] :FORWARD ACCEPT [512:393937] :OUTPUT ACCEPT [374:70793] -A FORWARD -d 10.128.199.248/32 -j DROP -A FORWARD -d 10.128.199.254/32 -j DROP COMMIT # Completed on Thu May 6 14:02:02 2021 # Generated by iptables-save v1.4.21 on Thu May 6 14:02:02 2021 *mangle :PREROUTING ACCEPT [64983188:19955544594] :INPUT ACCEPT [32316135:9267024863] :FORWARD ACCEPT [30855150:10615986243] :OUTPUT ACCEPT [31879646:5718904613] :POSTROUTING ACCEPT [62734647:16334877032] COMMIT # Completed on Thu May 6 14:02:02 2021 特殊客户端配置ccd 1 2 3 4 5 6 7 8 9 [dev][root@foxconn-vpn-192.168.83.44 ccd]# pwd /etc/openvpn/ccd [dev][root@foxconn-vpn-192.168.83.44 ccd]# ls lh.dmz [dev][root@foxconn-vpn-192.168.83.44 ccd]# cat lh.dmz iroute 10.0.0.0 255.0.0.0 #10.0.0.0/8子网网段应该被路由到lh.dmz #iroute控制了从OpenVPN服务器到远程客户端的路由 #和前面server.conf 配置相互 route 10.0.0.0 255.0.0.0 配合 是否允许client2子网(10.0.0.0/8)和其它客户端之间的网络流量。如果是，添加下面的语句到服务器配置文件中：(目前的环境暂时没有用到)\n1 2 3 client-to-client push \u0026#34;route 10.0.0.0 255.0.0.0\u0026#34; #这会让OpenVPN服务器通告client2的子网到其它客户端。 服务端脚本配置 钉钉发送接口脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #!/usr/bin/python # -*- coding: utf-8 -*- import requests import json import sys import os import time import hmac import hashlib import base64 import urllib def signature(): timestamp = long(round(time.time() * 1000)) secret = \u0026#39;xxxxxxxxxx\u0026#39; secret_enc = bytes(secret).encode(\u0026#39;utf-8\u0026#39;) string_to_sign = \u0026#39;{0}\\n{1}\u0026#39;.format(timestamp, secret) string_to_sign_enc = bytes(string_to_sign).encode(\u0026#39;utf-8\u0026#39;) hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.quote_plus(base64.b64encode(hmac_code)) return timestamp,sign def Dingtalk_Push(title, message, msgtype=None): (timestamp,sign) = signature() headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json;charset=utf-8\u0026#39;} token = \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; api_url = \u0026#34;https://oapi.dingtalk.com/robot/send?access_token=%s\u0026amp;timestamp=%s\u0026amp;sign=%s\u0026#34; % (token,timestamp,sign) json_data = { \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;: title, \u0026#34;text\u0026#34;: message.replace(\u0026#39;\\r\u0026#39;,\u0026#39;\\n\u0026#39;).replace(\u0026#39;\\\\n\u0026#39;,\u0026#39;\\n\u0026#39;) }, \u0026#34;at\u0026#34;: { \u0026#34;isAtAll\u0026#34;: True } } with open(\u0026#34;/tmp/dd_notice.log\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(json.dumps(json_data)) requests.post(api_url,json.dumps(json_data),headers=headers).text if __name__ == \u0026#39;__main__\u0026#39;: title = sys.argv[1] message = sys.argv[2] Dingtalk_Push(title, message) 线路检查-主备切换脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #!/bin/bash #Check To Fox Network Script PingCheck(){ ip=$1 status_code=0 for i in {1..3} do /usr/sbin/fping -c 10 -t 1 $ip \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [ $? != 0 ];then status_code=$(expr $status_code + 1) fi sleep 1 done echo $status_code } CheckLine(){ if [ -f /etc/openvpn/ccd/lh.fox ];then echo \u0026#34;lh.fox\u0026#34; elif [ -f /etc/openvpn/ccd/lh.dmz ];then echo \u0026#34;lh.dmz\u0026#34; fi } ChangeLine(){ S_Line=$1 D_Line=$2 mv /etc/openvpn/ccd/$S_Line /etc/openvpn/ccd/$D_Line kill $(ps -ef |grep \u0026#34;/sbin/openvpn --daemo[n]\u0026#34;|awk \u0026#39;{print $2}\u0026#39;) sleep 3 /sbin/openvpn --daemon --config /etc/openvpn/server.conf --log-append /var/log/openvpn.log } #Start Check M_IP=\u0026#34;192.168.255.10\u0026#34; B_IP=\u0026#34;192.168.255.6\u0026#34; M_S=$(PingCheck $M_IP) Line=$(CheckLine) if [ \u0026#34;$M_S\u0026#34; = \u0026#34;3\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$Line\u0026#34; = \u0026#34;lh.dmz\u0026#34; ];then B_S=$(PingCheck $B_IP) if [ \u0026#34;$B_S\u0026#34; = \u0026#34;3\u0026#34; ];then msg=\u0026#34;$(date +\u0026#34;%Y/%m/%d %H:%M:%S\u0026#34;)\\\\\\n壹城To厂区：主线路与备用线路均异常\u0026#34; else msg=\u0026#34;$(date +\u0026#34;%Y/%m/%d %H:%M:%S\u0026#34;)\\\\\\n壹城To厂区：主线路异常，切换备用线路\u0026#34; fi ChangeLine lh.dmz lh.fox /usr/bin/python /etc/openvpn/script/dd-notice.py \u0026#34;壹城To厂区网络告警\u0026#34; \u0026#34;$msg\u0026#34; elif [ \u0026#34;$M_S\u0026#34; = \u0026#34;0\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$Line\u0026#34; = \u0026#34;lh.fox\u0026#34; ];then msg=\u0026#34;$(date +\u0026#34;%Y/%m/%d %H:%M:%S\u0026#34;)\\\\\\n壹城To厂区：主线路恢复，切回主线路\u0026#34; ChangeLine lh.fox lh.dmz /usr/bin/python /etc/openvpn/script/dd-notice.py \u0026#34;壹城To厂区网络告警\u0026#34; \u0026#34;$msg\u0026#34; fi 定时任务 1 */5 * * * * /bin/bash -x /etc/openvpn/script/check-network.sh \u0026gt; /tmp/debug.log 2\u0026gt;\u0026amp;1 openvpn管理脚本（第一版） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 #!/bin/bash USER=$2 PASS=$(echo \u0026#34;${USER}@xxxx.com $(date)\u0026#34;|md5sum |cut -b 1-12) CA_PASS=\u0026#34;xxxxx.88\u0026#34; OVDIR=\u0026#34;/etc/openvpn\u0026#34; SER_ERSA=\u0026#34;${OVDIR}/easy-rsa/3.0.6\u0026#34; CLI_ERSA=\u0026#34;${OVDIR}/easy-rsa/3.0.6\u0026#34; KEYS_DIR=\u0026#34;${OVDIR}/client\u0026#34; DB_FILE=\u0026#34;${SER_ERSA}/pki/index.txt\u0026#34; if [ ! -f /bin/expect ];then echo \u0026#34;expect is not installed, start the installation...\u0026#34; yum -y install expect if [ $? != 0 ];then echo \u0026#34;Installation failed, please install manually\u0026#34; exit 1 fi fi function GenerateAkey(){ rlist=$(find /etc/openvpn/ -name \u0026#34;${USER}*\u0026#34;) if [ -n \u0026#34;$rlist\u0026#34; ];then echo \u0026#34;用户: ${USER} 已存在\u0026#34; exit 1 fi cd ${CLI_ERSA}/ /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa gen-req $USER expect \u0026#34;Enter PEM pass phrase:\u0026#34; send \u0026#34;${PASS}\\r\u0026#34; expect \u0026#34;Verifying - Enter PEM pass phrase\u0026#34; send \u0026#34;${PASS}\\r\u0026#34; expect \u0026#34;Common Name\u0026#34; send \u0026#34;\\r\u0026#34; expect eof EOF } function GenerateAconf(){ cat \u0026gt; ${KEYS_DIR}/${USER}/${USER}.ovpn \u0026lt;\u0026lt;EOF client dev tun proto udp remote xxx.xxx.xxx.xxx 1194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert ${USER}.crt key ${USER}.key remote-cert-tls server tls-auth ta.key 1 cipher AES-256-CBC verb 3 EOF } function ImportAkey(){ cd ${SER_ERSA}/ ./easyrsa import-req ${CLI_ERSA}/pki/reqs/${USER}.req ${USER} /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa sign client $USER expect \u0026#34;Confirm request details:\u0026#34; send \u0026#34;yes\\r\u0026#34; expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF } function CreateUser(){ GenerateAkey ImportAkey mkdir ${KEYS_DIR}/${USER} cp ${SER_ERSA}/pki/issued/${USER}.crt ${KEYS_DIR}/${USER}/ cp ${CLI_ERSA}/pki/private/${USER}.key ${KEYS_DIR}/${USER}/ cp ${OVDIR}/{ca.crt,ta.key} ${KEYS_DIR}/${USER}/ cd ${KEYS_DIR}/ GenerateAconf zip -r ${USER}.zip ${USER}/ if [ -d ${USER} ];then rm -rf ${USER} fi echo -e \u0026#34;\\033[32mPlease Download ${KEYS_DIR}/${USER}.zip\\033[0m\u0026#34; if [ -f ${SER_ERSA}/pki/issued/${USER}.crt ];then #/bin/python2.7 /etc/openvpn/script/mail.py reg ${USER} ${PASS} ${KEYS_DIR}/${USER}.zip echo -e \u0026#34;\\033[32mSenMail To ${USER}@fujfu.com\\033[0m\u0026#34; else echo -e \u0026#34;\\033[31mUser creation failed\\033[0m\u0026#34; fi } function RemoveUser(){ rlist=$(find /etc/openvpn/ -name \u0026#34;${USER}*\u0026#34;) if [ ! -n \u0026#34;$rlist\u0026#34; ];then echo \u0026#34;用户: ${USER} 不存在\u0026#34; exit 1 fi echo -e \u0026#34;\\033[31m${rlist}\\033[0m\u0026#34; read -p \u0026#34;上述文件将被移除,请确认(yes/no): \u0026#34; yn if [ \u0026#34;$yn\u0026#34; = \u0026#34;yes\u0026#34; ];then for f in $rlist;do rm -rf $f if [ $? = 0 ];then echo -e \u0026#34;\\033[31m$f\\033[0m \\033[32m[del]\\033[0m\u0026#34; else echo -e \u0026#34;\\033[31m$f\\033[0m \\033[31m[fail]\\033[0m\u0026#34; fi sleep 0.5 done sed -i \u0026#34;/=${USER}$/d\u0026#34; ${DB_FILE} if [ $? = 0 ];then cd ${SER_ERSA}/ /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa update-db expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF echo -e \u0026#34;\\033[31m${DB_FILE}\\033[0m \\033[32m[update]\\033[0m\u0026#34; else echo -e \u0026#34;\\033[31m${DB_FILE}\\033[0m \\033[32m[fail]\\033[0m\u0026#34; fi else exit 0 fi } function RevokeUser(){ rlist=$(find /etc/openvpn/ -name \u0026#34;${USER}*\u0026#34;) if [ ! -n \u0026#34;$rlist\u0026#34; ];then echo \u0026#34;用户: ${USER} 不存在\u0026#34; exit 1 fi cd ${SER_ERSA}/ /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa revoke ${USER} expect \u0026#34;Continue with revocation\u0026#34; send \u0026#34;yes\\r\u0026#34; expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF } function Gencrl(){ cd ${SER_ERSA}/ /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa gen-crl expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF } case \u0026#34;$1\u0026#34; in start) /usr/sbin/openvpn --daemon --config /etc/openvpn/server.conf --log-append /var/log/openvpn.log ;; restart) /usr/bin/pkill --signal SIGHUP --exact openvpn ;; add) if [ -n \u0026#34;$2\u0026#34; ];then CreateUser else echo $\u0026#34;Usage: $0 add username\u0026#34; fi ;; remove) if [ -n \u0026#34;$2\u0026#34; ];then RevokeUser Gencrl RemoveUser else echo $\u0026#34;Usage: $0 remove username\u0026#34; fi ;; *) echo $\u0026#34;Usage: $0 {start|restart|add|remove} username\u0026#34; exit 1 esac openvpn管理脚本（第二版） 为了简化客户端配置文件的管理，OpenVPN 提供了一种便捷的内联文件方式，允许用户将根证书（ca.crt）、客户端证书（client.crt）以及客户端私钥（client.key）等必要文件直接嵌入到配置文件中。通过这种方式，用户无需分别管理多个文件，从而提高了配置的便捷性和安全性。 具体来说，用户可以在OpenVPN的配置文件（通常是.ovpn文件）中，通过特定的指令将这些证书和密钥文件的内容直接包含进来。这样，当配置文件被加载时，OpenVPN会自动读取并应用这些内联的证书和密钥信息，无需用户手动指定证书和密钥文件的路径。 这种方法不仅使得文件管理更为集中和高效，而且也减少了配置错误的可能性，因为所有的信息都在一个地方，用户只需维护一个配置文件即可。这对于用户来说是一个很大的便利，特别是对于那些需要频繁部署和更新VPN配置的场景。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 #!/bin/bash #获取传入的第二个参数 USER=$2 PASS=$(echo \u0026#34;${USER} $(date)\u0026#34; | md5sum | cut -b 1-12) CA_PASS=\u0026#34;xxxx.88\u0026#34; OVDIR=\u0026#34;/etc/openvpn\u0026#34; SER_ERSA=\u0026#34;${OVDIR}/easy-rsa/3.0.6\u0026#34; CLI_ERSA=\u0026#34;${OVDIR}/easy-rsa/3.0.6\u0026#34; KEYS_DIR=\u0026#34;${OVDIR}/client\u0026#34; DB_FILE=\u0026#34;${SER_ERSA}/pki/index.txt\u0026#34; #判断是否安装expect if [ ! -f /bin/expect ]; then echo \u0026#34;expect is not installed, start the installation...\u0026#34; yum -y install expect if [ $? != 0 ]; then echo \u0026#34;Installation failed, please install manually\u0026#34; exit 1 fi fi function GenerateAkey() { #判断用户是否存在 rlist=$(find /etc/openvpn/ -name \u0026#34;${USER}*\u0026#34;) if [ -n \u0026#34;$rlist\u0026#34; ]; then echo \u0026#34;用户: ${USER} 已存在\u0026#34; exit 1 fi cd ${CLI_ERSA}/ || exit # 生成客户单的证书 /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa gen-req $USER expect \u0026#34;Enter PEM pass phrase:\u0026#34; send \u0026#34;${PASS}\\r\u0026#34; expect \u0026#34;Verifying - Enter PEM pass phrase\u0026#34; send \u0026#34;${PASS}\\r\u0026#34; expect \u0026#34;Common Name\u0026#34; send \u0026#34;\\r\u0026#34; expect eof EOF } #生成单个配置文件 new_client() { # Generates the custom client.ovpn { cat /etc/openvpn/server/client-common.txt echo \u0026#34;\u0026lt;ca\u0026gt;\u0026#34; cat /etc/openvpn/easy-rsa/3.0.6/pki/ca.crt echo \u0026#34;\u0026lt;/ca\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;cert\u0026gt;\u0026#34; sed -ne \u0026#39;/BEGIN CERTIFICATE/,$ p\u0026#39; /etc/openvpn/easy-rsa/3.0.6/pki/issued/\u0026#34;$USER\u0026#34;.crt echo \u0026#34;\u0026lt;/cert\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;key\u0026gt;\u0026#34; cat /etc/openvpn/easy-rsa/3.0.6/pki/private/\u0026#34;$USER\u0026#34;.key echo \u0026#34;\u0026lt;/key\u0026gt;\u0026#34; echo \u0026#34;key-direction 1\u0026#34; echo \u0026#34;\u0026lt;tls-auth\u0026gt;\u0026#34; sed -ne \u0026#39;/BEGIN OpenVPN Static key/,$ p\u0026#39; /etc/openvpn/server/ta.key echo \u0026#34;\u0026lt;/tls-auth\u0026gt;\u0026#34; } \u0026gt;~/\u0026#34;$USER\u0026#34;.ovpn } function ImportAkey() { cd ${SER_ERSA}/ || exit #导入client签约证书 ./easyrsa import-req ${CLI_ERSA}/pki/reqs/${USER}.req ${USER} #给client端证书做签名 /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa sign client $USER expect \u0026#34;Confirm request details:\u0026#34; send \u0026#34;yes\\r\u0026#34; expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF } #创建证书目录，进行归纳 压缩，发送邮件 function CreateUser() { GenerateAkey ImportAkey echo \u0026#34; client dev tun proto tcp remote xxx.xxx.xxx.xxx 11191 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server cipher AES-256-CBC block-outside-dns verb 3\u0026#34; \u0026gt;/etc/openvpn/server/client-common.txt new_client echo -e \u0026#34;\\033[32mPlease Download ${KEYS_DIR}/${USER}-yc.zip\\033[0m\u0026#34; echo -e \u0026#34;\u0026#34; #判断client 生成文件存在 if [ -f ${SER_ERSA}/pki/issued/${USER}.crt ]; then #通过Python发送邮件 ，包含证书和使用方法 # /bin/python2.7 /etc/openvpn/script/mail.py ${USER} ${PASS} ${KEYS_DIR}/${USER}-yc.zip # /bin/python2.7 /etc/openvpn/script/mail.py ${USER} ${PASS} ~/\u0026#34;$USER\u0026#34;.ovpn echo -e \u0026#34;\\033[32mSenMail To ${USER}@fujfu.com\\033[0m\u0026#34; echo -e \u0026#34;\\033[32mPrivate Key Password: \\033[0m\\033[31m${PASS}\\033[0m\u0026#34; else echo -e \u0026#34;\\033[31mUser creation failed\\033[0m\u0026#34; fi } #移除账号文件 function RemoveUser() { #判断用户是否存在 rlist=$(find /etc/openvpn/ -name \u0026#34;${USER}*\u0026#34;) if [ ! -n \u0026#34;$rlist\u0026#34; ]; then echo \u0026#34;用户: ${USER} 不存在\u0026#34; exit 1 fi echo -e \u0026#34;\\033[31m${rlist}\\033[0m\u0026#34; read -p \u0026#34;上述文件将被移除,请确认(yes/no): \u0026#34; yn if [ \u0026#34;$yn\u0026#34; = \u0026#34;yes\u0026#34; ]; then for f in $rlist; do rm -rf $f if [ $? = 0 ]; then echo -e \u0026#34;\\033[31m$f\\033[0m \\033[32m[del]\\033[0m\u0026#34; else echo -e \u0026#34;\\033[31m$f\\033[0m \\033[31m[fail]\\033[0m\u0026#34; fi done #删除openvpn的证书中的用户 sed -i \u0026#34;/=${USER}$/d\u0026#34; ${DB_FILE} if [ $? = 0 ]; then cd ${SER_ERSA}/ # update-db 命令来更新证书数据库 /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa update-db expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF echo -e \u0026#34;\\033[31m${DB_FILE}\\033[0m \\033[32m[update]\\033[0m\u0026#34; else echo -e \u0026#34;\\033[31m${DB_FILE}\\033[0m \\033[32m[fail]\\033[0m\u0026#34; fi else exit 0 fi } #删除账号 function RevokeUser() { rlist=$(find /etc/openvpn/ -name \u0026#34;${USER}*\u0026#34;) if [ ! -n \u0026#34;$rlist\u0026#34; ]; then echo \u0026#34;用户: ${USER} 不存在\u0026#34; exit 1 fi cd ${SER_ERSA}/ #gen-crl会生成一份吊销证书的名单，放在pki/crl.pem文件里 /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa revoke ${USER} expect \u0026#34;Continue with revocation\u0026#34; send \u0026#34;yes\\r\u0026#34; expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF } # 撤销证书并创建CRL # 这是CA特定的任务。 # 要永久撤消已颁发的证书，请提供导入期间使用的短名称： # ./easyrsa revoke EntityName # 要创建包含所有已撤销的证书的更新CRL，请执行以下操作： # ./easyrsa gen-crl function Gencrl() { cd ${SER_ERSA}/ /bin/expect \u0026lt;\u0026lt;EOF spawn ./easyrsa gen-crl expect \u0026#34;Enter pass phrase for\u0026#34; send \u0026#34;${CA_PASS}\\r\u0026#34; expect eof EOF } case \u0026#34;$1\u0026#34; in start) /usr/sbin/openvpn --daemon --config /etc/openvpn/server.conf --log-append /var/log/openvpn.log ;; restart) /usr/bin/pkill --signal SIGHUP --exact openvpn ;; add) if [ -n \u0026#34;$2\u0026#34; ]; then CreateUser else echo $\u0026#34;Usage: $0 add username\u0026#34; fi ;; remove) if [ -n \u0026#34;$2\u0026#34; ]; then RevokeUser Gencrl RemoveUser else echo $\u0026#34;Usage: $0 remove username\u0026#34; fi ;; *) echo $\u0026#34;Usage: $0 {start|restart|add|remove} username\u0026#34; exit 1 ;; esac 上面生成下方格式配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 client dev tun proto udp sndbuf 0 rcvbuf 0 remote openvpnserver 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server comp-lzo verb 3 cipher AES-256-CBC script-security 3 key-direction 1 auth-nocache auth-user-pass # ca ca.crt \u0026lt;ca\u0026gt; -----BEGIN CERTIFICATE----- ...... ...... ...... -----END CERTIFICATE----- \u0026lt;/ca\u0026gt; # cert client.crt \u0026lt;cert\u0026gt; Certificate: Data: Version: 3 (0x2) Serial Number: 89:53:31:72:cf:04:52:82:fb:e9:53:fe:82:fc:13:25 Signature Algorithm: sha256WithRSAEncryption Issuer: CN=192.168.1.10 Validity Not Before: Nov 14 03:32:32 2023 GMT Not After : Feb 16 03:32:32 2026 GMT Subject: CN=client Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: ...... ...... ...... Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Basic Constraints: CA:FALSE X509v3 Subject Key Identifier: 6E:0A:D2:EF:CE:55:4F:56:C7:57:D1:77:37:0D:AD:28:EF:A2:C8:41 X509v3 Authority Key Identifier: keyid:1A:CE:DD:A8:65:0E:3D:32:E0:74:17:15:5D:F7:6E:3B:7D:C1:59:05 DirName:/CN=192.168.1.10 serial:D8:A8:9B:71:84:77:C8:9B X509v3 Extended Key Usage: TLS Web Client Authentication X509v3 Key Usage: Digital Signature Netscape Comment: Easy-RSA (3.0.8) Generated Certificate Netscape Cert Type: SSL Client Signature Algorithm: sha256WithRSAEncryption ...... ...... ...... -----BEGIN CERTIFICATE----- ...... ...... ...... -----END CERTIFICATE----- \u0026lt;/cert\u0026gt; # key client.key \u0026lt;key\u0026gt; -----BEGIN PRIVATE KEY----- ...... ...... ...... -----END PRIVATE KEY----- \u0026lt;/key\u0026gt; # tls-crypt ta.key 1 \u0026lt;tls-crypt\u0026gt; # # 2048 bit OpenVPN static key # -----BEGIN OpenVPN Static key V1----- ...... ...... ...... -----END OpenVPN Static key V1----- \u0026lt;/tls-crypt\u0026gt; 开通账号自动发送邮件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 #!/usr/bin/env python # -*- coding: utf-8 -*- import datetime import smtplib,os,datetime,sys from email.header import Header from email.mime.text import MIMEText from email.mime.base import MIMEBase from email.mime.application import MIMEApplication import email.MIMEMultipart def GenHtml(UserName,UserPass): Data = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html xmlns=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=utf-8\u0026#34; /\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; table { border-collapse: collapse; font-family: Futura, Arial, sans-serif; } caption { font-size: larger; margin: 1em auto; } th,td { padding: .65em; } th { background: #555 nonerepeat scroll 0 0; border: 1px solid #777; color: #fff; } td { border: 1px solid#000000; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;table style=\u0026#34;tablestyle\u0026#34; border=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td bgcolor=\u0026#34;#66B3FF\u0026#34; style=\u0026#34;color: #F3F3FA\u0026#34; align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;b\u0026gt;开通时间\u0026lt;/b\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td align=\u0026#34;center\u0026#34;\u0026gt;%s\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td bgcolor=\u0026#34;#66B3FF\u0026#34; style=\u0026#34;color: #F3F3FA\u0026#34; align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;b\u0026gt;账号\u0026lt;/b\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026lt;b\u0026gt;%s\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td bgcolor=\u0026#34;#66B3FF\u0026#34; style=\u0026#34;color: #F3F3FA\u0026#34; align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;b\u0026gt;密码\u0026lt;/b\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026lt;b\u0026gt;%s\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td bgcolor=\u0026#34;#66B3FF\u0026#34; style=\u0026#34;color: #F3F3FA\u0026#34; align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;b\u0026gt;有效期\u0026lt;/b\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026lt;b\u0026gt;7天\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td bgcolor=\u0026#34;#66B3FF\u0026#34; style=\u0026#34;color: #F3F3FA\u0026#34; align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;b\u0026gt;证书及配置文件\u0026lt;/b\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;\u0026lt;b\u0026gt;见附件\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/br\u0026gt; \u0026lt;h2\u0026gt;使用说明:\u0026lt;/h2\u0026gt; \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;b\u0026gt;Windows\u0026lt;/b\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt;下载 openvpn 的安装程序 \u0026lt;a href=\u0026#34;https://aliyuncs.com/software/OpenVPN-2.5.3-I601-amd64.msi\u0026#34; title=\u0026#34;OpenVPN-2.5.3-I601-amd64.msi\u0026#34;\u0026gt;OpenVPN-2.5.3-I601-amd64.msi\u0026lt;/a\u0026gt; 安装完成，将证书拷贝至 OpenVPN\\config 下即可，openvpn 的软件请使用管理员权限打开。\u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;b\u0026gt;MacOS\u0026lt;/b\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt;下载 Tunnelblick 等，支持openvpn 的客户端双击 *.ovpn 即可导入。\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;下载 Tunnelblick 安装程序 \u0026lt;a href=\u0026#34;https://aliyuncs.com/software/Tunnelblick_3.8.5a_build_5671.dmg\u0026#34; title=\u0026#34;Tunnelblick_3.8.5a_build_5671.dmg\u0026#34;\u0026gt;Tunnelblick_3.8.5a_build_5671.dmg\u0026lt;/a\u0026gt; \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;b\u0026gt;温馨提示\u0026lt;/b\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt;请妥善保管好你的账号、密码以及密钥文件，如发生信息丢失，请及时联系管理员。\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; % (Data, UserName, UserPass) return html def VpnMailTo(html, mailSubject, mailto, key_file): mailuser = \u0026#34;yunwei@qq.com\u0026#34; mailpass = \u0026#34;xxxxxxx\u0026#34; server = smtplib.SMTP_SSL(\u0026#39;smtp.qq.com\u0026#39;, 465) server.login(mailuser, mailpass) main_msg = email.MIMEMultipart.MIMEMultipart() text_msg = email.MIMEText.MIMEText(html, _subtype=\u0026#39;html\u0026#39;, _charset=\u0026#39;utf-8\u0026#39;) main_msg.attach(text_msg) contype = \u0026#39;application/octet-stream\u0026#39; maintype, subtype = contype.split(\u0026#39;/\u0026#39;, 1) files = [\u0026#34;/etc/openvpn/script/docs/办公室内网OpenVPN使用手册.docx\u0026#34;, key_file] #for f in files: # data = open(f, \u0026#39;rb\u0026#39;) # file_msg = email.MIMEBase.MIMEBase(maintype, subtype) # file_msg.set_payload(data.read()) # data.close() # email.Encoders.encode_base64(f) # basename = os.path.basename(f) # file_msg.add_header(\u0026#39;Content-Disposition\u0026#39;,\u0026#39;attachment\u0026#39;, filename = f) # main_msg.attach(f) for fs in files: with open(fs,\u0026#39;rb\u0026#39;) as f: basename = os.path.basename(fs) part = MIMEApplication(f.read()) part.add_header(\u0026#39;Content-Disposition\u0026#39;, \u0026#39;attachment\u0026#39;, filename = basename) main_msg.attach(part) main_msg[\u0026#39;From\u0026#39;] = \u0026#39;VPNService \u0026lt;yunwei@qq.com\u0026gt;\u0026#39; main_msg[\u0026#39;Subject\u0026#39;] = Header(mailSubject, \u0026#39;utf8\u0026#39;).encode() main_msg[\u0026#39;To\u0026#39;] = \u0026#34;,\u0026#34;.join(mailto) main_msg[\u0026#39;Date\u0026#39;] = email.Utils.formatdate() server.sendmail(\u0026#39;yunwei@qq.com\u0026#39;, mailto, main_msg.as_string()) def VpnReg(User, Pass, KeysFile): mailSubject = User + \u0026#34; 你好，办公网络环境的VPN账号已开通成功！\u0026#34; html = GenHtml(User,Pass) MailTo = [] MailTo.append(User+\u0026#34;@qq.com\u0026#34;) VpnMailTo(html, mailSubject, MailTo, KeysFile) if __name__ == \u0026#39;__main__\u0026#39;: User = sys.argv[1] Pass = sys.argv[2] KeysFile = sys.argv[3] VpnReg(User,Pass,KeysFile) openvpn管理 1 2 3 4 [dev][root@foxconn-vpn-192.168.1.1 script]# pwd /etc/openvpn/script [dev][root@foxconn-vpn-192.168.1.1 script]# ./vpncli -h Usage: ./vpncli {start|restart|add|remove} username openvpn-client客户端 OpenVPN 客户端配置 - 主节点 客户端标识 物理 IP / 虚拟 IP 目标网络范围 主节点（openvpn-client01） 10.134.18.10 / 192.168.255.10 10.0.0.0/8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #客户段 client #使用的tcp协议 dev tun ## \u0026#34;dev tun\u0026#34;将会创建一个路由IP隧道 proto tcp #vpn server的地址 remote xxx.xxx.xxx.xxx 1194 # 启用该指令，与服务器连接中断后将自动重新连接， # 这在网络不稳定的情况下(例如：笔记本电脑无线网络)非常有用 resolv-retry infinite # 大多数客户端不需要绑定本机特定的端口号 nobind # 持久化选项可以尽量避免访问在重启时由于用户权限降低而无法访问的某些资源 persist-key persist-tun #ca证书 ca ca.crt #客户端证书 cert lh.dmz.crt #客户端秘钥 key lh.dmz.key # 通过检查证书具有正确的密钥使用设置来验证服务器证书 # 这是防止此处讨论的潜在攻击的重要预防措施： # http://openvpn.net/howto.html#mitm # 要使用此功能，EasyRSA生成服务器证书的时候进行相关设置 remote-cert-tls server # 如果在服务器上使用tls-auth密钥，那么每个客户端也必须拥有密钥 tls-auth ta.key 1 ## 选择一个加密算法，服务器使用的算法选项，也必须在这里指定它 # 注意，v2.4客户端/服务器将自动以TLS模式协商AES-256-GCM。 cipher AES-256-CBC #信息日志 verb 3 启动客户端 1 openvpn --daemon --cd /etc/openvpn/lh.dmz/ --config lh.dmz.ovpn --log-append /var/log/openvpn_dmz.log --askpass /etc/openvpn/lh.dmz/lh.dmz.pass OpenVPN 客户端配置 - 备节点（通过代理） 客户端标识 物理 IP / 虚拟 IP 目标网络范围 备节点（openvpn-client02） 10.128.19.10 / 192.168.255.6 10.0.0.0/8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 client dev tun proto tcp remote xxx.xxx.xxx.xxx 1194 resolv-retry infinite route 192.168.0.0 255.255.0.0 route xxx.xxx.xxx.xxx 255.255.255.255 #公网路由 nobind persist-key persist-tun ca ca.crt cert lh.fox.crt key lh.fox.key remote-cert-tls server tls-auth ta.key 1 cipher AES-256-CBC verb 3 #http-proxy 10.xxx.xxx.xxx 3128 #http-proxy 10.xxx.xxx.xxx 3128 # 如果通过HTTP代理方式来连接到实际的VPN服务器 # 在此处指定代理服务器的主机名(或IP)和端口号 # 如果代理服务器需要身份认证，请参考官方手册 http-proxy 10.xxx.xxx.xxx 3128 # 连接失败时自动重试 http-proxy-retry 启动客户端 1 openvpn --daemon --cd /etc/openvpn/lh.fox --config lh.fox.ovpn --log-append /var/log/openvpn.log --askpass /etc/openvpn/lh.fox/lh.fox.pass #这个vpn在连接时，会使用到密码 交换机配置 交换机添加路由\n10.0.0.0/8 这个网段是 异地 的网段\n访问 10.0.0.0/8的 数据，下一跳 192.168.83.44 （openvpn-server）\n参考文档：https://www.gaoyufu.cn/archives/openvpn\n","date":"2024-07-29T09:30:50Z","image":"https://www.ownit.top/title_pic/76.jpg","permalink":"https://www.ownit.top/p/202407290930/","title":"高可用openvpn全局单向A访问B区的网络"},{"content":"1、背景介绍 目前公司使用PowerDNS进行DNS管理，但由于采用的是单节点架构，存在不可用的风险。为提升系统的稳定性和可靠性，我们计划对现有架构进行重构。通过引入高可用性设计，我们将优化系统架构，使其能够在故障情况下依然保持服务的连续性和高效性，从而提升整体的业务稳定性。\n2、环境介绍 系统：Cnetos7 软件： pdns-4.1.8-1.el7.MIND.x86_64 pdns-recursor-4.1.11-1.el7.MIND.x86_64 （相关信息已经脱敏）\n名称 ip 组件 matser 172.17.20.20 nginx，mysql，PowerDNS Authoritative ，PowerDNS Recursor （主） slave 172.17.20.21 nginx，mysql，PowerDNS Authoritative ，PowerDNS Recursor (备) nginx（53）：作为upstream 代理 53 端口\nmysql（3306）：作为PowerDNS的后端存储\nPowerDNS Authoritative（5300）：用于管理企业私有域名\nPowerDNS Recursor（5301）： 用于DNS解析转发、缓存\n3、组件介绍 PowerDNS全家桶中包含PowerDNS Authoritative、Recursor、DNSList（暂不使用）三个组件。\nPowerDNS Authoritative：DNS权威服务器，用于提供企业私有域名的管理和解析； PowerDNS Recursor：DNS递归服务器，用于接受客户端DNS查询请求，并根据目标域转发配置转发到不同的上游DNS服务器进行解析，并对DNS解析记录进行缓存； PowerDNS-Admin：DNS权威服务器的Web管理页面； PowerDNS-Monitor：使用Grafana提供权威服务器和递归服务器的监控页面 PowerDNS权威服务器支持多种复制模式，本架构采用MySQL作为后端存储，并通过MySQL复制实现主备数据同步。\nPowerDNS（PDNS）成立于20世纪90年代末，是开源DNS软件、服务和支持的主要供应商，它们提供的权威认证DNS服务器和递归认证DNS服务器都是100%开源的软件，同时也和红帽等开源方案提供商一样提供了付费的技术支持版本。同时官方表示为了避免和软件使用者出现竞争，他们只提供服务支持而不提供DNS托管服务。\n熟悉DNS工作原理的同学可以大致地将DNS记录的查询分为两种：查询本地缓存和向上递归查询。和其他的如BIND、dnsmasq等将这些功能集成到一起的DNS软件不同，PowerDNS将其一分为二，分为了PowerDNS Authoritative Server和PowerDNS Recursor，分别对应这两种主要的需求，而我们常说的pdns指的就是PowerDNS Authoritative Server (后面简称PDNS Auth)，主要用途就是作为权威域名服务器，当然也可以作为普通的DNS服务器提供DNS查询功能。\n对于PowerDNS-Recursor，PowerDNS官网介绍其是一个内置脚本能力的高性能的DNS递归查询服务器，并且已经为一亿五千万个互联网连接提供支持。 4、MySQL安装 可参照博客安装 ： https://blog.csdn.net/heian_99/article/details/106644755\nMySQL主从同步 master创建账号 1 2 3 grant replication slave on *.* to repl@\u0026#39;172.17.20.%\u0026#39; identified by \u0026#39;123\u0026#39;; flush privileges; show master status; #查询master的状态 ，进行同步 slave库同步 注意：主从库的server_id 不能设置一样，auto.cnf 设置也不能一样，不满会出现mysql主从同步失败的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 mysql\u0026gt; change master to master_host=\u0026#39;172.17.20.20′,master_user=\u0026#39;repl\u0026#39;,master_password=\u0026#39;123\u0026#39;,master_log_file=\u0026#39;mysql-bin.000006′,master_log_pos=407; CHANGE MASTER TO MASTER_HOST=\u0026#39;172.17.20.20\u0026#39;, MASTER_PORT=3306, MASTER_USER=\u0026#39;repl\u0026#39;, MASTER_PASSWORD=\u0026#39;123\u0026#39;, MASTER_LOG_FILE=\u0026#39;mysql-bin.000006\u0026#39;, MASTER_LOG_POS=407; mysql\u0026gt; start slave; mysql\u0026gt; show slave status\\G 创建powerdns数据库 需要在主从配置完成后创建\n在主库执行以下命令\n1 2 3 4 mysql -uroot -p CREATE DATABASE powerdns; GRANT ALL ON powerdns.* TO \u0026#39;powerdns\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;VMware1!\u0026#39;; FLUSH PRIVILEGES; 导入PowerDNS 的数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 CREATE TABLE domains ( id INT AUTO_INCREMENT, name VARCHAR(255) NOT NULL, master VARCHAR(128) DEFAULT NULL, last_check INT DEFAULT NULL, type VARCHAR(6) NOT NULL, notified_serial INT DEFAULT NULL, account VARCHAR(40) CHARACTER SET \u0026#39;utf8\u0026#39; DEFAULT NULL, PRIMARY KEY (id) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE UNIQUE INDEX name_index ON domains(name); CREATE TABLE records ( id BIGINT AUTO_INCREMENT, domain_id INT DEFAULT NULL, name VARCHAR(255) DEFAULT NULL, type VARCHAR(10) DEFAULT NULL, content VARCHAR(64000) DEFAULT NULL, ttl INT DEFAULT NULL, prio INT DEFAULT NULL, change_date INT DEFAULT NULL, disabled TINYINT(1) DEFAULT 0, ordername VARCHAR(255) BINARY DEFAULT NULL, auth TINYINT(1) DEFAULT 1, PRIMARY KEY (id) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE INDEX nametype_index ON records(name,type); CREATE INDEX domain_id ON records(domain_id); CREATE INDEX ordername ON records (ordername); CREATE TABLE supermasters ( ip VARCHAR(64) NOT NULL, nameserver VARCHAR(255) NOT NULL, account VARCHAR(40) CHARACTER SET \u0026#39;utf8\u0026#39; NOT NULL, PRIMARY KEY (ip, nameserver) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE TABLE comments ( id INT AUTO_INCREMENT, domain_id INT NOT NULL, name VARCHAR(255) NOT NULL, type VARCHAR(10) NOT NULL, modified_at INT NOT NULL, account VARCHAR(40) CHARACTER SET \u0026#39;utf8\u0026#39; DEFAULT NULL, comment TEXT CHARACTER SET \u0026#39;utf8\u0026#39; NOT NULL, PRIMARY KEY (id) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE INDEX comments_name_type_idx ON comments (name, type); CREATE INDEX comments_order_idx ON comments (domain_id, modified_at); CREATE TABLE domainmetadata ( id INT AUTO_INCREMENT, domain_id INT NOT NULL, kind VARCHAR(32), content TEXT, PRIMARY KEY (id) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE INDEX domainmetadata_idx ON domainmetadata (domain_id, kind); CREATE TABLE cryptokeys ( id INT AUTO_INCREMENT, domain_id INT NOT NULL, flags INT NOT NULL, active BOOL, content TEXT, PRIMARY KEY(id) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE INDEX domainidindex ON cryptokeys(domain_id); CREATE TABLE tsigkeys ( id INT AUTO_INCREMENT, name VARCHAR(255), algorithm VARCHAR(50), secret VARCHAR(255), PRIMARY KEY (id) ) Engine=InnoDB CHARACTER SET \u0026#39;latin1\u0026#39;; CREATE UNIQUE INDEX namealgoindex ON tsigkeys(name, algorithm); 创建文件导入数据库\n1 2 3 4 5 6 touch /opt/schema.mysql.sql 把mysql的内容写到这个文件里面 mysql\u0026gt; use powerdns Database changed mysql\u0026gt; source /opt/schema.mysql.sql Query OK, 0 rows affected (0.01 sec) 5、PowerDNS Authoritative Server安装 参考官网文档：https://doc.powerdns.com/authorit文章来源(Source)：https://www.dqzboy.comative/installation.html PowerDNS已经集成到epel源中，所以我们首先需要安装elel源 1 2 3 4 5 6 7 [root@localhost ~]# wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm [root@localhost ~]# rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm [root@localhost ~]# yum clean all [root@localhost ~]# yum makecache [root@localhost ~]# yum install -y pdns pdns-backend-mysql 修改配置文件 /etc/pdns/pdns.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [root@master ~]# cp /etc/pdns/pdns.conf /etc/pdns/pdns.conf_`date \u0026#39;+%Y_%m_%d\u0026#39;` [root@master ~]# tree /etc/pdns/ /etc/pdns/ └── pdns.conf 0 directories, 1 file [root@master ~]# cat /etc/pdns/pdns.conf | grep -Ev \u0026#34;^#|^$\u0026#34; launch=bind setgid=pdns setuid=pdns 添加配置 生产环境如下 ############### cache-ttl=60 config-dir=/etc/pdns disable-axfr=yes distributor-threads=3 #数据库的连接信息 gmysql-dbname=powerdns gmysql-host=127.0.0.1 gmysql-password=VMware1! gmysql-user=powerdns guardian=no launch=gmysql ## pdns config 本地地址和启动端口 local-address=127.0.0.1 local-port=5300 log-dns-details=no disable-syslog=no negquery-cache-ttl=60 query-cache-ttl=60 query-logging=no receiver-threads=12 cache-ttl=0 setuid=pdns setgid=pdns ## pdns API 激活 API 服务 api-key=3jEkItlrSHfuqJbr api=yes #web页面也信息 webserver-address=0.0.0.0 webserver-password=bdata.pdns # webserver-allow-from指定允许访问webserver和API的IP白名单，多个IP可以使用英文逗号隔开 webserver-allow-from=0.0.0.0/0 webserver-port=8281 webserver=yes loglevel=9 启动Pdns服务 1 2 3 4 5 [root@master pdns]# systemctl enable pdns Created symlink from /etc/systemd/system/multi-user.target.wants/pdns.service to /usr/lib/systemd/system/pdns.service. [root@master pdns]# systemctl start pdns.service [root@master pdns]# systemctl status pdns.service 6、PowerDNS Recursor安装 1 [root@master pdns]# yum install pdns-recursor -y 修改配置文件 /etc/pdns-recursor/recursor.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 [root@master pdns]# tree /etc/pdns-recursor/ /etc/pdns-recursor/ └── recursor.conf [root@master pdns]# cp /etc/pdns-recursor/recursor.conf /etc/pdns-recursor/recursor.conf_`date \u0026#39;+%Y_%m_%d\u0026#39;` [root@master pdns]# cat /etc/pdns-recursor/recursor.conf | grep -Ev \u0026#34;^#|^$\u0026#34; security-poll-suffix= setgid=pdns-recursor setuid=pdns-recursor 添加配置 生产环境如下 ############### #允许所有用户端请求 allow-from=0.0.0.0/0,::/0 ## pdns-re API 激活 API 服务 api-key=3jEkItlrSHfuqJbr disable-packetcache=no export-etc-hosts=yes #用户解析的自定义配置文件 与forward-zones相同，从文件中解析 forward-zones-file=/etc/pdns-recursor/forward #/etc/hosts 文件的路径或等效文件。该文件可用于使用export-etc-hosts权威地提供数据。 etc-hosts-file=/etc/pdns-recursor/hosts #本地监听地址 local-address=0.0.0.0 local-port=5301 max-cache-entries=1000000 max-cache-ttl=60 max-mthreads=1024 max-packetcache-entries=500000 packetcache-servfail-ttl=0 packetcache-ttl=0 quiet=yes setgid=pdns-recursor setuid=pdns-recursor threads=6 #web页面也信息 webserver-address=0.0.0.0 webserver-password=bdata.pdns webserver-port=8282 webserver-allow-from=172.17.0.0/16 webserver=yes loglevel=9 创建forward 和hosts 文件 1. Forward 配置 forward 配置用于指定 PowerDNS 在无法解析某个 DNS 请求时，将请求转发给其他上游 DNS 服务器进行解析。这在多种情况下都很有用，比如公司内部网络中的 DNS 服务器需要解析外部互联网域名时。\n主要用途：\n外部解析：如果本地 DNS 服务器没有相应的记录，可以通过 forward 配置将请求转发给公共 DNS 服务器，如 Google DNS（8.8.8.8）。 负载均衡和冗余：可以配置多个上游 DNS 服务器，提高解析请求的成功率和响应速度。 配置示例：\n1 forward-zones=.=8.8.8.8;8.8.4.4 上面的示例表示将所有（. 代表所有域名）未解析的请求转发给 Google 的 DNS 服务器。\n2. Hosts 文件 hosts 文件是一个静态的 DNS 记录文件，用于手动定义特定域名的 IP 地址。这类似于传统的 /etc/hosts 文件，用于快速、本地化地解析一些常用或特定的域名，而无需通过外部 DNS 服务器。\n主要用途：\n本地解析：快速解析常用域名，避免每次都去查询外部 DNS。 自定义解析：为本地网络设备或特定服务自定义域名解析记录。 简化开发和测试：开发和测试环境中，直接在 hosts 文件中添加记录，方便快捷。 配置示例：\n假设 hosts 文件内容如下：\n1 2 192.168.1.1 server1.local 192.168.1.2 server2.local 这表示将 server1.local 解析为 192.168.1.1，将 server2.local 解析为 192.168.1.2。\n总结 Forward 配置：用于指定 PowerDNS 在无法解析请求时，将请求转发给其他上游 DNS 服务器，确保解析的成功率和速度。 Hosts 文件：用于手动定义特定域名的 IP 地址，实现本地化、静态的 DNS 解析。 通过合理配置和使用 forward 和 hosts 文件，可以大大提升 PowerDNS 系统的灵活性和解析效率，满足各种不同的网络需求。\n1 touch {forward,hosts} 借鉴配置\n1 2 3 4 5 6 7 8 9 [root@prometheus-server pdns-recursor]# cat forward +fjf.com=127.0.0.1:5300 +sit.com=127.0.0.1:5300 +fjf=127.0.0.1:5300 +cloudcs.fjf=172.31.0.10 +168.192.in-addr.arpa=127.0.0.1:5300 +30.172.in-addr.arpa=172.31.0.10 +31.172.in-addr.arpa=172.31.0.10 +.=223.5.5.5,223.6.6.6,119.29.29.29 1 2 3 4 5 6 [root@prometheus-server pdns-recursor]# cat hosts 192.168.82.22 mvn.test.com 192.168.84.47 gitlab.test.com 192.168.81.11 mirrors.test.cn 192.168.82.22 img.test.cn 192.168.82.22 test.xxx.com 启动Pdns-recursor 1 2 3 4 5 [root@master pdns-recursor]# systemctl enable pdns-recursor.service Created symlink from /etc/systemd/system/multi-user.target.wants/pdns-recursor.service to /usr/lib/systemd/system/pdns-recursor.service. [root@master pdns-recursor]# systemctl start pdns-recursor [root@master pdns-recursor]# systemctl status pdns-recursor.service 7、PowerAdmin安装 1 2 3 wget https://nchc.dl.sourceforge.net/project/poweradmin/poweradmin-2.1.7.tgz yum -y install php56u php56u-fpm php56u-fpm tengine php56u-mcrypt php56u-pdo php56u-soap php56u-mysqlnd https://linux.cn/article-5623-2.html #跟随此连接安装即可 安装步骤\n1 2 打开浏览器 ，打开页面 http://172.17.20.20:90/install/ 需要移除从PowerAdmin的根目录中移除“install”文件夹，这一点很重要。使用以下命令： 添加主域名 添加子域名 8、成功测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [root@master pdns-recursor]# cat /etc/resolv.conf # Generated by NetworkManager nameserver 172.17.20.20 [root@master pdns-recursor]# nslookup www.fjf.com Server:\t172.17.20.20 Address:\t172.17.20.20#53 Non-authoritative answer: Name:\twww.fjf.com Address: 172.17.20.21 [root@master pdns-recursor]# nslookup test.fjf.com Server:\t172.17.20.20 Address:\t172.17.20.20#53 Non-authoritative answer: Name:\ttest.fjf.com Address: 172.17.20.21 [root@master pdns-recursor]# nslookup 112.efoxconn.com Server:\t172.17.20.20 Address:\t172.17.20.20#53 Non-authoritative answer: Name:\t112.efoxconn.com Address: 10.134.192.116 9、备服务器同上步骤 备份服务器 按照上面配置，可以进行安装 和配置文件scp\n1 2 3 4 5 6 7 8 9 10 11 12 yum install -y pdns pdns-backend-mysql yum install pdns-recursor -y [root@master pdns-recursor]# scp -r /etc/pdns/* 172.17.20.21:/etc/pdns/ [root@master pdns-recursor]# scp -r /etc/pdns-recursor/* 172.17.20.21:/etc/pdns-recursor/ [root@slave pdns]# systemctl start pdns \u0026amp;\u0026amp;systemctl status pdns [root@slave pdns]# systemctl start pdns-recursor \u0026amp;\u0026amp; systemctl status pdns-recursor.service [root@slave pdns]# systemctl enable pdns \u0026amp;\u0026amp; systemctl enable pdns-recursor 10、nginx负载均衡 安装nginx\n1 yum install -y nginx 把这个配置加入 nginx.conf 后面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 stream { # 添加socket转发的代理 upstream dns { server 172.17.20.21:5301 max_fails=3 fail_timeout=3s; server 172.17.20.21:5301 max_fails=3 fail_timeout=3s; } # 提供转发的服务，即访问localhost:30001，会跳转至代理bss_num_socket指定的转发地址 server { listen 53 udp; proxy_pass dns; proxy_timeout 3s; proxy_connect_timeout 3s; } } 1 2 3 4 5 6 7 8 9 [root@master ~]# netstat -ltunp |grep 53 tcp 0 0 127.0.0.1:5300 0.0.0.0:* LISTEN 39851/pdns_server tcp 0 0 0.0.0.0:53 0.0.0.0:* LISTEN 41809/nginx: master tcp 0 0 0.0.0.0:5301 0.0.0.0:* LISTEN 40930/pdns_recursor tcp6 0 0 :::5300 :::* LISTEN 39851/pdns_server udp 0 0 127.0.0.1:5300 0.0.0.0:* 39851/pdns_server udp 0 0 0.0.0.0:5301 0.0.0.0:* 40930/pdns_recursor udp 0 0 0.0.0.0:53 0.0.0.0:* 41809/nginx: master udp6 0 0 :::5300 :::* 39851/pdns_server 备份服务器的nginx也如上配置即可\n","date":"2024-07-28T17:21:50Z","image":"https://www.ownit.top/title_pic/68.jpg","permalink":"https://www.ownit.top/p/202407281721/","title":"PowerDNS架构解析与安装部署指南"},{"content":"1、问题背景 在软件开发过程中，版本控制是一个至关重要的环节。Git 作为一种流行的分布式版本控制系统，被广泛应用于各种项目中。然而，近期我们发现在进行项目发版时，Git 克隆项目的时间显著增加，严重影响了发版的效率。经过分析，我们发现问题主要出在项目文件过大，导致克隆过程缓慢。\n原因：开发误操作上传Jar包，导致项目变大，后面就算删除jar，但是有commit记录，依旧导致文件过大。 正常大小：226M 异常大小：2.6GB\n2、环境介绍 项目名称: xxxx_adc_backend 代码托管平台: GitLab 主要分支: pre-fjfsim 清理工具: BFG Repo-Cleaner 3、清理原因 在开发过程中，项目中引入了一些大文件，这些文件不仅增加了代码库的体积，还影响了代码的推送和拉取效率。为了提高项目的整体性能和维护性，我们决定采用 BFG Repo-Cleaner 进行清理。\n4、清理步骤 1. 开发人员禁止推送代码 在开始清理之前，需要确保所有开发人员停止推送代码，避免在清理过程中产生新的提交。\n2. 运维备份代码 运维人员需要对 xxxx_adc_backend 项目进行备份，以便在清理过程中出现问题时可以快速回滚。 3、查询大文件记录 参考文档：https://blog.csdn.net/cysear/article/details/102823671 注意：记录是commit中，最好定位到有问题的分支，在问题分支上操作。比如我的大文件记录，在pre-fjfsim 上，所以我指定的是 pre-fjfsim\n1 2 #克隆仓库的镜像 git clone --mirror -b pre-fjfsim git@gitlab.fujfu.com:ownit/ownit_test.git git clone --mirror：克隆仓库的镜像。镜像克隆会克隆所有分支和标签，但不会克隆工作目录和历史记录。 -b pre-fjfsim：指定克隆的分支为 pre-fjfsim。\ngit@gitlab.fujfu.com:ownit/ownit_test.git：仓库的 URL。\n1 2 #查找大文件的 SHA-1 哈希值 git rev-list --objects --all | grep \u0026#34;$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk \u0026#39;{print$1}\u0026#39;)\u0026#34; git rev-list --objects --all：列出所有对象的 SHA-1 哈希值。 1 grep \u0026#34;$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk \u0026#39;{print$1}\u0026#39;)\u0026#34; ：这部分是一个复杂的管道命令，用于过滤出大文件的 SHA-1 哈希值。\ngit verify-pack -v .git/objects/pack/*.idx：验证打包文件，并显示每个对象的详细信息。 sort -k 3 -n：根据第三列（文件大小）进行数字排序。 tail -5：显示排序后的最后五行，通常是最大的五个文件。 awk '{print$1}'：打印每行的第一个字段，即 SHA-1 哈希值。 grep：使用这些 SHA-1 哈希值作为搜索模式，从 git rev-list --objects --all 的输出中过滤出相关的对象。 4、使用 BFG 清理大文件 BFG Repo-Cleaner 是一个高效的工具，可以帮助我们快速清理 Git 历史中的大文件。以下是具体的清理步骤： 参考记录：https://rtyley.github.io/bfg-repo-cleaner/\n克隆项目 1 git clone --mirror -b pre-fjfsim git@gitlab.xxxx.com:ownit/ownit_test.git 清除大文件 1 2 java -jar bfg.jar --delete-files xxxx_adc_backend_mac.tgz ownit_test.git java -jar bfg.jar --delete-files xxxx_adc_backend_mac_2024_0202.tgz ownit_test.git 删除文件重构索引 1 2 cd ownit_test git reflog expire --expire=now --all \u0026amp;\u0026amp; git gc --prune=now --aggressive 查看容量 1 git count-objects -vH 更新远程 1 git push -f 5、开发人员检查 清理完成后，开发人员需要检查代码库是否正常。如果一切正常，则可以继续推送代码。如果发现异常，可以根据备份代码进行回滚。\n5、总结 通过这次清理，我们成功地从 xxxx_adc_backend 项目中移除了不必要的大文件，不仅减轻了代码库的负担，还提高了代码管理的效率。BFG Repo-Cleaner 以其高效和稳定性，成为了我们清理 Git 历史大文件的首选工具。\n","date":"2024-07-18T11:55:47Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/202407181155/","title":"处理.git文件夹过大出现臃肿问题"},{"content":"1、引言 在当今数字化时代，网络安全已成为企业和个人用户关注的焦点。IP黑白名单作为一种有效的网络安全策略，允许我们精确控制对Web资源的访问权限。通过白名单，我们可以确保只有可信的IP地址能够访问敏感资源；而黑名单则可以阻止恶意IP的访问，从而减少安全风险。\n选择Nginx OpenResty与Redis作为实现黑白名单的解决方案，是基于以下几个原因：\n高性能：Nginx以其轻量级和高性能著称，适合处理高并发请求。 灵活性：OpenResty通过集成Lua脚本，提供了强大的定制能力。 可扩展性：Redis作为一个内存数据结构存储，支持数据的快速读写，适合实现动态的黑白名单管理。 2、简介 1、什么是OpenResty？ OpenResty是一个基于Nginx的全功能Web平台，它集成了一系列精心设计的Lua库、第三方模块和一个基于LuaJIT的轻量级Web框架。OpenResty的核心是Nginx，但它通过Lua语言扩展了Nginx的功能，使其能够构建能够处理超高并发的动态Web应用。\n2、OpenResty与Nginx的关系 OpenResty是在Nginx的基础上构建的，它保留了Nginx的所有功能，并通过Lua语言扩展了其能力。这意味着你可以使用OpenResty来实现Nginx的所有功能，同时还能够利用Lua脚本来实现更复杂的业务逻辑。\n3、环境安装 1、环境版本 1 2 3 centos 7 redis 7.2 nginx version: openresty/1.25.3.1 2、环境安装 1、添加OpenResty仓库\n1 2 3 4 5 # 由于公共库中找不到openresty，所以需要添加openresty的源仓库 yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo # 注意，如果上面命令提示不存在,那就先安装一下 yum install -y yum-utils 安装OpenResty 1 2 3 4 # 安装openresty yum install -y openresty # 安装OpenResty管理工具，帮助我们安装第三方的Lua模块 yum install -y openresty-opm 3、目录结构 ​ 默认安装在/usr/local/openresty 看到里面有一个nginx目录，进去可以看到跟我们平常用的nginx是一模一样的，OpenResty就是在Nginx基础上集成了一些Lua模块\n到这里我们就安装好了\n4、启动和运行 OpenResty底层是基于Nginx的，查看OpenResty目录的nginx目录，结构与windows中安装的nginx基本一致\n5、安装配置redis\n1 sudo yum install redis -y Redis配置主要包括设置持久化选项、网络配置、安全性设置等。以下是一个基本的配置示例：\n1 2 3 4 5 # redis.conf port 6379 bind 127.0.0.1 protected-mode yes requirepass \u0026#34;yourpassword\u0026#34; 4、白名单实现 图例说明： 客户端：发送HTTP请求的用户或应用。 Nginx (OpenResty)：处理和管理HTTP请求，执行Lua脚本进行访问控制。 Lua脚本：在Nginx中运行，负责从Redis获取白名单并进行IP检查。 Redis：存储白名单数据，用于快速查询。 交互流程： 客户端发送请求到Nginx。 Nginx调用Lua脚本进行访问控制。 Lua脚本连接Redis，并查询IP是否在白名单中。 Lua脚本返回查询结果给Nginx。 Nginx根据结果决定是否允许请求，并返回响应给客户端 定义白名单的作用与重要性 白名单是一种安全策略，用于定义一组被信任的IP地址或实体，它们被允许访问特定的资源或服务。在Web应用中，白名单的作用尤为显著：\n安全性增强：限制访问权限，仅允许特定的IP地址访问敏感资源。 防止滥用：减少恶意用户或爬虫对服务的滥用。 流量管理：通过控制访问源，更有效地管理网络流量。 通过OpenResty Lua脚本实现白名单逻辑 在OpenResty中，我们可以使用Lua脚本来实现白名单逻辑。Lua脚本可以在Nginx的配置文件中直接编写，或者存储在外部文件中，并在配置文件中引用。\nLua脚本实现步骤： 定义白名单：在Redis中存储白名单IP地址。 访问控制：在Nginx配置中使用access_by_lua_block或access_by_lua_file指令调用Lua脚本。 脚本逻辑：检查请求的IP地址是否在白名单中，如果不在，则拒绝访问。 案例演示 jenkins.ownit.top.conf 这个是nginx的配置文件 最主要内容:access_by_lua_file /opt/nginx/lua_script/white.lua;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 upstream jenkins-uat { server 192.168.102.20:91; } server { listen 80; server_name jenkins.ownit.top; #白名单 或者 黑名单 #include /opt/nginx/whitelist/corporation.conf; location / { rewrite ^/(.*)$ https://$host/$1 permanent; } access_log /www/wwwlogs/dns.ownit.top.log; error_log /www/wwwlogs/dns.ownit.top.error.log; } server { listen 443 ssl; server_name jenkins.ownit.top; #白名单 或者 黑名单 #include /opt/nginx/whitelist/corporation.conf; #ssl on; ssl_certificate /opt/nginx/ssl/ownit.top.crt; ssl_certificate_key /opt/nginx/ssl/ownit.top.key; include ssl.conf; location / { access_by_lua_file /opt/nginx/lua_script/white.lua; # nginx的lua脚本 proxy_pass http://jenkins-uat; include https_proxy.conf; } access_log /www/wwwlogs/dns.ownit.top.log; error_log /www/wwwlogs/dns.ownit.top.error.log; } white.lua文件\n通过使用Lua脚本，在接收到HTTP请求时检查请求的IP地址是否在Redis存储的白名单中。如果IP不在白名单中，则拒绝访问。\n思路 获取客户端IP和请求路径：在Lua脚本中获取客户端的IP地址和请求路径。 连接Redis：使用resty.redis模块连接Redis数据库。 权限校验：对连接的Redis进行认证。 检查IP是否在白名单中：从Redis中检查IP是否存在于白名单集合中。 返回结果：如果IP在白名单中，允许访问；否则，返回403 Forbidden状态码，拒绝访问。 释放Redis连接：将Redis连接返回到连接池中，以便复用。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 -- 获取客户端IP和请求路径 local client_ip = ngx.var.remote_addr local path = ngx.var.uri -- Redis相关配置 local redis_key_white_list = \u0026#34;map_request_white_list\u0026#34; -- 连接Redis local redis = require \u0026#34;resty.redis\u0026#34; local red = redis:new() red:set_timeout(1000) -- 设置超时（毫秒） local ok, err = red:connect(\u0026#34;127.0.0.1\u0026#34;, 6379) if not ok then ngx.log(ngx.ERR, \u0026#34;Redis连接失败: \u0026#34;, err) return ngx.exit(500) end -- 权限校验 local res, err = red:auth(\u0026#34;123456\u0026#34;) if not res then ngx.say(\u0026#34;failed to authenticate: \u0026#34;, err) return end -- 检查IP是否在白名单中 local is_in_whitelist, err = red:sismember(redis_key_white_list, client_ip) if is_in_whitelist == 1 then ngx.log(ngx.INFO, \u0026#34;IP在白名单中: \u0026#34;, client_ip) else ngx.status = ngx.HTTP_FORBIDDEN ngx.say(\u0026#34;Access Denied\u0026#34;) return ngx.exit(ngx.HTTP_FORBIDDEN) end -- 返还redis连接到连接池 local ok, err = red:set_keepalive(10000, 100) if not ok then ngx.log(ngx.ERR, \u0026#34;设置keepalive失败: \u0026#34;, err) end 详细解释 获取客户端IP和请求路径\n1 2 local client_ip = ngx.var.remote_addr local path = ngx.var.uri 这两行代码从Nginx变量中获取客户端的IP地址和请求路径，client_ip用于后续的白名单检查。\nRedis相关配置\n1 local redis_key_white_list = \u0026#34;map_request_white_list\u0026#34; 定义存储白名单的Redis键。\n连接Redis\n1 2 3 4 5 6 7 8 local redis = require \u0026#34;resty.redis\u0026#34; local red = redis:new() red:set_timeout(1000) -- 设置超时（毫秒） local ok, err = red:connect(\u0026#34;127.0.0.1\u0026#34;, 6379) if not ok then ngx.log(ngx.ERR, \u0026#34;Redis连接失败: \u0026#34;, err) return ngx.exit(500) end 使用resty.redis模块创建Redis连接对象，并设置连接超时时间。尝试连接到Redis服务器，如果连接失败，记录错误日志并返回500错误。\n权限校验\n1 2 3 4 5 local res, err = red:auth(\u0026#34;123456\u0026#34;) if not res then ngx.say(\u0026#34;failed to authenticate: \u0026#34;, err) return end 对Redis进行认证，如果认证失败，输出错误信息并停止执行。\n检查IP是否在白名单中 1 2 3 4 5 6 7 8 local is_in_whitelist, err = red:sismember(redis_key_white_list, client_ip) if is_in_whitelist == 1 then ngx.log(ngx.INFO, \u0026#34;IP在白名单中: \u0026#34;, client_ip) else ngx.status = ngx.HTTP_FORBIDDEN ngx.say(\u0026#34;Access Denied\u0026#34;) return ngx.exit(ngx.HTTP_FORBIDDEN) end 使用SISMEMBER命令检查IP是否在Redis的白名单集合中。如果IP在白名单中，记录信息日志；否则，返回403 Forbidden状态码并拒绝访问。\n释放Redis连接 1 2 3 4 local ok, err = red:set_keepalive(10000, 100) if not ok then ngx.log(ngx.ERR, \u0026#34;设置keepalive失败: \u0026#34;, err) end 将Redis连接返回到连接池中，以便后续请求复用该连接。如果设置失败，记录错误日志。\n在Nginx中配置Lua脚本 在Nginx的location配置中，使用access_by_lua_file指令来调用上述Lua脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 http { ​```lua http { server { listen 80; server_name example.com; location / { access_by_lua_file /opt/nginx/lua_script/white.lua; proxy_pass http://backend; } } } 上述配置在接收到HTTP请求时，会首先执行/opt/nginx/lua_script/white.lua脚本进行白名单检查。如果通过检查，则继续将请求转发到后端服务器。\n成功访问： 禁止访问： 5、黑名单实现 黑名单的作用与场景 黑名单是一种网络安全机制，用于识别并阻止恶意IP地址对服务器资源的访问。它在多种场景中发挥着重要作用：\n防止恶意攻击：通过封禁已知的攻击者IP，减少服务器遭受的恶意攻击。 打击爬虫滥用：限制爬虫对网站资源的过度访问，保护数据不被滥用。 减轻服务器负载：通过限制特定IP的访问频率，减轻服务器压力，提高服务稳定性。 DDoS防御：快速响应DDoS攻击，封禁攻击源IP，保护服务可用性。 使用Lua脚本与Redis实现动态IP封禁 OpenResty结合Redis可以实现一个高效的动态IP封禁系统。以下是实现的关键步骤：\n环境配置：确保Nginx OpenResty和Redis环境准备就绪。 Lua脚本编写：编写Lua脚本来动态查询和更新Redis中的黑名单状态。 Nginx配置整合：在Nginx配置文件中集成Lua脚本，实现访问控制。 案例演示 通过使用Lua脚本，在接收到HTTP请求时检查请求的IP地址是否在黑名单中，或控制IP的访问频率，并进行相应的处理。\n思路 获取客户端IP：在Lua脚本中获取客户端的IP地址。 连接Redis：使用resty.redis模块连接Redis数据库。 权限校验：对连接的Redis进行认证。 检查IP是否在黑名单中：从Redis中检查IP是否存在于黑名单集合中。 访问频次控制：对每个IP的访问频次进行限制，如果超过指定的频次，则将IP加入黑名单。 返回结果：如果IP在黑名单中，返回403 Forbidden状态码，拒绝访问；否则，允许请求继续。 Nginx 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 upstream jms-uat { server 192.168.82.105:81; } server { listen 80; server_name jms.ownit.top; # 白名单 或者 黑名单 # include /opt/nginx/whitelist/corporation.conf; rewrite ^/(.*)$ https://$host/$1 permanent; access_log /www/wwwlogs/dns.ownit.top.log; error_log /www/wwwlogs/dns.ownit.top.error.log; } server { listen 443 ssl; server_name jms.ownit.top; # 白名单 或者 黑名单 # include /opt/nginx/whitelist/corporation.conf; ssl_certificate /opt/nginx/ssl/ownit.top.crt; ssl_certificate_key /opt/nginx/ssl/ownit.top.key; include ssl.conf; location = /core/auth/login/ { access_by_lua_file /opt/nginx/lua_script/login.lua; proxy_pass http://jms-uat; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; } location / { if ($request_uri !~ \\.(html|htm|jpg|png|ico|js|css)$) { access_by_lua_file /opt/nginx/lua_script/rule.lua; } proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; proxy_pass http://jms-uat; include https_proxy.conf; client_max_body_size 0; } access_log /www/wwwlogs/dns.ownit.top.log; error_log /www/wwwlogs/dns.ownit.top.error.log; } location = /core/auth/login/\n1 2 3 4 5 6 location = /core/auth/login/ { access_by_lua_file /opt/nginx/lua_script/login.lua; proxy_pass http://jms-uat; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; } 解释 location = /core/auth/login/： 这是一个精确匹配的location块，仅对请求路径严格等于/core/auth/login/的请求生效。 access_by_lua_file /opt/nginx/lua_script/login.lua;： 使用OpenResty的Lua模块，指定在访问控制阶段执行/opt/nginx/lua_script/login.lua脚本。这个脚本通常用于执行认证、权限检查或其他预处理逻辑。 proxy_pass http://jms-uat;： 将匹配到的请求代理到上游服务器http://jms-uat。 proxy_set_header Upgrade $http_upgrade;： 设置Upgrade请求头，支持WebSocket等协议升级。$http_upgrade变量包含原始请求中的Upgrade头字段的值。 proxy_set_header Connection \u0026quot;upgrade\u0026quot;;： 设置Connection请求头为upgrade，通常与Upgrade头一起使用，以确保连接升级。 location /\n1 2 3 4 5 6 7 8 9 10 11 location / { # 如果该location 下存在静态资源文件可以做一个判断 if ($request_uri !~ \\.(html|htm|jpg|png|ico|js|css)$) { access_by_lua_file /opt/nginx/lua_script/rule.lua; 加上了这条配置，则会根据 rule.lua 的规则进行限流 } proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; proxy_pass http://jms-uat; include https_proxy.conf; client_max_body_size 0; } 解释 location /： 这是一个通配符匹配的location块，表示所有路径的请求都会匹配到这个块，除非有其他更精确的匹配块。 if ($request_uri !~ \\.(html|htm|jpg|png|ico|js|css)$)： 使用if指令对请求路径进行检查。仅在请求路径不匹配指定的静态资源文件扩展名（如.html, .htm, .jpg, .png, .ico, .js, .css）时，执行后续的Lua脚本。这种检查有助于将动态资源与静态资源区分开来。 access_by_lua_file /opt/nginx/lua_script/rule.lua;： 使用OpenResty的Lua模块，指定在访问控制阶段执行/opt/nginx/lua_script/rule.lua脚本。这个脚本通常用于实现限流、防火墙或其他动态访问控制逻辑。 proxy_set_header Upgrade $http_upgrade;： 设置Upgrade请求头，支持WebSocket等协议升级。$http_upgrade变量包含原始请求中的Upgrade头字段的值。 proxy_set_header Connection \u0026quot;upgrade\u0026quot;;： 设置Connection请求头为upgrade，通常与Upgrade头一起使用，以确保连接升级。 proxy_pass http://jms-uat;： 将匹配到的请求代理到上游服务器http://jms-uat。 include https_proxy.conf;： 包含额外的配置文件https_proxy.conf，该文件可能包含HTTPS代理相关的其他配置项。 client_max_body_size 0;： 设置客户端请求体的最大大小为0，表示不限制请求体的大小。 这两个location块主要用于处理不同路径的请求，并在访问控制阶段使用Lua脚本进行相应的逻辑处理。精确匹配的/core/auth/login/路径专用于特定的认证或预处理，而通配符匹配的/路径则处理所有其他请求，并进行动态访问控制逻辑。两者共同确保了Nginx服务器的灵活性和安全性。\nLua脚本 (rule.lua) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 -- 连接池超时回收毫秒 local pool_max_idle_time = 10000 -- 连接池大小 local pool_size = 100 -- redis 连接超时时间 local redis_connection_timeout = 100 -- redis host local redis_host = \u0026#34;192.168.102.20\u0026#34; -- redis port local redis_port = \u0026#34;6379\u0026#34; -- redis auth local redis_auth = \u0026#34;123456\u0026#34; -- 封禁IP时间（秒） local ip_block_time = 120 -- 指定ip访问频率时间段（秒） local ip_time_out = 10 -- 指定ip访问频率计数最大值（次） local ip_max_count = 60 -- 错误日志记录 local function errlog(msg, ex) ngx.log(ngx.ERR, msg, ex) end -- 释放连接池 local function close_redis(red) if not red then return end local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.say(\u0026#34;redis connct err:\u0026#34;, err) return red:close() end end -- 连接redis local redis = require \u0026#34;resty.redis\u0026#34; local client = redis:new() local ok, err = client:connect(redis_host, redis_port) -- 连接失败返回服务器错误 if not ok then return end -- 设置超时时间 client:set_timeout(redis_connection_timeout) -- 优化验证密码操作 local connCount, err = client:get_reused_times() -- 新建连接，需要认证密码 if 0 == connCount then local ok, err = client:auth(redis_auth) if not ok then errlog(\u0026#34;failed to auth: \u0026#34;, err) return end elseif err then errlog(\u0026#34;failed to get reused times: \u0026#34;, err) return end -- 获取请求ip local function getIp() local clientIP = ngx.req.get_headers()[\u0026#34;X-Real-IP\u0026#34;] if clientIP == nil then clientIP = ngx.req.get_headers()[\u0026#34;x_forwarded_for\u0026#34;] end if clientIP == nil then clientIP = ngx.var.remote_addr end return clientIP end local clientIp = getIp() local incrKey = \u0026#34;limit:all:count:\u0026#34; .. clientIp local blockKey = \u0026#34;limit:all:block:\u0026#34; .. clientIp -- 查询ip是否被禁止访问，如果存在则返回403错误代码 local is_block, err = client:get(blockKey) if tonumber(is_block) == 1 then ngx.exit(ngx.HTTP_FORBIDDEN) close_redis(client) end local ip_count, err = client:incr(incrKey) if tonumber(ip_count) == 1 then client:expire(incrKey, ip_time_out) end -- 如果超过单位时间限制的访问次数，则添加限制访问标识，限制时间为ip_block_time if tonumber(ip_count) \u0026gt; tonumber(ip_max_count) then client:set(blockKey, 1) client:expire(blockKey, ip_block_time) end close_redis(client) 成功访问： 失败访问： 参考文档： https://www.cnblogs.com/KingArmy/p/18019489 https://blog.csdn.net/qq_45503196/article/details/134648292\n","date":"2024-07-15T16:47:03Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/202407151647/","title":"使用Nginx OpenResty与Redis实现高效IP黑白名单管理"},{"content":"1、简介 在现代的微服务架构中，对系统的实时监控变得尤为重要。Prometheus作为一款开源的系统监控与报警工具，以其强大的数据模型和灵活的查询语言，成为了监控领域的佼佼者。然而，对于特定的硬件设备，如基于Padavan固件的路由器，直接集成Prometheus可能并非易事。因此，开发一个针对Padavan设备的Prometheus exporter，能够极大地简化监控集成过程，提升运维效率。\n本文将以padavan_exporter项目为例，探讨如何基于此项目进行二次开发，以适应更广泛的监控需求。我们将深入分析代码结构，了解其工作原理，并探索如何扩展其功能。 项目地址：https://github.com/Bpazy/padavan_exporter 2、项目背景 padavan_exporter是一个专门为基于Padavan固件的路由器设计的Prometheus exporter。它通过SSH连接至路由器，收集包括CPU使用率、内存状态、网络接口状态、网络连接统计在内的关键指标，然后以Prometheus兼容的格式暴露这些指标，供Prometheus服务器抓取。\n3、环境介绍 在开始之前，我们需要确保开发环境的正确配置。以下是开发此项目所需的环境和工具：\nGo 语言：Padavan Exporter 是用 Go 语言编写的，因此我们需要安装 Go 语言环境。 Prometheus：用于采集和存储来自 Exporter 的指标数据。 Grafana：用于可视化 Prometheus 存储的数据（可选，但推荐）。 Padavan 固件设备：实际运行的 Padavan 固件设备，用于测试和验证。 4、设计思路 项目主要包括以下几个部分：\n命令行参数解析：使用 kingpin 库解析命令行参数，获取 SSH 连接信息和 Web 服务监听地址。 SSH 客户端初始化：使用 golang.org/x/crypto/ssh 库建立 SSH 连接，获取 Padavan 设备的性能数据。 Prometheus 收集器：实现 Prometheus 收集器，用于从 SSH 客户端获取数据并提供给 Prometheus 服务器。 HTTP 服务器：启动一个 HTTP 服务器，提供 /metrics 接口供 Prometheus 服务器抓取数据。 5、main.go 1. 命令行参数解析 在 init 函数中，使用 kingpin 库解析命令行参数。定义了四个命令行参数：\nweb.listen-address：监听的 HTTP 服务地址，默认为 :9100。 padavan.ssh.host：Padavan 设备的 SSH 主机地址，默认为 127.0.0.1:22。 padavan.ssh.username：SSH 登录的用户名，默认为 admin。 padavan.ssh.password：SSH 登录的密码，默认为 admin。 debug：是否开启调试模式。 通过解析这些参数，设置日志级别为 Debug，并输出解析后的参数值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func init() { // 定义命令行参数 la = kingpin.Flag(\u0026#34;web.listen-address\u0026#34;, \u0026#34;Address on which to expose metrics and web interface\u0026#34;).Default(\u0026#34;:9100\u0026#34;).String() ph = kingpin.Flag(\u0026#34;padavan.ssh.host\u0026#34;, \u0026#34;Padavan ssh host\u0026#34;).Default(\u0026#34;127.0.0.1:22\u0026#34;).String() pu = kingpin.Flag(\u0026#34;padavan.ssh.username\u0026#34;, \u0026#34;Padavan ssh username\u0026#34;).Default(\u0026#34;admin\u0026#34;).String() pp = kingpin.Flag(\u0026#34;padavan.ssh.password\u0026#34;, \u0026#34;Padavan ssh password\u0026#34;).Default(\u0026#34;admin\u0026#34;).String() isDebug := kingpin.Flag(\u0026#34;debug\u0026#34;, \u0026#34;Debug mode\u0026#34;).Bool() kingpin.Parse() if *isDebug { log.SetLevel(log.DebugLevel) } log.Debugf(\u0026#34;web.listen-address(%s) padavan.ssh.host(%s) padavan.ssh.username(%s) padavan.ssh.password(%s)\u0026#34;, *la, *ph, *pu, *pp) } 2. 初始化 SSH 客户端 initSshClient 函数用于创建和返回一个 SSH 客户端。该客户端用于与 Padavan 设备进行 SSH 连接，以收集监控数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func initSshClient() *ssh.Client { sshConfig := ssh.ClientConfig{ User: *pu, Auth: []ssh.AuthMethod{ssh.Password(*pp)}, Timeout: 5 * time.Second, HostKeyCallback: ssh.InsecureIgnoreHostKey(), } log.Printf(\u0026#34;Connecting to %s\u0026#34;, *ph) sshClient, err := ssh.Dial(\u0026#34;tcp\u0026#34;, *ph, \u0026amp;sshConfig) if err != nil { log.Fatalf(\u0026#34;create ssh client failed: %+v\u0026#34;, err) } return sshClient } 3. 初始化 Prometheus 注册表和收集器 在 main 函数中，初始化 Prometheus 注册表（pedantic registry）和 SSH 客户端。然后注册各种收集器到 Prometheus 注册表。这些收集器将从 Padavan 设备获取不同类型的性能数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func main() { reg := prometheus.NewPedanticRegistry() sc := initSshClient() reg.MustRegister(collector.NewLoadAverageCollector(sc)) reg.MustRegister(collector.NewNetDevController(sc)) reg.MustRegister(collector.NewCpuCollector(sc)) reg.MustRegister(collector.NewMemoryCollector(sc)) reg.MustRegister(collector.NewNetconnCollector(sc)) gatherers := prometheus.Gatherers{reg} h := promhttp.HandlerFor(gatherers, promhttp.HandlerOpts{ ErrorLog: log.StandardLogger(), ErrorHandling: promhttp.ContinueOnError, }) http.HandleFunc(\u0026#34;/metrics\u0026#34;, func(w http.ResponseWriter, r *http.Request) { h.ServeHTTP(w, r) }) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintln(w, homePage()) }) log.Printf(\u0026#34;Start server at %s\u0026#34;, *la) log.Fatal(http.ListenAndServe(*la, nil)) } 4. 启动 HTTP 服务器 在 main 函数中，启动一个 HTTP 服务器，提供 /metrics 接口供 Prometheus 服务器抓取数据。同时提供一个简单的首页，通过 / 路径访问，显示一些基本信息和有用的链接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func homePage() string { return ` \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;padavan_exporter\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;padavan_exporter\u0026lt;/h2\u0026gt; \u0026lt;span\u0026gt;See docs at \u0026lt;a href=\u0026#34;https://github.com/Bpazy/padavan_exporter\u0026#34;\u0026gt;https://github.com/Bpazy/padavan_exporter\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;span\u0026gt; Useful endpoints: \u0026lt;/span\u0026gt; \u0026lt;br\u0026gt; \u0026lt;span\u0026gt; \u0026lt;a href=\u0026#34;/metrics\u0026#34;\u0026gt;metrics\u0026lt;/a\u0026gt; \u0026lt;span\u0026gt; - available service metrics \u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` } 5、整体代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/Bpazy/padavan_exporter/collector\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; log \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;golang.org/x/crypto/ssh\u0026#34; \u0026#34;gopkg.in/alecthomas/kingpin.v2\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) var ( ph *string // Padavan address pu *string // Padavan username pp *string // Padavan password la *string // Address on which to expose metrics and web interface ) // init 函数用于初始化命令行参数。 // 通过 kingpin 解析命令行参数，包括 web 服务监听地址、Padavan SSH 主机地址、用户名、密码等。 // 并根据是否开启 debug 模式设置日志级别。 func init() { // 定义命令行参数 la = kingpin.Flag(\u0026#34;web.listen-address\u0026#34;, \u0026#34;Address on which to expose metrics and web interface\u0026#34;).Default(\u0026#34;:9100\u0026#34;).String() ph = kingpin.Flag(\u0026#34;padavan.ssh.host\u0026#34;, \u0026#34;Padavan ssh host\u0026#34;).Default(\u0026#34;127.0.0.1:22\u0026#34;).String() pu = kingpin.Flag(\u0026#34;padavan.ssh.username\u0026#34;, \u0026#34;Padavan ssh username\u0026#34;).Default(\u0026#34;admin\u0026#34;).String() pp = kingpin.Flag(\u0026#34;padavan.ssh.password\u0026#34;, \u0026#34;Padavan ssh password\u0026#34;).Default(\u0026#34;admin\u0026#34;).String() // 解析命令行参数，并在 debug 模式下设置日志级别 isDebug := kingpin.Flag(\u0026#34;debug\u0026#34;, \u0026#34;Debug mode\u0026#34;).Bool() kingpin.Parse() if *isDebug { log.SetLevel(log.DebugLevel) } log.Debugf(\u0026#34;web.listen-address(%s) padavan.ssh.host(%s) padavan.ssh.username(%s) padavan.ssh.password(%s)\u0026#34;, *la, *ph, *pu, *pp) } // main 函数作为程序入口点，负责初始化 Prometheus 监控数据收集器、SSH 客户端，并启动 HTTP 服务提供监控数据。 func main() { // 初始化 Prometheus 注册表和 SSH 客户端 reg := prometheus.NewPedanticRegistry() sc := initSshClient() // 注册各种收集器到 Prometheus reg.MustRegister(collector.NewLoadAverageCollector(sc)) reg.MustRegister(collector.NewNetDevController(sc)) reg.MustRegister(collector.NewCpuCollector(sc)) reg.MustRegister(collector.NewMemoryCollector(sc)) reg.MustRegister(collector.NewNetconnCollector(sc)) gatherers := prometheus.Gatherers{reg} h := promhttp.HandlerFor(gatherers, promhttp.HandlerOpts{ ErrorLog: log.StandardLogger(), ErrorHandling: promhttp.ContinueOnError, }) // 处理 /metrics 请求以提供监控数据 http.HandleFunc(\u0026#34;/metrics\u0026#34;, func(w http.ResponseWriter, r *http.Request) { h.ServeHTTP(w, r) }) // 处理根路径请求，提供简单信息页面 http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { _, _ = fmt.Fprintln(w, homePage()) }) log.Printf(\u0026#34;Start server at %s\u0026#34;, *la) // 启动 HTTP 服务 log.Fatal(http.ListenAndServe(*la, nil)) } func homePage() string { return ` \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;padavan_exporter\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;padavan_exporter\u0026lt;/h2\u0026gt; \u0026lt;span\u0026gt;See docs at \u0026lt;a href=\u0026#34;https://github.com/Bpazy/padavan_exporter\u0026#34;\u0026gt;https://github.com/Bpazy/padavan_exporter\u0026lt;/a\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;span\u0026gt; Useful endpoints: \u0026lt;/span\u0026gt; \u0026lt;br\u0026gt; \u0026lt;span\u0026gt; \u0026lt;a href=\u0026#34;/metrics\u0026#34;\u0026gt;metrics\u0026lt;/a\u0026gt; \u0026lt;span\u0026gt; - available service metrics \u0026lt;/span\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` } // initSshClient 函数用于初始化并返回一个 SSH 客户端。 // 该客户端用于与 Padavan 设备进行 SSH 连接以收集监控数据。 func initSshClient() *ssh.Client { // 创建 SSH 客户端配置，包括用户名、密码和一些连接选项 sshConfig := ssh.ClientConfig{ User: *pu, Auth: []ssh.AuthMethod{ssh.Password(*pp)}, Timeout: 5 * time.Second, HostKeyCallback: ssh.InsecureIgnoreHostKey(), } log.Printf(\u0026#34;Connecting to %s\u0026#34;, *ph) // 尝试建立 SSH 连接 sshClient, err := ssh.Dial(\u0026#34;tcp\u0026#34;, *ph, \u0026amp;sshConfig) if err != nil { log.Fatalf(\u0026#34;create ssh client failed: %+v\u0026#34;, err) } return sshClient } 6、collector.go 1、整体思路 建立 SSH 会话并执行命令：通过 SSH 客户端连接到远程设备并执行命令，获取命令的输出结果。 获取文件内容：通过执行 cat 命令获取远程设备上指定文件的内容。 解析浮点数：将字符串形式的数字解析为浮点数。 2、功能解释 import 部分\n1 2 3 4 5 6 import ( \u0026#34;fmt\u0026#34; log \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;golang.org/x/crypto/ssh\u0026#34; \u0026#34;strconv\u0026#34; ) 这部分代码导入了所需的包。fmt 用于格式化输出，log 用于日志记录，ssh 用于 SSH 连接，strconv 用于字符串和数值之间的转换。\nmustGetContent 函数\n1 2 3 4 5 6 7 func mustGetContent(sshClient *ssh.Client, path string) string { rsp, err := execCommand(sshClient, \u0026#34;cat \u0026#34;+path) if err != nil { panic(err) } return rsp } 这个函数接受一个 SSH 客户端和一个文件路径，通过调用 execCommand 函数执行 cat 命令获取文件内容。如果执行命令失败，会引发 panic。成功执行后，返回文件内容作为字符串。\nexecCommand 函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 func execCommand(sshClient *ssh.Client, command string) (string, error) { session, err := sshClient.NewSession() if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;create ssh session failed: %+v\u0026#34;, err) } defer session.Close() rsp, err := session.CombinedOutput(command) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;execute ssh command failed: %+v\u0026#34;, err) } return string(rsp), nil } 这个函数接受一个 SSH 客户端和一个命令，创建一个新的 SSH 会话并执行命令，获取命令的输出结果。如果创建会话或执行命令失败，返回相应的错误信息。成功执行后，返回命令输出结果作为字符串。\nmustParseFloat 函数\n1 2 3 4 5 6 7 8 func mustParseFloat(fs string) float64 { float, err := strconv.ParseFloat(fs, 32) if err != nil { log.Printf(\u0026#34;%+v\u0026#34;, err) return 0 } return float } 这个函数接受一个字符串，尝试将其解析为浮点数。如果解析失败，记录错误信息并返回 0。成功解析后，返回浮点数。\n3、整体代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import ( \u0026#34;fmt\u0026#34; log \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;golang.org/x/crypto/ssh\u0026#34; \u0026#34;strconv\u0026#34; ) // mustGetContent 从指定路径读取文件内容 func mustGetContent(sshClient *ssh.Client, path string) string { // 执行SSH命令读取文件内容 rsp, err := execCommand(sshClient, \u0026#34;cat \u0026#34;+path) if err != nil { // 如果发生错误，panic panic(err) } // 返回文件内容 return rsp } // execCommand 执行SSH命令并返回结果 func execCommand(sshClient *ssh.Client, command string) (string, error) { // 创建一个新的SSH会话 session, err := sshClient.NewSession() if err != nil { // 如果创建会话失败，返回错误 return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;create ssh session failed: %+v\u0026#34;, err) } // 确保会话在函数结束时关闭 defer session.Close() // 执行命令并获取结果 rsp, err := session.CombinedOutput(command) if err != nil { // 如果执行命令失败，返回错误 return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;execute ssh command failed: %+v\u0026#34;, err) } // 返回命令执行结果 return string(rsp), nil } // mustParseFloat 解析字符串为浮点数 func mustParseFloat(fs string) float64 { // 尝试解析字符串为浮点数 float, err := strconv.ParseFloat(fs, 32) if err != nil { // 如果解析失败，记录错误并返回0 log.Printf(\u0026#34;%+v\u0026#34;, err) return 0 } // 返回解析后的浮点数 return float } 7、memory.go 1、整体思路 这个代码片段的目的是通过 Prometheus 采集远程设备的内存指标。通过 SSH 连接远程设备，读取 /proc/meminfo 文件的内容，然后解析出相关的内存信息，最后将这些信息作为 Prometheus 指标发送。\n2、功能解释 正则表达式\n1 2 3 var ( meminfoReg = regexp.MustCompile(`(\\w+):\\s+(\\d+) kB`) ) 这个正则表达式用于匹配 /proc/meminfo 文件中的每一行，捕获内存指标的名称和数值。\nmemoryCollector 结构体\n1 2 3 4 type memoryCollector struct { metrics map[string]*prometheus.Desc // 存储指标描述信息 sc *ssh.Client // SSH客户端，用于远程收集指标信息 } memoryCollector 结构体包含两个字段：一个用于存储 Prometheus 指标描述信息的字典 metrics，另一个是 SSH 客户端 sc，用于连接远程设备。\nDescribe 方法\n1 2 3 func (m *memoryCollector) Describe(ch chan\u0026lt;- *prometheus.Desc) { // metrics created when Collect } 这个方法是 Prometheus 采集器接口的一部分，用于描述指标。但在这个实现中，指标是在 Collect 方法中动态创建的，所以这里没有具体实现。\nCollect 方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 func (m *memoryCollector) Collect(ch chan\u0026lt;- prometheus.Metric) { // 通过SSH客户端获取远程机器的/proc/meminfo文件内容 content := mustGetContent(m.sc, \u0026#34;/proc/meminfo\u0026#34;) // 创建Scanner用于逐行读取内容 scanner := bufio.NewScanner(strings.NewReader(content)) // 使用正则表达式匹配行内容，获取指标名和值 for scanner.Scan() { parts := meminfoReg.FindStringSubmatch(scanner.Text()) if len(parts) != 3 { // 如果匹配不成功，则跳过当前行 continue } // 指标名 key := parts[1] // 解析指标值 value, err := strconv.ParseFloat(parts[2], 64) if err != nil { continue // 如果解析失败，则跳过当前行 } value *= 1024 // 将值从kB转换为B var desc *prometheus.Desc var ok bool // 根据指标名，获取或创建对应的Desc对象 switch key { case \u0026#34;MemTotal\u0026#34;: desc, ok = m.metrics[\u0026#34;MemTotal\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_total_bytes\u0026#34;, \u0026#34;Total memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;MemTotal\u0026#34;] = desc } case \u0026#34;MemFree\u0026#34;: desc, ok = m.metrics[\u0026#34;MemFree\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_free_bytes\u0026#34;, \u0026#34;Free memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;MemFree\u0026#34;] = desc } case \u0026#34;Buffers\u0026#34;: desc, ok = m.metrics[\u0026#34;Buffers\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_buffers_bytes\u0026#34;, \u0026#34;Buffers memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;Buffers\u0026#34;] = desc } case \u0026#34;Cached\u0026#34;: desc, ok = m.metrics[\u0026#34;Cached\u0026#34;] if (!ok) { desc = prometheus.NewDesc(\u0026#34;node_memory_cached_bytes\u0026#34;, \u0026#34;Cached memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;Cached\u0026#34;] = desc } default: continue // 如果不是我们关心的指标，则跳过当前行 } // 向通道发送指标 ch \u0026lt;- prometheus.MustNewConstMetric(desc, prometheus.GaugeValue, value) } } 这个方法是 Prometheus 采集器接口的另一部分，用于收集指标数据并向 Prometheus 发送。具体步骤如下：\n通过 mustGetContent 函数获取远程设备上 /proc/meminfo 文件的内容。 使用 bufio.Scanner 逐行读取文件内容。 使用正则表达式匹配每一行，提取内存指标名称和值。 根据指标名称创建或获取对应的 prometheus.Desc 对象。 将解析后的指标值乘以 1024，从 kB 转换为字节。 使用 prometheus.MustNewConstMetric 函数创建 Prometheus 指标并发送到通道 ch。 NewMemoryCollector 函数\n1 2 3 4 5 6 func NewMemoryCollector(sc *ssh.Client) *memoryCollector { return \u0026amp;memoryCollector{ sc: sc, metrics: map[string]*prometheus.Desc{}, } } 这个函数用于创建一个新的 memoryCollector 实例，初始化 SSH 客户端和指标描述信息的字典。\n3、整体流程 初始化：通过 NewMemoryCollector 函数创建一个 memoryCollector 实例，传入 SSH 客户端。\n收集指标：\nCollect 方法通过 SSH 客户端获取远程设备上的 /proc/meminfo 文件内容。 使用正则表达式解析文件内容，提取内存指标。 将解析后的指标转换为 Prometheus 格式并发送到通道 ch。 描述指标：Describe 方法用于向 Prometheus 描述指标，但具体实现中是在 Collect 方法中动态创建指标。\n4、整体代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 package collector import ( \u0026#34;bufio\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;golang.org/x/crypto/ssh\u0026#34; \u0026#34;regexp\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) /** * @Author: 南宫乘风 * @Description: * @File: memory.go * @Email: 1794748404@qq.com * @Date: 2024-05-22 15:05 */ var ( meminfoReg = regexp.MustCompile(`(\\w+):\\s+(\\d+) kB`) ) // memoryCollector 收集内存相关指标信息 type memoryCollector struct { metrics map[string]*prometheus.Desc // 存储指标描述信息 sc *ssh.Client // SSH客户端，用于远程收集指标信息 } // Describe 向通道发送描述指标的Desc对象 // 这个方法不具体实现，因为metrics在Collect方法中动态创建。 func (m *memoryCollector) Describe(ch chan\u0026lt;- *prometheus.Desc) { // metrics created when Collect } // Collect 向通道发送内存指标数据 // @param ch 用于发送指标的通道 func (m *memoryCollector) Collect(ch chan\u0026lt;- prometheus.Metric) { // 通过SSH客户端获取远程机器的/proc/meminfo文件内容 content := mustGetContent(m.sc, \u0026#34;/proc/meminfo\u0026#34;) // 创建Scanner用于逐行读取内容 scanner := bufio.NewScanner(strings.NewReader(content)) // 使用正则表达式匹配行内容，获取指标名和值 for scanner.Scan() { parts := meminfoReg.FindStringSubmatch(scanner.Text()) if len(parts) != 3 { // 如果匹配不成功，则跳过当前行 continue } // 指标名 key := parts[1] // 解析指标值 value, err := strconv.ParseFloat(parts[2], 64) if err != nil { continue // 如果解析失败，则跳过当前行 } value *= 1024 // 将值从kB转换为B var desc *prometheus.Desc var ok bool // 根据指标名，获取或创建对应的Desc对象 switch key { case \u0026#34;MemTotal\u0026#34;: desc, ok = m.metrics[\u0026#34;MemTotal\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_total_bytes\u0026#34;, \u0026#34;Total memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;MemTotal\u0026#34;] = desc } case \u0026#34;MemFree\u0026#34;: desc, ok = m.metrics[\u0026#34;MemFree\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_free_bytes\u0026#34;, \u0026#34;Free memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;MemFree\u0026#34;] = desc } case \u0026#34;Buffers\u0026#34;: desc, ok = m.metrics[\u0026#34;Buffers\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_buffers_bytes\u0026#34;, \u0026#34;Buffers memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;Buffers\u0026#34;] = desc } case \u0026#34;Cached\u0026#34;: desc, ok = m.metrics[\u0026#34;Cached\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_cached_bytes\u0026#34;, \u0026#34;Cached memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;Cached\u0026#34;] = desc } default: continue // 如果不是我们关心的指标，则跳过当前行 } // 向通道发送指标 ch \u0026lt;- prometheus.MustNewConstMetric(desc, prometheus.GaugeValue, value) } } // NewMemoryCollector 创建一个新的memoryCollector实例 // @param sc SSH客户端 // @return 返回memoryCollector实例的指针 func NewMemoryCollector(sc *ssh.Client) *memoryCollector { return \u0026amp;memoryCollector{ sc: sc, metrics: map[string]*prometheus.Desc{}, } } 8、调用 NewMemoryCollector 的原理 NewMemoryCollector 函数用于创建一个新的 memoryCollector 实例。这个实例负责从 Padavan 设备中收集内存相关的指标信息。调用 NewMemoryCollector 的过程和原理如下：\n1、NewMemoryCollector 函数的定义 NewMemoryCollector 函数接收一个 *ssh.Client 作为参数并返回一个 *memoryCollector 实例。\n1 2 3 4 5 6 func NewMemoryCollector(sc *ssh.Client) *memoryCollector { return \u0026amp;memoryCollector{ sc: sc, metrics: map[string]*prometheus.Desc{}, } } 2、memoryCollector 的定义 memoryCollector 结构体包含两个字段：\nmetrics：一个 map[string]*prometheus.Desc，用于存储指标的描述信息。 sc：一个 *ssh.Client，用于通过 SSH 连接到 Padavan 设备以收集监控数据。 1 2 3 4 type memoryCollector struct { metrics map[string]*prometheus.Desc // 存储指标描述信息 sc *ssh.Client // SSH客户端，用于远程收集指标信息 } 3、在 main 函数中调用 NewMemoryCollector 在 main 函数中，首先初始化 Prometheus 注册表和 SSH 客户端，然后调用 NewMemoryCollector 函数创建一个 memoryCollector 实例，并将其注册到 Prometheus 注册表中。\n1 reg.MustRegister(collector.NewMemoryCollector(sc)) 4、memoryCollector 的工作原理 初始化 SSH 客户端： initSshClient 函数创建一个 SSH 客户端，用于连接到 Padavan 设备。 创建 memoryCollector 实例： 调用 NewMemoryCollector(sc)，传入 SSH 客户端，返回一个 memoryCollector 实例。 注册收集器： 使用 reg.MustRegister(collector.NewMemoryCollector(sc)) 将 memoryCollector 注册到 Prometheus 注册表中。这样 Prometheus 就可以通过 /metrics 接口调用 memoryCollector 的 Collect 方法收集内存指标数据。 收集指标数据： memoryCollector 的 Collect 方法通过 SSH 客户端连接到 Padavan 设备，读取 /proc/meminfo 文件，解析文件内容并生成 Prometheus 指标，然后将指标发送到通道中供 Prometheus 收集。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 func (m *memoryCollector) Collect(ch chan\u0026lt;- prometheus.Metric) { // 通过SSH客户端获取远程机器的/proc/meminfo文件内容 content := mustGetContent(m.sc, \u0026#34;/proc/meminfo\u0026#34;) // 创建Scanner用于逐行读取内容 scanner := bufio.NewScanner(strings.NewReader(content)) // 使用正则表达式匹配行内容，获取指标名和值 for scanner.Scan() { parts := meminfoReg.FindStringSubmatch(scanner.Text()) if len(parts) != 3 { // 如果匹配不成功，则跳过当前行 continue } // 指标名 key := parts[1] // 解析指标值 value, err := strconv.ParseFloat(parts[2], 64) if err != nil { continue // 如果解析失败，则跳过当前行 } value *= 1024 // 将值从kB转换为B var desc *prometheus.Desc var ok bool // 根据指标名，获取或创建对应的Desc对象 switch key { case \u0026#34;MemTotal\u0026#34;: desc, ok = m.metrics[\u0026#34;MemTotal\u0026#34;] if !ok { desc = prometheus.NewDesc(\u0026#34;node_memory_total_bytes\u0026#34;, \u0026#34;Total memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;MemTotal\u0026#34;] = desc } case \u0026#34;MemFree\u0026#34;: desc, ok = m.metrics[\u0026#34;MemFree\u0026#34;] if (!ok) { desc = prometheus.NewDesc(\u0026#34;node_memory_free_bytes\u0026#34;, \u0026#34;Free memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;MemFree\u0026#34;] = desc } case \u0026#34;Buffers\u0026#34;: desc, ok = m.metrics[\u0026#34;Buffers\u0026#34;] if (!ok) { desc = prometheus.NewDesc(\u0026#34;node_memory_buffers_bytes\u0026#34;, \u0026#34;Buffers memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;Buffers\u0026#34;] = desc } case \u0026#34;Cached\u0026#34;: desc, ok = m.metrics[\u0026#34;Cached\u0026#34;] if (!ok) { desc = prometheus.NewDesc(\u0026#34;node_memory_cached_bytes\u0026#34;, \u0026#34;Cached memory in bytes.\u0026#34;, nil, nil) m.metrics[\u0026#34;Cached\u0026#34;] = desc } default: continue // 如果不是我们关心的指标，则跳过当前行 } // 向通道发送指标 ch \u0026lt;- prometheus.MustNewConstMetric(desc, prometheus.GaugeValue, value) } } ","date":"2024-07-09T10:24:28Z","image":"https://www.ownit.top/title_pic/27.jpg","permalink":"https://www.ownit.top/p/202407091024/","title":"基于Padavan Exporter的Prometheus客户端二次开发实践"},{"content":"1、简介 在现代 IT 基础设施中，管理大量服务器是一项复杂而繁琐的任务。特别是在检查服务器的存活状态以及 SSH 登录等任务上，手动操作非常耗时且容易出错。本文将介绍如何使用 Python 脚本实现对多台服务器的批量检查和管理，包括检查服务器是否在线，以及通过密码或 SSH 密钥登录服务器。\n2、背景 在我们测试机房环境，为了方便管理和使用。需要统一 账号，登录方式，以及堡垒机安全验证。在之前架构基础上，我们需要梳理整合现有所有测试机器。 需要批量管理和监控多台服务器。例如，检查服务器是否存活、是否可以通过 SSH 登录等。手动执行这些任务效率低且容易出错。通过编写自动化脚本，可以大大提高工作效率和准确性。\n3、环境介绍 1、依赖库 paramiko：用于 SSH 登录。 tqdm：用于显示进度条。 concurrent.futures：用于多线程处理。 可以通过以下命令安装这些库：\n1 pip install paramiko tqdm 2、文件结构 hosts：包含服务器 IP 地址的文件，每行一个 IP 地址。 ssh_key：SSH 私钥文件路径。 script.py：主脚本文件。 4、Python实现步骤 方便统计使用，归档文件 后期整理维护\n第一步：读取 IP 地址 首先，我们需要读取 hosts 文件中的 IP 地址。每行一个 IP 地址。\n1 2 3 # 读取 IP 地址 with open(\u0026#39;hosts\u0026#39;, \u0026#39;r\u0026#39;) as file: ip_addresses = [line.strip() for line in file.readlines()] 第二步：检查 IP 是否存活 我们使用 ping 命令检查每个 IP 是否存活。通过 subprocess 模块执行 ping 命令，并检查返回码来判断 IP 是否存活。\n1 2 3 4 5 6 7 8 9 10 11 import subprocess def is_alive(ip): try: #这里注意判断 # For Unix/Linux/Mac result = subprocess.run([\u0026#39;ping\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;1\u0026#39;, ip], stdout=subprocess.PIPE, stderr=subprocess.PIPE) except FileNotFoundError: # For Windows result = subprocess.run([\u0026#39;ping\u0026#39;, \u0026#39;-n\u0026#39;, \u0026#39;1\u0026#39;, ip], stdout=subprocess.PIPE, stderr=subprocess.PIPE) return result.returncode == 0 第三步：尝试 SSH 登录 我们使用 paramiko 库尝试通过密码和 SSH 密钥登录服务器。为了处理 RSA 格式的密钥，我们使用 paramiko.RSAKey.from_private_key_file 函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import paramiko from paramiko import SSHClient, AutoAddPolicy, RSAKey def ssh_login_with_password(ip, username, password): try: client = SSHClient() client.set_missing_host_key_policy(AutoAddPolicy()) client.connect(ip, username=username, password=password, timeout=5) client.close() return True except Exception as e: return False def ssh_login_with_key(ip, username, key_path): try: client = SSHClient() client.set_missing_host_key_policy(AutoAddPolicy()) key = RSAKey.from_private_key_file(key_path) client.connect(ip, username=username, pkey=key, timeout=5) client.close() return True except Exception as e: return False 第四步：并行处理 IP 地址 为了提高效率，我们使用 concurrent.futures.ThreadPoolExecutor 实现多线程处理。每个线程会检查一个 IP 的存活状态，并尝试通过密码和 SSH 密钥登录。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from concurrent.futures import ThreadPoolExecutor, as_completed from tqdm import tqdm def check_ip(ip): if not is_alive(ip): return (\u0026#39;non_alive\u0026#39;, ip) else: if ssh_login_with_password(ip, USERNAME, PASSWORD): return (\u0026#39;ssh_password_success\u0026#39;, ip) elif ssh_login_with_key(ip, USERNAME, KEY_PATH): return (\u0026#39;ssh_key_success\u0026#39;, ip) else: return (\u0026#39;ssh_failures\u0026#39;, ip) with ThreadPoolExecutor(max_workers=10) as executor: futures = {executor.submit(check_ip, ip): ip for ip in ip_addresses} for future in tqdm(as_completed(futures), total=len(ip_addresses), desc=\u0026#34;Checking IPs\u0026#34;): result, ip = future.result() if result == \u0026#39;non_alive\u0026#39;: non_alive_ips.append(ip) elif result == \u0026#39;ssh_password_success\u0026#39;: ssh_password_success.append(ip) elif result == \u0026#39;ssh_key_success\u0026#39;: ssh_key_success.append(ip) elif result == \u0026#39;ssh_failures\u0026#39;: ssh_failures.append(ip) 第五步：生成结果文件 最后，我们将检查结果写入一个文件中，按照指定的格式记录每个 IP 的状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 写入结果到文件 with open(\u0026#39;output.txt\u0026#39;, \u0026#39;w\u0026#39;) as output_file: output_file.write(\u0026#34;[no_alive]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(non_alive_ips) + \u0026#34;\\n\u0026#34;) output_file.write(\u0026#34;[password]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(ssh_password_success) + \u0026#34;\\n\u0026#34;) output_file.write(\u0026#34;[key]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(ssh_key_success) + \u0026#34;\\n\u0026#34;) output_file.write(\u0026#34;[fail]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(ssh_failures) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Results have been written to output.txt\u0026#34;) 完整的代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 # -*- coding: utf-8 -*- # @Time : 2024-06-27 11:46 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : kvm.py # @Software: PyCharm import os import subprocess from paramiko import SSHClient, AutoAddPolicy from tqdm import tqdm from concurrent.futures import ThreadPoolExecutor, as_completed # 读取 IP 地址 with open(\u0026#39;hosts\u0026#39;, \u0026#39;r\u0026#39;) as file: ip_addresses = [line.strip() for line in file.readlines()] # 初始化列表 non_alive_ips = [] ssh_password_success = [] ssh_key_success = [] ssh_failures = [] # 检查 IP 存活状态 def is_alive(ip): # For Windows result = subprocess.run([\u0026#39;ping\u0026#39;, \u0026#39;-n\u0026#39;, \u0026#39;1\u0026#39;, ip], stdout=subprocess.PIPE, stderr=subprocess.PIPE) return result.returncode == 0 # 尝试使用密码进行 SSH 登录 def ssh_login_with_password(ip, username, password): try: client = SSHClient() client.set_missing_host_key_policy(AutoAddPolicy()) client.connect(ip, username=username, password=password, timeout=5) client.close() return True except Exception as e: return False # 尝试使用 SSH 密钥进行登录 def ssh_login_with_key(ip, username, key_path): try: client = SSHClient() client.set_missing_host_key_policy(AutoAddPolicy()) client.connect(ip, username=username, key_filename=key_path, timeout=5) client.close() return True except Exception as e: return False # 用户名和密码/密钥路径配置 USERNAME = \u0026#39;root\u0026#39; PASSWORD = \u0026#39;xxxx.88\u0026#39; KEY_PATH = r\u0026#39;E:\\Code\\Gitlab_Code\\aliyun\\kvm_centos\\ssh_key\u0026#39; def check_ip(ip): if not is_alive(ip): return \u0026#39;non_alive\u0026#39;, ip else: if ssh_login_with_password(ip, USERNAME, PASSWORD): return \u0026#39;ssh_password_success\u0026#39;, ip elif ssh_login_with_key(ip, USERNAME, KEY_PATH): return \u0026#39;ssh_key_success\u0026#39;, ip else: return \u0026#39;ssh_failures\u0026#39;, ip # 检查每个 IP 地址 # 使用多线程检查 IP 地址 # 使用ThreadPoolExecutor来并发执行任务，最大工作线程数为10 with ThreadPoolExecutor(max_workers=10) as executor: # 提交检查每个IP地址的任务，并将任务对象与IP地址映射关系存储在字典futures中 futures = {executor.submit(check_ip, ip): ip for ip in ip_addresses} # 遍历所有完成的任务，使用tqdm显示进度条 for future in tqdm(as_completed(futures), total=len(ip_addresses), desc=\u0026#34;Checking IPs\u0026#34;): # 获取任务执行结果和对应的IP地址 result, ip = future.result() # 根据检查结果，将IP地址添加到相应的列表中 if result == \u0026#39;non_alive\u0026#39;: non_alive_ips.append(ip) elif result == \u0026#39;ssh_password_success\u0026#39;: ssh_password_success.append(ip) elif result == \u0026#39;ssh_key_success\u0026#39;: ssh_key_success.append(ip) elif result == \u0026#39;ssh_failures\u0026#39;: ssh_failures.append(ip) # 输出结果 print(\u0026#34;Non-alive IPs:\u0026#34;, non_alive_ips) print(\u0026#34;SSH login with password successful:\u0026#34;, ssh_password_success) print(\u0026#34;SSH login with key successful:\u0026#34;, ssh_key_success) print(\u0026#34;Alive but SSH login failed:\u0026#34;, ssh_failures) # 写入结果到文件 with open(\u0026#39;output.txt\u0026#39;, \u0026#39;w\u0026#39;) as output_file: output_file.write(\u0026#34;[no_alive]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(non_alive_ips) + \u0026#34;\\n\u0026#34;) output_file.write(\u0026#34;[password]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(ssh_password_success) + \u0026#34;\\n\u0026#34;) output_file.write(\u0026#34;[key]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(ssh_key_success) + \u0026#34;\\n\u0026#34;) output_file.write(\u0026#34;[fail]\\n\u0026#34;) output_file.write(\u0026#34;\\n\u0026#34;.join(ssh_failures) + \u0026#34;\\n\u0026#34;) print(\u0026#34;Results have been written to output.txt\u0026#34;) 5、Ansbile实现步骤 1、上面生成的文件作为hosts使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [fail] 192.168.84.37 192.168.84.38 192.168.99.160 192.168.99.176 192.168.99.254 192.168.102.200 192.168.102.248 192.168.102.249 192.168.102.250 192.168.102.251 #可以定义环境变量，方便登录使用 [fail:vars] ansible_user=root ansible_password=\u0026#34;xxxxxx.88\u0026#34; ansible_ssh_private_key_file=/opt/ansible/ssh_key 2、给密码登录的添加公钥 1 ansible password -i ./all_host -m authorized_key -a \u0026#34;user={{ ansible_user }} state=present key=\u0026#39;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;~/.ssh/id_rsa.pub\u0026#39;) }}\u0026#39;\u0026#34; -u root --ask-pass 作用：这条命令会提示用户输入 SSH 密码，并将运行 Ansible 以 root 用户身份连接到 all_host 文件中列出的所有主机。然后，它会将当前用户的公钥添加到这些主机上指定用户的 authorized_keys 文件中，以实现无密码 SSH 登录。\nansible password -i ./all_host：\nansible：Ansible 命令的入口点。 password：这里应该是指 Ansible 的 inventory 文件中定义的模块名称 -i ./all_host：指定 Ansible inventory 文件的位置，这里是 ./all_host。 -m authorized_key：\n-m authorized_key：指定要使用的 Ansible 模块，这里是 authorized_key 模块，用于管理 ~/.ssh/authorized_keys 文件。 -a \u0026quot;user={{ ansible_user }} state=present key='{{ lookup('file', '~/.ssh/id_rsa.pub') }}'\u0026quot;：\n-a：为指定的模块传递参数。\n\u0026quot;user={{ ansible_user }} state=present key='{{ lookup('file', '~/.ssh/id_rsa.pub') }}'\u0026quot;\nuser={{ ansible_user }}：指定要在目标主机上操作的用户，这里使用了变量 {{ ansible_user }}，这个变量通常在 Ansible 的配置文件或命令行中定义。 state=present：确保公钥存在，如果不存在就添加。 key='{{ lookup('file', '~/.ssh/id_rsa.pub') }}'：从本地文件 ~/.ssh/id_rsa.pub 中读取公钥，并将其添加到目标主机的 authorized_keys 文件中。 -u root：\n-u root：以 root 用户身份连接到目标主机。 --ask-pass：\n--ask-pass：提示输入 SSH 密码。这在目标主机还没有配置无密码 SSH 登录时很有用。 6、自动化配置安全 hosts.allow 和 hosts.deny 文件是 TCP Wrappers 的一部分，用于在 Unix 和 Linux 系统上控制对服务的访问。TCP Wrappers 提供了一种通过 IP 地址、主机名或域名限制或允许访问服务的机制。\nhosts.allow 和 hosts.deny 文件的作用 hosts.allow：定义允许哪些主机访问哪些服务。 hosts.deny：定义拒绝哪些主机访问哪些服务。 这两个文件通常位于 /etc 目录下。\n格式 这两个文件的每一行包含一条访问控制规则，格式如下：\n1 php复制代码\u0026lt;服务列表\u0026gt; : \u0026lt;客户端列表\u0026gt; [: \u0026lt;选项\u0026gt;] 服务列表：要控制的服务名称，可以是单个服务名，也可以是多个服务名，以逗号分隔。 客户端列表：允许或拒绝访问的客户端，可以是 IP 地址、主机名或域名，也可以是多个客户端，以逗号分隔。 选项（可选）：可以包含日志记录或执行命令等额外操作。 使用示例 假设你有一台服务器，想控制对 SSH 服务的访问。\nhosts.allow 允许特定 IP 地址访问 SSH 服务：\n1 sshd : 192.168.1.100 允许特定子网访问 SSH 服务：\n1 sshd : 192.168.1.0/24 允许特定主机名访问 SSH 服务：\n1 sshd : trustedhost.example.com hosts.deny 拒绝所有其他主机访问 SSH 服务：\n1 sshd : ALL 使用场景 安全控制：通过限制对某些关键服务（如 SSH、FTP、SMTP 等）的访问，可以增强系统的安全性。 访问管理：在多用户环境中，可以根据需求灵活控制哪些用户或主机能够访问特定服务。 日志记录：结合日志选项，可以记录访问尝试，以便审计和监控。 示例需求 在 /etc/hosts.deny 中写入 sshd:ALL，拒绝所有主机的 SSH 访问。\n在/etc/hosts.allow 中允许特定 IP 地址段和单个 IP 地址的 SSH 访问：\nsshd:192.168.0.0/16:allow sshd:192.168.102.20:allow 如果文件有变动，则重启 sshd 服务。\nAnsible 剧本 编写一个 Ansible 剧本来自动执行上述操作。以下是完整的 Ansible 剧本代码：configure_ssh_hosts.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 --- - name: 配置 hosts.allow 和 hosts.deny hosts: test become: yes # 使用sudo权限 vars: hosts_deny_content: \u0026#34;sshd:ALL\u0026#34; hosts_allow_content: | sshd:192.168.0.0/16:allow sshd:192.168.102.20:allow tasks: - name: 更新 hosts.deny 文件 lineinfile: path: /etc/hosts.deny line: \u0026#34;{{ hosts_deny_content }}\u0026#34; create: yes register: hosts_deny_result - name: 更新 hosts.allow 文件 copy: content: \u0026#34;{{ hosts_allow_content }}\u0026#34; dest: /etc/hosts.allow register: hosts_allow_result - name: 如果配置发生变化则重启 sshd 服务 systemd: name: sshd state: restarted when: hosts_deny_result.changed or hosts_allow_result.changed - name: 确保 sshd 服务已启用并正在运行 systemd: name: sshd state: started enabled: yes 使用方式 定义剧本名称和目标主机：\n1 2 3 4 [root@ansible-yunwei ansible]# cat hosts [test] 192.168.102.20 192.168.102.30 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [root@ansible-yunwei ansible]# ansible-playbook -i ./hosts configure_ssh_hosts.yml PLAY [配置 hosts.allow 和 hosts.deny] ******************************************************************************************************************************* TASK [Gathering Facts] ******************************************************************************************************************************************* ok: [192.168.102.30] ok: [192.168.102.20] TASK [更新 hosts.deny 文件] ****************************************************************************************************************************************** ok: [192.168.102.30] ok: [192.168.102.20] TASK [更新 hosts.allow 文件] ***************************************************************************************************************************************** ok: [192.168.102.30] ok: [192.168.102.20] TASK [如果配置发生变化则重启 sshd 服务] *************************************************************************************************************************************** skipping: [192.168.102.20] skipping: [192.168.102.30] TASK [确保 sshd 服务已启用并正在运行] **************************************************************************************************************************************** ok: [192.168.102.30] ok: [192.168.102.20] PLAY RECAP ******************************************************************************************************************************************************* 192.168.102.20 : ok=4 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.102.30 : ok=4 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 注意事项 hosts.allow 文件中的规则优先于 hosts.deny 中的规则。如果一个主机被 hosts.allow 允许，则不会被 hosts.deny 拒绝。 确保规则的顺序和逻辑正确，以免意外拒绝合法访问或允许非法访问。 这些文件适用于支持 TCP Wrappers 的服务，不适用于所有服务。 通过合理配置 hosts.allow 和 hosts.deny 文件，可以有效控制服务访问，提高系统的安全性。\n","date":"2024-06-27T17:27:24Z","image":"https://www.ownit.top/title_pic/33.jpg","permalink":"https://www.ownit.top/p/202406271727/","title":"Python （Ansbile）脚本高效批量管理服务器和安全"},{"content":"1、背景 在很多组织中，需要对用户和系统进行统一的身份认证和授权管理。为了实现这一目标，通常会使用LDAP（轻量级目录访问协议）来构建集中化的身份认证和授权服务。而在生产环境中，为了保证高可用性和可扩展性，需要构建OpenLDAP主从集群来提供稳定的身份认证服务。\n2、环境 在开始部署之前，需要准备以下环境：\n操作系统：Centos7 OpenLDAP版本：OpenLDAP: slapd 2.4.44 服务器数量：2台（1台主服务器和1台从服务器） 网络环境：两台服务器应在同一局域网内，确保网络连接稳定 3、主节点安装 清参考下方链接 https://blog.csdn.net/heian_99/article/details/138963912\n4、从节点安装 假设master节点的地址为192.168.102.20。对于master节点，完全参照前述章节操作并配置好。\n对于slave节点，也参照前述章节安装并配置，但只到执行ldapdomain.ldif文件后就行。也就是设置了管理员用户账号就行，不用添加部门或其它人员信息。另外，phpLDAPadmin可以安装。\n1、配置master节点 在master节点上，我们需要导入相关的信息。\n创建syncprov_mod.ldif文件：\n1 2 3 4 5 6 7 cat \u0026gt; syncprov_mod.ldif \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; dn: cn=module,cn=config objectClass: olcModuleList cn: module olcModulePath: /usr/lib64/openldap olcModuleLoad: syncprov.la EOF 执行：\n1 ldapadd -Y EXTERNAL -H ldapi:/// -f syncprov_mod.ldif 创建syncprov.ldif文件：\n1 2 3 4 5 6 7 8 cat \u0026gt; syncprov.ldif \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; dn: olcOverlay=syncprov,olcDatabase={2}hdb,cn=config objectClass: olcOverlayConfig objectClass: olcSyncProvConfig olcOverlay: syncprov olcSpCheckpoint: 1 1 olcSpSessionLog: 1024 EOF 执行：\n1 ldapadd -Y EXTERNAL -H ldapi:/// -f syncprov.ldif 2、配置slave节点 在slave节点上配置主从。\n创建rp.ldif文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026gt; rp.ldif \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; dn: olcDatabase={2}hdb,cn=config changetype: modify add: olcSyncRepl olcSyncRepl: rid=001 provider=ldap://192.168.102.20:389 bindmethod=simple binddn=\u0026#34;cn=Manager,dc=fujfu,dc=com\u0026#34; credentials=examplePassword searchbase=\u0026#34;dc=fujfu,dc=com\u0026#34; scope=sub schemachecking=on type=refreshAndPersist retry=\u0026#34;30 5 300 3\u0026#34; attrs=\u0026#34;*,+\u0026#34; interval=00:00:02:00 EOF provider表示master的地址，其他的都是些基础信息。需要注意的是认证用户一定要使用超级管理员，如果使用普通用户连接master的话，slave将不会同步用户的密码字段信息。credentials是管理员的密码。 执行：\n1 ldapmodify -Y EXTERNAL -H ldapi:/// -f rp.ldif 除此之外，为了优化openldap的查询速度，我们添加了相关字段属性的索引。\n1 2 3 4 5 6 7 8 9 10 11 cat \u0026gt; index.ldif \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; dn: olcDatabase={2}hdb,cn=config changetype: modify add: olcDbIndex olcDbIndex: uid eq,pres olcDbIndex: uniqueMember eq,pres olcDbIndex: uidNumber,gidNumber eq,pres olcDbIndex: member,memberUid eq,pres olcDbIndex: entryUUID eq olcDbIndex: entryCSN eq EOF 执行：\n1 ldapadd -Y EXTERNAL -H ldapi:/// -f index.ldif 3、验证主从正确性 slave机器上配置完毕后，无需重启master机器和slave机器的slapd服务。\n在slave机器上查看openldap日志，如下：\n1 journalctl -u slapd -n 100 -f 通过上图，我们可以很明显的看到slave机器上slapd服务没有报错，而且已经在同步相关openldap数据。\n现在切换到master机器上查看openldap日志，如下：\n1 journalctl -u slapd -n 100 -f 通过上图我们也可以发现master没有报错，而且也看到slave机器已经在同步信息了。\n现在我们再使用phpLDAPadmin工具，登录slave查看相关信息，如下： 通过上图，我们可以看到slave节点上已经有账号信息了。这也就说明OpenLDAP的master-slave已经在正常同步数据。\n4、测试 可测试如下两点： 1、在master节点上修改用户字段信息，slave节点上应能同步到修改信息。 2、在slave节点修改用户字段信息，应该无法操作。因为我们采用的主从模式中，slave节点是自读的。\n如果测试无问题，说明我们的主从的确正常运行。 LDAP查看命令 附ldap查看配置命令： ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b cn=config ldapsearch -x cn=test -b dc=local,dc=cn\n","date":"2024-06-20T17:00:04Z","image":"https://www.ownit.top/title_pic/63.jpg","permalink":"https://www.ownit.top/p/202406201700/","title":"生产环境OpenLDAP主从集群"},{"content":"1、项目背景 在当前的IT运维环境中，我们的业务系统日益复杂，特别是针对特定的业务逻辑和定制化需求，传统的通用监控工具往往难以覆盖所有的监控场景。例如，考虑到一个复杂的电商平台，除了基础的服务器性能、网络状况等基础设施监控外，我们还迫切需要对特定业务指标进行监控，比如DNS解析成功率、API响应延迟、特定业务流程的错误率等，这些都是直接影响用户体验和业务连续性的关键因素。\n面对这样的需求，虽然Prometheus官方提供了一系列标准的Exporter，如node_exporter和snmp_exporter，它们在基础监控层面表现优秀，但对于上述提到的业务逻辑相关的监控需求却鞭长莫及。为了弥补这一空白，我们需要自定义一个专为业务定制的Prometheus Exporter，以实现对这些特定业务指标的精准监控。\n为此，我们选择使用Go语言（Golang）来编写这个Exporter，原因在于Go语言天生对并发友好，适合编写高性能的网络服务，且官方提供了成熟的Prometheus客户端库，便于快速集成和开发\n下面的代码地址：https://github.com/nangongchengfeng/Prometheus-Go-Template\n2、开发环境搭建 安装Go语言：确保机器上安装了Go语言（1.20版本以上）并配置了GOPATH。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 curl -L https://go.dev/dl/go1.21.3.linux-amd64.tar.gz -o ./go-linux-amd64.tar.gz 解压 sudo tar -zxvf go-linux-amd64.tar.gz -C /usr/local/lib/ 下面语句是给所有用户创建环境变量 # 下面内容需要多行复制 sudo tee -a ~/.bashrc \u0026lt;\u0026lt; EOF export GOROOT=/usr/local/lib/go/ export GOPATH=/home/${USER}/sdk/go export PATH=\\$PATH:\\$GOROOT/bin:\\$GOPATH/bin EOF # 单行 source ~/.bashrc 下载Prometheus包：通过go get命令下载client_golang包。\n1 2 go get github.com/prometheus/client_golang/prometheus go get github.com/prometheus/client_golang/prometheus/promhttp 开启Go模块代理\n1 2 3 4 5 6 7 8 go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.cn,direct 如果不行，请修改 如下设置： set GOPRIVATE=gitlab.xx.com set GOPROXY=https://goproxy.cn set GONOPROXY=gitlab.xx.com set GONOSUMDB=gitlab.xx.com 3、编写Exporter基础代码 创建HTTP服务：使用Go的http模块创建一个服务，指定/metrics路径，并使用promhttp.Handler()作为处理器。 启动服务：通过http.ListenAndServe启动HTTP服务监听8080端口。 1 2 3 4 5 6 7 8 9 10 11 package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) func main() { http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } 访问：ip+端口:/metrics 会出现如下页面，Go模块自带的监控项，仅仅展示了一个默认的采集器，并且通过接口调用隐藏了太多实施细节 4、理解Prometheus指标 指标类型：Prometheus支持四种主要的指标类型：\nCounter（累加指标） Gauge（测量指标） Summary（概略图） Histogram（直方图） 1. Counter（累加指标） 什么是Counter？\nCounter是一种只能增加的指标。它用来记录一些只会累加的数值，例如处理的请求数量、完成的任务数量等。 通俗解释：\n想象一个只能加法不能减法的计数器。每当你完成一个任务，就按一次加号，这个数字会一直增加，不会减少。 例子：\n记录一个网站的访问次数，每次有新的访问时，计数器增加1。 2. Gauge（测量指标） 什么是Gauge？\nGauge是一种可以任意增减的指标。它用来记录一些瞬时的数值，例如当前的温度、CPU使用率等。 通俗解释：\n想象一个温度计，温度可以升高也可以降低。它反映的是当前的状态，而不是累积的数值。 例子：\n记录服务器当前的内存使用量，这个数值会随着时间的推移而增加或减少。 3. Summary（概略图） 什么是Summary？\nSummary是一种用来记录一系列数值的分布情况的指标。它会统计这些数值的分位数（例如中位数、90%分位数等）。 通俗解释：\n想象你有一堆考试成绩，你想知道这些成绩的中位数是多少，90%的成绩在哪个分数以上。这种指标会帮助你了解一组数据的分布情况。 例子：\n记录API响应时间，通过Summary可以知道大多数请求的响应时间情况，比如中位数响应时间、90%请求的响应时间等。 4. Histogram（直方图） 什么是Histogram？\nHistogram也是一种记录数值分布的指标，但它会把数值划分到不同的区间（桶）中，统计每个区间的数量。 通俗解释：\n想象你有一堆不同高度的孩子，你把他们分成不同的身高区间，比如1米到1米1，1米1到1米2等，然后统计每个区间有多少孩子。Histogram会帮助你知道数据在不同区间的分布情况。 例子：\n记录API响应时间，通过Histogram可以知道不同响应时间范围内的请求数量，比如0到100ms有多少请求，100到200ms有多少请求等。 可以理解为柱状图，典型的应用如：请求持续时间，响应大小。可以对观察结果采样，分组及统计。例如设置一个name为web_request_duration_seconds的Histogram 的metrics,并设置区间值为[0.1,0.5,1]会对区间点生成一条统计数据.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 响应时间小于0.1s的请求有5次 web_request_duration_seconds_bucket{endpoint=\u0026#34;/query\u0026#34;,method=\u0026#34;GET\u0026#34;,le=\u0026#34;0.1\u0026#34;} 3 # 响应时间小于0.5s的请求有次 web_request_duration_seconds_bucket{endpoint=\u0026#34;/query\u0026#34;,method=\u0026#34;GET\u0026#34;,le=\u0026#34;0.5\u0026#34;} 5 # 响应时间小于1s的请求有7次 web_request_duration_seconds_bucket{endpoint=\u0026#34;/query\u0026#34;,method=\u0026#34;GET\u0026#34;,le=\u0026#34;1\u0026#34;} 7 # 总共7次请求 web_request_duration_seconds_bucket{endpoint=\u0026#34;/query\u0026#34;,method=\u0026#34;GET\u0026#34;,le=\u0026#34;+Inf\u0026#34;} 7 # 7次请求duration的总和 web_request_duration_seconds_sum{endpoint=\u0026#34;/query\u0026#34;,method=\u0026#34;GET\u0026#34;} 2.7190880529999997 5. Summary 和 Histogram 的区别 在使用 Prometheus 进行监控时，Summary 和 Histogram 都是用于测量和分析数据分布的强大工具，但它们在细节和应用场景上有所不同。下面将从两者的定义、使用场景和特点来详细阐述它们的区别。\nSummary\nSummary 用于提供一组数据的概述信息，它能够计算数据的分位数（例如中位数、90%分位数等）。这种指标特别适合需要了解整体数据分布情况但不需要精细区间划分的场景。\n特点：\n分位数计算：可以直接计算出指定分位数，例如中位数、90%分位数等。 适合样本量大时：当你需要了解大量请求的整体响应时间分布时，Summary 是非常有用的工具。 无需预定义桶：Summary 通过算法动态计算分位数，而无需提前划分数据区间（桶）。 使用场景：\nAPI 响应时间分析：需要知道绝大多数请求的响应时间情况，例如中位数响应时间和90%的请求响应时间。 整体性能评估：适合评估系统的整体性能，而不需要详细的区间数据。 Histogram\nHistogram 则是将数据划分到不同的区间（桶）中，统计每个区间的数据量。它适合需要详细了解数据在不同范围内分布情况的场景。\n特点：\n区间划分：将数据划分到不同的区间（桶）中，每个区间统计数据量。 精细数据分析：适合需要详细了解数据分布情况的场景，可以明确知道数据在不同区间的分布。 需要预定义桶：在使用前需要定义数据的区间（桶），适合事先已知数据范围的情况。 使用场景：\n详细响应时间分析：需要知道在不同响应时间范围内的请求数量，比如0-100ms、100-200ms等。 资源使用情况监控：可以用来监控系统资源使用情况的分布，例如内存使用量在不同范围内的分布。 5、定义和注册指标 引入依赖库：通过go get命令获取prometheus库。\n1 2 3 4 import ( \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;sync/atomic\u0026#34; ) 定义指标：创建Counter类型的指标，例如API请求次数的监控\n1 2 3 4 5 6 // APIRequestCounter 结构体，用于管理API请求次数的监控 type APIRequestCounter struct { Zone string APIRequestDesc *prometheus.Desc requestCount uint64 } 注册指标：使用prometheus.MustRegister将指标注册到默认的Registry中。\n1 2 3 4 5 6 apiRequestCounter := collector.NewAPIRequestCounter(*metricsNamespace) // 创建一个新的Prometheus指标注册表 registry := prometheus.NewRegistry() // 注册APIRequestCounter实例到Prometheus注册表 registry.MustRegister(apiRequestCounter) 6、实现自定义数据采集 实现Collector接口：创建自定义的Collector结构体来实现数据采集。\n1 2 3 4 5 6 // APIRequestCounter 结构体，用于管理API请求次数的监控 type APIRequestCounter struct { Zone string APIRequestDesc *prometheus.Desc requestCount uint64 } 定义指标描述符：使用prometheus.NewDesc创建指标描述符。\n1 2 3 4 5 6 7 8 9 10 11 12 // NewAPIRequestCounter 创建一个新的APIRequestCounter实例 func NewAPIRequestCounter(zone string) *APIRequestCounter { return \u0026amp;APIRequestCounter{ Zone: zone, APIRequestDesc: prometheus.NewDesc( \u0026#34;api_request_count_total\u0026#34;, \u0026#34;API请求总次数\u0026#34;, nil, prometheus.Labels{\u0026#34;zone\u0026#34;: zone}, ), } } 实现Describe和Collect方法：Describe方法发送指标描述符到channel，Collect方法执行数据采集并返回数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Describe 向Prometheus描述收集的指标 func (c *APIRequestCounter) Describe(ch chan\u0026lt;- *prometheus.Desc) { ch \u0026lt;- c.APIRequestDesc } // Collect 收集指标数据并发送到Prometheus func (c *APIRequestCounter) Collect(ch chan\u0026lt;- prometheus.Metric) { count := atomic.LoadUint64(\u0026amp;c.requestCount) ch \u0026lt;- prometheus.MustNewConstMetric( c.APIRequestDesc, prometheus.CounterValue, float64(count), ) } 7、实现数据采集逻辑 自定义数据采集函数：实现一个函数来模拟或获取系统状态数据。\n1 2 3 4 // IncrementRequestCount 增加API请求计数 func (c *APIRequestCounter) IncrementRequestCount() { atomic.AddUint64(\u0026amp;c.requestCount, 1) } 收集数据：在Collect方法中调用数据采集函数，并使用MustNewConstMetric创建指标数据。\n8、集成和测试 创建自定义Collector实例：根据需要创建多个Collector实例。 注册到Registry：使用NewPedanticRegistry创建一个新的Registry，并注册自定义Collector。 设置HTTP Handler：使用promhttp.HandlerFor创建一个HTTP Handler，用于处理/metrics路径的请求并返回指标数据。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 var ( // Set during go build // version string // gitCommit string // 命令行参数 listenAddr = flag.String(\u0026#34;web.listen-port\u0026#34;, \u0026#34;8080\u0026#34;, \u0026#34;An port to listen on for web interface and telemetry.\u0026#34;) metricsPath = flag.String(\u0026#34;web.telemetry-path\u0026#34;, \u0026#34;/metrics\u0026#34;, \u0026#34;A path under which to expose metrics.\u0026#34;) metricsNamespace = flag.String(\u0026#34;metric.namespace\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;Prometheus metrics namespace, as the prefix of metrics name\u0026#34;) ) func main() { // 解析命令行参数 flag.Parse() apiRequestCounter := collector.NewAPIRequestCounter(*metricsNamespace) // 创建一个新的Prometheus指标注册表 registry := prometheus.NewRegistry() // 注册APIRequestCounter实例到Prometheus注册表 registry.MustRegister(apiRequestCounter) // 设置HTTP服务器以处理Prometheus指标的HTTP请求 http.Handle(*metricsPath, promhttp.HandlerFor(registry, promhttp.HandlerOpts{})) // 设置根路径的处理函数，用于返回一个简单的HTML页面，包含指向指标页面的链接 http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(`\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;A Prometheus Exporter\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;A Prometheus Exporter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\u0026#39;/metrics\u0026#39;\u0026gt;Metrics\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;`)) }) // 模拟API请求的处理函数 http.HandleFunc(\u0026#34;/api\u0026#34;, func(w http.ResponseWriter, r *http.Request) { apiRequestCounter.IncrementRequestCount() // 模拟API处理时间 w.Write([]byte(\u0026#34;API请求处理成功\u0026#34;)) }) // 记录启动日志并启动HTTP服务器监听 log.Printf(\u0026#34;Starting Server at http://localhost:%s%s\u0026#34;, *listenAddr, *metricsPath) log.Fatal(http.ListenAndServe(\u0026#34;:\u0026#34;+*listenAddr, nil)) } 9、启动服务并测试 请下载：https://github.com/nangongchengfeng/Prometheus-Go-Template.git 启动测试\n启动HTTP服务：启动服务并监听8080端口。 访问指标数据：通过浏览器访问http://localhost:8080/metrics查看指标数据。 1 2 3 测试接口 http://localhost:8080/api http://localhost:8080/query 10、原理 1、Prometheus 监控总结 在使用 Prometheus 进行监控时，理解指标的采集方式和不同指标类型的应用场景至关重要。以下是对相关概念的总结：\n指标采集： Exporter 定期采集指标数据，并通过 HTTP 接口暴露。这些数据可以由 Prometheus 服务进行抓取和存储，便于后续的分析和展示。\n指标类型：\nCounter：适用于持续增长的指标，例如请求计数、任务完成次数等。它只会递增，不会减少。 Gauge：适用于可能增减的指标，例如当前内存使用量、CPU 使用率等。它既可以递增，也可以递减。 Histogram：适用于需要聚合和统计数据分布的场景，例如请求响应时间分布。它将数据划分到不同的区间（桶）中，并统计每个区间的数据量。 Summary：同样适用于需要聚合和统计分布的场景，但它通过计算分位数来描述数据分布情况，例如中位数、90% 分位数等。 数据聚合： Histogram 和 Summary 这两种类型的指标专注于数据的聚合和统计分布。它们能够提供关于数据分布的详细信息，便于深入分析系统性能和行为。\n自定义 Collector： 开发者可以通过实现 Collector 接口来自定义数据采集逻辑，以满足特定的监控需求。自定义 Collector 需要实现 describe 和 collect 两个方法，以便将自定义逻辑集成到 Prometheus 的数据采集流程中。\n2、Golang 中的监控器逻辑解析 在 Golang 中使用 Prometheus 进行监控时，了解监控器的逻辑和工作原理至关重要。下面将简要总结一下监控器的基本逻辑：\n监控器初始化： 默认情况下，当引入 registry 包时，会触发对 gocollect 这个采集器的初始化，这是因为 registry 在初始化时会调用 describe 接口，而 gocollect 正是实现了这个接口。\n数据采集流程： 当有 HTTP 请求到达时，http.handle 会调用 registry 的 gather 函数。在 gather 函数内部，会调用具体采集器的 collect 接口，这个实现就是 gocollect 中的 collect 函数。这一流程确保了数据的采集和处理。\n多类型处理： 上述流程仅是一种特殊情况，实际上，对于四种不同类型的监控器（Counter、Gauge、Histogram 和 Summary），都有对应的结构体继承了基本函数接口，并实现了相应的接口。每种监控器都有自己独特的逻辑和实现方式，但整体流程大致相同。\n自定义监控器： 如果需要实现自定义的监控器逻辑，可以新建一个结构体作为监控器，实现 describe 和 collect 接口。这样，就可以根据自己的需求实现特定的监控逻辑。最终，这些自定义监控器的实现原理也是调用了四种不同类型监控器的相关函数，以实现数据的采集和处理。\n参考项目和文档：\nhttps://github.com/crockitwood/go-prometheus-example.git\nPrometheus四种指标详细解释\nhttps://kingjcy.github.io/post/monitor/metrics/prometheus/library/client_golang/\n","date":"2024-05-29T18:10:56Z","image":"https://www.ownit.top/title_pic/57.jpg","permalink":"https://www.ownit.top/p/202405291810/","title":"Go开发Prometheus客户端实战步骤"},{"content":"1、引言 随着应用程序和业务数据的持续增长，有效地管理数据库存储空间成为维护系统性能的关键。在MongoDB这类NoSQL数据库中，定期清理过期数据变得尤为重要，这不仅能释放宝贵的存储资源，还能优化查询性能，确保数据库运行的高效与稳定。 本文将深入探讨一种自动化清理MongoDB中过期数据的策略，并通过一个实际的Python脚本示例，展示如何实现这一功能。\n2、需求背景 根据公司业务发展积累，在众多应用场景中，如日志记录、临时缓存、会话管理等，数据往往具有时效性，超过一定时间后便不再有用。如果不及时清理，这些过期数据会占用大量存储空间，增加数据库维护成本，甚至影响查询效率。 目前我们的 MongoDB数据库单表达到70G，冗余数据积累。导致空间占用极大。为了实现“降本增效” 清理过期的数据 （切忌：过期数据也需要使用mongodump备份）因此，我们需要一个自动化机制，能够根据数据的“最后修改日期”等时间戳字段，识别并删除过期记录。\n3、功能概述 本方案设计了一个Python脚本，集成了以下几个核心功能：\n配置文件读取：允许用户灵活配置数据库连接信息、目标集合名、数据过期天数以及批处理大小等参数。 动态时间阈值计算：根据用户设定的过期天数，计算出需删除数据的截止时间戳。 分批删除机制：为了减少对数据库的冲击，脚本采用分批删除策略，每次只处理一批数据，直至所有过期数据被清理完毕。 进度可视化：集成tqdm库，实时显示删除进度，使操作过程透明且直观。 错误处理：包含了对配置加载、数据库连接、数据操作等环节的异常处理，确保脚本的健壮性。 4、实现步骤 1、数据库表结构分析 假如我们有个：tag_logs 的集合 数据格式如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 db.getCollection(\u0026#34;tag_logs\u0026#34;).insert( { _id: ObjectId(\u0026#34;65dd5f067db3e415f0d3972f\u0026#34;), taskId: \u0026#34;65dd5efd7db3e415f0d39630\u0026#34;, modelId: \u0026#34;6285a9890d45000030004392\u0026#34;, name: \u0026#34;nihaogengx\u0026#34;, ruleResult: \u0026#34;NOT_HIT\u0026#34;, logic: \u0026#34;AND\u0026#34;, conditionResults: [ { name: \u0026#34;nihaogengx\u0026#34;, result: \u0026#34;NOT_HIT\u0026#34;, logic: \u0026#34;AND\u0026#34;, subRuleResults: [ { name: \u0026#34;nihaogengx\u0026#34;, result: \u0026#34;NOT_HIT\u0026#34;, variableCode: \u0026#34;var-instant-core-xxxxxx\u0026#34; } ] } ], type: \u0026#34;AUDIT_TAG\u0026#34;, createdDate: NumberLong(\u0026#34;1709006598851\u0026#34;), lastModifiedDate: NumberLong(\u0026#34;1709006598851\u0026#34;), _class: \u0026#34;com.fujfu.shinji.entity.TagResultDO\u0026#34; } ); 索引查询\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 db.createCollection(\u0026#34;tag_logs\u0026#34;); db.getCollection(\u0026#34;tag_logs\u0026#34;).createIndex({ taskId: NumberInt(\u0026#34;1\u0026#34;) }, { name: \u0026#34;idx_tagResult_taskId\u0026#34; }); db.getCollection(\u0026#34;tag_logs\u0026#34;).createIndex({ createdDate: NumberInt(\u0026#34;1\u0026#34;) }, { name: \u0026#34;createdDate_1\u0026#34;, background: true }); db.getCollection(\u0026#34;tag_logs\u0026#34;).createIndex({ lastModifiedDate: NumberInt(\u0026#34;-1\u0026#34;) }, { name: \u0026#34;lastModifiedDate_-1\u0026#34;, background: true }); 2、增加索引 我们是根据 lastModifiedDate 来获取过期的时间，所以这个必选加索引。如果没有索引，根据下方添加\n1 db.tag_logs.createIndex( { lastModifiedDate: -1 }, { background: true } ) 这个命令的作用是在 tag_logs 集合上创建一个索引。具体来说：\ndb.tag_logs.createIndex：这是在 tag_logs 集合上创建索引的方法。 { lastModifiedDate: -1 }：这是索引的键和排序顺序。具体解释如下： lastModifiedDate 是你希望创建索引的字段名。 -1 表示你希望按照该字段的降序排序来创建索引。如果你用的是 1，则表示按照升序排序。 { background: true }：这是索引创建的选项。具体解释如下： background: true 表示在后台创建索引。这意味着索引创建操作不会阻塞其他数据库操作，允许其他读写操作继续进行。这对于生产环境中的大型集合非常有用，因为它可以减少对应用程序正常操作的干扰。 3、脚本核心逻辑 config.ini\n1 2 3 4 5 6 [database] uri = mongodb://root:xxxx.88@mongo2.fat.xxxx.fjf:27017/?authSource=admin #Mongo连接字符串 db_name = xxx-xxx-engine # 数据库名称 collection_name = variable_result_1 # 集合名称 expired_days = 90 # 删除过期多少天的。 删除3个月之前的数据 batch_size=1000 #每次删除的条数 clean_expired_data.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 import configparser from pymongo import MongoClient, errors from datetime import datetime, timedelta from tqdm import tqdm def load_config(file_path=\u0026#39;config.ini\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Load configuration from the specified file.\u0026#34;\u0026#34;\u0026#34; config = configparser.ConfigParser() config.read(file_path) return config def get_mongo_client(uri): \u0026#34;\u0026#34;\u0026#34;Create and return a MongoDB client.\u0026#34;\u0026#34;\u0026#34; return MongoClient(uri) def get_cutoff_timestamp(days): \u0026#34;\u0026#34;\u0026#34;Calculate and return the cutoff timestamp.\u0026#34;\u0026#34;\u0026#34; cutoff_date = datetime.now() - timedelta(days=days) return int(cutoff_date.timestamp() * 1000) def delete_expired_documents(collection, cutoff_timestamp, batch_size): \u0026#34;\u0026#34;\u0026#34;Delete documents older than the cutoff timestamp in batches.\u0026#34;\u0026#34;\u0026#34; total_deleted = 0 all_documents = collection.count_documents({}) # 1. 查询出需要删除的集合数量 total_to_delete = collection.count_documents({\u0026#39;lastModifiedDate\u0026#39;: {\u0026#39;$lt\u0026#39;: cutoff_timestamp}}) print(f\u0026#34;集合总数: {all_documents}, 需要删除的文档数量: {total_to_delete}\u0026#34;) # 2. 使用 tqdm 显示进度条 with tqdm(total=total_to_delete, desc=\u0026#39;Deleting documents\u0026#39;, unit=\u0026#39;doc\u0026#39;) as pbar: while True: documents = collection.find( {\u0026#39;lastModifiedDate\u0026#39;: {\u0026#39;$lt\u0026#39;: cutoff_timestamp}}, limit=batch_size ) document_ids = [doc[\u0026#39;_id\u0026#39;] for doc in documents] if not document_ids: break result = collection.delete_many({\u0026#39;_id\u0026#39;: {\u0026#39;$in\u0026#39;: document_ids}}) deleted_count = result.deleted_count total_deleted += deleted_count # print(f\u0026#39;Deleted {deleted_count} documents\u0026#39;) # 3. 更新进度条 pbar.update(deleted_count) if deleted_count \u0026lt; batch_size: break return total_deleted def clean_mongo_expired_data(): \u0026#34;\u0026#34;\u0026#34;Main function to clean expired data from MongoDB.\u0026#34;\u0026#34;\u0026#34; config = load_config() try: uri = config[\u0026#39;database\u0026#39;][\u0026#39;uri\u0026#39;] db_name = config[\u0026#39;database\u0026#39;][\u0026#39;db_name\u0026#39;] collection_name = config[\u0026#39;database\u0026#39;][\u0026#39;collection_name\u0026#39;] expired_days = int(config[\u0026#39;database\u0026#39;][\u0026#39;expired_days\u0026#39;]) batch_size = int(config[\u0026#39;database\u0026#39;][\u0026#39;batch_size\u0026#39;]) client = get_mongo_client(uri) db = client[db_name] collection = db[collection_name] cutoff_timestamp = get_cutoff_timestamp(expired_days) total_deleted = delete_expired_documents(collection, cutoff_timestamp, batch_size) print(\u0026#39;Completed deletion\u0026#39;) print(f\u0026#39;Deleted {total_deleted} documents\u0026#39;) except (configparser.Error, ValueError, errors.PyMongoError) as e: print(f\u0026#39;Error occurred: {e}\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: clean_mongo_expired_data() requirements.txt python 环境版本：Python 3.8.10\n1 2 pymongo==4.3.3 tqdm==4.66.4 5、实战测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 python3 -m venv py3 #创建虚拟环境 source env_py/py3/bin/activate #加载环境 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple # 安装依赖 更改config.ini 启动程序 nohup python clean_expired_data.py \u0026amp; (py3) [root@jenkins mongodb_clean]# tail -f nohup.out 集合总数: 410565470, 需要删除的文档数量: 404724244 Deleting documents: 13%|█▎ | 53910000/404724244 [1:17:54\u0026lt;8:13:39, 11844.06doc/s] 6、性能分析 在数据库维护操作中，尤其是涉及大量数据删除的场景，采取批量删除策略是出于对系统性能和稳定性的关键考量。直接针对大量数据执行一次性删除操作可能会引发以下几个潜在问题，这些问题对于生产环境中的MongoDB数据库尤为敏感：\nIOPS（每秒输入/输出操作）激增 大规模数据删除会导致磁盘I/O操作显著增加，瞬间的高IOPS需求可能迅速消耗数据库的I/O资源。这不仅会减慢当前操作的速度，还可能影响到其他正在执行的重要数据库操作，如关键查询和事务处理。 锁竞争与阻塞 虽然MongoDB采用了更细粒度的锁机制，但在极端情况下，大量写操作仍可能引发锁争用，导致其他读写操作被阻塞。这会直接影响系统的并发性能。 资源消耗 大量数据的连续删除操作会消耗大量的CPU和内存资源。在资源有限的系统中，这可能导致系统响应变慢，甚至出现短暂的服务不可用状态。 日志膨胀 数据库的每一次写操作，包括删除，都会被记录到事务日志中。大量删除操作会导致日志文件迅速增大，不仅占用存储空间，还会增加日志回放和恢复的时间。 采用上述方式可以简单有效解决\n目前我删除 404724244（4亿条数据），自动每次删除1w条，持续删 （不影响业务运行） 7亿条数据 ","date":"2024-05-27T16:05:30Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202405271605/","title":"MongoDB数据库（10亿条数据）清理策略- 自动化过期数据删除实战"},{"content":"1、简介 在现代软件开发中，尤其是涉及到数据驱动的应用程序时，开发和测试环境中数据库的管理是至关重要的一环。为了确保开发和测试环境中的数据库始终处于一致的状态，自动化重置数据库成为了一种常见的实践。本文旨在介绍如何通过Shell脚本来自动化重置MySQL数据库，以便开发团队能够轻松地在每次测试或开发新功能前将数据库恢复到一个已知的初始状态。\n2、技术支撑 Shell脚本基础：熟悉基本的Shell命令和脚本编写技巧。 MySQL操作：了解如何通过命令行与MySQL数据库进行交互，包括备份、恢复和执行SQL脚本。 安全性考虑：确保数据库操作的安全性，特别是涉及到数据库密码和敏感数据时。 自动化实践：通过计划任务（如cron jobs）实现自动化的数据库重置。 3、数据库规则约束 1、新建Gitlab的仓库 2、根据数据库名称作为目录，地下有3（schema，init、append）目录 4、自动化脚本实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 #!/bin/bash # 定义一个数组来存储选项，这些选项将由 `ls fat-db` 命令的输出填充。 options=() read -ra options \u0026lt;\u0026lt;\u0026lt; $(ls fat-db) # 定义颜色代码，用于在终端中输出不同颜色的文本。 INFO=\u0026#39;\\033[0;32m\u0026#39; EINFO=\u0026#39;\\033[0m\u0026#39; # 如果 `fat-db` 目录存在，则删除它。 if [ -d fat-db ]; then rm -rf fat-db ;fi # 克隆 git 仓库到当前目录。 git clone https://xxxx.xxxx.com/group/fat-db.git \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # 定义一个函数，用于执行 SQL 脚本。 function exec_sql(){ # 设置数据库服务器的主机地址。 host=\u0026#34;127.0.0.1\u0026#34; # 为 MySQL 客户端设置环境变量密码。 export MYSQL_PWD=\u0026#34;xxxx.88\u0026#34; # 连接到数据库，并显示数据库中的所有表。 # 然后删除这些表。 read -ra list \u0026lt;\u0026lt;\u0026lt; `mysql -u root -h $host -e \u0026#34;use $1;show tables;\u0026#34;` tb_list=`echo ${list[@]:1} | sed \u0026#39;s/ /,/g\u0026#39;` echo -e \u0026#34;${INFO}开始删除${1}所有表 ${EINFO}\u0026#34; mysql -u root -h $host -e \u0026#34;use $1;drop tables $tb_list;\u0026#34; # 执行数据库的 schema 脚本。 schema_sqls=`find fat-db/\\$1/schema -type f -name *.sql` for schema_sql in $schema_sqls;do echo -e \u0026#34;${INFO}开始执行${schema_sql} ${EINFO}\u0026#34; mysql -u root -h $host $1 \u0026lt; $schema_sql done # 如果存在初始化脚本目录，则执行其中的 SQL 脚本。 if [ -d fat-db/$1/init ]; then init_sqls=`find fat-db/\\$1/init -type f -name *.sql` for init_sql in $init_sqls;do echo -e \u0026#34;${INFO}开始执行${init_sql} ${EINFO}\u0026#34; mysql -u root -h $host $1 \u0026lt; $init_sql done fi # 如果存在追加脚本目录，则根据日期排序后执行其中的 SQL 脚本。 if [ -d fat-db/$1/append ]; then append_sqls=`find fat-db/$1/append -type f -name *.sql | sort -t \u0026#39;/\u0026#39; -k 4` for append_sql in $append_sqls;do echo -e \u0026#34;${INFO}开始执行${append_sql} ${EINFO}\u0026#34; mysql -u root -h $host $1 \u0026lt; $append_sql done fi } # 定义一个函数，用于当没有参数时，通过 select 提供一个菜单来选择数据库。 function select_db() { # 将 \u0026#34;all\u0026#34; 和 \u0026#34;quit\u0026#34; 添加到选项菜单中。 options+=(\u0026#34;all\u0026#34; \u0026#34;quit\u0026#34;) # 使用 select 显示菜单并允许用户选择一个选项。 select option in \u0026#34;${options[@]}\u0026#34;; do # 根据用户选择的选项执行不同的操作。 case $option in \u0026#34;quit\u0026#34;) echo \u0026#34;Exiting...\u0026#34; break ;; \u0026#34;all\u0026#34;) # 如果选择 \u0026#34;all\u0026#34;，则对所有数据库执行 exec_sql 函数。 for db in ${options[@]}; do if [[ $db =~ \u0026#34;all\u0026#34; || $db =~ \u0026#34;quit\u0026#34; ]];then continue ;fi exec_sql $db done ;; *) # 如果用户选择了一个有效的数据库名称，则执行 exec_sql 函数。 if [[ \u0026#34;${options[@]}\u0026#34; =~ \u0026#34;$option\u0026#34; ]]; then echo \u0026#34;You selected: $option\u0026#34; exec_sql $option else echo \u0026#34;Please choose a valid option.\u0026#34; fi ;; esac done } # 定义一个函数，用于当有一个参数时执行。 function arges_db() { # 如果参数存在于选项数组中，则执行该数据库的 SQL 脚本。 if [[ ${options[@]/$1/} != ${options[@]} ]]; then exec_sql $1 # 如果参数是 \u0026#34;all\u0026#34;，则对所有数据库执行 SQL 脚本。 elif [[ $1 == \u0026#34;all\u0026#34; ]];then for db in ${options[@]}; do exec_sql $db done # 如果参数不在选项中且不是 \u0026#34;all\u0026#34;，则输出错误信息。 else echo \u0026#34;You selected $1 not exist!\u0026#34; fi } # 检查传入的参数数量。 if [[ $# == 0 ]];then # 如果没有参数，则调用 select_db 函数。 select_db elif [[ $# == 1 ]];then # 如果有一个参数，则调用 arges_db 函数并传入该参数。 arges_db $1 else # 如果参数数量不是0或1，则输出错误信息。 echo \u0026#34;database:${options[@]}\u0026#34; echo \u0026#34;example : $(basename $0)\u0026#34; echo \u0026#34;example : $(basename $0) ${options[1]}\u0026#34; echo \u0026#34;example : $(basename $0) all\u0026#34; fi 5、测试结果 执行 sh fatdb-init.sh，会列举当下的所有数据库 如果想重置loan_urge库，输入7后回车就可以自动执行 同理，想重置哪个库就输入库名前面的数字即可。8代表全部库，9代表退出当前操作\n6、结语： 自动化数据库重置脚本的开发与实施，是提高开发效率、保证测试质量的关键一环。通过综合考虑技术选型、精心设计核心逻辑、强化安全措施及无缝融入CI/CD流程，可构建出既高效又可靠的数据库管理自动化解决方案。实践过程中，不断迭代优化，结合具体项目需求灵活调整，是实现长期成功的关键。\n","date":"2024-05-24T14:37:11Z","image":"https://www.ownit.top/title_pic/45.jpg","permalink":"https://www.ownit.top/p/202405241437/","title":"自动化重置数据库功能的探索与实践"},{"content":"LDAP 简介 什么是目录服务(directory service)？\n一个目录，除了支持基本的查找和更新功能外，是一个特别设计用来搜索和浏览的专门的数据库。目录倾向于包含描述性的、基于属性的信息，并支持精细的过滤能力。目录通常不支持复杂的事务和回滚机制，这与关系数据库不一样。\n什么是 LDAP？\nLDAP 代表轻量目录访问协议(Lightweight Directory Access Protocol)。正如名称中所暗示的，它是一个用于访问目录服务的轻量协议，具体来说，是基于 X.500 的目录服务。\n什么样的信息可以存储在目录中？LDAP 信息模型是基于条目(entries)的。一个条目(entry)是一些属性的集合，它有一个全局唯一的可辨识名称(Distinguished Name，DN)。DN 用于无歧义地引用该条目。条目的每一个属性都由一个类型(type)和一个或多个值(values)构成。类型通常是一个助记符字符，比如“cn”代表 common name，“mail”代表 email address。值的语法取决于属性类型。比如，一个 cn 属性的值可能是 Babs Jensen，一个 mail 属性的值可能是 babs@example.com，一个 jpegPhoto 属性可能包含一个 JPEG(二进制)格式的照片。\n温馨提示：dn 必须是全局唯一的。\nLDAP 中，将数据组织成一个树形结构，这与现实生活中的很多数据结构可以对应起来，而不像设计关系型数据库的表，需要进行多种变化。如下图所展示的就是一个树形结构的数据。 在上图所示的树形结构中，树的根结点是一个组织的域名（node3.com，可以根据自己意愿配），其下分为 2 个部分，分别是 ou=admin 和 dc=hdp，dc=hdp 下面又分 ou=people 和 ou=group。admin 用来管理所有管理人员，people 用来管理登录系统的用户，group 用来管理系统中的用户组。当然，在该图中还可继续增加其他分支。\n对于图中所示的树形结构，使用关系数据库来保存数据的话，需要设置多个表，一层一层分别保存，当需要查找某个信息时，再逐层进行查询，最终得到结果。\n若使用目录来保存该图中的数据，则更直观。图中每个结点用一个条目来保存，不同类型的结点需要保存的数据可能不同，在 LDAP 中通过一个称为 objectClass 的类型来控制不同结点需要的数据（称为属性）。\n参考文章：https://blog.csdn.net/weixin_42578481/article/details/80863890\n名词解释：\nDC：domain component，一般为公司名，例如：dc=163,dc=com\nOU：organization unit，为组织单元，最多可以有四级，每级最长 32 个字符，可以为中文\nCN：common name，为用户名或者服务器名，最长可以到 80 个字符，可以为中文\nDN：distinguished name，为一条 LDAP 记录项的名字，有唯一性，例如：dc:”cn=admin,ou=developer,dc=163,dc=com”\n图形示例\n上边来了一堆的名词解释，看的云里雾里，还不是很明白，怎么跟自己的组织架构对应起来呢？看看下边的图是不是清晰明了 部署 OpenLDAP 说明 下面是 LDAP 中的一些术语：\nentry(条目)：LDAP 目录中的一个单位。每一个条目都由它的唯一的可辨识名称(Distinguished** Name ， DN**)来唯一确定。\nattribute(属性)：与一个 entry 直接关联的信息。比如，如果使用一个 LDAP entry 来代表一个组织的话，那么，与该组织相关联的属性可能包括一个地址、一个传真号码，等等。\n一个属性可能有一个单一的值，或者是未排序的、空格分隔开来的值的列表。有一些属性是可选的，有一些则是必须的。必须的属性使用 objectClass 定义来指定，可以在 /etc/openldap/slapd.d/cn=config/cn=schema/ 目录下的 schema 文件中找到。\n一个属性和它相应值的断言(assertion)也被称为是一个相对可辨识的名称(Relative** Distinguished Name ， RDN**)。与 Distinguished Name(它是全局唯一的)不同的是，Relative Distinguished Name 只在每个条目中是唯一的。\nLDIF：LDAP 数据交换格式(LDAP Data Interchange Format，LDIF)是一个 LDAP entry 的纯文本表现形式。它有如下格式： id 是可选的，是一个数字，它由应用程序决定，被用来编辑该条目。每一个条目可以包含任意数量的 attribute_type 和 attribute_value 对，只要它们都在相应的 schema file 中定义了就行。空白行代表了该条目的结束。\nschema 文件：简单来说，可以理解为模板文件，是对 OpenLDAP 配置文件中所能使用的属性进行限定的文件。\n安装 OpenLDAP 套件 OpenLDAP 套件包含如下软件包：\n软件包名 说明 openldap 包含运行 OpenLDAP server 和 client 端应用程序所需的库。 openldap-clients 包含命令行工具，用于查看和修改 LDAP server 上的目录。 openldap-servers 包含用于配置和运行一个 LDAP server 的服务和工具。这包括一个独立的 LDAP 服务程序，slapd。 compat-openldap 包含 OpenLDAP 兼容库。 要搭建一个基本的 LDAP 服务器，安装如下软件包：\n1 [root@gw ~]# yum install openldap openldap-clients openldap-servers 我这里安装的是 2.4.44 版本的 openldap：\n启动 OpenLDAP 服务 直接启动 OpenLDAP 服务并设置开机启动：\n1 2 [root@gw ~]# systemctl start slapd [root@gw ~]# systemctl enable slapd 配置 OpenLDAP 服务 OpenLDAP 的配置文件和目录：\n配置文件 说明 /etc/openldap/ldap.conf 使用 OpenLDAP 库的客户端应用程序的配置文件。包括 ldapadd、ldapsearch、Evolution 等。关于该文件中配置的详细说明，可执行 man ldap.conf 命令。 /etc/openldap/slapd.d/ 包含 slapd 配置文件的目录。关于该目录下配置的详细说明，可执行 man slapd-config 命令。 注意，OpenLDAP 服务进程已不再从/etc/openldap/slapd.conf 文件读取它的配置，它的是使用位于/etc/openldap/slapd.d/目录下的配置。但是，不要手动去修改里面的配置，而应该使用客户端工具，比如 ldapmodify。\n该目录下的配置文件的结构类似于下面这样(这里省略了一些配置文件，只是为了便于解释)：\n可以看到，树的根节点是“cn=config”条目，即 cn=config.ldif 文件，它包含全局的配置。其它的设置包含在 cn=config 文件夹下的各子条目中。\n实际来说，cn=config 文件夹下有如下这些文件：\ncn=schema.ldif 文件：包含 schema 相关的设置。“cn=schema,cn=config”条目包含了该系统的 schema(所有硬编码到 slapd 中的 schema)。\nolcDatabase={0}config.ldif 文件：该文件是特殊的，用于设置 OpenLDAP config 数据库自身。\nolcDatabase={-1}frontend.ldif 文件：该文件包含 OpenLDAP 前端(front end)的相关配置，并定义了全局的数据库选项，比如访问控制列表(ACL)。该文件是特殊的，它里面的设置会被其它数据库所继承，除非它们显式进行了重新定义。\nolcDatabase={1}monitor.ldif 文件：该文件包含 OpenLDAP 监控后端(monitor back end)的相关配置。当启用时，它是自动生成的，并且会使用 OpenLDAP 服务的运行状态信息来动态地更新它。\nolcDatabase={2}hdb.ldif 文件：该文件包含 hdb 数据库后端(database back end)的相关配置。默认情况下，OpenLDAP 使用 hdb 作为它的数据库后端。不过，官方文档中说，hdb 已弃用，推荐使用 mdb 作为数据库后端。\n这些文件是自动生成的，不要手动去编辑它，要使用 ldapmodify 这类工具。当然，我们可以使用 cat 命令去查看它的内容。\nldif 文件的语法：在 ldif 文件中，\u0026rsquo;#\u0026rsquo; 符号开头的行是注释语句。如果某一行以一个空格开头，该行会被认为是前一行的延续(即便上一行是一个注释语句)并且该单一的打头的空格会被移除。不同的条目(entries)之间以空行分隔开。\n生成哈希后的密码 执行 slappasswd 命令，输入你打算使用的密码，生成该密码哈希后的值：\n1 2 3 4 [root@gw ~]# slappasswd New password: # 输入你打算使用的密码 Re-enter new password: # 再次输入 {SSHA}XGQdioFMQPvxjs8z1YeT7RIyn3FJTURB # 得到经过哈希后的密码 设置 OpenLDAP config 数据库的密码 随便找个目录，创建个 ldif 文件，文件名随意，只要扩展名是 ldif 就行：\n1 2 3 4 5 [root@gw ~]# vim ldaprootpasswd.ldif dn: olcDatabase={0}config,cn=config changetype: modify add: olcRootPW olcRootPW: {SSHA}XGQdioFMQPvxjs8z1YeT7RIyn3FJTURB dn：这里输入我们想要进行更改的条目的 dn 名称。这里实际指向的是 cn=config/olcDatabase={0}config.ldif 这个文件，即 OpenLDAP 的 config 数据库文件。\nchangetype：值为 modify，表示我们想要对目标条目进行的是修改操作。\nadd：值为 olcRootPW，表示我们想要给目标条目添加一个 olcRootPW 属性。\nolcRootPW：这里输入前面得到的哈希后的密码字符串。表示我们想要把 olcRootPW 属性的值设为这个值。直接使用明文的密码也可以，但不建议这样做，因为明文密码直接写配置文件中，很容易泄露出去嘛。\nldapadd 是一个往 LDAP 中添加新条目(或给现有条目添加新属性)的工具。使用 ldapadd 命令实际执行该 ldif 文件，对 OpenLDAP 条目进行修改：\n1 [root@gw ~]# ldapadd -Y EXTERNAL -H ldapi:/// -f ldaprootpasswd.ldif -Y 选项：指定在连接 LDAP 服务器时所要使用的 SASL 认证机制。EXTERNAL 不知道是什么机制来的，就直接使用吧 -H 选项：所要连接的 LDAP 服务器的 URI 地址。 -f 选项：所要执行的 ldif 文件。 配置 LDAP 数据库 拷贝示例数据库配置文件到 /var/lib/ldap 目录(这个是默认的 hdb 数据库后端的数据目录)，并设置所属用户和组(与 slapd 服务程序的属主相同)。这个实际使用的数据库类型为 Oracle Berkeley DB。\n1 2 3 [root@gw ~]# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG [root@gw ~]# chown -R ldap:ldap /var/lib/ldap/DB_CONFIG [root@gw ~]# systemctl restart slapd 从 /etc/openldap/schema 目录下导入几个基本的 LDAP schema 文件：\n1 2 3 [root@gw ~]# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif [root@gw ~]# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldif [root@gw ~]# ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif 接下来，添加你的域到 LDAP。创建一个 ldif 文件，在下面的文件中，example 字符串可以使用你的域替代，olcRootPW 值则使用哈希得到的密码字符串替代：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [root@gw ~]# vim ldapdomain.ldif #修改olcDatabase={1}monitor.ldif文件的olcAccess属性值 dn: olcDatabase={1}monitor,cn=config changetype: modify replace: olcAccess olcAccess: {0}to * by dn.base=\u0026#34;gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\u0026#34; read by dn.base=\u0026#34;cn=Manager,dc=example,dc=com\u0026#34; read by * none #修改olcDatabase={2}hdb.ldif文件的olcSuffix属性值 dn: olcDatabase={2}hdb,cn=config changetype: modify replace: olcSuffix olcSuffix: dc=example,dc=com #修改olcDatabase={2}hdb.ldif文件的olcRootDN属性值 dn: olcDatabase={2}hdb,cn=config changetype: modify replace: olcRootDN olcRootDN: cn=Manager,dc=example,dc=com #向olcDatabase={2}hdb.ldif文件中添加olcRootPW属性和值 dn: olcDatabase={2}hdb,cn=config changetype: modify add: olcRootPW olcRootPW: {SSHA}XGQdioFMQPvxjs8z1YeT7RIyn3FJTURB #向olcDatabase={2}hdb.ldif文件中添加olcAccess属性和值，多个olcAccess项 dn: olcDatabase={2}hdb,cn=config changetype: modify add: olcAccess olcAccess: {0}to attrs=userPassword,shadowLastChange by dn=\u0026#34;cn=Manager,dc=example,dc=com\u0026#34; write by anonymous auth by self write by * none olcAccess: {1}to dn.base=\u0026#34;\u0026#34; by * read olcAccess: {2}to * by dn=\u0026#34;cn=Manager,dc=example,dc=com\u0026#34; write by * read olcAccess：ACL 规则，设置指定用户(by 后接的)对指定资源(条目和/或属性)的权限(read、write)。在 frontend 中设置的 ACL 规则会被附加到其它任意特定数据库的 ACL 规则后。特定数据库的 rootdn 总是具有读写那个数据库所有东西的权限。\nolcSuffix：用于指定特定数据库的 DN 后缀。比如，后缀为“dc=example,dc=com”的查询都将被转发到该后端数据库。dc 的值可以自行设定。如有多个，也还可再加，比如“dc=gate,dc=ykd,dc=fujfu,dc=com”。\nolcRootDN：用于指定管理该数据库的 Root DN 的名称。默认为空，表示不指定 root 用户。如果 Root DN 是位于 olcSuffix 指定的 DN 后缀的里面，那么也必须使用 olcRootPW 来设定 Root DN 的密码。\n执行该 ldif 文件，对 LDAP 实际进行修改：\n1 [root@gw ~]# ldapmodify -Y EXTERNAL -H ldapi:/// -f ldapdomain.ldif 接下来，我们添加一些条目到 LDAP 目录中。创建个 ldif 文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@gw ~]# vim baseldapdomain.ldif dn: dc=example,dc=com objectClass: top objectClass: dcObject objectclass: organization o: example com dc: example dn: cn=Manager,dc=example,dc=com objectClass: organizationalRole cn: Manager description: Directory Manager dn: ou=People,dc=example,dc=com objectClass: organizationalUnit ou: People dn: ou=Group,dc=example,dc=com objectClass: organizationalUnit ou: Group o 属性：在 schema 文件中，o 代表 organizationName，组织名称。\ndc 属性：在 schema 文件中，dc 代表 domainComponent，域部件。\nou 属性：在 schema 文件中，ou 代表 organizationalUnitName，组织单元名称。\n执行该 ldif 文件。执行该命令后，会提示你输入 LDAP root 用户密码。\n1 [root@gw ~]# ldapadd -x -W -D \u0026#34;cn=Manager,dc=example,dc=com\u0026#34; -f baseldapdomain.ldif -x 选项：使用简单认证(simple authentication)而不是 SASL。\n-W 选项：提示输入用于简单认证的密码。这与直接在命令行指定密码是不同的。\n-D 选项：使用指定的 DN 名称来绑定到 LDAP 目录。\n接下来，创建一个 LDAP 用户 tecmint，并为它设置一个密码：\n1 2 [root@gw ~]# useradd tecmint [root@gw ~]# passwd tecmint 创建一个 ldif 文件，为 LDAP group 进行定义：\n1 2 3 4 5 [root@gw ~]# vim ldapgroup.ldif dn: cn=Manager,ou=Group,dc=example,dc=com objectClass: top objectClass: posixGroup gidNumber: 1002 在上面的配置中，gidNumber 是/etc/group 文件中 tecmint 的 GID，并将它添加到 OpenLDAP 目录。\n执行该 LDIF 文件：\n1 [root@gw ~]# ldapadd -x -W -D \u0026#34;cn=Manager,dc=example,dc=com\u0026#34; -f ldapgroup.ldif 接下来，创建另一个 ldif 文件，添加 tecmint 用户的定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@gw ~]# vim ldapuser.ldif dn: uid=tecmint,ou=People,dc=example,dc=com objectClass: top objectClass: account objectClass: posixAccount objectClass: shadowAccount cn: tecmint uid: tecmint uidNumber: 1002 gidNumber: 1002 homeDirectory: /home/tecmint userPassword: {SSHA}E6DO8V0dK5km8ZTJzCE0QO++/EX8GFax #该用户的密码哈希后的字符串 loginShell: /bin/bash gecos: tecmint shadowLastChange: 0 shadowMax: 0 shadowWarning: 0 执行该 LDIF 文件：\n1 [root@gw ~]# ldapadd -x -W -D \u0026#34;cn=Manager,dc=example,dc=com\u0026#34; -f ldapuser.ldif 安装 phpLDAPadmin 安装 phpLDAPadmin：\n1 2 [root@gw ~]# yum install epel-release [root@gw ~]# yum install phpldapadmin 事实上，这不仅安装了 phpldapadmin，还安装了 http、php 等相关的依赖。phpldapadmin 其实就是运行在 httpd 程序的 php 模块上的一个 php 应用。\n修改 httpd 配置文件，修改 httpd 与 phpldapadmin 集成的配置文件，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@gw ~]# vim /etc/httpd/conf.d/phpldapadmin.conf Alias /phpldapadmin /usr/share/phpldapadmin/htdocs Alias /ldapadmin /usr/share/phpldapadmin/htdocs \u0026lt;Directory /usr/share/phpldapadmin/htdocs\u0026gt; \u0026lt;IfModule mod_authz_core.c\u0026gt; # Apache 2.4 Require all granted \u0026lt;/IfModule\u0026gt; \u0026lt;IfModule !mod_authz_core.c\u0026gt; # Apache 2.2 Order Deny,Allow Deny from all Allow from 127.0.0.1 Allow from ::1 \u0026lt;/IfModule\u0026gt; \u0026lt;/Directory\u0026gt; 修改 phpldapadmin 的配置文件，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [root@gw ~]# vim /etc/phpldapadmin/config.php \u0026lt;?php $config-\u0026gt;custom-\u0026gt;session[\u0026#39;blowfish\u0026#39;] = \u0026#39;036b0f2d39d36791dd3a8effa7c3411f\u0026#39;; # 值是自动生成的 $config-\u0026gt;custom-\u0026gt;appearance[\u0026#39;hide_template_warning\u0026#39;] = true; # 避免不必要的告警信息提示 $config-\u0026gt;custom-\u0026gt;appearance[\u0026#39;minimalMode\u0026#39;] = true; # 不要页面头和注脚，使页面更简洁 $config-\u0026gt;custom-\u0026gt;appearance[\u0026#39;friendly_attrs\u0026#39;] = array( \u0026#39;facsimileTelephoneNumber\u0026#39; =\u0026gt; \u0026#39;Fax\u0026#39;, \u0026#39;gid\u0026#39; =\u0026gt; \u0026#39;Group\u0026#39;, \u0026#39;mail\u0026#39; =\u0026gt; \u0026#39;Email\u0026#39;, \u0026#39;telephoneNumber\u0026#39; =\u0026gt; \u0026#39;Telephone\u0026#39;, \u0026#39;uid\u0026#39; =\u0026gt; \u0026#39;User Name\u0026#39;, \u0026#39;userPassword\u0026#39; =\u0026gt; \u0026#39;Password\u0026#39; ); $servers = new Datastore(); $servers-\u0026gt;newServer(\u0026#39;ldap_pla\u0026#39;); $servers-\u0026gt;setValue(\u0026#39;server\u0026#39;,\u0026#39;name\u0026#39;,\u0026#39;Local LDAP Server\u0026#39;); $servers-\u0026gt;setValue(\u0026#39;server\u0026#39;,\u0026#39;host\u0026#39;,\u0026#39;172.31.2.6\u0026#39;); # LDAP服务器的地址 $servers-\u0026gt;setValue(\u0026#39;server\u0026#39;,\u0026#39;port\u0026#39;,389); $servers-\u0026gt;setValue(\u0026#39;server\u0026#39;,\u0026#39;base\u0026#39;,array(\u0026#39;dc=example,dc=com\u0026#39;)); # LDAP域的base DNs $servers-\u0026gt;setValue(\u0026#39;login\u0026#39;,\u0026#39;auth_type\u0026#39;,\u0026#39;session\u0026#39;); $servers-\u0026gt;setValue(\u0026#39;appearance\u0026#39;,\u0026#39;password_hash\u0026#39;,\u0026#39;\u0026#39;); $servers-\u0026gt;setValue(\u0026#39;login\u0026#39;,\u0026#39;attr\u0026#39;,\u0026#39;dn\u0026#39;); # 取消注释 // $servers-\u0026gt;setValue(\u0026#39;login\u0026#39;,\u0026#39;attr\u0026#39;,\u0026#39;uid\u0026#39;); # 添加注释 $servers-\u0026gt;setValue(\u0026#39;unique\u0026#39;,\u0026#39;attrs\u0026#39;,array(\u0026#39;uid\u0026#39;,\u0026#39;sn\u0026#39;)); ?\u0026gt; 用户登录方式，在此我们使用的是 dn 方式进行登录，phpldapadmin 默认使用的是 uid 方式进行登录。\nunique 唯一性，在此我们使用的是 uid 和 sn 作为唯一的标志，这个一定要注意下。特别是从一个用户复制为另外一个用户时，要使用到该配置。\n启动 httpd：\n1 2 [root@gw ~]# systemctl start httpd [root@gw ~]# systemctl enable httpd 访问 phpLDAPadmin 访问 phpLDAPadmin 地址 http://​server_ip**​ /ldapadmin/ ： 上述截图中，登录 DN，如果是 OpenLDAP 管理员登录的话，使用 cn=Manager,dc=example,dc=com。\n如果是 OpenLDAP 普通用户登录的话，使用 uid=tecmint,ou=People,dc=example,dc=com。\n","date":"2024-05-16T15:44:43Z","image":"https://www.ownit.top/title_pic/76.jpg","permalink":"https://www.ownit.top/p/202405161544/","title":"生产环境 OpenLDAP 部署流程"},{"content":"黑猫投诉平台，舆论监控系统 BuzzMonitor https://github.com/nangongchengfeng/BuzzMonitor.git\n简介 \u0026ldquo;黑猫投诉\u0026quot;舆论监控系统是一款专为快速识别和响应网络投诉而设计的应用，旨在帮助企业或机构第一时间掌握公众意见和反馈。通过实时监控网站及其他在线平台，系统能够迅速侦测到关于品牌或服务的负面评论、投诉或提议。 流程 参考文档：https://blog.csdn.net/qq_45270849/article/details/135416529\n针对企业公司关键字 比如：美团外卖\n1、打开页面 https://tousu.sina.com.cn/company/view/?couid=1003626\u0026sid=26857\n2、找到商家列表，获取到couid的值，这就是关键字\n3、根据关键字组合，解密获取到api接口的数据\n定时任务 爬虫是定时任务驱动，使用apscheduler\n1 2 3 4 5 6 7 8 9 # 创建一个后台调度器 scheduler = BackgroundScheduler(timezone=\u0026#34;Asia/Shanghai\u0026#34;) # 添加一个每隔20秒执行一次的定时任务 测试 scheduler.add_job(func=send_alert, trigger=\u0026#34;interval\u0026#34;, seconds=20) # 添加一个每隔10分钟 执行的定时任务 scheduler.add_job(func=get_heimao, trigger=CronTrigger(minute=\u0026#39;*/10\u0026#39;)) # 启动调度器 scheduler.start() return app 发送钉钉告警 使用方式 1、数据库创建数据库名称和执行命令 1 2 3 4 5 6 7 8 9 10 11 12 pip3 install Flask-Migrate 初始化(只需要执行一次) flask db init 生成文件 flask db migrate 迁移数据库 flask db upgrade 记得 （我已经导入） 只需要在 app.py 中导入 models.py 中的类即可。 而且导入全部和导入一个，结果都是可以对所有的表进行创建。 2、设置爬虫的信息 接口分页获取数据库内容 1 2 3 4 5 6 http://127.0.0.1:5000/info/ # 默认值 page = int(request.args.get(\u0026#39;page\u0026#39;, 1)) per_page = int(request.args.get(\u0026#39;per_page\u0026#39;, 10)) ","date":"2024-04-24T22:31:16Z","image":"https://www.ownit.top/title_pic/49.jpg","permalink":"https://www.ownit.top/p/202404242231/","title":"Python实现“黑猫投诉平台，舆论监控系统”"},{"content":"ES的备份：ES快照备份 根据时间，每天零点在Linux机器crontab来调用api接口实现快照备份，通过快照备份，可以定准恢复到某一天的日志。 现象：（坑：但是恢复某一天日志，发现会少8小时的日志，基本是：0:00 - 08:00 时间段）\n1、引言： 在大规模分布式系统中，ELK（Elasticsearch、Logstash、Kibana）栈已成为日志管理和分析的标准工具集。然而，实际部署与运维过程中，往往会遭遇各种挑战，其中时区问题导致的日志时间滞后尤为常见。本文将聚焦于解决ELK日志收集与备份过程中出现的滞后8小时等时区问题，分享实战经验与填坑策略。\n2、问题现象与原因 1、剖析滞后8小时时区 现象描述：在Kibana上观察到的一个典型现象是，即使日志事件是在当地时间上午9点发生的，但在日志系统中显示的时间却是下午5点。这种时差问题会导致运维团队在实时监控和响应中遭遇不少困难，因为所有的日志数据都显示为8小时前的事件。\n问题根源\n这一问题通常由以下几个因素造成：\n日志源端时区设置错误： 日志生成时，很多系统默认使用格林尼治标准时间（GMT）来记录时间戳，而不是当地时间。如果日志源设备或服务的时区设置不正确，或未明确指定时区，就会在源头产生时间偏差。\n传输过程中的时区处理不当： 在日志数据的收集过程中，如使用Logstash或Filebeat等工具，若这些工具没有被正确配置为转换或识别正确的时区，它们在处理日志时会继续使用GMT时间戳，从而进一步传递错误的时间信息。\nElasticsearch存储与展示的时区配置不当： Elasticsearch在存储时间数据时，默认使用UTC时间。如果在Elasticsearch或Kibana中没有设置正确的时区，即使原始日志的时间戳是正确的，展示时也会因为时区转换错误而导致时间显示不正确\n2、时区问题拆解 Elasticserch 默认时区是？能改吗？ 官方文档强调：在 Elasticsearch 内部，日期被转换为 UTC时区并存储为一个表示自1970-01-01 00:00:00 以来经过的毫秒数的值。\nInternally, dates are converted to UTC (if the time-zone is specified) and stored as a long number representing milliseconds-since-the-epoch.\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/date.html\nElasticsearch date 类型默认时区：UTC。\n正如官方工程师强调（如下截图所示）：Elasticsearch 默认时区不可以修改 https://discuss.elastic.co/t/index-creates-in-different-timezone-other-than-utc/148941但，我们可以“曲线救国”，通过：\ningest pipeline 预处理方式写入的时候修改时区； logstash filter 环节做时区转换； 查询时指定时区； 聚合时指定时区。\nKibana 默认时区是？能改吗？ kibana 默认时区是浏览器时区。可以修改，修改方式如下：Stack Management -\u0026gt; Advanced Settings -\u0026gt;Timezone for data formatting. Logstash 默认时区是？能改吗？\n默认：UTC。可以通过中间：filter 环节进行日期数据处理，包括：转时区操作。 logstash 默认 UTC 时区。 Elasticsearch 默认 UTC 时区。 Kibana 默认浏览器时区，基本我们用就是：东八区。 如果基于Mysql 同步数据，Mysql 数据是：东八区。 3、填坑实战与解决方案 基于上面的分析，如何解决时区问题呢？。 实战项目中，时间问题就转嫁为：写入的时候转换成给定时区（如：东8区）就可以。 1、查看机器的时区和日志\n1 2 / # date Mon Apr 15 11:21:29 CST 2024 2、Filebeaet 收集配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 filebeat.inputs: - type: log enabled: true paths: - /app/logs/app/*.log multiline.pattern: ^\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}[.,:]0{0,1}\\d{3} multiline.negate: true multiline.match: after fields: logtype: ${LOGTYPE:app} fields_under_root: true fields: ip: ${POD_IP} output.logstash: hosts: [\u0026#34;logstash-server.default.svc.cloudcs.fjf:5044\u0026#34;] 3、logstash 中间 filter 环节处理 数据源端：APP日志； 数据目的端：Elasticsearch； 同步方式：logstash，本质借助：logstash_input_jdbc 插件同步； 时区处理：logstash filter 环节 ruby 脚本处理。\n1 2 {\u0026#34;@timestamp\u0026#34;:\u0026#34;2024-04-15T10:48:34.518811962+08:00\u0026#34;,\u0026#34;severity\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;cat-outer-gateway\u0026#34;,\u0026#34;trace\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;span\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;parent\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;exportable\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;thread\u0026#34;:\u0026#34;reactor-http-epoll-2\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;c.fujfu.gateway.filter.CallBackUriFilter\u0026#34;,\u0026#34;rest\u0026#34;:\u0026#34;path:/cat-payment-dock/api/minSheng/notify,method:POST\\n\u0026#34;} {\u0026#34;@timestamp\u0026#34;:\u0026#34;2024-04-15T10:55:29.711692942+08:00\u0026#34;,\u0026#34;severity\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;service\u0026#34;:\u0026#34;cat-outer-gateway\u0026#34;,\u0026#34;trace\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;span\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;parent\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;exportable\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;thread\u0026#34;:\u0026#34;reactor-http-epoll-4\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;c.fujfu.gateway.filter.CallBackUriFilter\u0026#34;,\u0026#34;rest\u0026#34;:\u0026#34;path:/cat-payment-dock/api/minSheng/notify,method:POST\\n\u0026#34;} 1 2 2024-04-15 11:32:57.582 INFO [HzgDfyOvQqCH4DsY5Al7Jw] 1 --- [nio-8050-exec-9] c.f.f.s.impl.FpProductCfgServiceImpl : capitalId:1724007481146916866-该用户渠道被限制，channelCodeList:[huawei, oppo, vivo, xiaomi, yingyongbao, yingyongbaotuiguang, Android_oppo, Android_yingyongbao, Android_huawei, Android_xiaomi, FYH-rongyao, FYH-vivo, FYH-xiaomi, FYH-huawei] ,userReqVO.getRegisterChannel():huawei 2024-04-15 11:32:57.695 INFO [TtPg4TgCQI+MFkLxSCZzMA] 1 --- [nio-8050-exec-2] c.f.f.s.f.request.JUZIFundProductCaller : 【桔子数科】请求明文：https://api-gateway.jzhlkj.com/bus/standard/credit/queryQuota，参数：{\u0026#34;channelUid\u0026#34;:\u0026#34;29443d29067c4182baec1115026cf8d5\u0026#34;} 下方的脚本就实现收集上方两种日志格式 和 写入的时候转换成给定时区这个功能。\ndate 的数据 就是 东八区 的时间。我直接把 东八区的时间 写入@timestamp 在传入到ES（东八区） 。然后在 Kibana中访问 也是东八区，形成一个闭环\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 input { beats { port =\u0026gt; 5044 } } filter { if [fields][json] == \u0026#34;true\u0026#34; { json { source =\u0026gt; \u0026#34;message\u0026#34; remove_field =\u0026gt; [\u0026#34;message\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;tags\u0026#34;,\u0026#34;ecs\u0026#34;] add_field =\u0026gt; { \u0026#34;loglevel\u0026#34; =\u0026gt; \u0026#34;%{[severity]}\u0026#34; } } } else if [fields][logtype] == \u0026#34;powerjob\u0026#34; { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{TIMESTAMP_ISO8601:date} \\s{0,1}(?\u0026lt;severity\u0026gt;.*?) (?\u0026lt;pid\u0026gt;.*?) --- \\[(?\u0026lt;thread\u0026gt;.*?)\\] (?\u0026lt;class\u0026gt;.*?) \\s*: (?\u0026lt;rest\u0026gt;.*+?)\u0026#34; } remove_field =\u0026gt; [\u0026#34;message\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;tags\u0026#34;] add_field =\u0026gt; { \u0026#34;loglevel\u0026#34; =\u0026gt; \u0026#34;%{[severity]}\u0026#34; } } mutate { update =\u0026gt; { \u0026#34;[fields][logtype]\u0026#34; =\u0026gt; \u0026#34;logstash\u0026#34; } } } else { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; [ \u0026#34;%{TIMESTAMP_ISO8601:date} (?\u0026lt;loglevel\u0026gt;.*?)\\s{1,2}\\| \\[(?\u0026lt;threadname\u0026gt;.*?)\\] (?\u0026lt;classname\u0026gt;.*?) \\[(?\u0026lt;codeline\u0026gt;.*?)\\] \\| \\[(?\u0026lt;traceid\u0026gt;.*?)\\] \\| (?\u0026lt;msg\u0026gt;.*+?)\u0026#34;, \u0026#34;%{TIMESTAMP_ISO8601:date} (?\u0026lt;loglevel\u0026gt;.*?)\\s{1,2}\\| \\[(?\u0026lt;threadname\u0026gt;.*?)\\] (?\u0026lt;classname\u0026gt;.*?) \\[(?\u0026lt;codeline\u0026gt;.*?)\\] \\| (?\u0026lt;msg\u0026gt;.*+?)\u0026#34;, \u0026#34;\\[%{TIMESTAMP_ISO8601:date}\\] \\[(?\u0026lt;loglevel\u0026gt;.*?)\\s{0,2}\\] \\[(?\u0026lt;threadname\u0026gt;.*?)\\] (?\u0026lt;classname\u0026gt;.*?) (?\u0026lt;codeline\u0026gt;.*?) - (?\u0026lt;msg\u0026gt;.*+?)\u0026#34;, \u0026#34;%{TIMESTAMP_ISO8601:date} \\[(?\u0026lt;threadname\u0026gt;.*?)\\] (?\u0026lt;loglevel\u0026gt;.*?)\\s{0,2} (?\u0026lt;classname\u0026gt;.*?) (?\u0026lt;codeline\u0026gt;.*?) - (?\u0026lt;msg\u0026gt;.*+?)\u0026#34;, \u0026#34;\\[%{TIMESTAMP_ISO8601:date}\\] \\[\\s{0,2}(?\u0026lt;loglevel\u0026gt;.*?)\\] \\[(?\u0026lt;threadid\u0026gt;.*?)\\] \\[(?\u0026lt;threadname\u0026gt;.*?)\\] \\[(?\u0026lt;classname\u0026gt;.*?)\\] : (?\u0026lt;msg\u0026gt;.*+?)\u0026#34; ]} remove_field =\u0026gt; [\u0026#34;message\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;tags\u0026#34;] } } ruby { code =\u0026gt;\u0026#39; arr = event.get(\u0026#34;host\u0026#34;)[\u0026#34;name\u0026#34;].split(\u0026#34;.\u0026#34;)[0] event.set(\u0026#34;projectname\u0026#34;,arr) \u0026#39; } date { match =\u0026gt; [\u0026#34;date\u0026#34;,\u0026#34;ISO8601\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss.SSS\u0026#34;] #获取date的时间 target =\u0026gt; \u0026#34;@timestamp\u0026#34; # 复制给@timestamp 字段 timezone =\u0026gt; \u0026#34;Asia/Shanghai\u0026#34; # 设置 上海地区 } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;elasticsearch-log.prod.server.fjf:9200\u0026#34;] user =\u0026gt; \u0026#34;xxxxx\u0026#34; password =\u0026gt; \u0026#34;xxxxx\u0026#34; manage_template =\u0026gt; false index =\u0026gt; \u0026#34;%{[fields][logtype]}-prod-%{+YYYY.MM.dd}\u0026#34; } } 修改前\n修改后 4、总结 数据写入时间不一致、数据滞后8小时等时区问题的本质是：各个处理端时区不一致，写入源的时区、Kibana默认是本地时区（如中国为：东8区时区），而 logstash、Elasticsearch 是UTC时区。\n参考文档：https://www.cnblogs.com/fat-girl-spring/p/15122906.html\n","date":"2024-04-15T11:39:49Z","image":"https://www.ownit.top/title_pic/30.jpg","permalink":"https://www.ownit.top/p/202404151139/","title":"ELK日志收集和备份填坑实战 （滞后8个小时等时区问题）"},{"content":"介绍 在传统的业务系统中，应用微服务化后，需要一个统一的入口来将各个服务进行整合，这个入口可以是Nginx、Apache、HAproxy等等。而在K8s中，同样需要一个工具来将应用的各个service整合到统一的入口，这个工具就叫Ingress控制器，Ingress的中文翻译即为“入口”。\nIngress-nginx： 它是由Kubernetes社区基于Nginx Web服务器开发的，并补充了一组用于实现额外功能的Lua插件，作为“官方”默认控制器支持当然最优。\nGithub​https://github.com/kubernetes/ingress-nginx\n说明文档​https://kubernetes.github.io/ingress-nginx/deploy/\nNginx-ingress 这是Nginx官方社区开发产品，Nginx ingress具有很高的稳定性，持续的向后兼容性，没有任何第三方模块，并且由于消除了Lua代码而保证了较高的速度。\nGithub​https://github.com/nginxinc/kubernetes-ingress\n说明文档​https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/ Nginx Ingress Controller 基于 Nginx 实现了 Kubernetes Ingress API，Nginx 是公认的高性能网关，但如果不对其进行一些参数调优，就不能充分发挥出高性能的优势。Nginx Ingress工作原理： Kubernetes中ingress-nginx优化配置 描述: 在K8s集群中部署安装 ingress-nginx 后默认并未进行相应的优化配置，本小结将针对于生产环境的中的 ingress-nginx 控制器进行优化。\n我们可以从 ingress-nginx-controller 资源的 Pod 、ConfigMap 、以及业务的 ingress 规则入手。\n我们当时默认直接安装环境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 kubectl get deployments.apps -n ingress-nginx ingress-nginx-controller # 整体就这个样子，但是没有任何优化，性能肯定欠缺 kubectl get deployments.apps -n ingress-nginx ingress-nginx-controller -o yaml apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; #暴露Prometheus 端口进行监控 prometheus.io/port: \u0026#34;10254\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: ingress-nginx app.kubernetes.io/version: 1.0.0 helm.sh/chart: ingress-nginx-4.0.1 k8s.kuboard.cn/name: ingress-nginx-controller name: ingress-nginx-controller namespace: ingress-nginx resourceVersion: \u0026#34;91407609\u0026#34; uid: a5452fcf-f99c-4866-b0a1-93fe9e165cb6 spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx spec: containers: - args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --watch-ingress-without-class=true env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so image: willdockerhub/ingress-nginx-controller:v1.0.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown ........... 初始化Pod内核参数 温馨提示: 我们需要针对承载 ingress-nginx 的相关 Pod 容器进行内核参数优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 在 spec.template.spec 对象下添加一个初始化 initContainers 容器 initContainers: - command: - /bin/sh - -c - | mount -o remount rw /proc/sys sysctl -w net.core.somaxconn=65535 sysctl -w net.ipv4.ip_local_port_range=\u0026#34;1024 65535\u0026#34; sysctl -w net.ipv4.tcp_tw_reuse=1 sysctl -w fs.file-max=1048576 sysctl -w fs.inotify.max_user_instances=16384 sysctl -w fs.inotify.max_user_watches=524288 sysctl -w fs.inotify.max_queued_events=16384 image: busybox:1.29.3 imagePullPolicy: IfNotPresent name: init-sysctl resources: {} securityContext: capabilities: add: - SYS_ADMIN drop: - ALL terminationMessagePath: /dev/termination-log terminationMessagePolicy: File #修改deployment kubectl edit deployments.apps -n ingress-nginx ingress-nginx-controller 注意：如果按照文档安装，指定机器的跑ingres，会调度失败，因为原先的pod已经占用了80和443端口（唯一方式：先删除pod（先把副本重置为0，修改完毕，在改成1），再修改deployment）\n由此看，定制化有优势，也有劣势。\n因为上面的错误，给无生成大量的\n主要是调度失败引起的。\n1 2 ingress-nginx-controller-5d9cbf7bd7-27q74 0/1 NodePorts 0 74s ingress-nginx-controller-5d9cbf7bd7-5lh9x 0/1 NodePorts 0 75s 1 2 批量清除（清除完成可能一直卡，需要自己看一下数量） kubectl get pods -n ingress-nginx -o custom-columns=NAME:.metadata.name,STATUS:.status.phase |grep \u0026#34;Failed\u0026#34; |awk \u0026#39;{print $1}\u0026#39; | xargs -r kubectl delete pod -n ingress-nginx Ingree的ConfigMap优化 我们需要按照需要将下述K/V配置项插入到 ingress-nginx 的 configMap 里的 data 对象下。\n1 2 3 4 5 6 7 8 9 10 11 spec: containers: - args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --watch-ingress-without-class=true ingress-nginx-controller 这一个就是 Ingree的configmap的配置文件\ningress-nginx 资源查看\n1 2 # 查看 Ingress-nginx 全局配置参数: kubectl get cm -n ingress-nginx ingress-nginx-controller -oyaml 默认没有任何数据 下面面是data的内容，部分可以再根据实际情况修改\n在Ingress Nginx中，ConfigMap的修改可以实现所谓的\u0026quot;热加载\u0026quot;。这意味着当你更改Ingress Nginx的ConfigMap时，Ingress控制器能够接收这些更改并应用它们，而不需要重启Pods。控制器会监听ConfigMap的变化，并自动更新其配置，从而反映出所做的修改。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [root@bt ingress]# kubectl apply -f ingress-nginx-controller.yaml #热加载一下 [root@bt ingress]# cat ingress-nginx-controller.yaml apiVersion: v1 data: compute-full-forwarded-for: \u0026#34;true\u0026#34; forwarded-for-header: X-Forwarded-For log-format-upstream: \u0026#39;{\u0026#34;@timestamp\u0026#34;:\u0026#34;$time_iso8601\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;$time_local\u0026#34;,\u0026#34;remote_addr\u0026#34;:\u0026#34;$remote_addr\u0026#34;,\u0026#34;http_x_forwarded_for\u0026#34;:\u0026#34;$http_x_forwarded_for\u0026#34;,\u0026#34;remote_port\u0026#34;:\u0026#34;$remote_port\u0026#34;,\u0026#34;remote_user\u0026#34;:\u0026#34;$remote_user\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;$host\u0026#34;,\u0026#34;upstream_addr\u0026#34;:\u0026#34;$upstream_addr\u0026#34;,\u0026#34;upstream_status\u0026#34;:\u0026#34;$upstream_status\u0026#34;,\u0026#34;upstream_response_time\u0026#34;:$upstream_response_time,\u0026#34;upstream_cache_status\u0026#34;:\u0026#34;$upstream_cache_status\u0026#34;,\u0026#34;request\u0026#34;:\u0026#34;$request\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;$status\u0026#34;,\u0026#34;request_time\u0026#34;:$request_time,\u0026#34;body_bytes_sent\u0026#34;:\u0026#34;$body_bytes_sent\u0026#34;,\u0026#34;http_referer\u0026#34;:\u0026#34;$http_referer\u0026#34;,\u0026#34;http_user_agent\u0026#34;:\u0026#34;$http_user_agent\u0026#34;}\u0026#39; proxy-body-size: 10g proxy-buffer-size: 256k proxy-connect-timeout: \u0026#34;90\u0026#34; proxy-read-timeout: \u0026#34;90\u0026#34; proxy-send-timeout: \u0026#34;90\u0026#34; use-forwarded-headers: \u0026#34;true\u0026#34; # 客户端请求头的缓冲区大小 client-header-buffer-size: \u0026#34;512k\u0026#34; # 设置用于读取大型客户端请求标头的最大值number和size缓冲区 large-client-header-buffers: \u0026#34;4 512k\u0026#34; # 读取客户端请求body的缓冲区大小 client-body-buffer-size: \u0026#34;128k\u0026#34; # 代理缓冲区大小 proxy-buffer-size: \u0026#34;256k\u0026#34; # 代理body大小 proxy-body-size: \u0026#34;50m\u0026#34; # 服务器名称哈希大小 server-name-hash-bucket-size: \u0026#34;128\u0026#34; # map哈希大小 map-hash-bucket-size: \u0026#34;128\u0026#34; # SSL加密套件 ssl-ciphers: \u0026#34;ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA\u0026#34; # ssl 协议 ssl-protocols: \u0026#34;TLSv1 TLSv1.1 TLSv1.2\u0026#34; kind: ConfigMap metadata: labels: app: ingress-nginx name: ingress-nginx-controller namespace: ingress-nginx 这个Json 更加全面，可以接入ELK展示\n1 {\u0026#34;@timestamp\u0026#34;:\u0026#34;$time_iso8601\u0026#34;,\u0026#34;request_time\u0026#34;:\u0026#34;$request_time\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;$status\u0026#34;,\u0026#34;body_bytes_sent\u0026#34;:\u0026#34;$body_bytes_sent\u0026#34;,\u0026#34;bytes_sent\u0026#34;:\u0026#34;$bytes_sent\u0026#34;,\u0026#34;remote_addr\u0026#34;:\u0026#34;$remote_addr\u0026#34;,\u0026#34;remote_user\u0026#34;:\u0026#34;$remote_user\u0026#34;,\u0026#34;remote_port\u0026#34;:\u0026#34;$remote_port\u0026#34;,\u0026#34;request_uri\u0026#34;:\u0026#34;$request_uri\u0026#34;,\u0026#34;args\u0026#34;:\u0026#34;$args\u0026#34;,\u0026#34;request_id\u0026#34;:\u0026#34;$request_id\u0026#34;,\u0026#34;connection\u0026#34;:\u0026#34;$connection\u0026#34;,\u0026#34;connection_requests\u0026#34;:\u0026#34;$connection_requests\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;$pid\u0026#34;,\u0026#34;request_length\u0026#34;:\u0026#34;$request_length\u0026#34;,\u0026#34;time_local\u0026#34;:\u0026#34;$time_local\u0026#34;,\u0026#34;http_referer\u0026#34;:\u0026#34;$http_referer\u0026#34;,\u0026#34;http_user_agent\u0026#34;:\u0026#34;$http_user_agent\u0026#34;,\u0026#34;http_x_forwarded_for\u0026#34;:\u0026#34;$http_x_forwarded_for\u0026#34;,\u0026#34;http_host\u0026#34;:\u0026#34;$http_host\u0026#34;,\u0026#34;server_name\u0026#34;:\u0026#34;$server_name\u0026#34;,\u0026#34;ssl_protocol\u0026#34;:\u0026#34;$ssl_protocol\u0026#34;,\u0026#34;ssl_cipher\u0026#34;:\u0026#34;$ssl_cipher\u0026#34;,\u0026#34;scheme\u0026#34;:\u0026#34;$scheme\u0026#34;,\u0026#34;request_method\u0026#34;:\u0026#34;$request_method\u0026#34;,\u0026#34;server_protocol\u0026#34;:\u0026#34;$server_protocol\u0026#34;,\u0026#34;pipe\u0026#34;:\u0026#34;$pipe\u0026#34;,\u0026#34;gzip_ratio\u0026#34;:\u0026#34;$gzip_ratio\u0026#34;,\u0026#34;http_cf_ray\u0026#34;:\u0026#34;$http_cf_ray\u0026#34;,\u0026#34;request_host\u0026#34;:\u0026#34;$host\u0026#34;,\u0026#34;is_completion\u0026#34;:\u0026#34;$request_completion\u0026#34;,\u0026#34;upstream\u0026#34;:\u0026#34;$upstream_addr\u0026#34;,\u0026#34;upstream_name\u0026#34;:\u0026#34;$proxy_host\u0026#34;,\u0026#34;upstream_status\u0026#34;:\u0026#34;$upstream_status\u0026#34;,\u0026#34;upstream_bytes_sent\u0026#34;:\u0026#34;$upstream_bytes_sent\u0026#34;,\u0026#34;upstream_bytes_received\u0026#34;:\u0026#34;$upstream_bytes_received\u0026#34;,\u0026#34;upstream_connect_time\u0026#34;:\u0026#34;$upstream_connect_time\u0026#34;,\u0026#34;upstream_header_time\u0026#34;:\u0026#34;$upstream_header_time\u0026#34;,\u0026#34;upstream_response_time\u0026#34;:\u0026#34;$upstream_response_time\u0026#34;,\u0026#34;upstream_response_length\u0026#34;:\u0026#34;$upstream_response_length\u0026#34;,\u0026#34;upstream_cache_status\u0026#34;:\u0026#34;$upstream_cache_status\u0026#34;,\u0026#34;nginx_host\u0026#34;:\u0026#34;$hostname\u0026#34;} TCP和UPD 1 2 3 4 5 6 7 8 9 10 - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --watch-ingress-without-class=true - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services #增加tcp配置 - --udp-services-configmap=$(POD_NAMESPACE)/udp-services #增加upd配置 除了 HTTP 流量之外，NGINX Ingress Controller 还可以负载 TCP 和 UDP 流量，因此您可以使用它来管理基于以下协议的各种应用和实用程序的流量，它们包括：\nMySQL、LDAP 和 MQTT —— 许多流行应用使用的基于 TCP 协议的应用 DNS、syslog 和 RADIUS —— 边缘设备和非事务性应用使用的基于 UDP 协议的实用程序 https://www.niewx.cn/2021/11/09/2021-11-09-Use-Nginx-Ingress-to-expose-TCP-and-udp-services/\nhttps://www.nginx-cn.net/blog/load-balancing-tcp-and-udp-traffic-in-kubernetes-with-nginx/\nIngress日志追踪 在当今互联网架构中，Web应用防火墙（WAF）与Nginx(Ingress)作为前端代理的整合越来越常见，特别是在提升Web应用程序的安全性方面。这种结构不仅增强了对应用的保护，还提升了系统的整体性能。在这种架构中，理解WAF与Nginx(ingress)如何协作，以及如何实现日志追踪的整合尤为重要。\n用户访问流程日志分析 总结：阿里云的WAF日志更加详细，Nginx层面的日志也是WAF过来的（数据一样的）\n下方四个截图的数据日志，就是一条的请求的访问。数据都是一样的，各个层面都有日志记录。毫无疑问 WAF更加专业 ，图像化更多。 阿里云的WAF Nginx中转\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 server { listen 80; server_name xx.xxx.com; # IP白名单 include /usr/local/openresty/nginx/whitelist/corporation.conf; rewrite ^/(.*)$ https://$host/$1 permanent; } server { listen 443 ssl; server_name kpi.xxxx.com; #proxy_read_timeout 180; #proxy_send_timeout 180; # IP白名单 include /usr/local/openresty/nginx/whitelist/corporation.conf; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/xxx.com.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/xx.com.key; include ssl.conf; location / { proxy_pass http://kubernetes-cluster; include https_proxy.conf; } } 下方的地址为INgress的地址端口（可以看出，Nginx走https，Ingree走http）\n1 2 3 4 [root@nginx sites]# cat kubernetes-cluster.conf upstream kubernetes-cluster { server 172.18.xxx.xxx:80; } Ingree（中转）\nIngress\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@jenkins ops]# kubectl get ingresses -n prod new-fujfu-kpi-frontend -oyaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: new-fujfu-kpi-frontend namespace: prod resourceVersion: \u0026#34;3381713262\u0026#34; spec: rules: - host: kpi.xxx.com http: paths: - backend: serviceName: new-fujfu-kpi-frontend servicePort: 80 path: / SVC\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@jenkins ops]# kubectl get svc -n prod new-fujfu-kpi-frontend -oyaml apiVersion: v1 kind: Service metadata: labels: app: new-fujfu-kpi-frontend name: new-fujfu-kpi-frontend namespace: prod spec: clusterIP: 172.21.14.20 ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: new-fujfu-kpi-frontend sessionAffinity: None type: ClusterIP Pod\n1 2 [root@jenkins ops]# kubectl get pod -n prod --show-labels | grep \u0026#34;new-fujfu-kpi-frontend\u0026#34; new-fujfu-kpi-frontend.prod-8464c5df44-vsdcw 1/1 Running 0 185d app=new-fujfu-kpi-frontend,notfixedvalue=value-20231009171117,pod-template-hash=8464c5df44 整个Ingree访问流程\nIngree的日志\n1 kubectl logs -f nginx-ingress-controller-7795b48595-n2dcr -n kube-system |grep \u0026#34;kpi.xxxx.com\u0026#34; Nginx（Pod）\nNginx前端 也只是Pod\npod中的Nginx配置\n1 2 3 4 5 6 7 8 9 10 root@new-fujfu-kpi-frontend:/etc/nginx/conf.d# cat default.conf server { listen 80; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; location / { try_files $uri /index.html; } } 1 2 [root@jenkins ops]# kubectl get pod -n prod --show-labels | grep \u0026#34;new-fujfu-kpi-frontend\u0026#34; new-fujfu-kpi-frontend.prod-8464c5df44-vsdcw 1/1 Running 0 185d app=new-fujfu-kpi-frontend,notfixedvalue=value-20231009171117,pod-template-hash=8464c5df44 日志时间不对，因为pod中差个8个小时引起的 日志分析与可视化 利用自定义日志数据进行日志分析与可视化是提高安全性和响应能力的关键。通过集成SIEM工具、ELK栈（Elasticsearch、Logstash、Kibana）或其他日志分析平台，可以对日志数据进行实时监控、异常检测和攻击行为分析。这些工具能够帮助团队快速识别并响应安全威胁，同时提供直观的数据可视化，以便深入理解和改进安全策略。\n参考文档：\n从逻辑上深入理解Kubernetes中Ingress及Nginx Ingress Controller的概念及原理\nhttps://www.cnblogs.com/varden/p/15128802.html\nhttps://www.modb.pro/db/405641\nKubernetes安装ingress-nginx\n","date":"2024-04-11T18:30:08Z","image":"https://www.ownit.top/title_pic/73.jpg","permalink":"https://www.ownit.top/p/202404111830/","title":"Ingress配置优化和追踪"},{"content":"背景 Nginx是一款功能强大的Web服务器，对于网络环境中的日志记录和配置至关重要。定制化Nginx日志格式可以帮助管理员更好地监控服务器性能、分析用户行为并做出相应优化。在本文中，我们将深入探讨Nginx日志格式的高级定制化策略，包括理解基础日志结构、日志格式自定义实例、模块集成与扩展以及日志切割与管理。\nNginx的自定义日志（指标全面）\nNginx之间的日志追踪（分析整个流程）\nNginx自定义日志格式 1. 理解基础日志结构 log_format 有一个默认的无需设置的 combined 日志格式，相当于apache 的 combined 日志格式，如下所示：\n1 2 3 log_format combined \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#39; \u0026#39; \u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#39; \u0026#39; \u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; \u0026#39;; Nginx默认日志格式包含多个字段，每个字段都提供了有用的信息来帮助分析服务器行为。以下是一些常见字段及其含义：\n$remote_addr: 客户端的IP地址 $remote_user: 客户端用户的名称 $time_local: 访问时间与时区 $request: 完整的HTTP请求行，包括请求方法、URI和协议 $status: 服务器响应的HTTP状态码 $body_bytes_sent: 发送给客户端的字节数，不包括响应头的大小 $http_referer: 客户端发送的HTTP Referer头部信息 $http_user_agent: 客户端发送的User-Agent头部信息 这些字段的组合可以提供全面的请求和响应信息，有助于分析用户行为和服务器性能。\n如果 nginx 位于负载均衡器， squid， nginx 反向代理之后， web 服务器无法直接获取到客户端真实的 IP 地址了。 $remote_addr 获取反向代理的 IP 地址。反向代理服务器在转发请求的 http 头信息中，可以增加 X-ForwardedFor 信息，用来记录 客户端 IP 地址和客户端请求的服务器地址。 如下所示：\n1 2 3 log_format porxy \u0026#39;$http_x_forwarded_for - $remote_user [$time_local] \u0026#39; \u0026#39; \u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#39; \u0026#39; \u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; \u0026#39;; 2. 日志格式自定义实例 通过Nginx的access_log指令，管理员可以轻松自定义日志格式\n具体log_format的语法如下：\n1 log_format name [escape=default|json|none] string ...; 每一个日志格式都有一个名称，然后在配置access_log的时候可以指定用哪种格式来进行记录日志。log_format只能定义在http块中。查看在Nginx配置文件中的定义：\n1 2 3 4 5 6 7 8 9 10 11 http { #可以定义多个log_format，名称只需要不一样即可 log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; } server { access_log /var/log/nginx/access.log main; # 注意这里的配置 error_log /var/log/nginx/error.log; } 如上在http块中定义了一个名称为main的日志格式，在server块中通过名称对该日志格式进行了引用，这样子日志的格式都会按找main中定义的一些属性来进行记录访问信息。\n通过上边对日志格式和日志文件的一些了解，我们就可以非常容易的把日志的格式设置成JSON了，只需要定义一个log_format，把它的格式定义成一个json字符串，然后在打印日志的时候引用这个格式就可以实现了，下边是一个完成的定义文件:（可根据自己的需求定制）\n第一种 程序友好型json格式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 log_format json escape=json \u0026#39;{\u0026#39; \u0026#39;\u0026#34;time_iso8601\u0026#34;: \u0026#34;$time_iso8601\u0026#34;, \u0026#39; # local time in the ISO 8601 standard format \u0026#39;\u0026#34;request_time\u0026#34;: \u0026#34;$request_time\u0026#34;, \u0026#39; # request processing time in seconds with msec resolution \u0026#39;\u0026#34;status\u0026#34;: \u0026#34;$status\u0026#34;, \u0026#39; # response status code \u0026#39;\u0026#34;body_bytes_sent\u0026#34;: \u0026#34;$body_bytes_sent\u0026#34;, \u0026#39; # the number of body bytes exclude headers sent to a client \u0026#39;\u0026#34;bytes_sent\u0026#34;: \u0026#34;$bytes_sent\u0026#34;, \u0026#39; # the number of bytes sent to a client \u0026#39;\u0026#34;remote_addr\u0026#34;: \u0026#34;$remote_addr\u0026#34;, \u0026#39; # client IP \u0026#39;\u0026#34;remote_user\u0026#34;: \u0026#34;$remote_user\u0026#34;, \u0026#39; # client HTTP username \u0026#39;\u0026#34;remote_port\u0026#34;: \u0026#34;$remote_port\u0026#34;, \u0026#39; # client port \u0026#39;\u0026#34;request_uri\u0026#34;: \u0026#34;$request_uri\u0026#34;, \u0026#39; # full path and arguments if the request \u0026#39;\u0026#34;args\u0026#34;: \u0026#34;$args\u0026#34;, \u0026#39; # args \u0026#39;\u0026#34;request_id\u0026#34;: \u0026#34;$request_id\u0026#34;, \u0026#39; # the unique request id \u0026#39;\u0026#34;connection\u0026#34;: \u0026#34;$connection\u0026#34;, \u0026#39; # connection serial number \u0026#39;\u0026#34;connection_requests\u0026#34;: \u0026#34;$connection_requests\u0026#34;, \u0026#39; # number of requests made in connection \u0026#39;\u0026#34;pid\u0026#34;: \u0026#34;$pid\u0026#34;, \u0026#39; # process pid \u0026#39;\u0026#34;request_length\u0026#34;: \u0026#34;$request_length\u0026#34;, \u0026#39; # request length (including headers and body) \u0026#39;\u0026#34;time_local\u0026#34;: \u0026#34;$time_local\u0026#34;, \u0026#39; \u0026#39;\u0026#34;http_referer\u0026#34;: \u0026#34;$http_referer\u0026#34;, \u0026#39; # HTTP referer \u0026#39;\u0026#34;http_user_agent\u0026#34;: \u0026#34;$http_user_agent\u0026#34;, \u0026#39; # user agent \u0026#39;\u0026#34;http_x_forwarded_for\u0026#34;: \u0026#34;$http_x_forwarded_for\u0026#34;, \u0026#39; # http_x_forwarded_for \u0026#39;\u0026#34;http_host\u0026#34;: \u0026#34;$http_host\u0026#34;, \u0026#39; # the request Host: header \u0026#39;\u0026#34;server_name\u0026#34;: \u0026#34;$server_name\u0026#34;, \u0026#39; # the name of the vhost serving the request \u0026#39;\u0026#34;ssl_protocol\u0026#34;: \u0026#34;$ssl_protocol\u0026#34;, \u0026#39; # TLS protocol \u0026#39;\u0026#34;ssl_cipher\u0026#34;: \u0026#34;$ssl_cipher\u0026#34;, \u0026#39; # TLS cipher \u0026#39;\u0026#34;scheme\u0026#34;: \u0026#34;$scheme\u0026#34;, \u0026#39; # http or https \u0026#39;\u0026#34;request_method\u0026#34;: \u0026#34;$request_method\u0026#34;, \u0026#39; # request method \u0026#39;\u0026#34;server_protocol\u0026#34;: \u0026#34;$server_protocol\u0026#34;, \u0026#39; # request protocol, like HTTP/1.1 or HTTP/2.0 \u0026#39;\u0026#34;pipe\u0026#34;: \u0026#34;$pipe\u0026#34;, \u0026#39; # \u0026#34;p\u0026#34; if request was pipelined, \u0026#34;.\u0026#34; otherwise \u0026#39;\u0026#34;gzip_ratio\u0026#34;: \u0026#34;$gzip_ratio\u0026#34;, \u0026#39; \u0026#39;\u0026#34;http_cf_ray\u0026#34;: \u0026#34;$http_cf_ray\u0026#34;, \u0026#39; \u0026#39;\u0026#34;request_host\u0026#34;: \u0026#34;$host\u0026#34;, \u0026#39; \u0026#39;\u0026#34;is_completion\u0026#34;: \u0026#34;$request_completion\u0026#34;, \u0026#39; \u0026#39;\u0026#34;upstream\u0026#34;: \u0026#34;$upstream_addr\u0026#34;, \u0026#39; # upstream backend server for proxied requests \u0026#39;\u0026#34;upstream_name\u0026#34;: \u0026#34;$proxy_host\u0026#34;, \u0026#39; # upstream name \u0026#39;\u0026#34;upstream_status\u0026#34;: \u0026#34;$upstream_status\u0026#34;, \u0026#39; # upstream status \u0026#39;\u0026#34;upstream_bytes_sent\u0026#34;: \u0026#34;$upstream_bytes_sent\u0026#34;, \u0026#39; # upstream bytes sent \u0026#39;\u0026#34;upstream_bytes_received\u0026#34;: \u0026#34;$upstream_bytes_received\u0026#34;, \u0026#39; # upstream bytes received \u0026#39;\u0026#34;upstream_connect_time\u0026#34;: \u0026#34;$upstream_connect_time\u0026#34;, \u0026#39; # upstream handshake time incl. TLS \u0026#39;\u0026#34;upstream_header_time\u0026#34;: \u0026#34;$upstream_header_time\u0026#34;, \u0026#39; # time spent receiving upstream headers \u0026#39;\u0026#34;upstream_response_time\u0026#34;: \u0026#34;$upstream_response_time\u0026#34;, \u0026#39; # time spend receiving upstream body \u0026#39;\u0026#34;upstream_response_length\u0026#34;: \u0026#34;$upstream_response_length\u0026#34;, \u0026#39; # upstream response length \u0026#39;\u0026#34;upstream_cache_status\u0026#34;: \u0026#34;$upstream_cache_status\u0026#34;, \u0026#39; # cache HIT/MISS where applicable \u0026#39;\u0026#34;nginx_host\u0026#34;: \u0026#34;$hostname\u0026#34;\u0026#39; \u0026#39;}\u0026#39;; json格式对程序友好，配合ELK等日志采集、分析系统使用很方便，方便对日志进行深度分析。但是多数人平时比较喜欢直接查看日志文件，这时json格式的日志文件看起来就不够清晰了，日志较长，冗余信息较多，使用shell命令进行统计和分析也不方便。\n1 {\u0026#34;time_iso8601\u0026#34;: \u0026#34;2024-04-10T16:55:26+08:00\u0026#34;, \u0026#34;request_time\u0026#34;: \u0026#34;0.009\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;body_bytes_sent\u0026#34;: \u0026#34;397\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;916\u0026#34;, \u0026#34;remote_addr\u0026#34;: \u0026#34;192.168.96.19\u0026#34;, \u0026#34;remote_user\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;remote_port\u0026#34;: \u0026#34;8145\u0026#34;, \u0026#34;request_uri\u0026#34;: \u0026#34;/default_sso_heartbeat.html\u0026#34;, \u0026#34;args\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;c002e29f30c5e85a01dc1927991a9175\u0026#34;, \u0026#34;connection\u0026#34;: \u0026#34;759864\u0026#34;, \u0026#34;connection_requests\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;pid\u0026#34;: \u0026#34;8459\u0026#34;, \u0026#34;request_length\u0026#34;: \u0026#34;1017\u0026#34;, \u0026#34;time_local\u0026#34;: \u0026#34;10/Apr/2024:16:55:26 +0800\u0026#34;, \u0026#34;http_referer\u0026#34;: \u0026#34;https://apollo.ownit.top/default_sso_heartbeat.html\u0026#34;, \u0026#34;http_user_agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\u0026#34;, \u0026#34;http_x_forwarded_for\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;http_host\u0026#34;: \u0026#34;apollo.ownit.top\u0026#34;, \u0026#34;server_name\u0026#34;: \u0026#34;apollo.ownit.top\u0026#34;, \u0026#34;ssl_protocol\u0026#34;: \u0026#34;TLSv1.2\u0026#34;, \u0026#34;ssl_cipher\u0026#34;: \u0026#34;ECDHE-RSA-AES256-GCM-SHA384\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;request_method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;server_protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;pipe\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;gzip_ratio\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;http_cf_ray\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;request_host\u0026#34;: \u0026#34;apollo.ownit.top\u0026#34;, \u0026#34;is_completion\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;upstream\u0026#34;: \u0026#34;192.168.102.40:80\u0026#34;, \u0026#34;upstream_name\u0026#34;: \u0026#34;kubernetes-cluster\u0026#34;, \u0026#34;upstream_status\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;upstream_bytes_sent\u0026#34;: \u0026#34;1100\u0026#34;, \u0026#34;upstream_bytes_received\u0026#34;: \u0026#34;880\u0026#34;, \u0026#34;upstream_connect_time\u0026#34;: \u0026#34;0.000\u0026#34;, \u0026#34;upstream_header_time\u0026#34;: \u0026#34;0.009\u0026#34;, \u0026#34;upstream_response_time\u0026#34;: \u0026#34;0.009\u0026#34;, \u0026#34;upstream_response_length\u0026#34;: \u0026#34;397\u0026#34;, \u0026#34;upstream_cache_status\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;nginx_host\u0026#34;: \u0026#34;bt\u0026#34;} 第二种 运维友好型自定义格式：\n1 2 log_format main escape=json \u0026#39;$remote_addr |$ipdb_raw |[$time_local] |$host |$request |$status |BodySent:$body_bytes_sent |ReqTime:$request_time |$request_completion |$http_x_forwarded_for |$proxy_host |$upstream_addr |$upstream_status |$upstream_cache_status |UpResTime:$upstream_response_time |UpConnTime:$upstream_connect_time |UpResLen:$upstream_response_length |$hostname-$request_id |$scheme |$request_body |$http_referer |$http_user_agent |$http_cookie\u0026#39;; 这种格式使用“|”作为分隔符，日志打印也不是很长，方便使用AWK等命令进行统计\n需注意以下几点:\n$ipdb_raw这个变量是ipip.net ip库的nginx模块，如果未使用这个模块会报错，去掉或换成GEOIP即可。 escape=json 表示以json格式输出，高版本的Nginx已经支持json格式，对于第二种自定义格式，开启这个参数能在access log中打印中文，不会乱码 $http_cookie是打印所有cookies,当然也可以打印session,对于高安全要求的情况，日志中不会允许泄漏用户cookies。但是某些情况需调试或打印非敏感的cookies,可以打印指定cookies，如打印nginx session sticky生产的cookies,假设名字叫backend： \u0026lsquo;TRACE:$cookie_backend\u0026rsquo;。 $hostname-$request_id 如果有做全局调用链分析的需求，这个参数可以做为全局的UUID，nginx对每个请求都生成一个唯一的UUID，传给后端工程打印出来即可。 模块集成与扩展 通过集成第三方模块，如ngx_http_geoip_module和ngx_http_log_module，可以进一步丰富日志信息。ngx_http_geoip_module允许在日志中添加客户端的地理位置信息，例如国家、城市等，这对于分析用户的地理分布非常有用。ngx_http_log_module则提供了灵活的日志配置选项，包括基于变量的日志记录和条件日志记录，使日志记录更加精细和高效。\n请参考：\nhttps://blog.csdn.net/fishinhouse/article/details/88966258 https://blog.csdn.net/zzhongcy/article/details/86214969\n日志切割与管理 日志文件的定期滚动分割是日志管理中的一个重要方面。这可以通过设置定时任务（如使用cron）来实现，以定期执行日志切割操作，例如每天或每周切割一次日志文件。此外，还需要考虑日志的归档和安全存储，确保日志数据在保持可访问性的同时，不会因过时或安全问题而遭到破坏。使用如logrotate等工具可以帮助实现这些日志管理任务，保证日志数据的完整性和安全性。\n通过深入探索这些策略，可以实现对Nginx服务器日志记录的精细化配置，从而为应用程序的监控和分析提供强大的支持\n在Nginx的使用过程中，如果不对Nginx的日志做自定义的配置的话，那么默认的access.log日志默认就只有这么一个日志文件，随着系统使用时间越来越就，日志文件就会越来越大，而且默认的日志记录的格式也不方便分析。所以我们在实际使用Nginx的过程中需要对其日志做一些配置。 在http配置块中进行配置，也可以在每个server块中配置不同的access_log用以区分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 http{ # 定义一个变量来存储年月日 map $time_iso8601 $date { default \u0026#34;date-not-found\u0026#34;; \u0026#39;~^(?\u0026lt;ymd\u0026gt;\\d{4}-\\d{2}-\\d{2})\u0026#39; $ymd; } # 定义一个变量来存储时分秒 map $time_iso8601 $time { default \u0026#34;time-not-found\u0026#34;; \u0026#39;~T(?\u0026lt;hms\u0026gt;\\d{2}:\\d{2}:\\d{2})\u0026#39; $hms; } # 定义日志格式为json格式 log_format json_format \u0026#39;{ \u0026#34;timestamp\u0026#34;: \u0026#34;$date $time\u0026#34;, \u0026#34;remote_addr\u0026#34;: \u0026#34;$remote_addr\u0026#34;, \u0026#34;remote_user\u0026#34;: \u0026#34;$remote_user\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;$request\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;$status\u0026#34;, \u0026#34;body_bytes_sent\u0026#34;: \u0026#34;$body_bytes_sent\u0026#34;, \u0026#34;http_referer\u0026#34;: \u0026#34;$http_referer\u0026#34;, \u0026#34;http_user_agent\u0026#34;: \u0026#34;$http_user_agent\u0026#34;, \u0026#34;http_x_forwarded_for\u0026#34;: \u0026#34;$http_x_forwarded_for\u0026#34; }\u0026#39;; # 定义一个变量来存储年月,可用来作为access.log的日志文件名，按月自动分割日志 map $time_iso8601 $logmonth { \u0026#39;~^(?\u0026lt;ym\u0026gt;\\d{4}-\\d{2})\u0026#39; $ym; default \u0026#39;date-not-found\u0026#39;; } # 定义一个变量来存储年月日，可用来作为access.log的日志文件名，按天自动分割日志 map $time_iso8601 $logdate { \u0026#39;~^(?\u0026lt;ymd\u0026gt;\\d{4}-\\d{2}-\\d{2})\u0026#39; $ymd; default \u0026#39;date-not-found\u0026#39;; } # 开启access.log日志，可设置日志名变量(这里选择上面的按天)以便分割日志；可设置日志格式，以便更直观分析日志 access_log logs/access-$logdate.log json_format; } Nginx的日志常见配置项 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 $args #请求中的参数值 $query_string #同 $args $arg_NAME #GET请求中NAME的值 $is_args #如果请求中有参数，值为\u0026#34;?\u0026#34;，否则为空字符串 $uri #请求中的当前URI(不带请求参数，参数位于$args)，可以不同于浏览器传递的$request_uri的值，它可以通过内部重定向，或者使用index指令进行修改，$uri不包含主机名，如\u0026#34;/foo/bar.html\u0026#34;。 $document_uri #同 $uri $document_root #当前请求的文档根目录或别名 $host #优先级：HTTP请求行的主机名\u0026gt;\u0026#34;HOST\u0026#34;请求头字段\u0026gt;符合请求的服务器名.请求中的主机头字段，如果请求中的主机头不可用，则为服务器处理请求的服务器名称 $hostname #主机名 $https #如果开启了SSL安全模式，值为\u0026#34;on\u0026#34;，否则为空字符串。 $binary_remote_addr #客户端地址的二进制形式，固定长度为4个字节 $body_bytes_sent #传输给客户端的字节数，响应头不计算在内；这个变量和Apache的mod_log_config模块中的\u0026#34;%B\u0026#34;参数保持兼容 $bytes_sent #传输给客户端的字节数 $connection #TCP连接的序列号 $connection_requests #TCP连接当前的请求数量 $content_length #\u0026#34;Content-Length\u0026#34; 请求头字段 $content_type #\u0026#34;Content-Type\u0026#34; 请求头字段 $cookie_name #cookie名称 $limit_rate #用于设置响应的速度限制 $msec #当前的Unix时间戳 $nginx_version #nginx版本 $pid #工作进程的PID $pipe #如果请求来自管道通信，值为\u0026#34;p\u0026#34;，否则为\u0026#34;.\u0026#34; $proxy_protocol_addr #获取代理访问服务器的客户端地址，如果是直接访问，该值为空字符串 $realpath_root #当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径 $remote_addr #客户端地址 $remote_port #客户端端口 $remote_user #用于HTTP基础认证服务的用户名 $request #代表客户端的请求地址 $request_body #客户端的请求主体：此变量可在location中使用，将请求主体通过proxy_pass，fastcgi_pass，uwsgi_pass和scgi_pass传递给下一级的代理服务器 $request_body_file #将客户端请求主体保存在临时文件中。文件处理结束后，此文件需删除。如果需要之一开启此功能，需要设置client_body_in_file_only。如果将次文件传 递给后端的代理服务器，需要禁用request body，即设置proxy_pass_request_body off，fastcgi_pass_request_body off，uwsgi_pass_request_body off，or scgi_pass_request_body off $request_completion #如果请求成功，值为\u0026#34;OK\u0026#34;，如果请求未完成或者请求不是一个范围请求的最后一部分，则为空 $request_filename #当前连接请求的文件路径，由root或alias指令与URI请求生成 $request_length #请求的长度 (包括请求的地址，http请求头和请求主体) $request_method #HTTP请求方法，通常为\u0026#34;GET\u0026#34;或\u0026#34;POST\u0026#34; $request_time #处理客户端请求使用的时间,单位为秒，精度毫秒； 从读入客户端的第一个字节开始，直到把最后一个字符发送给客户端后进行日志写入为止。 $request_uri #这个变量等于包含一些客户端请求参数的原始URI，它无法修改，请查看$uri更改或重写URI，不包含主机名，例如：\u0026#34;/cnphp/test.php?arg=freemouse\u0026#34; $scheme #请求使用的Web协议，\u0026#34;http\u0026#34; 或 \u0026#34;https\u0026#34; $server_addr #服务器端地址，需要注意的是：为了避免访问linux系统内核，应将ip地址提前设置在配置文件中 $server_name #服务器名 $server_port #服务器端口 $server_protocol #服务器的HTTP版本，通常为 \u0026#34;HTTP/1.0\u0026#34; 或 \u0026#34;HTTP/1.1\u0026#34; $status #HTTP响应代码 $time_iso8601 #服务器时间的ISO 8610格式 $time_local #服务器时间（LOG Format 格式） $cookie_NAME #客户端请求Header头中的cookie变量，前缀\u0026#34;$cookie_\u0026#34;加上cookie名称的变量，该变量的值即为cookie名称的值 $http_NAME #匹配任意请求头字段；变量名中的后半部分NAME可以替换成任意请求头字段，如在配置文件中需要获取http请求头：\u0026#34;Accept-Language\u0026#34;，$http_accept_language即可 $http_cookie $http_host #请求地址，即浏览器中你输入的地址（IP或域名） $http_referer #url跳转来源,用来记录从那个页面链接访问过来的 $http_user_agent #用户终端浏览器等信息 $http_x_forwarded_for $sent_http_NAME #可以设置任意http响应头字段；变量名中的后半部分NAME可以替换成任意响应头字段，如需要设置响应头Content-length，$sent_http_content_length即可 $sent_http_cache_control $sent_http_connection $sent_http_content_type $sent_http_keep_alive $sent_http_last_modified $sent_http_location $sent_http_transfer_encoding Nginx之间的日志追踪 在当今互联网架构中，Web应用防火墙（WAF）与Nginx作为前端代理的整合越来越常见，特别是在提升Web应用程序的安全性方面。这种结构不仅增强了对应用的保护，还提升了系统的整体性能。在这种架构中，理解WAF与Nginx如何协作，以及如何实现日志追踪的整合尤为重要。\nWAF与Nginx协作原理 WAF通常作为Nginx的上游组件部署，拦截向Web应用程序发起的请求。它的主要作用是评估每个请求中的潜在威胁，如SQL注入、跨站脚本（XSS）等，然后根据预定义的规则集来决定是否允许请求通过。通过与Nginx的紧密协作，WAF能够在请求到达应用服务器之前提供一层额外的保护。\n在请求和响应过程中，WAF与Nginx之间的交互涉及到请求的接收、分析、处理和转发。当WAF检测到潜在威胁时，可以决定直接阻止该请求，或者将其转发给Nginx进行进一步的处理。\n日志集成挑战与解决方案 在将WAF与Nginx整合进一致性和可追溯性的日志系统时，面临的挑战主要包括跨组件的日志同步和关联。日志记录需要从WAF到Nginx，再到应用服务器，形成一个连续的链路，以确保对整个请求-响应周期的完全可见性。\n为了解决这一挑战，一种方法是在所有组件中使用统一的日志ID。通过在HTTP头部插入一个唯一的跟踪标识符，例如通过X-Request-ID，可以跨多个系统跟踪单个请求，从而简化日志数据的关联和分析。\n用户访问流程日志分析 总结：阿里云的WAF日志更加详细，Nginx层面的日志也是WAF过来的（数据一样的）\n下方三个截图的数据日志，就是一条的请求的访问。数据都是一样的，各个层面都有日志记录。毫无疑问 WAF更加专业 ，图像化更多。 阿里云的WAF Nginx中转\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 server { listen 80; server_name bd.xxx.com ; rewrite ^/(.*)$ https://$host/$1 permanent; } server { listen 443 ssl; server_name bd.xxx.com; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/xx.com.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/xxx.com.key; include ssl.conf; location / { proxy_pass http://172.18.199.115:8085; include https_proxy.conf; } } Nginx前端 前端机器：172.18.199.115\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 server { listen 8085; server_name bd.xxxx.com; root /opt/mfbd/dist; index index.html index.htm; location / { try_files $uri /index.html; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Headers Content-Type; add_header Access-Control-Allow-Methods GET,POST,OPTIONS; add_header Access-Control-Allow-Credentials true; } } 日志分析与可视化 利用自定义日志数据进行日志分析与可视化是提高安全性和响应能力的关键。通过集成SIEM工具、ELK栈（Elasticsearch、Logstash、Kibana）或其他日志分析平台，可以对日志数据进行实时监控、异常检测和攻击行为分析。这些工具能够帮助团队快速识别并响应安全威胁，同时提供直观的数据可视化，以便深入理解和改进安全策略。\n综上所述，WAF与Nginx前端代理的整合不仅加强了Web应用的安全层，而且通过有效的日志追踪和分析，为团队提供了关键的洞察，以持续改进安全姿态和响应能力。\n参考文档：\nhttps://www.cnblogs.com/ouym/p/15393191.html\nhttps://www.iminling.com/2023/10/04/272.html\nhttps://opswill.com/articles/nginx-custom-access-log-format.html\n","date":"2024-04-10T17:25:30Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202404101725/","title":"Nginx日志格式化和追踪"},{"content":"MongoDB和LVM快照概述 MongoDB的重要性：MongoDB支持的灵活的文档模型，使其成为处理大量分散数据的理想选择，特别是在需要快速迭代和频繁更改数据结构的应用中。\nLVM（逻辑卷管理）快照技术基本概念：LVM允许在不停止数据库服务的情况下，创建数据在某一时间点的快照。这意味着可以在不影响数据库性能和用户体验的情况下进行备份。\n应用方面 1、备份（恢复）的重要性：\n备份MongoDB数据对于确保数据安全和业务连续性至关重要。无论是因为硬件故障、软件故障、还是人为错误，数据丢失的后果都可能是灾难性的。定期备份可以最小化这些风险，确保在出现问题时可以快速恢复。\n2、测试和开发环境的创建\n在测试和开发过程中，需要创建一个与生产环境相同的数据库副本。通过使用快照，可以在测试和开发环境中快速创建一个具有相同数据和结构的数据库副本，以便进行有效的测试和开发工作。\n3、数据分析和报告生成\n某些情况下，需要对数据库进行离线分析和报告生成。通过使用快照，在不影响生产环境的情况下，可以在备份中创建一个副本，用于数据分析和报告生成，而不会对生产环境的性能产生负面影响。\nLVM快照的优势 使用LVM快照作为MongoDB备份策略的优点包括：\n快速备份：能够迅速捕捉到数据的瞬时状态，减少备份时间。 最小化停机时间：备份过程中不需要停止服务，这对于需要24/7运行的业务至关重要。 有效地恢复数据：可以精确到备份时的状态恢复数据，提高恢复效率。 环境准备 进行MongoDB备份前的环境准备包括：\n确保已安装LVM并正确配置存储。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sudo yum install lvm2 首先，识别可用的磁盘或分区（例如，/dev/sdb）并创建物理卷： sudo pvcreate /dev/sdb 展示效果：该命令应返回“Physical volume \u0026#34;/dev/sdb\u0026#34; successfully created”的信息。 接着，创建一个卷组，加入刚创建的物理卷： sudo vgcreate vg0 /dev/sdb 展示效果：返回“Volume group \u0026#34;vg0\u0026#34; successfully created”表明卷组创建成功。 步骤三：创建逻辑卷 最后，在卷组内创建逻辑卷用于MongoDB数据存储： sudo lvcreate -L 10G -n mongo_data vg0 展示效果：应返回“Logical volume \u0026#34;mongo_data\u0026#34; created”的消息。 MongoDB数据库运行在支持LVM的存储卷上。 1 2 3 4 5 6 7 8 9 10 11 12 13 配置MongoDB以使用逻辑卷 修改MongoDB的配置文件（通常为/etc/mongod.conf），指定数据存储路径到刚创建的逻辑卷上的某个目录： storage: dbPath: \u0026#34;/mnt/mongo_data\u0026#34; 然后，将MongoDB数据目录移动到新的位置，并修改目录权限： sudo mkdir /mnt/mongo_data sudo mount /dev/vg0/mongo_data /mnt/mongo_data sudo rsync -av /var/lib/mongo/ /mnt/mongo_data sudo chown -R mongodb:mongodb /mnt/mongo_data 验证系统的性能和存储空间，确保快照操作不会导致性能瓶颈或空间耗尽。 1 2 htop htop将显示CPU和内存的实时使用情况，用户需要确保在创建快照期间，这些资源的使用不会达到饱和 步骤详解 确认MongoDB数据库的存储卷 首先，使用lvdisplay命令来查找MongoDB数据存储在哪个逻辑卷上：\n1 lvdisplay 这个命令会列出系统中所有的逻辑卷。你需要找到与MongoDB数据相关联的逻辑卷信息，比如卷名（Volume Name）、卷组名（VG Name）等。\n创建快照卷 **注意：在执行的时候需要关闭数据库，以防止有数据继续写入（**​systemctl stop mongod​ ）\n接着，根据确认的MongoDB数据卷，使用lvcreate命令创建一个新的快照卷。例如，如果MongoDB数据存储在名为mongo_data的逻辑卷上，位于卷组vg0中，可以执行：\n1 sudo lvcreate --size 1G --snapshot --name mongo_backup /dev/vg0/mongo_data 这个命令创建了一个名为mongo_backup的快照卷，大小为1GB。\n挂载快照卷并备份 创建快照卷后，需要将其挂载到系统的一个目录上，以便访问和备份数据。首先，创建一个挂载点，然后挂载快照卷：\n1 2 sudo mkdir /mnt/mongo_backup sudo mount /dev/vg0/mongo_backup /mnt/mongo_backup 挂载完成后，使用标准备份工具（如tar或rsync）备份数据。例如，使用tar创建一个压缩的备份文件：\n1 sudo tar czf /path/to/backup/mongo_backup.tar.gz /mnt/mongo_backup 卸载快照卷并删除 备份完成后，不要忘记卸载快照卷并删除它，以释放空间：\n1 2 sudo umount /mnt/mongo_backup sudo lvremove /dev/vg0/mongo_backup 在删除快照卷时，系统会询问你是否确定删除，输入y确认。\n快照恢复流程 准备恢复环境 在恢复数据之前，确保目标位置有足够的空间接收从快照中恢复的数据。可以使用df -h命令检查磁盘空间，并根据需要清理或添加存储空间。\n使用快照数据恢复 在进行数据恢复之前，确保MongoDB服务已停止，以避免数据恢复过程中的冲突。使用以下命令停止MongoDB服务：\n1 sudo systemctl stop mongod 然后，将快照数据恢复到MongoDB的数据目录中。首先，确认快照挂载点和MongoDB的数据目录路径。假设快照挂载在/mnt/mongo_backup，MongoDB数据目录为/var/lib/mongo，可以使用rsync进行数据恢复：\n1 sudo rsync -av /mnt/mongo_backup/ /var/lib/mongo/ 确保使用结束斜杠/来指示同步目录内容而非目录本身。\n启动MongoDB服务 数据恢复完成后，重启MongoDB服务，并验证数据的一致性和完整性：\n1 sudo systemctl start mongod 重启服务后，检查MongoDB的日志文件，确保没有启动错误，并进行必要的数据一致性检查。使用MongoDB客户端或应用程序验证数据完整性和访问性。\nMongoDB，作为一个高性能、开源、无模式的文档数据库，因其高可伸缩性和灵活性而受到广泛欢迎。然而，正如任何技术专家所知，无论数据库的强大与否，数据备份和恢复计划的重要性都不可忽视。本文将探讨如何利用逻辑卷管理（LVM）快照技术来备份MongoDB数据库，确保数据的安全和业务的连续性。\n业务实战 （数据分析和报告生成：某些情况下，需要对数据库进行离线分析和报告生成。通过使用快照，在不影响生产环境的情况下，可以在备份中创建一个副本，用于数据分析和报告生成，而不会对生产环境的性能产生负面影响）。\n财务报表的准确性对于公司的及时决策至关重要。当前，报表直接在业务数据库上运行，不仅耗时长，而且在月初出报时常常失败，这对财务部门和公司决策产生了影响。随着公司业务规模的持续增长，现有的报表运行方式已不再适用。\n我们的业务数据每日凌晨从多个业务数据库同步到数据仓库中。数据同步的耗时长意味着，同步完成时的数据已不再是某一时刻的精准快照（因为业务数据库在不断更新）。\n需求：我们需要一个能将数据从MongoDB业务数据库精准同步到阿里云MaxCompute数据仓库的简单且可靠方案。简单性至关重要，因为它能提高系统的可靠性。理想的解决方案中应避免使用复杂的自编脚本来处理主从同步的细节，例如确定binlog的位置或时间点等技术栈，而是应尽可能使用封装良好的技术解决方案。数据的精确性非常关键，任何错误都可能导致不可逆的后果。\n思路：利用操作系统提供的LVM快照技术，可以在某一时刻将基于MongoDB的业务数据库数据精准同步到另一个MongoDB数据库中。这样，MaxCompute就可以从后者获取数据，进行同步任务。\n实践：以核心系统的MongoDB数据库为例，我们将展示如何将数据精准同步到数据仓库中，从而为财务报表的生成提供可靠的数据基础。 实践步骤 1. 准备工作 首先，确保已经在服务器上安装并配置了MongoDB，并且有足够的磁盘空间用于创建快照。\n2. 自动化脚本 为了实现这一过程的自动化，我们编写了一个MongoDB脚本和一个Shell脚本。MongoDB脚本用于锁定数据库，触发快照的创建，然后解锁数据库。Shell脚本负责实际创建LVM快照。\nMongoDB 脚本 (snapshot.js) 这个JavaScript脚本是为了在MongoDB数据库中创建一个数据快照，同时确保数据一致性，而设计的。这是通过在不完全停止数据库服务的情况下“锁定”数据库来实现的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 conn = new Mongo(); db = conn.getDB(\u0026#34;admin\u0026#34;); db.auth(\u0026#39;root\u0026#39;,\u0026#39;xxx\u0026#39;); db = db.getSiblingDB(\u0026#39;数据库名称\u0026#39;); session = db.getMongo().startSession(); session.startTransaction(); db.fsyncLock(); run(\u0026#39;/opt/mongodb_snap.sh\u0026#39;); db.fsyncUnlock(); session.commitTransaction(); session.endSession(); conn.close(); 建立连接和认证：\nconn = new Mongo(); 创建一个新的MongoDB连接。 db = conn.getDB(\u0026quot;admin\u0026quot;); 选择admin数据库进行操作。 db.auth('root','xxx'); 使用root账号和对应密码进行认证。 db = db.getSiblingDB('fubaodai'); 切换到目标数据库fubaodai，这是实际要快照的数据库。 事务和锁定：\nsession = db.getMongo().startSession(); 启动一个新的会话。 session.startTransaction(); 在该会话中开始一个新的事务。 db.fsyncLock(); 锁定数据库，以准备进行快照。这个操作会阻止写操作，但读操作仍然可以继续，从而保证了数据的一致性而不完全停止服务。 执行快照脚本：\nrun('/opt/mongodb_snap.sh'); 执行Shell脚本/opt/mongodb_snap.sh来创建一个LVM快照。注意，run函数在这个上下文中并不是MongoDB Shell的内置函数，这里假设它是为了说明目的而使用的。在实际MongoDB Shell脚本中，你可能需要通过其他方式触发快照脚本，比如通过MongoDB的系统命令执行或外部程序调度。 解锁和结束：\ndb.fsyncUnlock(); 解锁数据库，恢复正常的读写操作。 session.commitTransaction(); 提交事务，虽然这里的事务主要用于fsync锁定和解锁操作。 session.endSession(); 结束会话。 conn.close(); 关闭与MongoDB的连接。 通过上述步骤，这个脚本使得数据库能够在确保数据一致性的同时，进行LVM快照创建，而不需要停机或重启服务。这对于需要24/7运行的生产环境尤为重要。\nShell 脚本 (mongodb_snap.sh) 1 2 3 #!/bin/bash name=mongodb-$(date +%Y%m%d%H%M%S) /usr/sbin/lvcreate -s -L 100G -n $name /dev/mapper/data-mongodb 这些脚本通过Cron任务在每天零点自动执行，以确保数据同步的准确性和及时性。\n要通过cron在凌晨自动执行mongo --nodb mongodb_snap.js命令，你需要按照以下步骤操作：\n在终端中运行crontab -e命令。这将打开一个文本编辑器，允许你编辑当前用户的cron作业。\n1 0 1 * * * /usr/bin/mongo --nodb /path/to/mongodb_snap.js 在每天的1:00 AM，执行/usr/bin/mongo --nodb /path/to/mongodb_snap.js命令。请确保根据你的环境替换/usr/bin/mongo和/path/to/mongodb_snap.js为mongo命令和mongodb_snap.js脚本的实际路径。\n3. 数据同步 完成LVM快照后，我们可以将快照数据同步到另一个MongoDB数据库实例，该实例将作为数据仓库同步任务的数据源。这保证了数据仓库中的数据既准确又是最新的。\n总结 MongoDB数据库的备份和恢复是确保数据安全和业务连续性的重要环节。通过使用LVM快照作为备份策略，我们能够快速备份MongoDB数据，最小化停机时间，并以高效的方式恢复数据。这种备份方式的优点在于它提供了高级别的数据保护和可靠性，使得企业能够在数据风险和灾难发生时保持高度的安全性和稳定性。\n","date":"2024-04-08T16:16:38Z","image":"https://www.ownit.top/title_pic/66.jpg","permalink":"https://www.ownit.top/p/202404081616/","title":"MongoDB快照（LVM）业务场景应用实战"},{"content":"简介 在现代DevOps环境中，Nginx作为负载均衡器与Kubernetes的Ingress资源的结合，为应用程序提供了强大的路由和安全解决方案。本文将深入探讨如何利用Nginx的灵活性和功能，实现高效、安全的外部访问控制，以及如何配置Ingress以优化流量管理和SSL/TLS支持。\nNginx 可以与 Kubernetes 的 Ingress 资源配合使用，以提供高级的路由和负载均衡功能。Ingress 允许你通过定义规则来管理外部访问集群内服务的路径。当与 Nginx Ingress 控制器结合使用时，你可以利用 Nginx 的强大功能来处理 HTTP 和 HTTPS 流量，包括 SSL/TLS 终端、虚拟主机、重写和更多。 环境 Nginx服务：部署在公网，IP地址为172.1x.1x9.90，作为集群的入口点。 内网环境：基于Kubernetes的集群，使用Ingress资源管理服务间的通信。 Nginx构建步骤 公网Nginx为入口 在现代的网络架构中，使用 Nginx 作为反向代理服务器是一种常见的做法，它可以帮助我们将公网流量有效地转发到内网的应用程序服务器。\nhttp_proxy.conf 1 2 3 4 5 6 [root@monitor conf]# cat http_proxy.conf proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; proxy_set_header Connection \u0026#34;\u0026#34;; 这是一个配置文件，通常用于设置 HTTP 代理服务器的行为。下面是每一行的解释：\nproxy_set_header Host $host; 这一行设置代理服务器向目标服务器发送请求时，将使用原始请求中的 Host 头部。$host 是 Nginx 配置中的变量，代表请求行中的主机名。 proxy_set_header X-Real-IP $remote_addr; 这里配置代理服务器向目标服务器发送请求时，会添加一个 X-Real-IP 头部，其值为发起请求的客户端的 IP 地址。$remote_addr 是 Nginx 配置中的变量，代表客户端的 IP 地址。 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 这一行配置代理服务器在向目标服务器发送请求时，会添加一个 X-Forwarded-For 头部，用于表示发起请求的原始客户端的 IP 地址。$proxy_add_x_forwarded_for 是一个变量，它包含了原始请求中的 X-Forwarded-For 头部的值，如果有的话。 proxy_http_version 1.1;\n这里指定代理服务器使用 HTTP 版本 1.1 与目标服务器进行通信。这是因为 HTTP/1.1 支持持久连接，可以提高性能和效率。 proxy_set_header Connection \u0026quot;\u0026quot;;\n这一行设置代理服务器向目标服务器发送请求时，Connection 头部将不会被发送。这通常用于防止目标服务器关闭连接，特别是在使用 HTTP/1.1 或 HTTP/2 时，持久连接是有益的。 这些配置通常用于确保代理服务器正确地转发客户端的请求到目标服务器，并且目标服务器能够接收到正确的原始请求信息。\nhttps_proxy.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@monitor conf]# cat https_proxy.conf add_header Front-End-Https on; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Connection \u0026#34;\u0026#34;; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-HTTPS on; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 256k; proxy_buffers 4 256k; proxy_busy_buffers_size 256k; proxy_temp_file_write_size 256k; proxy_max_temp_file_size 8m; ssl.conf 1 2 3 4 5 6 7 8 9 10 11 12 [root@monitor conf]# cat ssl.conf ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers \u0026#34;ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4\u0026#34;; ssl_prefer_server_ciphers on; ssl_stapling on; ssl_stapling_verify off; resolver 223.5.5.5 223.6.6.6 valid=300s; resolver_timeout 10s; Server的conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 server { listen 80; server_name admin-uat.xxx.net; rewrite ^/(.*)$ https://$host/$1 permanent; #它执行了一个永久重定向策略，强制所有HTTP流量转向HTTPS，同时引用了公司白名单配置文件，以确保只有经过授权的用户或IP地址可以访问此服务。 include /usr/local/openresty/nginx/whitelist/corporation.conf; } server { listen 443 ssl; server_name admin-uat.xxx.net; include /usr/local/openresty/nginx/whitelist/corporation.conf; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/xxx.net.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/xxx.net.key; include ssl.conf; location / { proxy_pass http://kubernetes-cluster; include https_proxy.conf; } } 这个服务器块负责处理加密的HTTPS流量，配置了SSL证书和密钥以保证传输安全，并且在location上下文中设置了代理转发至名为kubernetes-cluster的上游服务器组。\n流量进行转发 1 2 3 4 5 [root@monitor vhosts]# cat kubernetes-cluster.conf upstream kubernetes-cluster { server 192.168.82.42 weight=5; keepalive 16; } 这里定义了一个上游服务器池，包含了一台内网IP地址为192.168.82.42的服务器，权重设置为5，意味着相对其他可能存在的服务器，此服务器将接收到更多比例的请求。同时，保持16个活动连接（keepalive），有助于减少建立新连接的开销，提高性能。\n内网Nginx default.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 server { listen 80 default_server; # 监听80端口，默认服务器配置 index index.html index.htm index.php; # 默认的索引文件 root /usr/share/nginx/html; # 根目录配置 location / { proxy_pass http://kubernetes-cluster; # 反向代理到kubernetes-cluster include proxy.conf; # 包含proxy.conf文件 } location ~ ^/Bdata_Nginx_Status$ { stub_status on; # 启用Nginx状态页面 allow 127.0.0.1; allow 10.0.0.0/8; allow 172.16.0.0/12; allow 192.168.0.0/16; deny all; # 拒绝其他IP访问 } location /Bdata_Check_Status { check_status; # 启用Nginx健康检查 allow 127.0.0.1; allow 10.0.0.0/8; allow 172.16.0.0/12; allow 192.168.0.0/16; deny all; # 拒绝其他IP访问 } access_log /var/log/nginx/access.log main; # 访问日志路径和格式 } server { listen 443 ssl default_server; # 监听443端口，默认服务器配置，启用SSL index index.html index.htm index.php; # 默认的索引文件 root /usr/share/nginx/html; # 根目录配置 ssl_certificate /etc/nginx/ssl/server.crt; # SSL证书路径 ssl_certificate_key /etc/nginx/ssl/server.key; # SSL证书私钥路径 location / { proxy_pass https://kubernetes-cluster-https; # 反向代理到kubernetes-cluster-https include proxy.conf; # 包含proxy.conf文件 } location ~ ^/Bdata_Nginx_Status$ { stub_status on; # 启用Nginx状态页面 allow 127.0.0.1; allow 10.0.0.0/8; allow 172.16.0.0/12; allow 192.168.0.0/16; deny all; # 拒绝其他IP访问 } location /Bdata_Check_Status { check_status; # 启用Nginx健康检查 allow 127.0.0.1; allow 10.0.0.0/8; allow 172.16.0.0/12; allow 192.168.0.0/16; deny all; # 拒绝其他IP访问 } access_log /var/log/nginx/access.log main; # 访问日志路径和格式 } upstream kubernetes-cluster-https.conf：\n1 2 3 4 5 upstream kubernetes-cluster-https { server 172.31.154.47:443 weight=5; # 后端服务的地址和端口，设置权重为5 check interval=10000 rise=3 fall=2 timeout=1500 type=tcp; # 配置健康检查参数 keepalive 16; # 配置 keepalive 连接数 } kubernetes-cluster.conf：\n1 2 3 4 5 upstream kubernetes-cluster { server 172.31.154.47 weight=5; # 后端服务的地址和端口，设置权重为5 check interval=10000 rise=3 fall=2 timeout=1500 type=tcp; # 配置健康检查参数 keepalive 16; # 配置 keepalive 连接数 } 这些配置定义了负载均衡的 upstream 组，其中 kubernetes-cluster-https 用于处理 HTTPS 流量，而 kubernetes-cluster 用于处理 HTTP 流量。每个 upstream 组只包含一个后端服务器（IP 地址为 172.31.154.47），并分配了权重为 5。此外，还配置了健康检查参数和 keepalive 连接数。\nIngress服务 上面的“172.31.154.47” 为 Ingress的IP\n1 2 3 [dev][root@kubernetes-master-192.168.83.13 ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 172.31.154.47 \u0026lt;none\u0026gt; 80:30274/TCP,443:30994/TCP 4y22d 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [dev][root@kubernetes-master-192.168.83.13 ~]# kubectl get ingresses -n uat hire-admin NAME HOSTS ADDRESS PORTS AGE hire-admin hire-admin-uat.xxx.net 172.31.154.47 80 2y4d [dev][root@kubernetes-master-192.168.83.13 ~]# kubectl get ingresses -n uat hire-admin -oyaml apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx # 使用 nginx 作为 Ingress 控制器 name: hire-admin namespace: uat selfLink: /apis/extensions/v1beta1/namespaces/uat/ingresses/hire-admin spec: rules: - host: hire-admin-uat.xxx.net # Ingress 规则中的主机名 http: paths: - backend: serviceName: hire-admin # 后端服务的名称 servicePort: 8503 # 后端服务的端口 path: / # Ingress 路径 status: loadBalancer: ingress: - ip: 172.31.154.47 # 负载均衡器的 IP 地址 后端服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [dev][root@kubernetes-master-192.168.83.13 ~]# kubectl get svc -n uat hire-admin NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hire-admin ClusterIP 172.31.183.155 \u0026lt;none\u0026gt; 8503/TCP 2y4d [dev][root@kubernetes-master-192.168.83.13 ~]# kubectl get svc -n uat hire-admin -oyaml apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;hire-admin\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;hire-admin\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;uat\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;ports\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;port\u0026#34;:8503,\u0026#34;protocol\u0026#34;:\u0026#34;TCP\u0026#34;,\u0026#34;targetPort\u0026#34;:8503}],\u0026#34;selector\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;hire-admin\u0026#34;}}} creationTimestamp: \u0026#34;2022-03-30T02:37:57Z\u0026#34; labels: app: hire-admin name: hire-admin namespace: uat resourceVersion: \u0026#34;260185009\u0026#34; selfLink: /api/v1/namespaces/uat/services/hire-admin uid: dc559c3a-abfb-4f70-927d-75ac5227b433 spec: clusterIP: 172.31.183.155 # Service 的 ClusterIP ports: - name: http port: 8503 protocol: TCP targetPort: 8503 # Service 指向的目标端口 selector: app: hire-admin sessionAffinity: None type: ClusterIP status: loadBalancer: {} # 没有负载均衡器相关的状态信息 总结来说，通过深度整合Nginx与Kubernetes Ingress资源，我们可以实现高度定制化的流量路由和负载均衡策略，不仅保障了服务的安全性和高可用性，同时也极大地提升了内外网交互的灵活性与响应速度。\n","date":"2024-04-02T16:13:11Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202404021613/","title":"Nginx在Kubernetes集群中的进阶应用"},{"content":"背景 在现代软件开发实践中，持续集成（Continuous Integration, CI）是确保代码质量和快速响应软件缺陷的关键策略。GitLab 提供了强大的 CI/CD 功能，允许开发者自动化测试和部署流程。本文将介绍如何设置 GitLab 流水线计划任务，以实现对 dev 分支每小时的自动测试。\n需求： GitLab 流水线计划任务（pipeline-schedules）：实现每小时自动测试 dev 分支的更新\n环境 Gitlab企业版本：v16.9.2-ee GitLab 流水线是一系列作业（jobs）的集合，这些作业可以并行或顺序执行，以完成构建、测试、部署等任务。流水线可以由代码提交、定时计划或其他事件触发。\n计划任务是 GitLab 流水线的一个特性，它允许你按计划执行流水线，而不需要代码提交或其他触发条件。这对于定期运行测试、备份或其他维护任务特别有用。\n每小时自动测试 dev 分支的步骤 创建 .gitlab-ci.yml 配置文件 在项目的根目录下，创建或编辑 .gitlab-ci.yml 文件，这是 GitLab 流水线的配置文件。我们将定义两个作业：一个是常规的测试作业，另一个是计划任务作业。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 stages: - test - alert - jar before_script: - export PATH=/usr/lib/jvm/java-21/bin:${PATH} # 将java路径添加到PATH路径中。如果是java 11，使用/usr/lib/jvm/java-11/bin。 ci_test: stage: test script: - | if [ \u0026#34;$(git log origin/dev --since=\u0026#39;1 hour ago\u0026#39; --pretty=format:\u0026#39;%h\u0026#39;)\u0026#34; != \u0026#34;\u0026#34; ]; then mvn clean test else echo \u0026#34;No changes in the last hour\u0026#34; fi - export rules: - if: \u0026#39;$CI_PIPELINE_SOURCE == \u0026#34;schedule\u0026#34; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026#34;dev\u0026#34;\u0026#39; dingTalk_fail_alert: stage: alert script: - /bin/bash /opt/k8s/dingtalk/ding_ci_alert.sh fail when: on_failure allow_failure: true rules: - if: \u0026#39;$CI_PIPELINE_SOURCE == \u0026#34;schedule\u0026#34; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026#34;dev\u0026#34;\u0026#39; dingTalk_succes_alert: stage: alert script: - /bin/bash /opt/k8s/dingtalk/ding_ci_alert.sh success when: on_success allow_failure: true rules: - if: \u0026#39;$CI_PIPELINE_SOURCE == \u0026#34;schedule\u0026#34; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026#34;dev\u0026#34;\u0026#39; jar: stage: jar tags: - pdd cache: paths: - manage-analysis/target/*.jar - manage-listener/target/*.jar - manage-backend/target/*.jar policy: push key: ${CI_BUILD_REF_NAME} script: - mvn clean install -Dmaven.test.skip=true rules: - if: \u0026#39;$CI_PIPELINE_SOURCE != \u0026#34;schedule\u0026#34; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026#34;dev\u0026#34;\u0026#39; 配置钉钉告警机器人 配置钉钉告警机器人：关键字认证 “告警” 配置告警脚本脚本模板 在GitLab CI/CD 管道中，我们设计了一个名为 ding_ci_alert.sh 的Shell脚本，用于对接钉钉机器人发送构建状态的通知。针对不同的构建结果，我们有如下需求：\n当Maven测试任务失败时，我们会向脚本传递一个参数 \u0026quot;fail\u0026quot;，表示构建失败。此时，脚本不仅会调用钉钉机器人发送一条包含失败详情的告警消息，还会将此次失败的构建ID写入一个专门记录失败构建的文件中。 当后续的Maven测试任务成功时，我们将向脚本传递一个参数 \u0026quot;success\u0026quot;，以标识构建成功。这时，脚本首先会检查记录失败构建的文件中是否存在当前构建ID。如果存在，则调用钉钉机器人发送一条成功恢复的消息，并从记录文件中移除此构建ID；反之，若当前构建ID未在文件中找到，则不发送成功告警信息。 这样，我们通过灵活调用和智能判断，确保了在构建失败时及时通知相关人员，并在问题得到解决后发送成功通知，既避免了不必要的通知干扰，又保证了团队能够及时了解到构建状态的变化。在实际的博客文章中，您可以插入示例代码和详细说明，让读者更直观地理解这一过程。\nvim /opt/k8s/dingtalk/ding_ci_alert.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #!/bin/bash WEBHOOK_URL=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=xxxxx\u0026#34; # 检查是否提供了参数 if [ \u0026#34;$#\u0026#34; -ne 1 ]; then echo \u0026#34;Usage: $0 \u0026lt;success|fail\u0026gt;\u0026#34; exit 1 fi # 定义记录文件路径 RECORD_FILE=\u0026#34;/tmp/build_records.txt\u0026#34; # 根据传入的参数执行相应操作 case $1 in fail) # 构建失败，发送告警并记录 echo \u0026#34;构建失败，发送告警...\u0026#34; # 准备告警消息 JSON_DATA=$(printf \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;告警:构建失败通知\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;#### **[%s 的 Maven 测试未通过构建失败](%s/pipelines/%s)**\\n\\n\u0026gt; - **提交信息**: %s\\n\u0026gt; - **提交者名称**: %s\\n\u0026gt; - **提交时间**: %s\\n\\n请及时处理，更多详情可点击项目名称。\u0026#34; }, \u0026#34;at\u0026#34;: { \u0026#34;isAtAll\u0026#34;: false } }\u0026#39; \u0026#34;$CI_PROJECT_NAME\u0026#34; \u0026#34;$CI_PROJECT_URL\u0026#34; \u0026#34;$CI_PIPELINE_ID\u0026#34; \u0026#34;$CI_COMMIT_MESSAGE\u0026#34; \u0026#34;$CI_COMMIT_AUTHOR\u0026#34; \u0026#34;$CI_COMMIT_TIMESTAMP\u0026#34;) # 发送告警 curl \u0026#34;$WEBHOOK_URL\u0026#34; -X POST -H \u0026#39;Content-Type: application/json;charset=utf-8\u0026#39; -d \u0026#34;$JSON_DATA\u0026#34; # 记录失败的构建 echo \u0026#34;$CI_PROJECT_NAME failed\u0026#34; \u0026gt;\u0026gt; \u0026#34;$RECORD_FILE\u0026#34; ;; success) # 构建成功，检查记录文件 if grep -q \u0026#34;$CI_PROJECT_NAME failed\u0026#34; \u0026#34;$RECORD_FILE\u0026#34;; then echo \u0026#34;构建成功，发送告警...\u0026#34; # 准备成功告警消息 SUCCESS_JSON_DATA=$(printf \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;告警:构建成功通知\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;#### **[%s 的 Maven 测试已通过构建成功]**\\n\\n\u0026gt; - **提交信息**: %s\\n\u0026gt; - **提交者名称**: %s\\n\u0026gt; - **提交时间**: %s\\n\\n构建成功，可以进行下一步操作。\u0026#34; }, \u0026#34;at\u0026#34;: { \u0026#34;isAtAll\u0026#34;: false } }\u0026#39; \u0026#34;$CI_PROJECT_NAME\u0026#34; \u0026#34;$CI_COMMIT_MESSAGE\u0026#34; \u0026#34;$CI_COMMIT_AUTHOR\u0026#34; \u0026#34;$CI_COMMIT_TIMESTAMP\u0026#34;) # 发送成功告警 curl \u0026#34;$WEBHOOK_URL\u0026#34; -X POST -H \u0026#39;Content-Type: application/json;charset=utf-8\u0026#39; -d \u0026#34;$SUCCESS_JSON_DATA\u0026#34; # 清除记录 sed -i \u0026#34;/$CI_PROJECT_NAME failed/d\u0026#34; \u0026#34;$RECORD_FILE\u0026#34; else echo \u0026#34;没有找到构建失败的记录，不发送成功告警。\u0026#34; fi ;; *) echo \u0026#34;Invalid argument: $1. Expected \u0026#39;success\u0026#39; or \u0026#39;fail\u0026#39;.\u0026#34; exit 1 ;; esac exit 0 设置流水线计划 步骤1：登录GitLab并进入项目\n登录到您的GitLab账号。 导航到您想要设置流水线计划的项目。 步骤2：进入CI/CD设置\n在项目主页，点击左侧面板中的“CI/CD”选项卡。 在CI/CD设置区域，点击“Schedules”（调度）。 步骤3：创建新的流水线计划\n在“Schedules”页面，点击“New schedule”按钮以创建一个新的流水线计划。 步骤4：配置流水线计划详细信息\n描述：为您的流水线计划提供一个易于理解的描述，以便日后识别。 CRON 表达式：输入一个cron表达式来定义计划执行的时间规律。例如，每天凌晨1点执行，可以输入0 1 * * *。请参考cron表达式文档以获取更多信息。 分支/标签：可以选择流水线将在哪个分支或标签上执行，默认通常是当前项目的默认分支（如master/main）。 额外变量（可选）：如果需要，您可以为流水线提供额外的环境变量，这将在计划执行时注入到环境中。 保护（可选）：如有必要，您可以勾选“Protect this pipeline”，这样只有具有适当权限的用户才能执行或修改该流水线计划。 步骤5：保存流水线计划\n配置完所有选项后，滚动到底部并点击“Save changes”。 步骤6：验证流水线计划\n在指定的时间，GitLab将根据您设置的cron表达式自动触发流水线。您可以在项目的历史流水线记录中查看流水线计划的执行情况。 注意事项\n确保您的项目中有.gitlab-ci.yml文件，并且文件中定义了适当的CI/CD流水线配置，这样才能在计划时间到来时执行预定的任务。 如果流水线涉及到敏感信息，您可能需要预先配置好变量并通过密钥管理或GitLab的受保护变量功能来安全地传递这些信息。 某些高级设置，如运行特定job，需要在.gitlab-ci.yml文件中通过rules或only/except关键字进行定义。 注意：第二张图片 CI_PIPELINE_SOURCE 是自己定义的变量\n额 外 变 量 （ 可 选 ） ： 如 果 需 要 ， 您 可 以 为 流 水 线 提 供 额 外 的 环 境 变 量 ， 这 将 在 计 划 执 行 时 注 入 到 环 境 中 ， 一 般 我 们 会 配 置 一 个 变 量 名 字 叫 $CI PIPELINE_SOURCE ， 它 的 值 为 s c h ed u le 通 过 这 个 方 式 ， 我 们 就 能 够 确 保 .gitlab-ci.yml 文 件 里 面 只 有 定 义 了 以 下 规 则 的 stage 才 会 执 行 这 个 定 时 任 务 ：\n1 2 rules: - if: \u0026#39;$CI_PIPELINE_SOURCE == \u0026#34;schedule\u0026#34; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026#34;dev\u0026#34;\u0026#39; 保 护 （ 可 选 ） ： 如 有 必 要 ， 您 可 以 勾 选 \u0026ldquo;Protect this pipeline\u0026rdquo;, 这 样 只 有 具 有 适 当 权 限 的 用 户 才 能 执 行 或 修 改 该 流 水 线 计 划 。 验证流水线自动化 ","date":"2024-03-22T16:32:28Z","image":"https://www.ownit.top/title_pic/64.jpg","permalink":"https://www.ownit.top/p/202403221632/","title":"Gitlab的流水线任务【实现每小时自动测试 dev分支的更新】"},{"content":"推荐 Istio 多集群监控使用 Prometheus，其主要原因是基于 Prometheus 的分层联邦（Hierarchical Federation）。\n通过 Istio 部署到每个集群中的 Prometheus 实例作为初始收集器，然后将数据聚合到网格层次的 Prometheus 实例上。 网格层次的 Prometheus 既可以部署在网格之外（外部），也可以部署在网格内的集群中。\n使用 Istio 以及 Prometheus 进行生产规模的监控时推荐的方式是使用分层联邦并且结合一组记录规则。\n尽管安装 Istio 不会默认部署 Prometheus，入门指导中 Option 1: Quick Start 的部署按照 Prometheus 集成指导安装了 Prometheus。 此 Prometheus 部署刻意地配置了很短的保留窗口（6 小时）。此快速入门 Prometheus 部署同时也配置为从网格上运行的每一个 Envoy 代理上收集指标，同时通过一组有关它们的源的标签（instance、pod 和 namespace）来扩充指标 使用负载级别的聚合指标进行联邦 为了建立 Prometheus 联邦，请修改您的 Prometheus 生产部署配置来抓取 Istio Prometheus 联邦终端的指标数据。\n将以下的 Job 添加到配置中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 - job_name: \u0026#39;istio-prometheus\u0026#39; honor_labels: true metrics_path: \u0026#39;/federate\u0026#39; kubernetes_sd_configs: - role: pod namespaces: names: [\u0026#39;istio-system\u0026#39;] metric_relabel_configs: - source_labels: [__name__] regex: \u0026#39;workload:(.*)\u0026#39; target_label: __name__ action: replace params: \u0026#39;match[]\u0026#39;: - \u0026#39;{__name__=~\u0026#34;workload:(.*)\u0026#34;}\u0026#39; - \u0026#39;{__name__=~\u0026#34;pilot(.*)\u0026#34;}\u0026#39; 如果您使用的是 Prometheus Operator，请使用以下的配置：\n我这边就是使用 这种方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: istio-federation labels: app.kubernetes.io/name: istio-prometheus spec: namespaceSelector: matchNames: - istio-system selector: matchLabels: app: prometheus endpoints: - interval: 30s scrapeTimeout: 30s params: \u0026#39;match[]\u0026#39;: - \u0026#39;{__name__=~\u0026#34;workload:(.*)\u0026#34;}\u0026#39; - \u0026#39;{__name__=~\u0026#34;pilot(.*)\u0026#34;}\u0026#39; path: /federate targetPort: 9090 honorLabels: true metricRelabelings: - sourceLabels: [\u0026#34;__name__\u0026#34;] regex: \u0026#39;workload:(.*)\u0026#39; targetLabel: \u0026#34;__name__\u0026#34; action: replace 1 2 3 4 5 6 kubectl apply -f istio-federation.yaml -n monitoring [root@bt ~]# kubectl get servicemonitors.monitoring.coreos.com -n monitoring istio-federation NAME AGE istio-federation 44d 联邦配置的关键是首先匹配通过 Istio 部署的 Prometheus 中收集 Istio 标准指标的 Job。并且将收集到的指标重命名，方法为去除负载等级记录规则命名前缀 (workload:)。 这使得现有的仪表盘以及引用能够无缝地针对生产用 Prometheus 继续工作（并且不在指向 Istio 实例）。\n您可以在设置联邦时包含额外的指标（例如 envoy、go 等）。\n控制面指标也被生产用 Prometheus 收集并联邦。\n配置文件监控kubernetes-istio-pods Prometheus 的抓取作业配置文件，怎么实现 Prometheus 如何抓取和处理与 Kubernetes Istio pods 相关的指标数据？\n1、首先Istio内部集成安装Prometheus\n2、我们可以去istio查看 人家的配置怎么写\n3、根据人家的配置 ，修改一下，集成到Prometheus上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 global: evaluation_interval: 1m scrape_interval: 15s scrape_timeout: 10s rule_files: - /etc/config/recording_rules.yml - /etc/config/alerting_rules.yml - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token job_name: kubernetes-apiservers kubernetes_sd_configs: - role: endpoints relabel_configs: - action: keep regex: default;kubernetes;https source_labels: - __meta_kubernetes_namespace - __meta_kubernetes_service_name - __meta_kubernetes_endpoint_port_name scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token job_name: kubernetes-nodes kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - replacement: kubernetes.default.svc:443 target_label: __address__ - regex: (.+) replacement: /api/v1/nodes/$1/proxy/metrics source_labels: - __meta_kubernetes_node_name target_label: __metrics_path__ scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token job_name: kubernetes-nodes-cadvisor kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - replacement: kubernetes.default.svc:443 target_label: __address__ - regex: (.+) replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor source_labels: - __meta_kubernetes_node_name target_label: __metrics_path__ scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true - honor_labels: true job_name: kubernetes-service-endpoints kubernetes_sd_configs: - role: endpoints relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scrape - action: drop regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow - action: replace regex: (https?) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: (.+?)(?::\\d+)?;(\\d+) replacement: $1:$2 source_labels: - __address__ - __meta_kubernetes_service_annotation_prometheus_io_port target_label: __address__ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_service_name target_label: service - action: replace source_labels: - __meta_kubernetes_pod_node_name target_label: node - honor_labels: true job_name: kubernetes-service-endpoints-slow kubernetes_sd_configs: - role: endpoints relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow - action: replace regex: (https?) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: (.+?)(?::\\d+)?;(\\d+) replacement: $1:$2 source_labels: - __address__ - __meta_kubernetes_service_annotation_prometheus_io_port target_label: __address__ - action: labelmap regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_service_name target_label: service - action: replace source_labels: - __meta_kubernetes_pod_node_name target_label: node scrape_interval: 5m scrape_timeout: 30s - honor_labels: true job_name: prometheus-pushgateway kubernetes_sd_configs: - role: service relabel_configs: - action: keep regex: pushgateway source_labels: - __meta_kubernetes_service_annotation_prometheus_io_probe - honor_labels: true job_name: kubernetes-services kubernetes_sd_configs: - role: service metrics_path: /probe params: module: - http_2xx relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_probe - source_labels: - __address__ target_label: __param_target - replacement: blackbox target_label: __address__ - source_labels: - __param_target target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_service_name target_label: service - honor_labels: true job_name: kubernetes-pods kubernetes_sd_configs: - role: pod relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scrape - action: drop regex: true source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow - action: replace regex: (https?) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4}) replacement: \u0026#39;[$2]:$1\u0026#39; source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_port - __meta_kubernetes_pod_ip target_label: __address__ - action: replace regex: (\\d+);((([0-9]+?)(\\.|$)){4}) replacement: $2:$1 source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_port - __meta_kubernetes_pod_ip target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: drop regex: Pending|Succeeded|Failed|Completed source_labels: - __meta_kubernetes_pod_phase - honor_labels: true job_name: kubernetes-pods-slow kubernetes_sd_configs: - role: pod relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow - action: replace regex: (https?) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4}) replacement: \u0026#39;[$2]:$1\u0026#39; source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_port - __meta_kubernetes_pod_ip target_label: __address__ - action: replace regex: (\\d+);((([0-9]+?)(\\.|$)){4}) replacement: $2:$1 source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_port - __meta_kubernetes_pod_ip target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+) replacement: __param_$1 - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: drop regex: Pending|Succeeded|Failed|Completed source_labels: - __meta_kubernetes_pod_phase scrape_interval: 5m scrape_timeout: 30s 上面是人家的写的配置，修改配置为下方\n在prometheus-prometheus.yaml的配置文件中，官方内置了一个字段additionalScrapeConfigs用于添加自定义的抓取目标\nhttps://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#PrometheusSpec\n所以只需要按照官方的要求写入配置即可。新建额外抓取的信息prometheus-additional.yaml：\nprometheus-additional.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 - job_name: kubernetes-istio-pods honor_labels: true honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /metrics scheme: http follow_redirects: true relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: keep - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow] separator: ; regex: \u0026#34;true\u0026#34; replacement: $1 action: drop - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme] separator: ; regex: (https?) target_label: __scheme__ replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] separator: ; regex: (.+) target_label: __metrics_path__ replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip] separator: ; regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4}) target_label: __address__ replacement: \u0026#39;[$2]:$1\u0026#39; action: replace - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip] separator: ; regex: (\\d+);((([0-9]+?)(\\.|$)){4}) target_label: __address__ replacement: $2:$1 action: replace - separator: ; regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+) replacement: __param_$1 action: labelmap - separator: ; regex: __meta_kubernetes_pod_label_(.+) replacement: $1 action: labelmap - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_phase] separator: ; regex: Pending|Succeeded|Failed|Completed replacement: $1 action: drop kubernetes_sd_configs: - role: pod kubeconfig_file: \u0026#34;\u0026#34; follow_redirects: true 抓取的目标制定为targets列表，创建secret对象istio-additional-configs引用该配置\n1 kubectl create secret generic `istio-additional-configs` --from-file=./prometheus-additional.yaml -n monitoring 1 2 3 4 5 6 kubectl edit prometheus -n monitoring spec: additionalScrapeConfigs: key: prometheus-additional.yaml name: istio-additional-configs 或者\n在promethes-prometheus.yaml中添加additionalScrapeConfigs引用创建的secret\n1 2 3 4 5 6 7 serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: {} serviceMonitorSelector: {} version: v2.11.0 additionalScrapeConfigs: name: ingress-nginx-additional-configs key: prometheus-additional.yaml 重建promethes配置:\n1 $ kubectl delete -f ./prometheus-prometheus.yaml \u0026amp;\u0026amp; kubectl apply -f ./prometheus-prometheus.yaml Grafan监控 关于Grafan的监控面板，可以去官方进行寻找\nhttps://grafana.com/grafana/dashboards/\nIstio也提供了自身平台的监控大盘，如下： 可以看出Istio的默认监控大盘非常全面，该监控的都监控起来了 参考文档：https://www.cnblogs.com/justmine/p/12269587.html\n","date":"2024-03-04T11:44:38Z","image":"https://www.ownit.top/title_pic/51.jpg","permalink":"https://www.ownit.top/p/202403041144/","title":"Kube-Prometheus 监控Istio"},{"content":"Ansible批量主机机器到Jumpserver 1、背景 在现代 IT 环境中，随着机器数量的增加和复杂性的提高，手动管理和配置机器变得越来越困难和耗时。为了提高效率并确保一致性，自动化工具成为了不可或缺的一部分。Jumpserver 是一个功能强大的堡垒机和服务器管理平台，可以帮助管理员更好地管理和控制远程机器。\n最近，我们面临着一个挑战：需要将一批新的机器（约 K 台）导入到 Jumpserver 中，并按照预定义的分组进行组织。手动逐个导入这些机器将是一项繁琐且容易出错的任务。为了解决这个问题，我们决定使用自动化脚本来批量导入机器到 Jumpserver，并根据分组进行组织。\n2、环境准备 Ansible\n1 2 3 4 5 6 7 root@wq-1:~/file# ansible --version ansible 2.10.8 config file = /etc/ansible/ansible.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3/dist-packages/ansible executable location = /usr/bin/ansible python version = 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] Jumpserver 版本（3.8.1）\n‍\n3、操作流程 jumpserverAPI 文档 ⚓︎ https://docs.jumpserver.org/zh/master/dev/rest_api/\n‍\n1、获取 Jumpserver 的 token 需要替换 网站地址 和 用户 账号 和 密码。\n（注意）：Jumpserver 的版本不一样，接口文档的参数也会变得，请依照自己的版本接口调试。\n1 curl -X POST https://jms.网站.top/api/v1/authentication/auth/ -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxx\u0026#34;}\u0026#39; ​\n2、创建所有机器的模板账号 所有的机器可以统一使用同一 账号来管理。账号可以使用 ，密码或秘钥。根据自己情况来操作，我们这边需要这个模板账号的 id 进行绑定，这样就可以在 Jumpserver 中远程访问机器。\n步骤 1 ​\n步骤 2 ​​\n步骤 3 ​​\n我们需要这个 id 号。\n3、自动创建分组和机器脚本 add_jms_hosts.sh （会参入三个参数 分组名称 IP 地址 主机名称） 注意：这个脚本必选 配合 jq 。请先安装完成使用\n1 yum install -y jq 主机名称作为 id 来的，所以值必选唯一，请注意\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #!/bin/bash # JumpServer API 相关信息 JUMPSERVER_URL=\u0026#34;http://10.1.10.3/api/v1\u0026#34; JUMPSERVER_TOKEN=\u0026#34;8j85rhlicQ8Mjxxxxx\u0026#34; # 传入的参数 GROUP_NAME=\u0026#34;$1\u0026#34; IP=\u0026#34;$2\u0026#34; HOST_NAME=\u0026#34;$3\u0026#34; # 检查分组是否存在，不存在则创建 check_and_create_group() { echo \u0026#34;检查分组是否存在...\u0026#34; group_id=$(curl -s -H \u0026#34;Authorization: Bearer $JUMPSERVER_TOKEN\u0026#34; \u0026#34;$JUMPSERVER_URL/assets/nodes/\u0026#34; | jq -r --arg GROUP_NAME \u0026#34;$GROUP_NAME\u0026#34; \u0026#39;.[] | select(.name==$GROUP_NAME) | .id\u0026#39;) if [ -z \u0026#34;$group_id\u0026#34; ]; then echo \u0026#34;分组不存在，创建分组...\u0026#34; group_id=$(curl -s -X POST -H \u0026#34;Authorization: Bearer $JUMPSERVER_TOKEN\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;value\u0026#34;: \u0026#34;\u0026#39;$GROUP_NAME\u0026#39;\u0026#34;}\u0026#39; \u0026#34;$JUMPSERVER_URL/assets/nodes/\u0026#34; | jq -r \u0026#39;.id\u0026#39;) echo \u0026#34;创建后的分组ID: $group_id\u0026#34; else echo \u0026#34;找到的分组ID: $group_id\u0026#34; fi } # 创建或更新资产 create_or_update_asset() { echo \u0026#34;创建或更新资产 $HOST_NAME ...\u0026#34; asset_data=\u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;\u0026#39;$HOST_NAME\u0026#39;\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;\u0026#39;$IP\u0026#39;\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;protocols\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;port\u0026#34;: 22}], \u0026#34;is_active\u0026#34;: true, \u0026#34;nodes\u0026#34;: [\u0026#34;\u0026#39;$group_id\u0026#39;\u0026#34;], \u0026#34;accounts\u0026#34;: [{\u0026#34;template\u0026#34;: \u0026#34;f728c07f-003e-415e-a6be-d02e6fbf7f28\u0026#34;}] }\u0026#39; #此处的id，为模板账号 id response=$(curl -s -X POST -H \u0026#34;Authorization: Bearer $JUMPSERVER_TOKEN\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;$asset_data\u0026#34; \u0026#34;$JUMPSERVER_URL/assets/hosts/\u0026#34;) echo \u0026#34;资产创建或更新响应: $response\u0026#34; } check_and_create_group create_or_update_asset 执行 demo\n1 2 3 4 5 6 root@wq-1:~/file# ./add_jms_hosts.sh ownit 192.168.16.16 ownit-16 检查分组是否存在... 分组不存在，创建分组... 创建后的分组ID: b939d606-e334-494f-8a87-b1b464a941ce 创建或更新资产 ownit-16 ... 资产创建或更新响应: {\u0026#34;id\u0026#34;:\u0026#34;ec503228-77e8-4d44-9c5a-a322f45ad4ec\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;ownit-16\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;192.168.16.16\u0026#34;,\u0026#34;comment\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domain\u0026#34;:null,\u0026#34;platform\u0026#34;:{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Linux\u0026#34;},\u0026#34;nodes\u0026#34;:[{\u0026#34;id\u0026#34;:\u0026#34;b939d606-e334-494f-8a87-b1b464a941ce\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;ownit\u0026#34;}],\u0026#34;labels\u0026#34;:[],\u0026#34;protocols\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;ssh\u0026#34;,\u0026#34;port\u0026#34;:22}],\u0026#34;nodes_display\u0026#34;:[\u0026#34;/Default/ownit\u0026#34;],\u0026#34;category\u0026#34;:{\u0026#34;value\u0026#34;:\u0026#34;host\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;主机\u0026#34;},\u0026#34;type\u0026#34;:{\u0026#34;value\u0026#34;:\u0026#34;linux\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;Linux\u0026#34;},\u0026#34;connectivity\u0026#34;:{\u0026#34;value\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;未知\u0026#34;},\u0026#34;auto_config\u0026#34;:{\u0026#34;su_enabled\u0026#34;:true,\u0026#34;domain_enabled\u0026#34;:true,\u0026#34;ansible_enabled\u0026#34;:true,\u0026#34;id\u0026#34;:1,\u0026#34;ansible_config\u0026#34;:{\u0026#34;ansible_connection\u0026#34;:\u0026#34;smart\u0026#34;},\u0026#34;ping_enabled\u0026#34;:true,\u0026#34;ping_method\u0026#34;:\u0026#34;posix_ping\u0026#34;,\u0026#34;ping_params\u0026#34;:{},\u0026#34;gather_facts_enabled\u0026#34;:true,\u0026#34;gather_facts_method\u0026#34;:\u0026#34;gather_facts_posix\u0026#34;,\u0026#34;gather_facts_params\u0026#34;:{},\u0026#34;change_secret_enabled\u0026#34;:true,\u0026#34;change_secret_method\u0026#34;:\u0026#34;change_secret_posix\u0026#34;,\u0026#34;change_secret_params\u0026#34;:{},\u0026#34;push_account_enabled\u0026#34;:true,\u0026#34;push_account_method\u0026#34;:\u0026#34;push_account_posix\u0026#34;,\u0026#34;push_account_params\u0026#34;:{\u0026#34;sudo\u0026#34;:\u0026#34;/bin/whoami\u0026#34;,\u0026#34;shell\u0026#34;:\u0026#34;/bin/bash\u0026#34;,\u0026#34;home\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;groups\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;verify_account_enabled\u0026#34;:true,\u0026#34;verify_account_method\u0026#34;:\u0026#34;verify_account_posix\u0026#34;,\u0026#34;verify_account_params\u0026#34;:{},\u0026#34;gather_accounts_enabled\u0026#34;:true,\u0026#34;gather_accounts_method\u0026#34;:\u0026#34;gather_accounts_posix\u0026#34;,\u0026#34;gather_accounts_params\u0026#34;:{},\u0026#34;remove_account_enabled\u0026#34;:true,\u0026#34;remove_account_method\u0026#34;:\u0026#34;remove_account_posix\u0026#34;,\u0026#34;remove_account_params\u0026#34;:{},\u0026#34;platform\u0026#34;:1},\u0026#34;created_by\u0026#34;:\u0026#34;Administrator\u0026#34;,\u0026#34;gathered_info\u0026#34;:{},\u0026#34;org_id\u0026#34;:\u0026#34;00000000-0000-0000-0000-000000000002\u0026#34;,\u0026#34;org_name\u0026#34;:\u0026#34;Default\u0026#34;,\u0026#34;spec_info\u0026#34;:{},\u0026#34;is_active\u0026#34;:true,\u0026#34;date_verified\u0026#34;:null,\u0026#34;date_created\u0026#34;:\u0026#34;2024/01/26 09:50:54 +0800\u0026#34;} 4、Ansible 批量跑脚本 上面的脚本已经实现，现在就批量跑数据。（注意，使用 Ansible 跑，但是有个问题，有些不成功） 最后使用 shell 脚本来处理 的\n‍\nAnsible 的 hosts 定义\n1 2 3 4 5 6 7 8 9 root@wq-1:~/ansible# cat hosts [all:vars] ansible_ssh_user=root ansible_ssh_pass=dgadxxx [wq] 10.1.11.[1:90] [li] 10.1.17.[1:46] Ansible 的剧本文件（注意 hosts 的定义，我是按分组单个执行）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 {{ group_names[0] }} {{ inventory_hostname }} 这两个是Ansible的内置变量 ansible own -m setup 可以使用 这个查询那个有内置变量 [root@bt ~]# ansible own -m setup | head -n 50 192.168.102.20 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_all_ipv4_addresses\u0026#34;: [ \u0026#34;192.168.102.20\u0026#34;, \u0026#34;172.17.0.1\u0026#34;, \u0026#34;172.23.0.1\u0026#34;, \u0026#34;172.18.0.1\u0026#34;, \u0026#34;172.22.0.1\u0026#34;, \u0026#34;172.25.1.10\u0026#34;, \u0026#34;172.19.0.1\u0026#34; ], \u0026#34;ansible_all_ipv6_addresses\u0026#34;: [ \u0026#34;fe80::a095:f2ff:fe80:413b\u0026#34;, \u0026#34;fe80::42:b1ff:fe02:9e5f\u0026#34;, \u0026#34;fe80::42:20ff:fe5e:7cfd\u0026#34;, \u0026#34;fe80::6c19:a7ff:fe5e:a4f\u0026#34;, \u0026#34;fe80::ac72:32ff:feaa:3ef\u0026#34;, \u0026#34;fe80::c0a1:c1ff:fea2:63b9\u0026#34; ], \u0026#34;ansible_apparmor\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;disabled\u0026#34; }, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 root@wq-1:~/ansible# cat add_jms_host.yml - hosts: wq remote_user: root gather_facts: no tasks: - name: 复制 jq 到远程主机 copy: src: /usr/bin/jq dest: /usr/bin/jq mode: \u0026#39;0755\u0026#39; - name: 获取系统的主机名称 shell: hostname register: result - name: 打印变量 debug: msg: - \u0026#34;Group Name: {{ group_names[0] }}\u0026#34; - \u0026#34;Ansible Host: {{ inventory_hostname }}\u0026#34; - \u0026#34;Inventory Hostname: {{ result.stdout }}\u0026#34; - name: \u0026#34;执行添加到 JumpServer 的脚本\u0026#34; script: /root/file/add_jms_hosts.sh {{ group_names[0] }} {{ inventory_hostname }} {{ result.stdout }} args: executable: /bin/bash 执行的 demo\n1 ansible-playbook add_jms_host.yml -i ./hosts 上执行完毕，但是有问题，我 90 台机器，但实际添加的只有 30 台，其余是执行正常的，但是没有添加。\n或者使用下面的\n‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 - hosts: wq remote_user: root gather_facts: no tasks: - name: 复制 jq 到远程主机 copy: src: /usr/bin/jq dest: /usr/bin/jq mode: \u0026#39;0755\u0026#39; - name: 复制 add_jms_hosts.sh 到远程主机 copy: src: /root/file/add_jms_hosts.sh dest: /root/add_jms_hosts.sh mode: \u0026#39;0755\u0026#39; - name: 获取系统的主机名称 shell: hostname register: result - name: 打印变量 debug: msg: - \u0026#34;Group Name: {{ group_names[0] }}\u0026#34; - \u0026#34;Ansible Host: {{ inventory_hostname }}\u0026#34; - \u0026#34;Inventory Hostname: {{ result.stdout }}\u0026#34; - name: 执行添加到 JumpServer 的脚本 command: /bin/bash /root/add_jms_hosts.sh {{ group_names[0] }} {{ inventory_hostname }} {{ result.stdout }} ‍\n5、shell 执行批量数据 如果上面的执行还是不行，那就使用 下方的方式 100 % 可以。\n我先使用 ansible 跑出 分组 ip 和 主机名称\n‍\n1 2 3 4 5 6 7 8 9 10 11 12 - hosts: li remote_user: root gather_facts: no tasks: - name: 获取系统的主机名称 shell: hostname register: result - name: 打印变量 debug: msg: - \u0026#34;{{ group_names[0] }} {{ inventory_hostname }} {{ result.stdout }} \u0026#34; 执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ansible-playbook test.yml -i ./hosts \u0026gt;\u0026gt; 1.txt cat 1.txt | grep wq \u0026gt; /root/file/host （编辑host 删除 第一行 非 ip的） sed \u0026#39;s/. *\u0026amp;quot;(.* \\)\u0026#34;.*/\\1/\u0026#39; host \u0026gt; 1.txt (出去 双引号) 过滤后的数据格式 root@wq-1:~/file# cat 1.txt li 10.1.17.1 li-1 li 10.1.17.2 li-2 li 10.1.17.3 li-3 li 10.1.17.4 li-4 li 10.1.17.5 li-5 li 10.1.17.6 li-6 li 10.1.17.7 li-7 li 10.1.17.8 li-8 li 10.1.17.9 li-9 1 2 3 4 5 6 7 8 9 10 11 root@wq-1:~/file# cat star_add.sh #!/bin/bash while IFS= read -r line; do group=$(echo \u0026#34;$line\u0026#34; | awk \u0026#39;{print $1}\u0026#39;) ip=$(echo \u0026#34;$line\u0026#34; | awk \u0026#39;{print $2}\u0026#39;) name=$(echo \u0026#34;$line\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) echo \u0026#34;Group: $group, IP: $ip, Name: $name\u0026#34; # 在这里执行您的操作，例如调用其他脚本并传递这三个变量 ./add_jms_hosts.sh \u0026#34;$group\u0026#34; \u0026#34;$ip\u0026#34; \u0026#34;$name\u0026#34; done \u0026lt; 1.txt bash star_add.sh 注册到 jumpserver 上\n​\n‍\n4、总结 我们的解决方案是编写一个脚本，该脚本可以读取包含机器信息的文本文件，并使用 Jumpserver 提供的 API 进行自动导入。脚本首先通过逐行读取文本文件来获取每台机器的相关信息，例如 IP 地址、主机名和分组。然后，它使用 Jumpserver 的 API 调用来创建新的机器对象，并将其分配到相应的分组中。\n通过使用这个自动化脚本，我们可以大大减少手动操作的工作量，并确保导入的机器被正确地组织到 Jumpserver 中的相应分组中。这不仅提高了操作效率，还降低了出错的风险，并提供了一致性和可追溯性。\n总结起来，通过编写自动化脚本来批量导入机器到 Jumpserver，并根据预定义的分组进行组织，我们能够提高工作效率、降低错误率，并确保一致性和可追溯性。这个解决方案为我们节省了宝贵的时间和精力，使我们能够更好地管理和控制远程机器。\n","date":"2024-01-26T10:29:52Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/202401261029/","title":"自动化批量导入机器到Jumpserver：提高效率与一致性的关键步骤"},{"content":"Kube-Prometheus 监控Ingress Kube-Prometheus 是一个在 Kubernetes 上运行的 Prometheus 堆栈，它提供了一种方便的方式来监控你的 Kubernetes 集群。在本文中，我们将介绍如何使用 Kube-Prometheus 来监控 Kubernetes Ingress。\n1、前提条件 在开始之前，你需要确保你已经在你的 Kubernetes 集群上安装了以下组件：\nKube-Prometheus Ingress Controller（例如 NGINX Ingress Controller） Kubernetes v1.23.0集群 ‍\n如果没有，这可以参考下方博客进行部署构建\nKube-Prometheus 手动部署\nKubernetes安装ingress-nginx\n使用kubeadm部署一套Kubernetes v1.23.0集群\n‍\n2、Ingress介绍 在 Kubernetes 集群中，我们通常使用 “Nginx Ingress” 实现集群南北向流量的代理转发，Nginx Ingress 基于集群内 Ingress 资源配置生成具体的路由规则。Ingress 资源负责对外公开服务的管理，一般这类服务通过 HTTP 协议进行访问。通过 Nginx Ingress + Ingress 资源可以实现以下场景：\n‍\n一、通过 Nginx Ingress 将来自客户端的全部流量转发给单一 Service。\n​​\n二、通过 Nginx Ingress 实现更复杂的路由转发规则，将来自单一绑定 IP 地址的所有流量根据 URL 请求路径前缀转发给不同的 Service。\n​​\n三、根据 HTTP 请求头部携带的 Host 字段——通常由访问的域名决定，将来自单一绑定 IP 地址的流量分发给不同后端 Service，实现基于名称的虚拟主机（Name-based Virtual Hosting）能力。\n​​\n通常，围绕 Nginx Ingress 网关监控场景，我们通常会关注两类核心指标数据：\n工作负载资源 即 Nginx Ingress Controller Pod 的负载情况，当 CPU 、内存等资源水位处于饱和或过载，会导致集群对外服务不稳定。针对“工作负载监控”，一般建议关注 “USE” 指标，即：使用率（Utilization）、饱和度（Saturation）、错误率（Errors）。对此，阿里云 Prometheus 监控提供了预置性能监控大盘，可参考 《工作负载性能监控组件接入》 [1] 完成数据采集与大盘创建。\n入口请求流量 包括集群范围全局的流量、某个 Ingress 规则转发的流量、某个 Service 的流量，以及对应的成功率/错误率、延迟，乃至请求来源的地址、设备等信息的分析与统计。针对“入口请求流量监控”，一般建议关注 “RED” 指标，即：请求速率（Rate）、请求失败数（Errors）、请求延迟（Duration）。可通过本文最佳实践实现接入。\n3、Ingress配置 Ingress的监控端口：10254\n​​\n‍\n查看SVC，POD\n​​\n1、修改Ingress deployment内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Deployment metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;10254\u0026#34; .. spec: ports: - name: prometheus containerPort: 10254 .. 重新apply一下yaml文件让修改的配置生效 kubectl apply -f ingress-deploy.yml 或者\n因为已经部署可以直接编辑\n1 2 3 4 5 6 7 8 kubectl edit deployments.apps -n ingress-nginx ingress-nginx-controller ports: - containerPort: 10254 hostPort: 10254 name: prometheus protocol: TCP ​​\n之后保存退出\n注意：这边有个坑，因为我的Ingress是使用的hostnetwork 固定node01 上，\n所以保存重启 会失败，因为80 和 443 端口已经被占用。\n解决问题：备份原本Ingress的yaml文件，删除Ingress的deployment，在使用yaml文件重建 即可\n‍\nhostNetwork: true\nnodeName: node01\nnodeSelector:\nkubernetes.io/os: linux\n2、修改Ingress Service内容 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;10254\u0026#34; .. spec: ports: - name: prometheus port: 10254 targetPort: 10254 .. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion: v1 kind: Service metadata: annotations: #svc 这一块必须要加，不然不会监控到 prometheus.io/port: \u0026#39;10254\u0026#39; prometheus.io/scrape: \u0026#39;true\u0026#39; labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: ingress-nginx app.kubernetes.io/version: 1.0.0 helm.sh/chart: ingress-nginx-4.0.1 name: ingress-nginx-controller namespace: ingress-nginx spec: ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http - appProtocol: https name: https port: 443 protocol: TCP targetPort: https - name: prometheus port: 10254 protocol: TCP targetPort: prometheus selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx sessionAffinity: None type: ClusterIP 3、测试一下修改是否正常 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@bt app]# kubectl get po,svc -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-controller-d494d449b-k7z5q 1/1 Running 0 59m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller ClusterIP 172.21.20.203 \u0026lt;none\u0026gt; 80/TCP,443/TCP,10254/TCP 448d service/ingress-nginx-controller-admission ClusterIP 172.21.18.217 \u0026lt;none\u0026gt; 443/TCP 448d [root@bt app]# [root@bt app]# curl 172.21.20.203:10254/metrics # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 4.1684e-05 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 0.000148229 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000936162 go_gc_duration_seconds{quantile=\u0026#34;0.75\u0026#34;} 0.001216288 go_gc_duration_seconds{quantile=\u0026#34;1\u0026#34;} 0.002295067 go_gc_duration_seconds_sum 0.039017271 go_gc_duration_seconds_count 46 # HELP go_goroutines Number of goroutines that currently exist. ​​\n4、ServiceMonitor 新增Ingress ServiceMonitor\n查询标签\n1 2 3 4 [root@bt app]# kubectl get svc -n ingress-nginx --show-labels NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE LABELS ingress-nginx-controller ClusterIP 172.21.20.203 \u0026lt;none\u0026gt; 80/TCP,443/TCP,10254/TCP 448d app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/version=1.0.0,helm.sh/chart=ingress-nginx-4.0.1 ingress-nginx-controller-admission ClusterIP 172.21.18.217 \u0026lt;none\u0026gt; 443/TCP 448d app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/version=1.0.0,helm.sh/chart=ingress-nginx-4.0.1 根据此处的标签来填写下面的selector 匹配标签\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: ingress-nginx namespace: monitoring spec: endpoints: - interval: 15s port: prometheus namespaceSelector: matchNames: - ingress-nginx selector: matchLabels: #此处不是乱写的，要根据自己实际情况，查标签 app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/version: 1.0.0 --- # 在对应的ns中创建角色 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - pods verbs: - get - list - watch --- # 绑定角色 prometheus-k8s 角色到 Role apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s # Prometheus 容器使用的 serviceAccount，kube-prometheus默认使用prometheus-k8s这个用户 namespace: monitoring 5、添加报警规则 ‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: prometheus: k8s role: alert-rules name: nginx-ingress-rules namespace: monitoring spec: groups: - name: nginx-ingress-rules rules: - alert: NginxFailedtoLoadConfiguration expr: nginx_ingress_controller_config_last_reload_successful == 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;Nginx Ingress Controller配置文件加载失败\u0026#34; description: \u0026#34;Nginx Ingress Controller的配置文件加载失败，请检查配置文件是否正确。\u0026#34; - alert: NginxHighHttp4xxErrorRate expr: rate(nginx_ingress_controller_requests{status=~\u0026#34;^404\u0026#34;}[5m]) * 100 \u0026gt; 1 for: 1m labels: severity: warining annotations: description: Nginx high HTTP 4xx error rate ( namespaces {{ $labels.exported_namespace }} host {{ $labels.host }} ) summary: \u0026#34;Too many HTTP requests with status 404 (\u0026gt; 1%)\u0026#34; - alert: NginxHighHttp5xxErrorRate expr: rate(nginx_ingress_controller_requests{status=~\u0026#34;^5..\u0026#34;}[5m]) * 100 \u0026gt; 1 for: 1m labels: severity: warining annotations: description: Nginx high HTTP 5xx error rate ( namespaces {{ $labels.exported_namespace }} host {{ $labels.host }} ) summary: \u0026#34;Too many HTTP requests with status 5xx (\u0026gt; 1%)\u0026#34; - alert: NginxLatencyHigh expr: histogram_quantile(0.99, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[2m])) by (host, node)) \u0026gt; 3 for: 2m labels: severity: warining annotations: description: Nginx latency high ( namespaces {{ $labels.exported_namespace }} host {{ $labels.host }} ) summary: \u0026#34;Nginx p99 latency is higher than 3 seconds\u0026#34; - alert: NginxHighRequestRate expr: rate(nginx_ingress_controller_nginx_process_requests_total[5m]) * 100 \u0026gt; 1000 for: 1m labels: severity: warning annotations: description: Nginx ingress controller high request rate ( instance {{ $labels.instance }} namespaces {{ $labels.namespaces }} pod {{$labels.pod}}) summary: \u0026#34;Nginx ingress controller high request rate (\u0026gt; 1000 requests per second)\u0026#34; - alert: SSLCertificateExpiration15day expr: nginx_ingress_controller_ssl_expire_time_seconds \u0026lt; 1296000 for: 30m labels: severity: warning annotations: summary: SSL/TLS certificate for {{ $labels.host $labels.secret_name }} is about to expire description: The SSL/TLS certificate for {{ $labels.host $labels.secret_name }} will expire in less than 15 days. - alert: SSLCertificateExpiration7day expr: nginx_ingress_controller_ssl_expire_time_seconds \u0026lt; 604800 for: 30m labels: severity: critical annotations: summary: SSL/TLS certificate for {{ $labels.host $labels.secret_name }} is about to expire description: The SSL/TLS certificate for {{ $labels.host $labels.secret_name }} will expire in less than 7 days. 执行查询\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@bt app]# kubectl apply -f ingress_rule_yaml [root@bt app]# kubectl get prometheusrules.monitoring.coreos.com -n monitoring NAME AGE alertmanager-main-rules 24h etcd-rules 21h kube-prometheus-rules 24h kube-state-metrics-rules 24h kubernetes-monitoring-rules 24h nginx-ingress-rules 11s node-exporter-rules 24h prometheus-k8s-prometheus-rules 24h prometheus-operator-rules 24h ​​\n6、Grafan导入模版\nIngress-nginx模板ID：9614、14314\n​​\n​​\n‍\n6、监控指标 1 curl http://172.21.20.203:10254/metrics 访问上面的接口，会返回监控指标，这写数据Prometheus会定时拉去。\n接下来让我们看看这些数据的含义\n1 curl http://172.21.20.203:10254/metrics | grep -Ev \u0026#39;^#\u0026#39; |awk -F \u0026#39;{\u0026#39; \u0026#39;{print $1}\u0026#39; | sort | uniq | awk \u0026#39;{print $1}\u0026#39; ‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 [root@bt ~]# curl http://172.21.20.203:10254/metrics | grep -Ev \u0026#39;^#\u0026#39; |awk -F \u0026#39;{\u0026#39; \u0026#39;{print $1}\u0026#39; | sort | uniq | awk \u0026#39;{print $1}\u0026#39; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 291k 0 291k 0 0 6654k 0 --:--:-- --:--:-- --:--:-- 6781k go_gc_duration_seconds go_gc_duration_seconds_count go_gc_duration_seconds_sum go_goroutines go_info go_memstats_alloc_bytes go_memstats_alloc_bytes_total go_memstats_buck_hash_sys_bytes go_memstats_frees_total go_memstats_gc_cpu_fraction go_memstats_gc_sys_bytes go_memstats_heap_alloc_bytes go_memstats_heap_idle_bytes go_memstats_heap_inuse_bytes go_memstats_heap_objects go_memstats_heap_released_bytes go_memstats_heap_sys_bytes go_memstats_last_gc_time_seconds go_memstats_lookups_total go_memstats_mallocs_total go_memstats_mcache_inuse_bytes go_memstats_mcache_sys_bytes go_memstats_mspan_inuse_bytes go_memstats_mspan_sys_bytes go_memstats_next_gc_bytes go_memstats_other_sys_bytes go_memstats_stack_inuse_bytes go_memstats_stack_sys_bytes go_memstats_sys_bytes go_threads nginx_ingress_controller_bytes_sent_bucket nginx_ingress_controller_bytes_sent_count nginx_ingress_controller_bytes_sent_sum nginx_ingress_controller_config_hash nginx_ingress_controller_config_last_reload_successful nginx_ingress_controller_config_last_reload_successful_timestamp_seconds nginx_ingress_controller_ingress_upstream_latency_seconds nginx_ingress_controller_ingress_upstream_latency_seconds_count nginx_ingress_controller_ingress_upstream_latency_seconds_sum nginx_ingress_controller_leader_election_status nginx_ingress_controller_nginx_process_connections nginx_ingress_controller_nginx_process_connections_total nginx_ingress_controller_nginx_process_cpu_seconds_total nginx_ingress_controller_nginx_process_num_procs nginx_ingress_controller_nginx_process_oldest_start_time_seconds nginx_ingress_controller_nginx_process_read_bytes_total nginx_ingress_controller_nginx_process_requests_total nginx_ingress_controller_nginx_process_resident_memory_bytes nginx_ingress_controller_nginx_process_virtual_memory_bytes nginx_ingress_controller_nginx_process_write_bytes_total nginx_ingress_controller_request_duration_seconds_bucket nginx_ingress_controller_request_duration_seconds_count nginx_ingress_controller_request_duration_seconds_sum nginx_ingress_controller_requests nginx_ingress_controller_request_size_bucket nginx_ingress_controller_request_size_count nginx_ingress_controller_request_size_sum nginx_ingress_controller_response_duration_seconds_bucket nginx_ingress_controller_response_duration_seconds_count nginx_ingress_controller_response_duration_seconds_sum nginx_ingress_controller_response_size_bucket nginx_ingress_controller_response_size_count nginx_ingress_controller_response_size_sum nginx_ingress_controller_ssl_expire_time_seconds nginx_ingress_controller_success process_cpu_seconds_total process_max_fds process_open_fds process_resident_memory_bytes process_start_time_seconds process_virtual_memory_bytes process_virtual_memory_max_bytes promhttp_metric_handler_requests_in_flight promhttp_metric_handler_requests_total [root@bt ~]# 参考文档：\nhttps://help.aliyun.com/zh/arms/prometheus-monitoring/basic-metrics\nhttps://www.volcengine.com/docs/6731/802251\n‍\n上面go开头的不用管，这是监控软件自带监控本身数据指标的，我们重点关注下方的的指标数据\n此处根据自己的整理，以及参考ChatGPT进行汇总，仅供参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 这些指标通过 Prometheus 暴露，并可以用于监控和告警： nginx_ingress_controller_bytes_sent_bucket: 请求发送字节大小的分布情况（通常为直方图的一部分，用于计算请求字节量的分位数）。 nginx_ingress_controller_bytes_sent_count: 发送的总请求数量，每个请求发送的字节累加的总和。 nginx_ingress_controller_bytes_sent_sum: 发送的总字节量，累计所有请求发送的字节。 nginx_ingress_controller_config_hash: 当前Nginx配置的哈希值，可用来检测配置是否有变化。 nginx_ingress_controller_config_last_reload_successful: 标识最后一次重新加载Nginx配置是否成功（1表示成功，0表示失败）。 nginx_ingress_controller_config_last_reload_successful_timestamp_seconds: 最后一次成功重新加载Nginx配置的时间戳。 nginx_ingress_controller_ingress_upstream_latency_seconds: 记录从ingress到上游服务的延迟。 nginx_ingress_controller_ingress_upstream_latency_seconds_count: 上游服务延迟计数。 nginx_ingress_controller_ingress_upstream_latency_seconds_sum: 上游服务延迟总和。 nginx_ingress_controller_leader_election_status: 标识当前实例是否是领导者（leader election用于决定哪个Ingress controller实例是主控）。 nginx_ingress_controller_nginx_process_connections: Nginx进程当前的活跃连接数。 nginx_ingress_controller_nginx_process_connections_total: Nginx进程处理的总连接数。 nginx_ingress_controller_nginx_process_cpu_seconds_total: Nginx进程消耗的CPU时间总量。 nginx_ingress_controller_nginx_process_num_procs: Nginx进程数。 nginx_ingress_controller_nginx_process_oldest_start_time_seconds: 最老的Nginx进程启动时间。 nginx_ingress_controller_nginx_process_read_bytes_total: Nginx进程读操作的总字节数。 nginx_ingress_controller_nginx_process_requests_total: Nginx进程处理的总请求数量。 nginx_ingress_controller_nginx_process_resident_memory_bytes: Nginx进程的常驻内存大小。 nginx_ingress_controller_nginx_process_virtual_memory_bytes: Nginx进程的虚拟内存大小。 nginx_ingress_controller_nginx_process_write_bytes_total: Nginx进程写操作的总字节数。 nginx_ingress_controller_request_duration_seconds_bucket: 处理请求的持续时间的分布情况。 nginx_ingress_controller_request_duration_seconds_count: 请求持续时间的计数。 nginx_ingress_controller_request_duration_seconds_sum: 请求持续时间的总和。 nginx_ingress_controller_requests: 处理的总请求量。 nginx_ingress_controller_request_size_bucket: 请求大小的分布情况。 nginx_ingress_controller_request_size_count: 请求大小的计数。 nginx_ingress_controller_request_size_sum: 请求大小的总和。 nginx_ingress_controller_response_duration_seconds_bucket: 响应时间的分布情况。 nginx_ingress_controller_response_duration_seconds_count: 响应时间的计数。 nginx_ingress_controller_response_duration_seconds_sum: 响应时间的总和。 nginx_ingress_controller_response_size_bucket: 响应大小的分布情况。 nginx_ingress_controller_response_size_count: 响应大小的计数。 nginx_ingress_controller_response_size_sum: 响应大小的总和。 nginx_ingress_controller_ssl_expire_time_seconds: SSL证书到期时间。 nginx_ingress_controller_success: 成功处理的请求计数。 process_cpu_seconds_total: 进程消耗的CPU时间总量。 process_max_fds: 进程可以打开的最大文件描述符数量。 process_open_fds: 进程当前打开的文件描述符数量。 process_resident_memory_bytes: 进程的常驻内存大小。 process_start_time_seconds: 进程启动的开始时间。 process_virtual_memory_bytes: 进程的虚拟内存大小。 process_virtual_memory_max_bytes: 进程可以使用的最大虚拟内存大小。 promhttp_metric_handler_requests_in_flight: 当前正在处理的promhttp指标处理器的请求数量。 promhttp_metric_handler_requests_total: promhttp指标处理器处理的总请求数量。 ‍\n","date":"2024-01-19T10:19:00Z","image":"https://www.ownit.top/title_pic/49.jpg","permalink":"https://www.ownit.top/p/202401191019/","title":"Kube-Prometheus 监控Ingress实战"},{"content":"Kube-Prometheus 手动部署 使用 Prometheus Operator 监控 Kubernetes 集群（Prometheus Operator 已改名为 Kube-Prometheus）\n注意：\nPrometheus-operator 已经改名为 Kube-promethues\n介绍和功能 prometheus-operator 介绍\n当今 Cloud Native 概念流行，对于容器、服务、节点以及集群的监控变得越来越重要。Prometheus 作为 Kubernetes 监控的事实标准，有着强大的功能和良好的生态。但是它不支持分布式，不支持数据导入、导出，不支持通过 API 修改监控目标和报警规则，所以在使用它时，通常需要写脚本和代码来简化操作。\nPrometheus Operator 为监控 Kubernetes service、deployment、daemonsets 和 Prometheus 实例的管理提供了简单的定义等，简化在 Kubernetes 上部署、管理和运行 Prometheus 和 Alertmanager 集群。\nprometheus-operator 功能\n​创建/销毁​：在 Kubernetes namespace 中更加容易地启动一个 Prometheues 实例，一个特定应用程序或者团队可以更容易使用 Prometheus Operator。 ​便捷配置​：通过 Kubernetes 资源配置 Prometheus 的基本信息，比如版本、存储、副本集等。 ​通过标签标记目标服务​： 基于常见的 Kubernetes label 查询自动生成监控目标配置；不需要学习 Prometheus 特定的配置语言。 Prometheus Operator 的工作原理\n​​\n上面架构图中，各组件以不同的方式运行在 Kubernetes 集群中：\nOperator： 根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。 Prometheus：声明 Prometheus deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。 Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。 ServiceMonitor：声明指定监控的服务，描述了一组被 Prometheus 监控的目标列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。 Service：简单的说就是 Prometheus 监控的对象。 Alertmanager：定义 AlertManager deployment 期望的状态，Operator 确保这个 deployment 运行时一直与定义保持一致。 自定义资源\nPrometheus Operator 为我们提供了哪些自定义的 Kubernetes 资源，列出了 Prometheus Operator 目前提供的 ️4 类资源：\nPrometheus：声明式创建和管理 Prometheus Server 实例； ServiceMonitor：负责声明式的管理监控配置； PrometheusRule：负责声明式的管理告警配置； Alertmanager：声明式的创建和管理 Alertmanager 实例。 ‍\nPrometheus Operator 官方提供根据 Prometheus Operator CRD 自动管理 Prometheus Server 的工作机制原理图如下：\n​​\nServiceMonitor 可以通过 labelSelector 的方式去匹配一类 Service，Prometheus 也可以通过 labelSelector 去匹配多个 ServiceMonitor。Prometheus Operator 自动检测 Kubernetes API 服务器对上述任何对象的更改，并确保匹配的部署和配置保持同步。\n为了能让 prom 监控 k8s 内的应用，Prometheus-Operator 通过配置 servicemonitor 匹配到由 service 对象自动填充的 Endpoints，并配置 prometheus 监控这些 Endpoints 后端的 pods，ServiceMonitor.Spec 的 Endpoints 部分就是用于配置 Endpoints 的哪些端口将被 scrape 指标。 servicemonitor 对象很巧妙，它解耦了“监控的需求”和“需求的实现方”。servicemonitor 只需要用到 label-selector 这种简单又通用的方式声明一个 “监控需求”，也就是哪些 Endpoints 需要搜集，怎么收集就行了。让用户只关心需求，这是一个非常好的关注点分离。当然 servicemonitor 最后还是会被 operator 转化为原始的复 杂的 scrape config,但这个复杂度已经完全被 operator 屏蔽了。 ‍\nprometheus 在配置报警时需要操作哪些资源，及各资源起到的作用\n​​\n首先通过配置 servicemonitor/podmonitor 来获取应用的监控指标；Prometheus.spec.alerting 字段会匹配 Alertmanager 中的配置，匹配到 alertmanager 实例 然后通过 prometheusrule 对监控到的指标配置报警规则； 最后配置告警接收器，配置 alertmanagerconfig 来配置如何处理告警，包括如何接收、路由、抑制和发送警报等； ‍\n版本区别\n为了使用 Prometheus-Operator，这里我们直接使用 kube-prometheus 这个项目来进行安装，该项目和 Prometheus-Operator 的区别就类似于 Linux 内核和 CentOS/Ubuntu 这些发行版的关系，真正起作用的是 Operator 去实现的，而 kube-prometheus 只是利用 Operator 编写了一系列常用的监控资源清单。不过需要注意 Kubernetes 版本和 kube-prometheus​ 的兼容：\n​​\n​​\n‍\nPrometheus Operator 和 kube-prometheus 的区别 Prometheus Operator 和 kube-prometheus 都是用于简化 Prometheus 在 Kubernetes 集群中部署和管理的工具，但它们的功能和使用方式有所不同。\nPrometheus Operator: Prometheus Operator 是 CoreOS 开发的一个 Kubernetes Operator，它的目标是简化 Prometheus 在 Kubernetes 集群中的部署和管理过程。Prometheus Operator 通过定义一组 CRD (Custom Resource Definitions)，使用户可以以声明性的方式来部署和配置 Prometheus，Alertmanager，以及相关的监控组件。这些 CRD 包括 Prometheus​，ServiceMonitor​，PodMonitor​，Alertmanager ​等。 kube-prometheus: kube-prometheus 是一个预配置的 Prometheus、Alertmanager、Grafana 以及一系列的 Prometheus Exporters 的集合，用于监控 Kubernetes 集群。它使用 Prometheus Operator 管理 Prometheus 和 Alertmanager 实例，并提供了一系列预定义的 Grafana 仪表板和 Prometheus 规则。换句话说，kube-prometheus 是在 Prometheus Operator 的基础上，提供了一套完整的、开箱即用的 Kubernetes 集群监控解决方案。 总的来说，Prometheus Operator 提供了一种机制，让你可以在 Kubernetes 中更容易地管理和运行 Prometheus。而 kube-prometheus 则是使用这种机制，提供了一套完整的 Kubernetes 集群监控方案。\n‍\n准备步骤 拉取 Prometheus Operator 环境：kubeadm 部署 v1.23.1\n使用的版本：https://github.com/prometheus-operator/kube-prometheus/tree/release-0.10\n‍\n先从 Github 上将源码拉取下来，利用源码项目已经写好的 kubernetes 的 yaml 文件进行一系列集成镜像的安装，如 grafana、prometheus 等等。\n从 GitHub 拉取 Prometheus Operator 源码\n1 2 $ git clone https://github.com/prometheus-operator/kube-prometheus.git -b release-0.10 $ cd kube-prometheus/manifests/ 进行文件分类 由于它的文件都存放在项目源码的 manifests 文件夹下，所以需要进入其中进行启动这些 kubernetes 应用 yaml 文件。又由于这些文件堆放在一起，不利于分类启动，所以这里将它们分类。\n进入源码的 manifests 文件夹\n1 cd kube-prometheus/manifests/ ​​\n创建文件夹并且将 yaml 文件分类\n‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 创建文件夹 $ mkdir -p operator node-exporter alertmanager grafana kube-state-metrics prometheus serviceMonitor adapter blackbox # 移动 yaml 文件，进行分类到各个文件夹下 mv *-serviceMonitor* serviceMonitor/ mv prometheusOperator-* operator mv grafana-* grafana/ mv kubeStateMetrics-* kube-state-metrics mv alertmanager-* alertmanager/ mv nodeExporter-* node-exporter/ mv prometheusAdapter-* adapter mv prometheus-* prometheus/ mv blackboxExporter-* blackbox/ mv kubePrometheus-prometheusRule.yaml kubernetesControlPlane-prometheusRule.yaml prometheus/ ​​\n修改镜像源 国外镜像源某些镜像无法拉取，我们这里修改 prometheus-operator，prometheus，alertmanager，kube-state-metrics，node-exporter，prometheus-adapter 的镜像源为国内镜像源。我这里使用的是中科大的镜像源。\n1 2 3 4 5 6 7 # 查找 grep -rn \u0026#39;quay.io\u0026#39; * # 批量替换 sed -i \u0026#39;s/quay.io/quay.mirrors.ustc.edu.cn/g\u0026#39; `grep \u0026#34;quay.io\u0026#34; -rl *` # 再查找 grep -rn \u0026#39;quay.io\u0026#39; * grep -rn \u0026#39;image: \u0026#39; * 修改 Service 端口设置 为了可以从外部访问 prometheus​，alertmanager​，grafana​，我们这里修改 promethes​，alertmanager​，grafana ​的 service​ 类型为 NodePort​ 类型。\n修改 prometheus-service.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim prometheus/prometheus-service.yaml 修改prometheus Service端口类型为NodePort，设置nodePort端口为32101 apiVersion: v1 kind: Service metadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoring spec: type: NodePort ports: - name: web port: 9090 targetPort: web nodePort: 32101 selector: app: prometheus prometheus: k8s sessionAffinity: ClientIP 修改 grafana-service.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 vim grafana/grafana-service.yaml 修改garafana Service端口类型为NodePort，设置nodePort端口为32102 apiVersion: v1 kind: Service metadata: labels: app: grafana name: grafana namespace: monitoring spec: type: NodePort ports: - name: http port: 3000 targetPort: http nodePort: 32102 selector: app: grafana 修改数据持久化存储 prometheus 实际上是通过 emptyDir 进行挂载的，我们知道 emptyDir 挂载的数据的生命周期和 Pod 生命周期一致的，如果 Pod 挂掉了，那么数据也就丢失了，这也就是为什么我们重建 Pod 后之前的数据就没有了的原因，所以这里修改它的持久化配置。\nGlusterFS 存储的 StorageClass 配置\n1 2 3 4 5 6 7 8 9 10 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast #---SorageClass 名称 provisioner: kubernetes.io/glusterfs #---标识 provisioner 为 GlusterFS parameters: resturl: \u0026#34;http://10.10.249.63:8080\u0026#34; restuser: \u0026#34;admin\u0026#34; gidMin: \u0026#34;40000\u0026#34; gidMax: \u0026#34;50000\u0026#34; volumetype: \u0026#34;none\u0026#34; #---分布巻模式，不提供备份，正式环境切勿用此模式 NFS 存储的 StorageClass 配置\n1 2 3 4 5 6 7 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: nfs-client #---动态卷分配应用设置的名称，必须和集群中的\u0026#34;nfs-provisioner\u0026#34;应用设置的变量名称保持一致 parameters: archiveOnDelete: \u0026#34;true\u0026#34; #---设置为\u0026#34;false\u0026#34;时删除PVC不会保留数据,\u0026#34;true\u0026#34;则保留数据 修改 Prometheus 持久化 修改 prometheus-prometheus.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 $ vim prometheus/prometheus-prometheus.yaml prometheus是一种 StatefulSet 有状态集的部署模式，所以直接将 StorageClass 配置到里面，在下面的yaml中最下面添加持久化配置： apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: labels: prometheus: k8s name: k8s namespace: monitoring spec: alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web baseImage: quay.io/prometheus/prometheus nodeSelector: beta.kubernetes.io/os: linux replicas: 2 resources: requests: memory: 400Mi ruleSelector: matchLabels: prometheus: k8s role: alert-rules securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 1000 serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: {} serviceMonitorSelector: {} version: v2.7.2 storage: #----添加持久化配置，指定StorageClass为上面创建的fast volumeClaimTemplate: spec: storageClassName: fass #---指定为fast resources: requests: storage: 10Gi ‍\n因为我集群有多个 pvc 没有被使用到，清理空间\n集群有多个 pvc，怎么查询出没有被使用的（题外话）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl get pvc --all-namespaces -o json | jq -r \u0026#39;.items[] | \u0026#34;\\(.metadata.namespace) \\(.metadata.name)\u0026#34;\u0026#39; | while read -r line; do if ! kubectl get pods --all-namespaces -o json | jq -r \u0026#39;.items[] | select(any(.spec.volumes[]; .persistentVolumeClaim.claimName == \u0026#34;\u0026#39;$(echo $line | awk \u0026#39;{print $2}\u0026#39;)\u0026#39;\u0026#34;)) | \u0026#34;\\(.metadata.namespace) \\(.metadata.name)\u0026#34;\u0026#39; | grep -q \u0026#34;$(echo $line | awk \u0026#39;{print $1}\u0026#39;)\u0026#34;; then echo $line fi done #列出没有使用的 default storage-loki-0 jenkins maven-pvc kube-system grafana-data-grafana-0 kube-system prometheus-data-prometheus-0 kuboard prometheus-k8s-db-prometheus-k8s-0 删除你列出的 PVC： kubectl delete pvc storage-loki-0 -n default kubectl delete pvc maven-pvc -n jenkins kubectl delete pvc grafana-data-grafana-0 -n kube-system kubectl delete pvc prometheus-data-prometheus-0 -n kube-system kubectl delete pvc prometheus-k8s-db-prometheus-k8s-0 -n kuboard 修改 Grafana 持久化配置 创建 grafana-pvc.yaml 文件\n由于 Grafana 是部署模式为 Deployment，所以我们提前为其创建一个 grafana-pvc.yaml 文件，加入下面 PVC 配置。\n1 2 3 4 5 6 7 8 9 10 11 12 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: grafana namespace: monitoring #---指定namespace为monitoring spec: storageClassName: fast #---指定StorageClass为上面创建的fast accessModes: - ReadWriteOnce resources: requests: storage: 5Gi 修改 grafana-deployment.yaml 文件设置持久化配置，应用上面的 PVC\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 vim grafana/grafana-deployment.yaml 将 volumes 里面的 “grafana-storage” 配置注掉，新增如下配置，挂载一个名为 grafana 的 PVC ...... volumes: - name: grafana-storage #-------新增持久化配置 persistentVolumeClaim: claimName: grafana #-------设置为创建的PVC名称 #- emptyDir: {} #-------注释掉旧的配置 # name: grafana-storage - name: grafana-datasources secret: secretName: grafana-datasources - configMap: name: grafana-dashboards name: grafana-dashboards ...... ‍\n‍\n安装步骤 安装 Setup 1 2 3 4 5 6 7 8 9 10 11 12 [root@bt manifests]# pwd /app/kube-prometheus/manifests [root@bt manifests]# kubectl apply --server-side -f setup customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com serverside-applied customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com serverside-applied namespace/monitoring serverside-applied 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 等待 kubectl wait \\ --for condition=Established \\ --all CustomResourceDefinition \\ --namespace=monitoring [root@bt manifests]# kubectl get crd |grep coreos alertmanagerconfigs.monitoring.coreos.com 2024-01-17T07:04:28Z alertmanagers.monitoring.coreos.com 2024-01-17T07:04:28Z podmonitors.monitoring.coreos.com 2024-01-17T07:04:28Z probes.monitoring.coreos.com 2024-01-17T07:04:28Z prometheuses.monitoring.coreos.com 2024-01-17T07:04:29Z prometheusrules.monitoring.coreos.com 2024-01-17T07:04:29Z servicemonitors.monitoring.coreos.com 2024-01-17T07:04:29Z thanosrulers.monitoring.coreos.com 2024-01-17T07:04:29Z ​​\n这会创建一个名为 monitoring​ 的命名空间，以及相关的 CRD 资源对象声明。前面章节中我们讲解过 CRD 和 Operator 的使用，当我们声明完 CRD 过后，就可以来自定义资源清单了，但是要让我们声明的自定义资源对象生效就需要安装对应的 Operator 控制器，在 manifests​ 目录下面就包含了 Operator 的资源清单以及各种监控对象声明，比如 Prometheus、Alertmanager 等，直接应用即可：\n安装 Operator 1 kubectl apply -f operator/ 查看 Pod，等 pod 创建起来在进行下一步\n1 2 3 4 $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS prometheus-operator-5d6f6f5d68-mb88p 1/1 Running 0 ​​\n安装其它组件 1 2 3 4 5 6 7 8 kubectl apply -f adapter/ kubectl apply -f alertmanager/ kubectl apply -f node-exporter/ kubectl apply -f kube-state-metrics/ kubectl apply -f grafana/ kubectl apply -f prometheus/ kubectl apply -f serviceMonitor/ kubectl apply -f blackbox/ ​​\n上图服务已经全部启动，但是有 node-exporter 为什么启动失败？ 因为本地的宿主机已经启动相应的程序，占用 9100 的端口\n后续解决办法：1、修改 k8s 中的 node-exporter 的端口\n​​\n‍\n‍\n问题解决 ​​\n上面的图中有几个问题需要解决？\n1、kube-controller-manager/0 (0/0 up)\n2、kube-scheduler/0 (0/0 up)\n3、node-exporter 失败\n4、没有监控 etcd 数据库\n必须提前设置一些 Kubernetes 中的配置，否则 kube-scheduler 和 kube-controller-manager 无法监控到数据。\n‍\n由于 Kubernetes 集群是由 kubeadm 搭建的，其中 kube-scheduler 和 kube-controller-manager 默认绑定 IP 是 127.0.0.1 地址。Prometheus Operator 是通过节点 IP 去访问，所以我们将 kube-scheduler 绑定的地址更改成 0.0.0.0。\n修改 kube-scheduler 和 kube-controller-manager 编辑 /etc/kubernetes/manifests/kube-scheduler.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 vim /etc/kubernetes/manifests/kube-scheduler.yaml 将 command 的 bind-address 地址更改成 0.0.0.0 ...... spec: containers: - command: - kube-scheduler - --bind-address=0.0.0.0 #改为0.0.0.0 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true ...... 编辑 /etc/kubernetes/manifests/kube-controller-manager.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 vim /etc/kubernetes/manifests/kube-controller-manager.yaml 将 command 的 bind-address 地址更改成 0.0.0.0 spec: containers: - command: - kube-controller-manager - --allocate-node-cidrs=true - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf - --bind-address=0.0.0.0 #改为0.0.0.0 ...... 创建 kube-scheduler \u0026amp; controller-manager 对应 Service 因为 Prometheus Operator 配置监控对象 serviceMonitor 是根据 label 选取 Service 来进行监控关联的，而通过 Kuberadm 安装的 Kubernetes 集群只创建了 kube-scheduler \u0026amp; controller-manager 的 Pod 并没有创建 Service，所以 Prometheus Operator 无法这两个组件信息，这里我们收到创建一下这俩个组件的 Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: v1 kind: Service metadata: namespace: kube-system name: kube-controller-manager labels: k8s-app: kube-controller-manager spec: selector: component: kube-controller-manager type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10252 targetPort: 10252 protocol: TCP --- apiVersion: v1 kind: Service metadata: namespace: kube-system name: kube-scheduler labels: k8s-app: kube-scheduler spec: selector: component: kube-scheduler type: ClusterIP clusterIP: None ports: - name: http-metrics port: 10251 targetPort: 10251 protocol: TCP ​​\n如果是二进制部署还得创建对应的 Endpoints 对象将两个组件挂入到 kubernetes 集群内，然后通过 Service 提供访问，才能让 Prometheus 监控到。\n上面如果不起效，请看下面的步骤\n‍\n‍\n‍\n注意\n小结一下, 通过之前的这些配置, Kubernetes 组件的 Metrics 监听端口分别为:\nController Manager: (Kubernetes v1.23+)\n端口: 10257 协议: https Scheduler: (Kubernetes v1.23+)\n端口: 10259 协议: https Kube Proxy\n端口: 10249 协议: http etcd\n端口: 2381 协议: http 可以通过 netstat​ 命令查看之前的配置是否全部生效:\n1 2 3 4 5 6 7 8 9 10 11 12 13 tcp 0 0 127.0.0.1:10249 0.0.0.0:* LISTEN 3454/kube-proxy tcp 0 0 192.168.102.30:2381 0.0.0.0:* LISTEN 6742/etcd tcp 0 0 127.0.0.1:2381 0.0.0.0:* LISTEN 2577/etcd tcp6 0 0 :::10257 :::* LISTEN 18313/kube-controll tcp6 0 0 :::10259 :::* LISTEN 18245/kube-schedule # 测试etcd指标 curl -k http://localhost:2381/metrics # 测试 kube-proxy 指标 curl -k http://localhost:10249/metrics ‍\n查看 serviceMonitor 的配置\nserviceMonitor/kubernetesControlPlane-serviceMonitorKubeScheduler.yaml\nserviceMonitor/kubernetesControlPlane-serviceMonitorKubeControllerManager.yaml\n看看 他们的匹配规则\n（是下方两个）\napp.kubernetes.io/name: kube-controller-manager\napp.kubernetes.io/name: kube-scheduler\n在高版本，业务端口就是 Prometheus 的端口，使用 https 协议\n1 2 3 4 5 6 7 8 9 10 11 12 [root@master01 manifests]# curl -k https://192.168.102.30:10257/metrics { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: {}, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;forbidden: User \\\u0026#34;system:anonymous\\\u0026#34; cannot get path \\\u0026#34;/metrics\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: {}, \u0026#34;code\u0026#34;: 403 } 上面的错误就是没有权限验证，无法收集 ‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 cat svc_error.yaml apiVersion: v1 kind: Service metadata: namespace: kube-system name: kube-controller-manager labels: k8s-app: kube-controller-manager app.kubernetes.io/name: kube-controller-manager spec: selector: component: kube-controller-manager type: ClusterIP # clusterIP: None ports: - name: https-metrics port: 10257 targetPort: 10257 protocol: TCP --- apiVersion: v1 kind: Service metadata: namespace: kube-system name: kube-scheduler labels: k8s-app: kube-scheduler app.kubernetes.io/name: kube-scheduler spec: selector: component: kube-scheduler type: ClusterIP # clusterIP: None ports: - name: https-metrics port: 10259 targetPort: 10259 protocol: TCP kubectl apply -f svc_error.yaml ​​\n‍\nnodeExporter 端口被占用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@bt node-exporter]# grep -nr \u0026#34;9100\u0026#34; nodeExporter-daemonset.yaml:29: - --web.listen-address=127.0.0.1:9100 nodeExporter-daemonset.yaml:57: - --secure-listen-address=[$(IP)]:9100 nodeExporter-daemonset.yaml:59: - --upstream=http://127.0.0.1:9100/ nodeExporter-daemonset.yaml:68: - containerPort: 9100 nodeExporter-daemonset.yaml:69: hostPort: 9100 nodeExporter-service.yaml:15: port: 9100 把这个修改成9101 即可 vim node-exporter/nodeExporter-daemonset.yaml 或者 cd node-exporter/ sed -i \u0026#39;s/9100/9101/g\u0026#39; nodeExporter-daemonset.yaml nodeExporter-service.yaml kubectl apply -f nodeExporter-daemonset.yaml kubectl apply -f nodeExporter-service.yaml 重启 Prometheus（热加载）\n1 2 3 4 5 6 7 8 IP=`kubectl get svc -n monitoring -owide | grep prometheus-k8s | awk \u0026#39;{print $3}\u0026#39;` curl -X POST \u0026#34;http://$IP:9090/-/reload\u0026#34; if [ $? -ne 0 ]; then echo \u0026#34;failed\u0026#34; else echo \u0026#34;succeed\u0026#34; fi ‍\n​​\n‍\n监控 ETCD etcd 默认会在 2379 端口上暴露 metrics，例如： curl -k http://localhost:2381/metrics\n先用 curl 命令检测一下，是否正常。\n1 curl -k http://localhost:2381/metrics 修改 ETCD yaml 文件\n1 2 3 4 vim /etc/kubernetes/manifests/etcd.yaml ... --listen-metrics-urls=http://0.0.0.0:2381\t# 默认是127.0.0.1,改成任意节点访问 ... 配置 ServiceMonitor，Service\nService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat etcd_svc.yaml apiVersion: v1 kind: Service metadata: labels: k8s-app: etcd-k8s name: etcd-k8s namespace: kube-system spec: ports: - name: http-etcd port: 2381 targetPort: 2381 selector: component: etcd ServiceMonitor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat etcd-ServiceMonitor.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: etcd-k8s namespace: monitoring labels: k8s-app: etcd-k8s spec: jobLabel: k8s-app selector: matchLabels: k8s-app: etcd-k8s namespaceSelector: matchNames: - kube-system endpoints: - port: http-etcd interval: 15s ​​\n‍\nGrafan 画图（3070​）\n​​\n‍\n配置告警\n对于部分核心指标，建议配置到告警规则里，这样出问题时能及时发现。\nEtcd 官方也提供了一个告警规则,点击查看–\u0026gt; etcd3_alert.rules.yml，内容比较多就不贴在这里来了。\n以下是基于该文件去除过时指标后的版本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 # these rules synced manually from https://github.com/etcd-io/etcd/blob/master/Documentation/etcd-mixin/mixin.libsonnet groups: - name: etcd rules: - alert: etcdInsufficientMembers annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: insufficient members ({{ $value }}).\u0026#39; expr: | sum(up{job=~\u0026#34;.*etcd.*\u0026#34;} == bool 1) by (job) \u0026lt; ((count(up{job=~\u0026#34;.*etcd.*\u0026#34;}) by (job) + 1) / 2) for: 3m labels: severity: critical - alert: etcdNoLeader annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: member {{ $labels.instance }} has no leader.\u0026#39; expr: | etcd_server_has_leader{job=~\u0026#34;.*etcd.*\u0026#34;} == 0 for: 1m labels: severity: critical - alert: etcdHighNumberOfLeaderChanges annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: instance {{ $labels.instance }} has seen {{ $value }} leader changes within the last hour.\u0026#39; expr: | rate(etcd_server_leader_changes_seen_total{job=~\u0026#34;.*etcd.*\u0026#34;}[15m]) \u0026gt; 3 for: 15m labels: severity: warning - alert: etcdHighNumberOfFailedGRPCRequests annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.\u0026#39; expr: | 100 * sum(rate(grpc_server_handled_total{job=~\u0026#34;.*etcd.*\u0026#34;, grpc_code!=\u0026#34;OK\u0026#34;}[5m])) BY (job, instance, grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) BY (job, instance, grpc_service, grpc_method) \u0026gt; 1 for: 10m labels: severity: warning - alert: etcdHighNumberOfFailedGRPCRequests annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.\u0026#39; expr: | 100 * sum(rate(grpc_server_handled_total{job=~\u0026#34;.*etcd.*\u0026#34;, grpc_code!=\u0026#34;OK\u0026#34;}[5m])) BY (job, instance, grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) BY (job, instance, grpc_service, grpc_method) \u0026gt; 5 for: 5m labels: severity: critical - alert: etcdHighNumberOfFailedProposals annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: {{ $value }} proposal failures within the last hour on etcd instance {{ $labels.instance }}.\u0026#39; expr: | rate(etcd_server_proposals_failed_total{job=~\u0026#34;.*etcd.*\u0026#34;}[15m]) \u0026gt; 5 for: 15m labels: severity: warning - alert: etcdHighFsyncDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: 99th percentile fync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) \u0026gt; 0.5 for: 10m labels: severity: warning - alert: etcdHighCommitDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) \u0026gt; 0.25 for: 10m labels: severity: warning - alert: etcdHighNodeRTTDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: node RTT durations {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99,rate(etcd_network_peer_round_trip_time_seconds_bucket[5m])) \u0026gt; 0.5 for: 10m labels: severity: warning 实际上只需要对前面提到的几个核心指标配置上告警规则即可：\n​sum(up{job=~\u0026quot;.\\*etcd.\\*\u0026quot;} == bool 1) by (job) \u0026lt; ((count(up{job=~\u0026quot;.\\*etcd.\\*\u0026quot;}) by (job) + 1) / 2) ​ ： 当前存活的 etcd 节点数是否小于 (n+1)/2\n集群中存活小于 (n+1)/2 那么整个集群都会不可用\n​etcd_server_has_leader{job=~\u0026quot;.\\*etcd.\\*\u0026quot;} == 0​ ：etcd 是否存在 leader\n为 0 则表示不存在 leader，同样整个集群不可用 ​rate(etcd_server_leader_changes_seen_total{job=~\u0026quot;.\\*etcd.\\*\u0026quot;}[15m]) \u0026gt; 3​ : 15 分钟 内集群 leader 切换次数是否超过 3 次\n频繁 Leader 切换会影响集群稳定性 ​histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\u0026quot;.\\*etcd.\\*\u0026quot;}[5m])) \u0026gt; 0.5​ ：5 分钟内 WAL fsync 调用延迟 p99 大于 500ms\n​histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\u0026quot;.\\*etcd.\\*\u0026quot;}[5m])) \u0026gt; 0.25​ ：5 分钟内 DB fsync 调用延迟 p99 大于 500ms\n​histogram_quantile(0.99,rate(etcd_network_peer_round_trip_time_seconds_bucket[5m])) \u0026gt; 0.5​： 5 分钟内 节点之间 RTT 大于 500 ms\n只包含上述指标告警策略的精简版\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # these rules synced manually from https://github.com/etcd-io/etcd/blob/master/Documentation/etcd-mixin/mixin.libsonnet groups: - name: etcd rules: - alert: etcdInsufficientMembers annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: insufficient members ({{ $value }}).\u0026#39; expr: | sum(up{job=~\u0026#34;.*etcd.*\u0026#34;} == bool 1) by (job) \u0026lt; ((count(up{job=~\u0026#34;.*etcd.*\u0026#34;}) by (job) + 1) / 2) for: 3m labels: severity: critical - alert: etcdNoLeader annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: member {{ $labels.instance }} has no leader.\u0026#39; expr: | etcd_server_has_leader{job=~\u0026#34;.*etcd.*\u0026#34;} == 0 for: 1m labels: severity: critical - alert: etcdHighFsyncDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: 99th percentile fync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) \u0026gt; 0.5 for: 10m labels: severity: warning - alert: etcdHighCommitDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) \u0026gt; 0.25 for: 10m labels: severity: warning - alert: etcdHighNodeRTTDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: node RTT durations {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99,rate(etcd_network_peer_round_trip_time_seconds_bucket[5m])) \u0026gt; 0.5 for: 10m labels: severity: warning ‍\n创建 PrometheusRule\n使用 PrometheusRule 对象来存储这部分规则。\n比较重要的是 label，后续会根据 label 来关联到此告警规则。\n这里的 label 和默认生成的内置 PrometheusRule 对象 label 一致，这样就不会影响到其他 rule。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 cat \u0026gt; pr.yaml \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: labels: prometheus: k8s role: alert-rules name: etcd-rules namespace: monitoring spec: groups: - name: etcd rules: - alert: etcdInsufficientMembers annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: insufficient members ({{ $value }}).\u0026#39; expr: | sum(up{job=~\u0026#34;.*etcd.*\u0026#34;} == bool 1) by (job) \u0026lt; ((count(up{job=~\u0026#34;.*etcd.*\u0026#34;}) by (job) + 1) / 2) for: 3m labels: severity: critical - alert: etcdNoLeader annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: member {{ $labels.instance }} has no leader.\u0026#39; expr: | etcd_server_has_leader{job=~\u0026#34;.*etcd.*\u0026#34;} == 0 for: 1m labels: severity: critical - alert: etcdHighFsyncDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: 99th percentile fsync durations are {{ $value }}s(normal is \u0026lt; 10ms) on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) \u0026gt; 0.5 for: 3m labels: severity: warning - alert: etcdHighCommitDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: 99th percentile commit durations {{ $value }}s(normal is \u0026lt; 120ms) on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;}[5m])) \u0026gt; 0.25 for: 3m labels: severity: warning - alert: etcdHighNodeRTTDurations annotations: message: \u0026#39;etcd cluster \u0026#34;{{ $labels.job }}\u0026#34;: node RTT durations {{ $value }}s on etcd instance {{ $labels.instance }}.\u0026#39; expr: | histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m])) \u0026gt; 0.5 for: 3m labels: severity: warning EOF kubectl -n monitoring apply -f pr.yaml 配置到 Prometheus\n​kubectl -n monitoring get prometheus​\n对上述对象进行修改，添加 ruleSelector 字段来通过 label 筛选前面的告警规则。\n1 2 3 4 5 6 7 8 查看配置文件 kubectl get prometheuses.monitoring.coreos.com -n monitoring k8s -oyaml ruleSelector: matchLabels: prometheus: k8s role: alert-rules ​​\n​​\n‍\n参考文档：\nhttp://www.mydlq.club/article/10\nhttps://www.lixueduan.com/posts/etcd/17-monitor/\n‍\n","date":"2024-01-18T09:12:41Z","image":"https://www.ownit.top/title_pic/13.jpg","permalink":"https://www.ownit.top/p/202401180912/","title":"Kube-Prometheus 手动部署"},{"content":"1、Istio简介 Istio 是一个开源服务网格，它透明地分层到现有的分布式应用程序上。 Istio 强大的特性提供了一种统一和更有效的方式来保护、连接和监视服务。 Istio 是实现负载平衡、服务到服务身份验证和监视的路径——只需要很少或不需要更改服务代码。它强大的控制平面带来了重要的特点，包括：\n使用 TLS 加密、强身份认证和授权的集群内服务到服务的安全通信 自动负载均衡的 HTTP, gRPC, WebSocket，和 TCP 流量 通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行细粒度控制 一个可插入的策略层和配置 API，支持访问控制、速率限制和配额 对集群内的所有流量(包括集群入口和出口)进行自动度量、日志和跟踪 Istio 主要由两部分组成，分别是数据平面和控制平面。\n数据平面：数据平面或者数据层是由代理服务的集合所组成的，它们会使用扩展的 Envoy 代理服务器，表现形式是每个 Kubernetes pod 中的 sidecar 容器。这些 sidecar 会协调和控制所有微服务之间的网络通信，同时还会收集和报告有用的遥测数据。 控制平面：控制平面或者控制层由一个名为 istiod 的二进制文件组成，负责将高层级的路由规则和流量控制行为转换成 Envoy 的特定配置，然后在运行时将它们传播到 sidecar 中。除此之外，控制平面还提供安全措施，通过内置的身份标识和证书管理，实现强大的服务间和终端用户认证，同时根据服务的身份标识执行安全策略。 2、Istio 核心功能 Istio 是一个与 Kubernetes 紧密结合的服务网格(Service Mesh)，用于服务治理。\n注意，Istio 是用于服务治理的，主要有流量管理、服务间安全、可观测性这几种功能。在微服务系统中，会碰到很多棘手的问题，Istio 只能解决其中一小部分。\n服务治理有三种方式，第一种是每个项目中包含单独的治理逻辑，这样比较简单；第二种是将逻辑封装到 SDK 中，每个项目引用 SDK 即可，不增加或只需要少量配置或代码即可；第三种是下沉到基础设施层。Istio 便是第三种方式。 3、Istio 原理 Istio 可以的作用原理是拦截 Kubernetes 部署 Pod 的事件，然后从 Pod 中注入一个名为 Envoy 的容器，这个容器会拦截外部到业务应用的流量。由于所有流量都被 Envoy “劫持” 了，所以 Istio 可以对流量进行分析例如收集请求信息，以及一系列的流量管理操作，也可以验证授权信息。当 Envoy 拦截流量并执行一系列操作之后，如果请求没问题，就会转发流量到业务应用的 Pod 中。 【左：普通 Pod；Istio；右：Istio 代理了出入口流量】\n当然，由于 Envoy 需要拦截流量之后转发给业务应用，这样就多了一层转发，会导致系统响应速度会有所下降，但是增加的响应时间几乎可以忽略不计。\n每个 Pod 都有一个 Envoy 负责拦截、处理和转发进出 Pod 的所有网络流量，这种方式被称为 Sidecar。\n以下是 Istio Sidecar 的一些主要功能：\n流量管理：Envoy 代理可以根据 Istio 配置的路由规则（如 VirtualService 和 DestinationRule）实现流量的转发、分割和镜像等功能。 安全通信：Envoy 代理负责在服务之间建立安全的双向 TLS 连接，确保服务间通信的安全性。 遥测数据收集：Envoy 代理可以收集关于网络流量的详细遥测数据（如延迟、成功率等），并将这些数据上报给 Istio 的遥测组件，以便进行监控和分析。 策略执行：Envoy 代理可以根据 Istio 配置的策略规则（如 RateLimit 和 AuthorizationPolicy）执行限流、访问控制等策略。 由于 Pod 是通过 Envoy 暴露端口的，所有进出口流量都需要经过 Envoy 的检查，所以很容易判断访问来源，如果请求方不是在 Istio 中的服务，那么 Envoy 便会拒绝访问。 在 Istio 中，Envoy 这一块称为数据平面，而负责管理集群的 istiod 组件称为控制平面。\n注意，这里是 istiod ，是 Istio 负责管理集群的一种组件。\n4、Istio 安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 istioctl(可以通过curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.1 sh 安装) [root@bt ~]# istioctl profile list Istio configuration profiles: ambient default demo empty external minimal openshift preview remote 或者 安装profile=demo的 istio istioctl manifest apply --set profile=demo \\ --set cni.enabled=true --set cni.components.cni.namespace=kube-system \\ --set values.gateways.istio-ingressgateway.type=ClusterIP 设置自动 istio 注入，可以通过 namespace 去做\n1 2 3 4 5 #给ns1的命名空间添加istio标签，那么在这个标签里，所有创建的 pod 都会被自动注入 istio kubectl label ns ns1 istio-injection=enabled # 查看labels kubectl get ns --show-labels 使用 istio 单独的注入一个pod\n1 2 3 4 5 6 7 8 9 10 11 12 # 创建 istioctl kube-inject -f pod1.yaml | kubectl apply -f - # 查看pod数量 kubectl get pods #此时READY数为2 #这个pod新增的docker，就是 pilot，envoy ## 在创建一个 pod 测试 sed \u0026#39;s/pod1/pod2/\u0026#39; pod1.yaml | kubectl apply -f - kubectl get pods # 发现这个 pod2 是没有被注入的 安装kiali ：图形化工具，可以直观的查看流量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubectl get pods -n istio-system # 安装 kubectl apply -f istio-1.10.3/samples/addons/kiali.yaml # 或者可以选择直接全部都安装上 kubectl apply -f istio-1.10.3/samples/addons/ kubectl get svc -n istio-system # kiali 的PORT：20001，TYPE为 ClusterIP # 修改 kiali 的 svc kubectl edit svc kiali -n istio-system sessionAffinity: None type: LoadBalancer #把ClusterIP修改为NodePort或者LoadBalancer # 查看并验证 kubectl get svc -n istio-system # 查看 kiali 的 ip 和端口，使用浏览器访问 # 进入浏览器后可以测试一下，进入 Graph 页面，选择 istio-system 和 ns1 的 Namespace，可以看到我们现在环境中的拓扑图 Kiali 的 Graph 数据主要来自两个来源：Prometheus 和 Istio 本身的遥测数据。\nPrometheus：Prometheus 是一个开源监控和警报工具，它用于收集和存储 Istio 服务网格中的指标数据。Istio 使用 Envoy 代理收集遥测数据，这些数据随后被 Prometheus 抓取和存储。Kiali 使用这些 Prometheus 数据来生成服务之间的流量、错误率、延迟等指标。\nIstio 遥测数据：Istio 服务网格生成的遥测数据包括请求、响应、延迟以及 Envoy 代理的其他性能指标。这些数据由 Istio 组件（例如 Mixer 和 Pilot）以及 Envoy 代理本身生成。Kiali 从这些遥测数据中获取服务拓扑信息，以创建服务之间的依赖关系图。\nKiali 将这两个数据源的信息整合在一起，生成 Graph，它展示了服务网格的拓扑结构、服务之间的流量以及其他性能指标。这有助于用户更好地理解服务之间的依赖关系，发现潜在的性能问题，并优化服务网格配置。\n可能失败的原因 如果你的 Kiali 一直显示 Empty Graph。请关注以下几种可能的情况：\n集群版本低于 1.23 ，需要升级 Kubernetes 集群。 安装了 Kubesphere，说多了都是泪，Kubesphere 太重了，笔者花了一晚上时间重新安装集群。 访问的地址不正确，没有配置对 /productpage 的访问地址，请求流量没有打入集群。 Pod 没有被注入 istio-proxy。 你可以在 Kiali 的 Workloads 查看每个负载的 Pod 信息，正常情况应当如下所示： 修复 Kiali Grafana 问题 点击右上角的消息，可能会提示配置不正确，因为 kiali 需要从 Grafana 拉取数据。 编辑 configmap 。\n1 2 3 4 5 6 7 8 kubectl edit configmap kiali -n istio-system grafana: \\n enabled: true \\n url: \\\u0026#34;http://grafana.istio-system.svc.cluster.local:3000\\\u0026#34; \\ \\n in_cluster_url: \\\u0026#34;http://grafana.istio-system.svc.cluster.local:3000\\\u0026#34;\\n 如果上方不行，就添加下方 grafana: \\n enabled: true \\n url: \\\u0026#34;http://grafana.istio-system.svc.cluster.local:3000\\\u0026#34; 如果使用的是可视化工具，添加就简单了。\n1 2 3 4 grafana: enabled: true url: \u0026#34;http://grafana.istio-system.svc.cluster.local:3000\u0026#34; in_cluster_url: \u0026#34;http://grafana.istio-system\u0026gt;.svc.cluster.local:3000\u0026#34; 然后使用 kubectl describe configmap kiali -n istio-system 查看配置是否正确。\n5、Istio核心资源 和Kubernetes资源一致，Istio的配置也是通过声明式自定义资源配置来加载的。常用的核心资源有VirtualService、DestinationRule、Gateway、ServiceEntry、Sidecar等。\nGateway 类似 Nginx 需要创建一个反向代理时需要绑定的域名配置。\nIstio VistualService 中可以限制外部能够访问的路由地址，\nDestinationRule 则可以配置访问的 Pod 策略。\n可以为 Istio VistualService 绑定一个 Istio DestinationRule\n通过 DestinationRule 我们还可以定义版本子集等，通过更加丰富的策略转发流量。 istio-ingressgateway（邮局的入口）： 当一个请求（邮件）到达你的应用时，它首先会到达istio-ingressgateway。这就像一个邮局的入口，所有的邮件都必须先经过这里。 Gateway（邮局的接收员）： Gateway就像是邮局的接收员，它的工作是检查每个进来的请求（邮件），看看它应该被发送到哪里。在你的例子中，Gateway会把所有的请求都发送到bookinfo服务。 VirtualService（邮局的分拣员）： 当请求（邮件）到达bookinfo服务时，VirtualService就像是一个分拣员，它会根据请求的路径或者其他属性，决定请求应该被发送到哪个服务。在你的例子中，productpage的VirtualService可能会把请求发送到productpage服务。 DestinationRule（邮局的包装员）： 当请求（邮件）被发送到一个特定的服务时，DestinationRule就像是一个包装员，它会根据规则，决定如何处理这个请求（邮件）。例如，它可能会决定请求应该被发送到哪个版本的服务，或者请求应该如何被负载均衡。 Envoy（邮局的快递员）： 当productpage服务需要访问其他服务（如reviews）时，它会发送一个请求。这个请求就像是一个新的邮件，它会被Envoy（快递员）拿到，然后根据VirtualService和DestinationRule的规则，被发送到正确的服务。 在 Istio 中，VirtualService 和 DestinationRule 是两个关键的自定义资源定义（CRD），它们用于配置和控制服务间的流量路由。\n它们之间的关系可以概括为：\nVirtualService 定义了流量的路由规则，\nDestinationRule 定义了流量到达目的地后如何进行负载分发和连接池管理。\nVirtualService 用于定义流量的路由规则。 当请求从一个服务到另一个服务时，VirtualService 可以指定如何将流量路由到不同的目的地（例如，不同的服务实例，版本或子集）。VirtualService 还可以根据请求的属性（如请求头、路径、来源等）对流量进行匹配和分发。此外，VirtualService 可以配置复杂的路由行为，如重试、超时和故障注入等。\nDestinationRule 被用于控制流量的分发和连接池管理。 DestinationRule 定义了服务的子集（即服务的不同版本或变体），并指定如何根据负载均衡策略（如轮询、随机、最少连接等）将流量分发到这些子集。此外，DestinationRule 还可以配置连接池设置（如最大连接数、空闲超时等）和传输层安全策略（如 TLS 设置）。\n总之，VirtualService 和 DestinationRule 在 Istio 中共同实现了流量的精细控制。VirtualService 用于定义流量的路由规则，而 DestinationRule 则负责处理流量到达目的地后的负载分发和连接池管理。 Istio 的做法是 Gateway 监控入口流量， 通过 VirtualService 设置流量进入的策略，并指向 Service。 而 DestinationRule 则定义了流量流向 Pod 的策略。\nVirtualService VirtualService（虚拟服务）和Kubernetes的Service类似，但是两种并不是对等的资源类型。VirtualService基于Istio和对应平台提供的基本连通性和服务发现能力，将请求路由到对应的目标。每一个VirtualService包含一组路由规则，Istio将每个请求根据路由匹配到指定的目标地址。\n和Kubernetes的Service不同的是，Kubernetes的Service只提供了最简单的服务发现和负载均衡的能力，如果想要实现更加细粒度的流量分发，比如灰度、蓝绿等流量管理，Kubernetes的Service显得比较吃力或者无法实现，而VirtualService在流量管理方面有着比较好的灵活性和有效性，可以在代码零侵入的情况下实现更加丰富的流量管理，比如灰度等。\n一个典型的用例是将流量发送到被指定服务的不同版本，比如80%的流量发送给v1版本，20%的流量发送给新版本。或者将某个登录用户指定到新版本，其他用户指定到旧版本，可以实现AB测试等功能。\n接下来看一个VirtualService的配置示例，根据特定用户将流量分发至不同版本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v3 上面参数说明:\napiVersion：对应的API版本 kind：创建的资源类型，和Kubernetes的Service类似 metadata：元数据，和Kubernetes资源类似，可以定义annotations、labels、name等 metadata.name：VirtualService的名称 spec：关于VirtualService的定义 spec.hosts：VirtualService的主机，即用户指定的目标或路由规则的目标，客户端向服务端发送请求时使用的一个或多个地址，可以是IP地址、DNS名称，或者是依赖于底层平台的一个简称（比如Kubernetes的Service短名称），隐式或显式地指向一个完全限定域名（FQDN），当然也可以是一个通配符\u0026quot;*\u0026quot; spec.http：路由规则配置，用来指定流量的路由行为，通过该处的配置将HTTP/1.1、HTTP2和gRPC等流量发送到hosts字段指定的目标 spec.http[].match：路由规则的条件，可以根据一些条件制定更加细粒度的路由。比如当前示例的headers，表示匹配headers里面的end-user字段，并且值为jason的请求 route：路由规则，destination字段指定了符合此条件的流量的实际目标地址。比如该示例的请求，如果请求头包含end-user=jason字段，则该请求会被路由到reviews的v2版本。如果没有匹配到该请求头，则流量会被路由到v3版本（版本由DestinationRule划分） 注意：VirtualService路由规则按照从上往下的顺序进行匹配，第一个规则有最高的优先级，如果不满足第一个路由规则，则流量会选择下一个规则。\n除了上述的路由匹配外，VirtualService也支持域名+路径的方式进行路由。比如后端有两个服务，一个是reviews，通过http://​**bookinfo.com/reviews访问；另一个是ratings，通过http://​bookinfo.com/ratings**访问。此时可以配置VirtualService如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews spec: hosts: - bookinfo.com http: - match: - uri: prefix: /reviews route: - destination: host: reviews - match: - uri: prefix: /ratings route: - destination: host: ratings DestinationRule 将VirtualService理解为Kubernetes Service层面，DestinationRule理解为Service后端真实的目标地址，即VirtualService用于Service层面的路由管控，DestinationRule用于对后端真实的服务再做进一步的划分。比如存在一个Service名为paycenter，指向后端多个paycenter的Pod（该Pod可能是不同的Deployment创建的），而DestinationRule可以对后端的多个Pod区分新旧版本，划分成不同的subnet，之后VirtualService可以针对不同的版本进行流量管控。\n在下面的示例中，目标规则为 my-svc 目标服务配置了 3 个具有不同负载均衡策略的子集：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: my-destination-rule spec: host: my-svc trafficPolicy: loadBalancer: simple: RANDOM subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v3 labels: version: v3 上面参数说明：\ntrafficPolicy：这是一个用于定义流量策略的部分。在这个例子中，定义了三个子集（subsets）的流量策略 loadBalancer：用于指定负载均衡算法的配置 subsets：这里定义了三个子集（v1、v2、v3），每个子集通过 labels 来选择对应版本的服务实例。第一个子集 v1 对应版本为 v1 的服务实例，并使用 simple: RANDOM 负载均衡策略。这意味着请求将随机路由到 v1 子集中的服务实例；第二个子集 v2 对应版本为 v2 的服务实例，并使用 simple: ROUND_ROBIN 负载均衡策略。这意味着请求将以循环方式（Round Robin）路由到 v2 子集中的服务实例；第三个子集 v3 对应版本为 v3 的服务实例。在这里没有指定负载均衡策略，因此将使用默认的负载均衡策略。 每个子集都是基于一个或多个 labels 定义的，在 Kubernetes 中它是附加到像 Pod 这种对象上的键/值对。这些标签应用于 Kubernetes 服务的 Deployment 并作为 metadata 来识别不同的版本。\n除了定义子集之外，此目标规则对于所有子集都有默认的流量策略，而对于该子集， 则有特定于子集的策略覆盖它。定义在 subsets 上的默认策略，为 v1 和 v3 子集设置了一个简单的随机负载均衡器。在 v2 策略中，轮询负载均衡器被指定在相应的子集字段上\n默认情况下，Istio 使用轮询的负载均衡策略，实例池中的每个实例依次获取请求。Istio 同时支持如下的负载均衡模型， 可以在 DestinationRule 中为流向某个特定服务或服务子集的流量指定这些模型。\n随机：请求以随机的方式转发到池中的实例 权重：请求根据指定的百分比转发到池中的实例 最少请求：请求被转发到最少被访问的实例 Gateway Istio同样支持网关功能，可以使用Gateway在网格最外层接收HTTP/TCP流量，并将流量转发到网格内的某个服务。\n在安装Istio时，可以在istio-system命名空间下安装ingressgateway的Pod，用来充当Ingress Gateway。其中Ingress Gateway为入口网关，可以将网格内的服务“暴露”出去，一般和VirtualService配置使用，并配置一个可以被外部服务访问的域名，从而外部服务可以通过该域名访问网格内的服务。\n配置Gateway和Istio其他资源类似，kind指定为Gateway即可。比如配置一个VirtualService和Gateway实现对网格内的某个服务进行发布，首先创建一个Gateway，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;httpbin.example.com\u0026#34; 上面参数说明：\nselector：必选字段。选择由哪个ingressgateway的Pod发布服务，默认为istio-system命名空间下具有istio=ingressgateway标签的Pod servers：必选字段。表示发布的服务列表，用于描述发布服务的属性，比如代理监听的端口、协议和端口的名称等 hosts：必选字段。Gateway发布的服务地址，也就是允许用户访问的域名，可以配置为“*”，表示任何域名都可以被代理，本示例为http://httpbin.example.com可被路由 之后配置一个VirtualService与之匹配，即可通过该域名访问VirtualService配置的服务。比如将http://httpbin.example.com的/status和/delay代理到httpbin服务的8000端口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin.example.com\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: host: httpbin port: number: 8000 之后将域名解析至ingressgateway Pod的Service上即可访问该域名。\n","date":"2024-01-12T17:19:48Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202401121719/","title":"Istio安装和基础原理"},{"content":"背景 在本地集群进行测试时，我们常常面临一个棘手的问题：Service Type不支持LoadBalancer，而我们只能选择使用NodePort作为替代。这种情况下，我们通常会配置Service为NodePort，并使用externalIPs将流量导入Kubernetes环境。然而，这些解决方案都存在明显的缺陷，使得在私有环境部署Kubernetes的用户在这个生态中感觉自己像是二等公民。\n值得注意的是，Kubernetes默认并未提供负载均衡器的实现。在Kubernetes生态中，网络负载均衡器的实现通常依赖于各种IaaS平台，如Google Cloud Platform (GCP)、Amazon Web Services (AWS)、Azure等。因此，如果你不在这些IaaS平台上运行Kubernetes，你创建的Service中的type: LoadBalancers将一直处于Pending状态，如下所示：\n1 2 3 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.93.15.64 \u0026lt;pending\u0026gt; 8080:31322/TCP,443:31105/TCP 1h 这种情况下，我们急需一个解决方案，一个可以在本地环境中实现负载均衡的解决方案。这就是我们今天要介绍的主角——MetalLB。\nMetallb 通过标准路由协议能解决该问题。MetalLB 也是 CNCF 的沙箱项目，最早发布在 https://github.com/google/metallb 开发，后来迁移到 https://github.com/metallb/metallb 中。\n原理 Metallb 会在 Kubernetes 内运行，监控服务对象的变化，一旦察觉有新的LoadBalancer 服务运行，并且没有可申请的负载均衡器之后，\nMetalLB 通过 MetalLB hooks 为 Kubernetes 中提供网络负载均衡器的实现。简单的说，它允许你在非云供应商提供的（私有的） Kubernetes 中创建 type: LoadBalancer 的 services。\nMetalLB 有两大功能：\n地址分配：在 IaaS 平台申请 LB，会自动分配一个公网 IP，因此，MetalLB也需要管理 IP 地址的分配工作 IP 外部声明：当 MetalLB 获取外部地址后，需要对外声明该 IP 在 k8s 中使用，并可以正常通信。 IP 声明可以使用 ARP、 NDP、或 BGP 协议，因此 MetalLB 有两种工作模式：\nBGP 工作模式，使用 BGP 协议分配地址池 L2 工作模式，使用 ARP/NDP 协议分配地址池 Layer2模式 在2层模式下，Metallb会在Node节点中选出一台作为Leader，与服务IP相关的所有流量都会流向该节点。在该节点上， kube-proxy将接收到的流量传播到对应服务的Pod。当leader节点出现故障时，会由另一个节点接管。从这个角度来看，2层模式更像是高可用，而不是负载均衡，因为同时只能在一个节点负责接收数据。\n在二层模式中会存在以下两种局限性：单节点瓶颈和故障转移慢的情况。\n由于Layer 2 模式会使用单个选举出来的Leader来接收服务IP的所有流量，这就意味着服务的入口带宽被限制为单个节点的带宽，单节点的流量处理能力将成为整个集群的接收外部流量的瓶颈。\n在故障转移方面，目前的机制是MetalLB通过发送2层数据包来通知各个节点，并重新选举Leader，这通常能在几秒内完成。但如果是计划外的事故导致的，此时在有故障的客户端刷新其缓存条目之前，将无法访问服务IP。\nBGP模式 BGP模式是真正的负载均衡，该模式需要路由器支持BGP协议 ，群集中的每个节点会与网络路由器建议基于BGP的对等会话，并使用该会话来通告负载均衡的IP。MetalLB发布的路由彼此等效，这意味着路由器将使用所有的目标节点，并在它们之间进行负载平衡。数据包到达节点后，kube-proxy负责流量路由的最后一跳，将数据包发送到对应服务的Pod。\n负载平衡的方式取决于您特定的路由器型号和配置，常见的有基于数据包哈希对每个连接进行均衡，这意味着单个TCP或UDP会话的所有数据包都将定向到群集中的单个计算机。\nBGP模式也存在着自身的局限性，该模式通过对数据包头中的某些字段进行哈希处理，并将该哈希值用作后端数组的索引，将给定的数据包分配给特定的下一跳。但路由器中使用的哈希通常不稳定，因此只要后端节点数量发生变化时，现有连接就会被随机地重新哈希，这意味着大多数现有连接将被转发到另一后端，而该后端并不清楚原有的连接状态。为了减少这种麻烦，建议使用更加稳定的BGP算法，如：ECMP散列算法。\n系统要求 在开始部署MetalLB之前，我们需要确定部署环境能够满足最低要求：\n一个k8s集群，要求版本不低于1.13.0，且没有负载均衡器相关插件 k8s集群上的CNI组件和MetalLB兼容 预留一段IPv4地址给MetalLB作为LoadBalance的VIP使用 如果使用的是MetalLB的BGP模式，还需要路由器支持BGP协议 如果使用的是MetalLB的Layer2模式，因为使用了memberlist算法来实现选主，因此需要确保各个k8s节点之间的7946端口可达（包括TCP和UDP协议），当然也可以根据自己的需求配置为其他端口 安装要求 准备 （根据情况而定） 如果你在 IPVS 模式下使用 kube-proxy，从 Kubernetes v1.14.2 开始，必须启用严格 ARP 模式。\n注意，如果使用 kube-router 作为 service-proxy，则不需要这样做，因为它默认启用严格 ARP。\n部署Layer2模式需要把k8s集群中的ipvs配置打开strictARP，开启之后k8s集群中的kube-proxy会停止响应kube-ipvs0网卡之外的其他网卡的arp请求，而由MetalLB接手处理。\nstrict ARP开启之后相当于把 将 arp_ignore 设置为 1 并将 arp_announce 设置为 2 启用严格的 ARP，这个原理和LVS中的DR模式对RS的配置一样，可以参考之前的文章中的解释。\n可以通过编辑当前集群的 kube-proxy 配置来实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 查看kube-proxy中的strictARP配置 $ kubectl get configmap -n kube-system kube-proxy -o yaml | grep strictARP strictARP: false # 手动修改strictARP配置为true $ kubectl edit configmap -n kube-system kube-proxy configmap/kube-proxy edited # 使用命令直接修改并对比不同 $ kubectl get configmap kube-proxy -n kube-system -o yaml | sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | kubectl diff -f - -n kube-system # 确认无误后使用命令直接修改并生效 $ kubectl get configmap kube-proxy -n kube-system -o yaml | sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | kubectl apply -f - -n kube-system # 重启kube-proxy确保配置生效 $ kubectl rollout restart ds kube-proxy -n kube-system # 确认配置生效 $ kubectl get configmap -n kube-system kube-proxy -o yaml | grep strictARP strictARP: true 部署MetalLB(Layer2模式) MetalLB的部署也十分简单，官方提供了manifest文件部署（yaml部署），helm3部署和Kustomize部署三种方式，这里我们还是使用manifest文件部署。\n大多数的官方教程为了简化部署的步骤，都是写着直接用kubectl命令部署一个yaml的url，这样子的好处是部署简单快捷，但是坏处就是本地自己没有存档，不方便修改等操作，因此我个人更倾向于把yaml文件下载到本地保存再进行部署\n1 2 3 4 5 6 7 # 下载v0.12.1的两个部署文件 $ wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml $ wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml # 如果使用frr来进行BGP路由管理，则下载这两个部署文件 $ wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml $ wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb-frr.yaml 下载官方提供的yaml文件之后，我们再提前准备好configmap的配置，github上面有提供一个参考文件，layer2模式需要的配置并不多，这里我们只做最基础的一些参数配置定义即可：\nprotocol这一项我们配置为layer2 addresses这里我们可以使用CIDR来批量配置（198.51.100.0/24），也可以指定首尾IP来配置（192.168.0.150-192.168.0.200），指定一段和k8s节点在同一个子网的IP。也可以指定宿主机的网络分配。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.102.50-192.168.102.60 还可以指定多个网段\n1 2 3 addresses: - 192.168.12.0/24 - 192.168.144.0/20 除了自动分配IP外，Metallb 还支持在定义服务的时候，通过 spec.loadBalancerIP 指定一个静态IP 。\n使用kubectl log -f [matellb-contoller-pod]能看到配置更新过程\n接下来就可以开始进行部署，整体可以分为三步：\n部署namespace 部署deployment和daemonset 配置configmap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # 创建namespace $ kubectl apply -f namespace.yaml namespace/metallb-system created $ kubectl get ns NAME STATUS AGE default Active 8d kube-node-lease Active 8d kube-public Active 8d kube-system Active 8d metallb-system Active 8s nginx-quic Active 8d # 部署deployment和daemonset，以及相关所需的其他资源 $ kubectl apply -f metallb.yaml Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ podsecuritypolicy.policy/controller created podsecuritypolicy.policy/speaker created serviceaccount/controller created serviceaccount/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created role.rbac.authorization.k8s.io/config-watcher created role.rbac.authorization.k8s.io/pod-lister created role.rbac.authorization.k8s.io/controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created rolebinding.rbac.authorization.k8s.io/config-watcher created rolebinding.rbac.authorization.k8s.io/pod-lister created rolebinding.rbac.authorization.k8s.io/controller created daemonset.apps/speaker created deployment.apps/controller created # 这里主要就是部署了controller这个deployment来检查service的状态 $ kubectl get deploy -n metallb-system NAME READY UP-TO-DATE AVAILABLE AGE controller 1/1 1 1 86s # speaker则是使用ds部署到每个节点上面用来协商VIP、收发ARP、NDP等数据包 $ kubectl get ds -n metallb-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE speaker 3 3 3 3 3 kubernetes.io/os=linux 64s $ kubectl get pod -n metallb-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES controller-55bbcf48c8-qst68 1/1 Running 0 2d19h 172.20.28.118 node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-8wctf 1/1 Running 0 2d19h 192.168.102.41 node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-txdsv 1/1 Running 0 2d19h 192.168.102.30 master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-xdzrr 1/1 Running 0 2d19h 192.168.102.40 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl apply -f configmap-layer2.yaml configmap/config created kubectl wait --for=condition=Ready pods --all -n metallb-system # pod/controller-57fd9c5bb-d5z9j condition met # pod/speaker-6hz2h condition met # pod/speaker-7pzb4 condition met # pod/speaker-trr9v condition met 部署测试服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # mentallb/whoami.yaml apiVersion: apps/v1 kind: Deployment metadata: name: whoami labels: app: containous name: whoami spec: replicas: 2 selector: matchLabels: app: containous task: whoami template: metadata: labels: app: containous task: whoami spec: containers: - name: containouswhoami image: containous/whoami resources: ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: whoami spec: ports: - name: http port: 80 selector: app: containous task: whoami type: LoadBalancer 或者 nginx\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.23-alpine ports: - name: http containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx spec: #loadBalancerIP: x.y.z.a # 指定公网IP ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: nginx type: LoadBalancer 可见分配的 EXTERNAL-IP 为 192.168.102.52\n分别在虚拟机和宿主机试试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 curl 192.168.102.52 [root@bt metallb]# curl 192.168.102.52 Hostname: whoami-665585b57c-kngqc IP: 127.0.0.1 IP: 172.20.28.123 RemoteAddr: 127.0.0.6:47635 GET / HTTP/1.1 Host: 192.168.102.52 User-Agent: curl/7.29.0 Accept: */* X-B3-Sampled: 1 X-B3-Spanid: 0268f13fd66b2858 X-B3-Traceid: c6984aca633971850268f13fd66b2858 X-Forwarded-Proto: http X-Request-Id: 6664e21c-b2d4-9abe-af63-6d79485cbdd8 这时如果你 ping 这个 ip 的话，会发现无法 ping 通\n这个是正常的，因为它是一个虚拟IP地址，所以根本无法ping 通（此虚拟IP与物理网卡共用同一个 MAC 地址，那么这个IP是如何工作的呢？又是如何收到流量请求的呢？很值得思考）。\n那如何测试这个虚拟IP是否能正常提供服务呢，其实只需要使用 telnet 命令就可以了，如\n1 2 3 4 [root@bt metallb]# telnet 192.168.102.52 80 Trying 192.168.102.52... Connected to 192.168.102.52. Escape character is \u0026#39;^]\u0026#39;. 注意：\n注意：并非所有的LoadBalancer都允许设置 loadBalancerIP。\n如果LoadBalancer支持该字段，那么将根据用户设置的 loadBalancerIP 来创建负载均衡器。\n如果没有设置 loadBalancerIP 字段，将会给负载均衡器指派一个临时 IP。\n如果设置了 loadBalancerIP，但LoadBalancer并不支持这种特性，那么设置的 loadBalancerIP 值将会被忽略掉。\n我们在创建LoadBalancer服务的时候，默认情况下k8s会帮我们自动创建一个nodeport服务，这个操作可以通过指定Service中的allocateLoadBalancerNodePorts字段来定义开关，默认情况下为true\n不同的loadbalancer实现原理不同，有些是需要依赖nodeport来进行流量转发，有些则是直接转发请求到pod中。对于MetalLB而言，是通过kube-proxy将请求的流量直接转发到pod，因此我们需要关闭nodeport的话可以修改service中的spec.allocateLoadBalancerNodePorts字段，将其设置为false，那么在创建svc的时候就不会分配nodeport。\n但是需要注意的是如果是对已有service进行修改，关闭nodeport（从true改为false），k8s不会自动去清除已有的ipvs规则，这需要我们自行手动删除。\n我们重新定义创建一个svc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Service metadata: name: nginx-lb-service namespace: nginx-quic spec: allocateLoadBalancerNodePorts: false externalTrafficPolicy: Cluster internalTrafficPolicy: Cluster selector: app: nginx-lb ports: - protocol: TCP port: 80 # match for service access port targetPort: 80 # match for pod access port type: LoadBalancer loadBalancerIP: 10.31.8.100 此时再去查看对应的svc状态和ipvs规则会发现已经没有nodeport相关的配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.8.62.180:80 rr -\u0026gt; 10.8.65.18:80 Masq 1 0 0 -\u0026gt; 10.8.65.19:80 Masq 1 0 0 -\u0026gt; 10.8.66.14:80 Masq 1 0 0 -\u0026gt; 10.8.66.15:80 Masq 1 0 0 TCP 10.31.8.100:80 rr -\u0026gt; 10.8.65.18:80 Masq 1 0 0 -\u0026gt; 10.8.65.19:80 Masq 1 0 0 -\u0026gt; 10.8.66.14:80 Masq 1 0 0 -\u0026gt; 10.8.66.15:80 Masq 1 0 0 $ kubectl get svc -n nginx-quic NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-lb-service LoadBalancer 10.8.62.180 10.31.8.100 80/TCP 23s 如果是把已有服务的spec.allocateLoadBalancerNodePorts从true改为false，原有的nodeport不会自动删除，因此最好在初始化的时候就规划好相关参数\n1 2 3 4 5 $ kubectl get svc -n nginx-quic nginx-lb-service -o yaml | egrep \u0026#34; allocateLoadBalancerNodePorts: \u0026#34; allocateLoadBalancerNodePorts: false $ kubectl get svc -n nginx-quic NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-lb-service LoadBalancer 10.8.62.180 10.31.8.100 80:31405/TCP 85m 参考文档： https://tinychen.com/20220519-k8s-06-loadbalancer-metallb https://juejin.cn/post/6906447786112516110\n","date":"2023-12-28T09:32:55Z","image":"https://www.ownit.top/title_pic/68.jpg","permalink":"https://www.ownit.top/p/202312280932/","title":"MetalLB：本地Kubernetes集群的LoadBalancer负载均衡利器"},{"content":"使用kubeadm部署一套Kubernetes v1.23.0集群 1、前置知识点 1.1 生产环境可部署Kubernetes集群的两种方式 目前生产部署Kubernetes集群主要有两种方式：\n• kubeadm\nKubeadm是一个K8s部署工具，提供kubeadm init和kubeadm join，用于快速部署Kubernetes集群。\n• 二进制包\n从github下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。\n这里采用kubeadm搭建集群。\nkubeadm工具功能：\n• kubeadm init： 初始化一个Master节点\n• kubeadm join： 将工作节点加入集群\n• kubeadm upgrade： 升级K8s版本\n• kubeadm token： 管理 kubeadm join 使用的令牌\n• kubeadm reset： 清空 kubeadm init 或者 kubeadm join 对主机所做的任何更改\n• kubeadm version： 打印 kubeadm 版本\n• kubeadm alpha： 预览可用的新功能\n1.2 准备环境 服务器要求：\n• 建议最小硬件配置：2核CPU、2G内存、20G硬盘\n• 服务器最好可以访问外网，会有从网上拉取镜像需求，如果服务器不能上网，需要提前下载对应镜像并导入节点\n软件环境：\n软件 版本 操作系统 CentOS7.9_x64 （mini） Docker 20-ce Kubernetes 1.23 服务器规划：\n角色 IP k8s-master 192.168.31.71 k8s-node1 192.168.31.72 k8s-node2 192.168.31.73 架构图：\n​\n1.3 操作系统初始化配置【所有节点】 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 关闭防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭selinux sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config # 永久 setenforce 0 # 临时 # 关闭swap swapoff -a # 临时 sed -ri \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab # 永久 # 根据规划设置主机名 hostnamectl set-hostname \u0026lt;hostname\u0026gt; # 在master添加hosts cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 192.168.31.71 k8s-master 192.168.31.72 k8s-node1 192.168.31.73 k8s-node2 EOF # 将桥接的IPv4流量传递到iptables的链 cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 生效 # 时间同步 yum install ntpdate -y ntpdate time.windows.com 2. 安装Docker/kubeadm/kubelet 【所有节点】\n这里使用Docker作为容器引擎，也可以换成别的，例如containerd\n2.1 安装Docker 1 2 3 wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo yum -y install docker-ce systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 配置镜像下载加速器：\n1 2 3 4 5 6 7 8 9 cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://b9pmyelo.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF systemctl restart docker docker info 2.2 添加阿里云YUM软件源 1 2 3 4 5 6 7 8 9 cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2.3 安装kubeadm，kubelet和kubectl 由于版本更新频繁，这里指定版本号部署：\n1 2 yum install -y kubelet-1.23.0 kubeadm-1.23.0 kubectl-1.23.0 systemctl enable kubelet ‍\n3. 部署Kubernetes Master 在192.168.31.71（Master）执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubeadm init \\ --apiserver-advertise-address=192.168.31.71 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.23.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all pod 172.20.20.0/20 svc 172.21.20.0/20 •\t\u0026ndash;apiserver-advertise-address 集群通告地址\n•\t\u0026ndash;image-repository 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址\n•\t\u0026ndash;kubernetes-version K8s版本，与上面安装的一致\n•\t\u0026ndash;service-cidr 集群内部虚拟网络，Pod统一访问入口\n•\t\u0026ndash;pod-network-cidr Pod网络，与下面部署的CNI网络组件yaml中保持一致\n‍\n初始化完成后，最后会输出一个join命令，先记住，下面用。\n拷贝kubectl使用的连接k8s认证文件到默认路径：\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看工作节点：\n1 2 3 kubectl get nodes NAME STATUS ROLES AGE VERSION localhost.localdomain NotReady control-plane,master 20s v1.23.0 注：由于网络插件还没有部署，还没有准备就绪 NotReady，先继续\n参考资料：\nhttps://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node\n4. 加入Kubernetes Node 在192.168.31.72/73（Node）执行。\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\n1 2 kubeadm join 192.168.31.71:6443 --token 7gqt13.kncw9hg5085iwclx \\ --discovery-token-ca-cert-hash sha256:66fbfcf18649a5841474c2dc4b9ff90c02fc05de0798ed690e1754437be35a01 默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，可以直接使用命令快捷生成：\n1 kubeadm token create --print-join-command 参考资料：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/\n5. 部署容器网络（CNI） Calico是一个纯三层的数据中心网络方案，是目前Kubernetes主流的网络方案。\n下载YAML：\n1 wget https://docs.projectcalico.org/manifests/calico.yaml 下载完后还需要修改里面定义Pod网络（CALICO_IPV4POOL_CIDR），与前面kubeadm\ninit的 \u0026ndash;pod-network-cidr指定的一样。\n修改完后文件后，部署：\n1 2 kubectl apply -f calico.yaml kubectl get pods -n kube-system 等Calico Pod都Running，节点也会准备就绪。\n注：以后所有yaml文件都只在Master节点执行。\n安装目录：/etc/kubernetes/\n组件配置文件目录：/etc/kubernetes/manifests/\n参考资料：https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network\n6. 部署 Dashboard Dashboard是官方提供的一个UI，可用于基本管理K8s资源。\nYAML下载地址：\n1 https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml 课件中文件名是：kubernetes-dashboard.yaml\n默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vi recommended.yaml ... kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard type: NodePort ... kubectl apply -f recommended.yaml kubectl get pods -n kubernetes-dashboard 访问地址：https://NodeIP:30001\n创建service account并绑定默认cluster-admin管理员集群角色：\n1 2 3 4 5 6 7 8 9 10 11 # 创建用户 kubectl create serviceaccount dashboard-admin -n kube-system # 用户授权 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取用户Token kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 使用输出的token登录Dashboard。\n‍\n7.K8S 网络和局域网打通 参考文档：办公环境下 kubernetes 网络互通方案\n在 kubernetes 的网络模型中，基于官方默认的 CNI 网络插件 Flannel，这种 Overlay Network（覆盖网络）可以轻松的实现 pod 间网络的互通。当我们把基于 spring cloud 的微服务迁移到 k8s 中后，无须任何改动，微服务 pod 可以通过 Eureka 注册后可以互相轻松访问。除此之外，我们可以通过 ingress + ingress controller ，在每个节点上，把基于 http 80端口、https 443端口的用户请求流量引入到集群服务中。\n但是实际使用中，我们出现了以下需求：\n1.办公室网络 和 k8s pod 网络不通。开发在电脑完成某个微服务模块开发后，希望本地启动后，能注册到 k8s 中开发环境的服务中心进行调试，而不是本地起一堆依赖的服务。 2.办公室网络 和 k8s svc 网络不通。在 k8s 中运行的 mysql、redis 等，无法通过 ingress 7层暴露，电脑无法通过客户端工具直接访问；如果我们通过 service 的 NodePort 模式，会导致维护量工作量巨大。 网络互通配置 k8s 集群中新加一台配置不高（2核4G）的 node 节点（node-30）专门做路由转发，连接办公室网络和 k8s 集群 pod、svc\nnode-30 IP 地址 192.168.102.30 内网 DNS IP 地址 192.168.102.20 pod 网段 172.20.20.0/20，svc 网段 172.21.20.0/20 办公网段 192.168.0.0/16 给 node-30节点打上污点标签（taints），不让 k8s 调度 pod 来占用资源：\n1 kubectl taint nodes node-30 forward=node-30:NoSchedule node-30节点，做snat：\n1 2 3 4 5 6 7 8 9 10 11 12 # 开启转发 # vim /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 # sysctl -p (kubeadm init --apiserver-advertise-address=192.168.102.30 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.23.0 --service-cidr=172.21.20.0/20 --pod-network-cidr=172.20.20.0/20 --ignore-preflight-errors=all ) # 来自办公室访问pod、service snat SVC iptables -t nat -A POSTROUTING -s 192.168.0.0/16 -d 172.21.20.0/20 -j MASQUERADE POD iptables -t nat -A POSTROUTING -s 192.168.0.0/16 -d 172.20.20.0/20 -j MASQUERADE 在办公室的出口路由器上，设置静态路由，将 k8s pod 和 service 的网段，路由到 node-30 节点上\n1 2 3 4 5 6 7 8 9 路由器上 ip route 172.20.0.0 255.255.255.0 192.168.102.30 ip route 172.21.0.0 255.255.255.0 192.168.102.30 Linux主机 route add -net 172.20.20.0/20 gw 192.168.102.30 route add -net 172.21.20.0/20 gw 192.168.102.30 sudo ip route add 172.20.0.0/16 via 192.168.102.30 dev eth0 ​\nDNS 解析配置 以上步骤操作后，我们就可以在本地电脑通过访问 pod ip 和 service ip 去访问服务。但是在 k8s 中，由于 pod ip 随时都可能在变化，service ip 也不是开发、测试能轻松获取到的。我们希望内网 DNS 在解析 *.cluster.local，去coreDNS寻找解析结果。\n例如，我们约定将（项目A 、开发环境一 、数据库mysql）部署到 ProjectA-dev1 这个 namespace 下，由于本地到 k8s 集群 service 网络已经打通，我们在本地电脑使用 mysql 客户端连接时，只需要填写mysql.ProjectA-dev1.svc.cluster.local即可，DNS 查询请求到了内网DNS后，走向 CoreDNS，从而解析出 service ip。\n由于内网 DNS 在解析 *.cluster.local，需要访问 CoreDNS 寻找解析结果。这就需要保证网络可达\n‍\n方案一， 最简单的做法，我们把内网DNS架设在node-30这台节点上，那么他肯定访问到kube-dns 10.96.0.10\n1 2 # kubectl get svc -n kube-system |grep kube-dns kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 20d 方案二，由于我们实验场景内网DNS IP地址 192.168.102.20 ，并不在node-30上，我们需要打通192.168.102.20 访问 svc网段172.21.20.0/20 即可\n1 2 3 4 5 6 7 #内网DNS（IP 192.168.102.20） 添加静态路由 route add -net 172.21.0.0/16 gw 192.168.102.30 route add -net 172.20.0.0/16 gw 192.168.102.30 # node-30（IP 192.168.102.30） 做snat iptables -t nat -A POSTROUTING -s 192.168.102.30 -d 172.21.0.0/16 -j MASQUERADE iptables -t nat -A POSTROUTING -s 192.168.102.30 -d 172.20.0.0/16 -j MASQUERADE 方案三（实验选择），由于我们实验场景内网DNS IP 192.168.102.20 并不在node-30上，我们可以用nodeSelector在node-30部署 一个nginx ingress controller， 用4层暴露出来coredns 的TCP/UDP 53端口。 ","date":"2023-12-11T10:30:51Z","image":"https://www.ownit.top/title_pic/27.jpg","permalink":"https://www.ownit.top/p/202312111030/","title":"使用kubeadm部署一套Kubernetes v1.23.0集群"},{"content":"Kubernetes安装ingress-nginx 1 、Ingress 简介 1.1 kubernetes访问方式 在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes 目前 提供了以下几种方案：\nK8s集群对外暴露服务的方式目前只有三种：Loadbalancer；NodePort；Ingress\nLoadbalancer 缺点：需要阿里云等公有云支持，而且需要额外支付费用 NodePort 缺点：要暴露端口，端口范围只能是 30000-32767 Ingress 好处：Ingress 不会公开任意端口或协议。可能就是带来一些学习成本，需要了解Traefik和Nginx的常用配置和反向代理。 一图看Ingress流程,由图可知，ingress充当的是代理的角色，把外部来的请求，根据路由地址转发到k8s中匹配到的后端service，而且service又连接了deployment，一个deployment又跑了N个Pod，达到了流量转发的目的。\n‍\nservice只能通过四层负载就是ip+端口的形式来暴露\nNodePort：会占用集群机器的很多端口，当集群服务变多的时候，这个缺点就越发明显 LoadBalancer：每个Service都需要一个LB，比较麻烦和浪费资源，并且需要 k8s之外的负载均衡设备支持 ingress可以提供7层的负责对外暴露接口，而且可以调度不同的业务域，不同的url访问路径的业务流量。\nIngress：K8s 中的一个资源对象，作用是定义请求如何转发到 service 的规则 Ingress Controller：具体实现反向代理及负载均衡的程序，对Ingress定义的规则进行解析，根据配置的规则来实现请求转发，有很多种实现方式，如 Nginx、Contor、Haproxy等 ‍\n‍\n1.2 Ingress 组成 ingress controller\n将新加入的Ingress转化成Nginx的配置文件并使之生效\ningress服务\n将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可\n‍\n1.3 Ingress 工作原理 1.ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，\n2.然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，\n3.再写到nginx-ingress-control的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，\n4.然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。\n​\n用户编写 Ingress Service规则， 说明每个域名对应 K8s集群中的哪个Service Ingress控制器会动态感知到 Ingress 服务规则的变化，然后生成一段对应的Nginx反向代理配置 Ingress控制器会将生成的Nginx配置写入到一个运行中的Nginx服务中，并动态更新 然后客户端通过访问域名，实际上Nginx会将请求转发到具体的Pod中，到此就完成了整个请求的过程 ​\n1.4 Ingress可以解决什么问题 1.动态配置服务\n如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指向我们新的k8s服务. 而如果用了Ingress, 只需要配置好这个服务, 当服务启动时, 会自动注册到Ingress的中, 不需要而外的操作.\n2.减少不必要的端口暴露\n配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要用NodePort方式\n2 、部署配置Ingress 2.1 部署文件介绍、准备 https://kubernetes.github.io/ingress-nginx/deploy/\n网上的资料一般是基于v0.30.0来安装，但是对于kubernetes@1.22来说要安装ingress-nginx@v1.0.0以上版本（目前最新版本是v1.0.4，本文采用v1.0.0），原因是 kubectl@v1.22版本不再支持v1beta1\n如果安装ingress-nginx@v0.30.0版本后启动pod有如下问题\n1 Failed to list *v1beta1.Ingress: the server could not find the requested resource 有一个版本的支持情况（https://github.com/kubernetes/ingress-nginx/）\ningress 官方网站 ingress 仓库地址\ningress-nginx v1.0 最新版本 v1.0\n适用于 Kubernetes 版本 v1.19+ （包括 v1.19 ）\nKubernetes-v1.22+ 需要使用 ingress-nginx\u0026gt;=1.0，因为networking.k8s.io/v1beta 已经移除\n​\n2.2 直接部署 ingress-nginx 直接部署比较简单，直接拉去 girhub 的文件就可以了，如果遇到长时间无响应，可以终止任务从新拉取。\n拉取镜像yaml文件\n1 2 3 4 5 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml #更换国内镜像 sed -i \u0026#39;s@k8s.gcr.io/ingress-nginx/controller:v1.0.0\\(.*\\)@willdockerhub/ingress-nginx-controller:v1.0.0@\u0026#39; deploy.yaml sed -i \u0026#39;s@k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.0\\(.*\\)$@hzde0128/kube-webhook-certgen:v1.0@\u0026#39; deploy.yaml ‍\n2.3 优化yaml配置文件 1、默认 ingress-nginx 随机提供 nodeport 端口，开启 hostNetwork 启用80、443端口。\n‍\n‍\n需要在原deploy.yaml文件上修改修改后再部署\n修改点如下 1：k8s.gcr.io/ingress-nginx/controller:v1.0.0@sha256:0851b34f69f69352bf168e6ccf30e1e20714a264ab1ecd1933e4d8c0fc3215c6 改为：willdockerhub/ingress-nginx-controller:v1.0.0\n2：k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.0@sha256:f3b6b39a6062328c095337b4cadcefd1612348fdd5190b1dcbcb9b9e90bd8068 改为：jettech/kube-webhook-certgen:v1.0.0\n#3：Deployment修改点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller spec: dnsPolicy: ClusterFirstWithHostNet #既能使用宿主机DNS，又能使用集群DNS 原文line：319 hostNetwork: true #与宿主机共享网络 nodeName: k8s-master-1 #设置只能在k8s-master-1节点运行 tolerations: #设置能容忍master污点 - key: node-role.kubernetes.io/master operator: Exists containers: - name: controller image: willdockerhub/ingress-nginx-controller:v1.0.0 imagePullPolicy: IfNotPresent 如果不关心 ingressClass 或者很多没有 ingressClass 配置的 ingress 对象， 添加参数 ingress-controller --watch-ingress-without-class=true 。 优化上传等参数 # Source: ingress-nginx/templates/controller-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: # header默认大小上限是4k，超过会返回400错误 client_header_buffer_size: \u0026#34;16k\u0026#34; large_client_header_buffers: \u0026#34;4 16k\u0026#34; # 请求体默认大小上限是1m，超过会返回413错误 proxy-body-size: \u0026#34;10m\u0026#34; ‍\n修改后的yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 apiVersion: v1 kind: Namespace metadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx automountServiceAccountToken: true --- # Source: ingress-nginx/templates/controller-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: # header默认大小上限是4k，超过会返回400错误 client_header_buffer_size: \u0026#34;16k\u0026#34; large_client_header_buffers: \u0026#34;4 16k\u0026#34; # 请求体默认大小上限是1m，超过会返回413错误 proxy-body-size: \u0026#34;10m\u0026#34; --- # Source: ingress-nginx/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx rules: - apiGroups: - \u0026#39;\u0026#39; resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - \u0026#39;\u0026#39; resources: - nodes verbs: - get - apiGroups: - \u0026#39;\u0026#39; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - \u0026#39;\u0026#39; resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch --- # Source: ingress-nginx/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - \u0026#39;\u0026#39; resources: - namespaces verbs: - get - apiGroups: - \u0026#39;\u0026#39; resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - \u0026#39;\u0026#39; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - \u0026#39;\u0026#39; resources: - configmaps resourceNames: - ingress-controller-leader verbs: - get - update - apiGroups: - \u0026#39;\u0026#39; resources: - configmaps verbs: - create - apiGroups: - \u0026#39;\u0026#39; resources: - events verbs: - create - patch --- # Source: ingress-nginx/templates/controller-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-service-webhook.yaml apiVersion: v1 kind: Service metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller-admission namespace: ingress-nginx spec: type: ClusterIP ports: - name: https-webhook port: 443 targetPort: webhook appProtocol: https selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller --- # Source: ingress-nginx/templates/controller-service.yaml apiVersion: v1 kind: Service metadata: annotations: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: http appProtocol: http - name: https port: 443 protocol: TCP targetPort: https appProtocol: https selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller --- # Source: ingress-nginx/templates/controller-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller revisionHistoryLimit: 10 minReadySeconds: 0 template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet nodeName: node01 tolerations: - key: node-role.kubernetes.io/master operator: Exists containers: - name: controller image: willdockerhub/ingress-nginx-controller:v1.0.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE runAsUser: 101 allowPrivilegeEscalation: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: webhook containerPort: 8443 protocol: TCP volumeMounts: - name: webhook-cert mountPath: /usr/local/certificates/ readOnly: true resources: requests: cpu: 100m memory: 90Mi nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- # Source: ingress-nginx/templates/controller-ingressclass.yaml # We don\u0026#39;t support namespaced ingressClass yet # So a ClusterRole and a ClusterRoleBinding is required apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: nginx namespace: ingress-nginx spec: controller: k8s.io/ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml # before changing this value, check the required kubernetes version # https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook name: ingress-nginx-admission webhooks: - name: validate.nginx.ingress.kubernetes.io matchPolicy: Equivalent rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses failurePolicy: Fail sideEffects: None admissionReviewVersions: - v1 clientConfig: service: namespace: ingress-nginx name: ingress-nginx-controller-admission path: /networking/v1/ingresses --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - \u0026#39;\u0026#39; resources: - secrets verbs: - get - create --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-create namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: template: metadata: name: ingress-nginx-admission-create labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: create image: hzde0128/kube-webhook-certgen:v1.0 imagePullPolicy: IfNotPresent args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 2000 --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-patch namespace: ingress-nginx annotations: helm.sh/hook: post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: template: metadata: name: ingress-nginx-admission-patch labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: patch image: hzde0128/kube-webhook-certgen:v1.0 imagePullPolicy: IfNotPresent args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 2000 开始执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ kubectl apply -f deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created configmap/ingress-nginx-controller created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx created service/ingress-nginx-controller-admission created service/ingress-nginx-controller created deployment.apps/ingress-nginx-controller created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created serviceaccount/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created 查看生成状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [root@bt ingress]# kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-hdd2b 0/1 Completed 0 5d12h pod/ingress-nginx-admission-patch-4x2nj 0/1 Completed 1 5d12h pod/ingress-nginx-controller-6db5ddb4ff-jv4rr 1/1 Running 0 5d12h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller ClusterIP 172.21.23.131 \u0026lt;none\u0026gt; 80/TCP,443/TCP 5d12h service/ingress-nginx-controller-admission ClusterIP 172.21.21.247 \u0026lt;none\u0026gt; 443/TCP 5d12h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 5d12h NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-6db5ddb4ff 1 1 1 5d12h NAME COMPLETIONS DURATION AGE job.batch/ingress-nginx-admission-create 1/1 3s 5d12h job.batch/ingress-nginx-admission-patch 1/1 4s 5d12h [root@bt ingress]# kubectl get endpoints -n ingress-nginx ingress-nginx-controller NAME ENDPOINTS AGE ingress-nginx-controller 192.168.102.40:443,192.168.102.40:80 5d12h [root@bt ingress]# kubectl get endpoints -n ingress-nginx ingress-nginx-controller-admission NAME ENDPOINTS AGE ingress-nginx-controller-admission 192.168.102.40:8443 5d12h 这个时候ingress-nginx就安装完了，在kubernets之外的机器上访问每个节点，nginx都可以访问了，先不用管404的错误\n1 2 3 4 5 6 7 8 9 [root@bt ingress]# curl http://node01 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;404 Not Found\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;404 Not Found\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@bt ingress]# ‍\n3、 测试验证 配置nginx简单yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 apiVersion: apps/v1 kind: Deployment metadata: namespace: heian name: ingress-heian annotations: k8s.kuboard.cn/workload: ingress-heian deployment.kubernetes.io/revision: \u0026#39;1\u0026#39; k8s.kuboard.cn/service: ClusterIP k8s.kuboard.cn/ingress: \u0026#39;true\u0026#39; labels: app: ingress-heian spec: selector: matchLabels: app: ingress-heian revisionHistoryLimit: 10 template: metadata: labels: app: ingress-heian spec: affinity: {} securityContext: seLinuxOptions: {} imagePullSecrets: [] restartPolicy: Always initContainers: [] containers: - image: \u0026#39;wangyanglinux/myapp:v1\u0026#39; imagePullPolicy: IfNotPresent name: ingress-heian volumeMounts: - name: tz-config mountPath: /usr/share/zoneinfo/Asia/Shanghai - name: tz-config mountPath: /etc/localtime - name: timezone mountPath: /etc/timezone resources: limits: cpu: 100m memory: 100Mi requests: cpu: 10m memory: 10Mi env: - name: TZ value: Asia/Shanghai - name: LANG value: C.UTF-8 lifecycle: {} ports: - name: web containerPort: 80 protocol: TCP terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumes: - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Shanghai type: \u0026#39;\u0026#39; - name: timezone hostPath: path: /etc/timezone type: \u0026#39;\u0026#39; dnsPolicy: ClusterFirst dnsConfig: options: [] schedulerName: default-scheduler terminationGracePeriodSeconds: 30 progressDeadlineSeconds: 600 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 0 maxSurge: 1 replicas: 1 --- apiVersion: v1 kind: Service metadata: namespace: heian name: ingress-heian annotations: k8s.kuboard.cn/workload: ingress-heian labels: app: ingress-heian spec: selector: app: ingress-heian type: ClusterIP ports: - port: 80 targetPort: 80 protocol: TCP name: ingress-web-1 nodePort: 0 sessionAffinity: None --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: heian name: ingress-heian annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;0\u0026#34; nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;600\u0026#34; nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;600\u0026#34; kubernetes.io/tls-acme: \u0026#34;true\u0026#34; cert-manager.io/cluster-issuer: \u0026#34;example-issuer\u0026#34; labels: app: ingress-heian spec: rules: - host: ingress.ownit.top #域名 http: paths: - pathType: Prefix path: / backend: service: name: ingress-heian port: number: 80 测试完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@bt heian]# kubectl get pod,svc,ingress -n heian NAME READY STATUS RESTARTS AGE pod/ingress-heian-7ff9cbd9d5-w5f7v 1/1 Running 0 5d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-heian ClusterIP 172.21.24.68 \u0026lt;none\u0026gt; 80/TCP 5d NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/ingress-heian \u0026lt;none\u0026gt; ingress.ownit.top 192.168.102.40 80 5d [root@bt heian]# curl https://ingress.ownit.top Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@bt heian]# curl https://ingress.ownit.top/hostname.html ingress-heian-7ff9cbd9d5-w5f7v ​\n4、NGINX作为反向代理 1、安装nginx\n1 yum install nginx nginx-mod-stream -y 2、配置kubernetes-cluster.conf\n1 2 3 4 5 [root@bt nginx]# cat kubernetes-cluster.conf upstream kubernetes-cluster { server 192.168.102.40 weight=5; keepalive 16; } 3、配置ingress访问接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@bt nginx]# cat ingress.ownit.top.conf server { listen 80; server_name ingress.ownit.top; rewrite ^/(.*)$ https://$host/$1 permanent; # IP白名单 include /opt/nginx/whitelist/corporation.conf; } server { listen 443 ssl; server_name ingress.ownit.top; # IP白名单 include /opt/nginx/whitelist/corporation.conf; #ssl on; ssl_certificate /opt/nginx/ssl/ownit.top.crt; ssl_certificate_key /opt/nginx/ssl/ownit.top.key; include ssl.conf; location / { proxy_pass http://kubernetes-cluster; include https_proxy.conf; } access_log /www/wwwlogs/dns.ownit.top.log; error_log /www/wwwlogs/dns.ownit.top.error.log; } 4、则可以HTTPS访问到k8s里面的ingress域名\n​\n​\n‍\n","date":"2023-12-04T09:42:25Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/202312040942/","title":"Kubernetes安装ingress-nginx"},{"content":"Prometheus监控Padavan路由器 1、背景 近期在Synology（群辉）中安装一套Prometheus监控程序，目前已经监控Synology，然后家中有有路由器（Padavan）型号，也准备使用Prometheus+Grafan进行监控。\n‍\n环境：\nPrometheus：2.48.0\nGrafan：10\nPadavan：固件版本:3.4.3.9-099_22-05-1\n​​\n2、监控步骤 ‍\n1、padavan_exporter 正确使用 地址：https://github.com/Bpazy/padavan_exporter/blob/master/README-zh_CN.md\npadavan_exporter有两种方式（不是放到Padavan系统上的）\n安装到Linux操作系统，通过ssh连接到路由器进行数据获取 安装到Docker上，通过ssh连接到路由器进行数据获取 Linux操作系统 在Linux操作系统下载padavan_exporter\n参数使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ ./padavan_exporter --help Flags: --help Show context-sensitive help (also try --help-long and --help-man). --web.listen-address=\u0026#34;:9100\u0026#34; Address on which to expose metrics and web interface --padavan.ssh.host=\u0026#34;127.0.0.1:22\u0026#34; Padavan ssh host --padavan.ssh.username=\u0026#34;admin\u0026#34; Padavan ssh username --padavan.ssh.password=\u0026#34;admin\u0026#34; Padavan ssh password --debug Debug mode 安装 padavan_exporter 并使用正确的参数启动（点击跳转项目首页）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 wget https://github.com/Bpazy/padavan_exporter/releases/download/v0.0.2/padavan_exporter-linux-amd64-v0.0.2 sudo mv ./padavan_exporter-linux-amd64-v0.0.2 /usr/local/bin/padavan_exporter sudo chmod u=rwx,o=rx,g=rx /usr/local/bin/padavan_exporter sudo chgrp root /usr/local/bin/padavan_exporter sudo chown root /usr/local/bin/padavan_exporter # 自定义 systemd service sudo touch /lib/systemd/system/padavan_exporter.service sudo cat \u0026lt;\u0026lt;EOF \u0026gt;/lib/systemd/system/padavan_exporter.service [Unit] Description=Node exporter After=network.target [Service] Type=simple ExecStart=/usr/local/bin/padavan_exporter --padavan.ssh.host=\u0026#34;127.0.0.1:22\u0026#34; --padavan.ssh.username=\u0026#34;admin\u0026#34; --padavan.ssh.password=\u0026#34;admin\u0026#34; Restart=on-failure [Install] WantedBy=multi-user.target EOF # 加载自定义的 service sudo systemctl daemon-reload # 开机自启 sudo systemctl enable padavan_exporter # 启动 sudo systemctl start padavan_exporter ‍\nDocker Compose（推荐） 当然更好的方式是使用 Docker Compose，你可以参考本项目预置的 docker-compose.yml 文件。\n‍\n此处要注意，我把9100端口 映射到 9101，因为 我的 9100端口已经被占用，你们要注意\n1 2 3 4 5 6 7 8 version: \u0026#39;3\u0026#39; services: padavan_exporter: image: bpazy/padavan_exporter:latest restart: always ports: - \u0026#34;9101:9100\u0026#34; command: [\u0026#34;--padavan.ssh.host=192.168.123.1:22\u0026#34;, \u0026#34;--padavan.ssh.username=admin\u0026#34;, \u0026#34;--padavan.ssh.password=admin\u0026#34;] ​​\n测试数据 ​​\nPrometheus修改配置重载 1 2 3 4 - job_name: \u0026#39;padavan\u0026#39; static_configs: - targets: [\u0026#39;192.168.123.200:9101\u0026#39;] 2、Grafana 导入图形 ‍\n这个图在Dashboard: https://grafana.com/grafana/dashboards/15978 进行修改 根补充\n​​\n如果导入没数据，请编辑 然后在Prometheus上进行调试 下载Json文件导入 Padavan-1701611865144.json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 { \u0026#34;__inputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;DS_PROMETHEUS\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;pluginId\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;pluginName\u0026#34;: \u0026#34;Prometheus\u0026#34; } ], \u0026#34;__elements\u0026#34;: {}, \u0026#34;__requires\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Grafana\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;10.2.2\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Graph (old)\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Prometheus\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;panel\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;timeseries\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Time series\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;list\u0026#34;: [ { \u0026#34;builtIn\u0026#34;: 1, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;datasource\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;grafana\u0026#34; }, \u0026#34;enable\u0026#34;: true, \u0026#34;hide\u0026#34;: true, \u0026#34;iconColor\u0026#34;: \u0026#34;rgba(0, 211, 255, 1)\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Annotations \u0026amp; Alerts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;dashboard\u0026#34; } ] }, \u0026#34;description\u0026#34;: \u0026#34;Padavan exporter\u0026#39;s Dashboard 南宫乘风 fix \u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;fiscalYearStartMonth\u0026#34;: 0, \u0026#34;gnetId\u0026#34;: 15978, \u0026#34;graphTooltip\u0026#34;: 2, \u0026#34;id\u0026#34;: null, \u0026#34;links\u0026#34;: [ { \u0026#34;asDropdown\u0026#34;: false, \u0026#34;icon\u0026#34;: \u0026#34;external link\u0026#34;, \u0026#34;includeVars\u0026#34;: false, \u0026#34;keepTime\u0026#34;: false, \u0026#34;tags\u0026#34;: [], \u0026#34;targetBlank\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;如果不显示数据，请手动修改 页面中的标签数据（请点击，请查询博客教程）\u0026#34;, \u0026#34;tooltip\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;link\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://blog.csdn.net/heian_99\u0026#34; } ], \u0026#34;liveNow\u0026#34;: false, \u0026#34;panels\u0026#34;: [ { \u0026#34;collapsed\u0026#34;: false, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 1, \u0026#34;w\u0026#34;: 24, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 0 }, \u0026#34;id\u0026#34;: 9, \u0026#34;panels\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;基础信息\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;row\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisBorderShow\u0026#34;: true, \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;负载\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 0, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;insertNulls\u0026#34;: false, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 2, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;decimals\u0026#34;: 2, \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] } }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 11, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 1 }, \u0026#34;id\u0026#34;: 15, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [ \u0026#34;max\u0026#34;, \u0026#34;min\u0026#34;, \u0026#34;mean\u0026#34; ], \u0026#34;displayMode\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;node_load1{job=\\\u0026#34;padavan\\\u0026#34;}\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;1分钟负载\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;node_load5{job=\\\u0026#34;padavan\\\u0026#34;}\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;5分钟负载\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;node_load15{job=\\\u0026#34;padavan\\\u0026#34;}\u0026#34;, \u0026#34;hide\u0026#34;: false, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;15分钟负载\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;C\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;设备负载\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisBorderShow\u0026#34;: false, \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 25, \u0026#34;gradientMode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;insertNulls\u0026#34;: false, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;smooth\u0026#34;, \u0026#34;lineStyle\u0026#34;: { \u0026#34;fill\u0026#34;: \u0026#34;solid\u0026#34; }, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;fieldMinMax\u0026#34;: true, \u0026#34;mappings\u0026#34;: [], \u0026#34;min\u0026#34;: 0, \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;percent\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 11, \u0026#34;y\u0026#34;: 1 }, \u0026#34;id\u0026#34;: 14, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [ \u0026#34;max\u0026#34;, \u0026#34;min\u0026#34;, \u0026#34;mean\u0026#34; ], \u0026#34;displayMode\u0026#34;: \u0026#34;table\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;100- (\\r\\n (\\r\\n sum without (mode) (rate(node_cpu_seconds_total{job=\\\u0026#34;padavan\\\u0026#34;, cpu=\\\u0026#34;cpu\\\u0026#34;}[5m]))\\r\\n -\\r\\n sum without (mode) (rate(node_cpu_seconds_total{job=\\\u0026#34;padavan\\\u0026#34;,cpu=\\\u0026#34;cpu\\\u0026#34;, mode=\\\u0026#34;idle\\\u0026#34;}[5m]))\\r\\n ) / \\r\\n sum without (mode) (rate(node_cpu_seconds_total{job=\\\u0026#34;padavan\\\u0026#34;,cpu=\\\u0026#34;cpu\\\u0026#34;}[5m]))\\r\\n) * 100\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;legendFormat\u0026#34;: \u0026#34;cpu使用率\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;CPU使用率\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;1H路由器数据\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;palette-classic\u0026#34; }, \u0026#34;custom\u0026#34;: { \u0026#34;axisBorderShow\u0026#34;: true, \u0026#34;axisCenteredZero\u0026#34;: false, \u0026#34;axisColorMode\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;axisLabel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;axisPlacement\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;barAlignment\u0026#34;: 0, \u0026#34;drawStyle\u0026#34;: \u0026#34;bars\u0026#34;, \u0026#34;fillOpacity\u0026#34;: 100, \u0026#34;gradientMode\u0026#34;: \u0026#34;hue\u0026#34;, \u0026#34;hideFrom\u0026#34;: { \u0026#34;legend\u0026#34;: false, \u0026#34;tooltip\u0026#34;: false, \u0026#34;viz\u0026#34;: false }, \u0026#34;insertNulls\u0026#34;: false, \u0026#34;lineInterpolation\u0026#34;: \u0026#34;linear\u0026#34;, \u0026#34;lineWidth\u0026#34;: 1, \u0026#34;pointSize\u0026#34;: 5, \u0026#34;scaleDistribution\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;linear\u0026#34; }, \u0026#34;showPoints\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;spanNulls\u0026#34;: false, \u0026#34;stacking\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;percent\u0026#34; }, \u0026#34;thresholdsStyle\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;off\u0026#34; } }, \u0026#34;decimals\u0026#34;: 2, \u0026#34;fieldMinMax\u0026#34;: false, \u0026#34;mappings\u0026#34;: [], \u0026#34;thresholds\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;absolute\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: null }, { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 80 } ] }, \u0026#34;unit\u0026#34;: \u0026#34;bytes\u0026#34; }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 11, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 10 }, \u0026#34;id\u0026#34;: 8, \u0026#34;options\u0026#34;: { \u0026#34;legend\u0026#34;: { \u0026#34;calcs\u0026#34;: [], \u0026#34;displayMode\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;placement\u0026#34;: \u0026#34;bottom\u0026#34;, \u0026#34;showLegend\u0026#34;: true }, \u0026#34;tooltip\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;multi\u0026#34;, \u0026#34;sort\u0026#34;: \u0026#34;none\u0026#34; } }, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.2.2\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;increase(node_network_receive_bytes_total{job=\\\u0026#34;padavan\\\u0026#34;,device=~\u0026#39;br0|eth2.2\u0026#39;}[1h])\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;time_series\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 下载\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;exemplar\u0026#34;: false, \u0026#34;expr\u0026#34;: \u0026#34;increase(node_network_transmit_bytes_total{job=\\\u0026#34;padavan\\\u0026#34;,device=~\u0026#39;eth2|br0|eth2.2\u0026#39;}[1h])\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;30m\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 上传\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; } ], \u0026#34;title\u0026#34;: \u0026#34;1H路由器数据\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34; }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;links\u0026#34;: [] }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;fill\u0026#34;: 1, \u0026#34;fillGradient\u0026#34;: 0, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 11, \u0026#34;y\u0026#34;: 10 }, \u0026#34;hiddenSeries\u0026#34;: false, \u0026#34;id\u0026#34;: 4, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;alertThreshold\u0026#34;: true }, \u0026#34;percentage\u0026#34;: false, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.2.2\u0026#34;, \u0026#34;pointradius\u0026#34;: 2, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:250\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ppp0 receive rate\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#5794F2\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:251\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ppp0 receive total\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#B877D9\u0026#34; } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;increase(node_network_receive_bytes_total{job=\\\u0026#34;padavan\\\u0026#34;, device=~\\\u0026#34;eth2.2|br0\\\u0026#34;}[1000w])\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 接收总数\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;rate(node_network_receive_bytes_total{device=~\\\u0026#34;eth2.2|br0\\\u0026#34;, job=\\\u0026#34;padavan\\\u0026#34;}[1m])\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 接收速率\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeRegions\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;网络下载总计\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:268\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Receive Total\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true }, { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:269\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Receive Rate\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true } ], \u0026#34;yaxis\u0026#34;: { \u0026#34;align\u0026#34;: false } }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;links\u0026#34;: [] }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;fill\u0026#34;: 1, \u0026#34;fillGradient\u0026#34;: 0, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 11, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 19 }, \u0026#34;hiddenSeries\u0026#34;: false, \u0026#34;id\u0026#34;: 6, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;alertThreshold\u0026#34;: true }, \u0026#34;percentage\u0026#34;: false, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.2.2\u0026#34;, \u0026#34;pointradius\u0026#34;: 2, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:824\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ppp0 download bandwidth\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#5794F2\u0026#34; }, { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:825\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ppp0 upload bandwidth\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#73BF69\u0026#34; } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;rate(node_network_transmit_bytes_total{device=~\\\u0026#34;eth2.2|br0\\\u0026#34;, job=\\\u0026#34;padavan\\\u0026#34;}[1m])*8\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 上传\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;rate(node_network_receive_bytes_total{device=~\\\u0026#34;eth2.2|br0\\\u0026#34;, job=\\\u0026#34;padavan\\\u0026#34;}[1m])*8\u0026#34;, \u0026#34;instant\u0026#34;: false, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 下载\u0026#34;, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeRegions\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;网络带宽\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:838\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;decbits\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true }, { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:839\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;short\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: false } ], \u0026#34;yaxis\u0026#34;: { \u0026#34;align\u0026#34;: false } }, { \u0026#34;aliasColors\u0026#34;: {}, \u0026#34;bars\u0026#34;: false, \u0026#34;dashLength\u0026#34;: 10, \u0026#34;dashes\u0026#34;: false, \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;links\u0026#34;: [] }, \u0026#34;overrides\u0026#34;: [] }, \u0026#34;fill\u0026#34;: 1, \u0026#34;fillGradient\u0026#34;: 0, \u0026#34;gridPos\u0026#34;: { \u0026#34;h\u0026#34;: 9, \u0026#34;w\u0026#34;: 12, \u0026#34;x\u0026#34;: 11, \u0026#34;y\u0026#34;: 19 }, \u0026#34;hiddenSeries\u0026#34;: false, \u0026#34;id\u0026#34;: 2, \u0026#34;legend\u0026#34;: { \u0026#34;avg\u0026#34;: false, \u0026#34;current\u0026#34;: false, \u0026#34;max\u0026#34;: false, \u0026#34;min\u0026#34;: false, \u0026#34;show\u0026#34;: true, \u0026#34;total\u0026#34;: false, \u0026#34;values\u0026#34;: false }, \u0026#34;lines\u0026#34;: true, \u0026#34;linewidth\u0026#34;: 1, \u0026#34;nullPointMode\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;alertThreshold\u0026#34;: true }, \u0026#34;percentage\u0026#34;: false, \u0026#34;pluginVersion\u0026#34;: \u0026#34;10.2.2\u0026#34;, \u0026#34;pointradius\u0026#34;: 2, \u0026#34;points\u0026#34;: false, \u0026#34;renderer\u0026#34;: \u0026#34;flot\u0026#34;, \u0026#34;seriesOverrides\u0026#34;: [ { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:723\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ppp0 transmit rate\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#73BF69\u0026#34;, \u0026#34;fill\u0026#34;: 0, \u0026#34;yaxis\u0026#34;: 2 }, { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:724\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ppp0 transmit total\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#B877D9\u0026#34; } ], \u0026#34;spaceLength\u0026#34;: 10, \u0026#34;stack\u0026#34;: false, \u0026#34;steppedLine\u0026#34;: false, \u0026#34;targets\u0026#34;: [ { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;increase(node_network_transmit_bytes_total{job=\\\u0026#34;padavan\\\u0026#34;, device=~\\\u0026#34;eth2.2|br0\\\u0026#34;}[1000w])\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 上传总计\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;datasource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;${DS_PROMETHEUS}\u0026#34; }, \u0026#34;editorMode\u0026#34;: \u0026#34;code\u0026#34;, \u0026#34;expr\u0026#34;: \u0026#34;rate(node_network_transmit_bytes_total{device=~\\\u0026#34;eth2.2|br0\\\u0026#34;, job=\\\u0026#34;padavan\\\u0026#34;}[1m])\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{device}} 上传速率\u0026#34;, \u0026#34;range\u0026#34;: true, \u0026#34;refId\u0026#34;: \u0026#34;B\u0026#34; } ], \u0026#34;thresholds\u0026#34;: [], \u0026#34;timeRegions\u0026#34;: [], \u0026#34;title\u0026#34;: \u0026#34;网络上传总计\u0026#34;, \u0026#34;tooltip\u0026#34;: { \u0026#34;shared\u0026#34;: true, \u0026#34;sort\u0026#34;: 0, \u0026#34;value_type\u0026#34;: \u0026#34;individual\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;xaxis\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;show\u0026#34;: true, \u0026#34;values\u0026#34;: [] }, \u0026#34;yaxes\u0026#34;: [ { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:741\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;decbytes\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Transmit Total\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true }, { \u0026#34;$$hashKey\u0026#34;: \u0026#34;object:742\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;Bps\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Transmit Rate\u0026#34;, \u0026#34;logBase\u0026#34;: 1, \u0026#34;show\u0026#34;: true } ], \u0026#34;yaxis\u0026#34;: { \u0026#34;align\u0026#34;: false, \u0026#34;alignLevel\u0026#34;: 0 } } ], \u0026#34;refresh\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;schemaVersion\u0026#34;: 38, \u0026#34;tags\u0026#34;: [ \u0026#34;jokersy\u0026#34; ], \u0026#34;templating\u0026#34;: { \u0026#34;list\u0026#34;: [] }, \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;10s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;30m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;2h\u0026#34;, \u0026#34;1d\u0026#34; ] }, \u0026#34;timezone\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Padavan\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;5BoX3my7z\u0026#34;, \u0026#34;version\u0026#34;: 30, \u0026#34;weekStart\u0026#34;: \u0026#34;\u0026#34; } ","date":"2023-12-03T22:16:28Z","image":"https://www.ownit.top/title_pic/70.jpg","permalink":"https://www.ownit.top/p/202312032216/","title":"使用Prometheus监控Padavan路由器"},{"content":"1、简介 在现代的IT环境中，对于服务器和网络设备的监控是至关重要的。Synology（群辉）作为一种流行的网络存储解决方案，为用户提供了高性能和可靠的存储服务。然而，了解Synology设备的运行状况和性能指标对于确保其正常运行和及时采取措施至关重要。\nPrometheus是一个功能强大的开源监控系统，它提供了灵活的数据模型和丰富的查询语言，可用于收集、存储和可视化各种应用程序和设备的监控指标。通过将Prometheus与Synology（群辉）集成，您可以实时监控Synology（Snmp协议）设备的关键指标，如CPU使用率、内存使用率、磁盘空间、网络流量等，以便及时发现问题并采取适当的措施。 2、环境准备 Synology（群辉）：7.2 .1 prometheus：2.48.0 Grafan ：10 Snmp_exporter: 0.22.0 (注意版本：新版本配置变更)\nSnmp版本问题：https://github.com/prometheus/snmp_exporter/blob/main/auth-split-migration.md 从snmp_exporter v0.23.0 版本开始，配置文件格式snmp_exporter已更改。v0.22.0 及之前版本的配置文件将不起作用 3、Synology配置 1、开启SNMP协议 2、安装Docker 在套件中心，直接搜索 “docker” 进行安装 7.0 和 6.0 的版本不一样 7.0 3、Node-Exporter 安装 Node-Exporter 是监控 Synology的底层系统，相当于Linux操作系统， 这个可以使用docker的镜像安装，不过这里 大佬已经封装好插件，直接使用。 添加源\n1 2 https://spk7.imnks.com/ http://spk.bobohome.store:8880 4、Docker安装组件 参考： https://github.com/ddiiwoong/synology-prometheus Docker目录下，创建monitor目录，后续的配置存放里面 1、Snmp_exporter 安装 我使用dockerfile 进行安装 snmp的文件：https://github.com/ddiiwoong/synology-prometheus/blob/master/snmp-synology/snmp.yml\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#34;3.8\u0026#34; services: snmp-exporter: image: ricardbejarano/snmp_exporter:0.22.0 container_name: snmp_exporter volumes: - ./snmp-synology/snmp.yml:/etc/snmp_exporter/snmp.yml ports: - 9116:9116 command: - \u0026#34;--config.file=/etc/snmp_exporter/snmp.yml\u0026#34; 启动成功 测试 2、Prometheus安装 这次我才用ssh到nas系统，直接执行命令进行创建 prometheus.yml 配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 global: scrape_interval: 15s evaluation_interval: 15s alerting: alertmanagers: - static_configs: - targets: [\u0026#39;alertmanager:9093\u0026#39;] rule_files: - \u0026#34;/etc/prometheus/rules/*\u0026#34; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] labels: group: \u0026#39;prometheus\u0026#39; - job_name: node static_configs: - targets: [\u0026#39;192.168.123.200:9100\u0026#39;] - job_name: \u0026#39;snmp\u0026#39; metrics_path: /snmp static_configs: - targets: - 192.168.123.200 params: module: [synology] relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 192.168.123.200:9116 放到指定位置,然后创建prometheus\n1 2 3 4 5 6 7 8 9 docker run -d -p 9090:9090 -u root \\ -v /volume1/docker/monitor/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \\ -v /volume1/docker/monitor/prometheus/rules:/etc/prometheus/rules \\ -v /volume1/docker/monitor/prometheus/data:/etc/prometheus/data \\ --name prometheus \\ prom/prometheus:latest \\ --storage.tsdb.path=/etc/prometheus/data \\ --storage.tsdb.retention.time=90d \\ --config.file=/etc/prometheus/prometheus.yml 3、Grafan安装 参考文档：https://blog.csdn.net/wayne_primes/article/details/112467639\n1 2 3 4 5 6 7 8 9 10 docker run \\ -d --name grafana -p 3000:3000 \\ grafana/grafana grafana 将配置文件拷贝至宿主机方便修改配置 docker exec -it grafana cat /etc/grafana/grafana.ini \u0026gt; /data/grafana/grafana.ini mkdir -p /data/grafana/data #修改目录权限否则启动后容器中用户无法创建数据文件夹和文件 chmod 777 /data/grafana/data Grafan创建命令\n1 2 3 4 5 6 7 docker run -d -p 3000:3000 -u root \\ --name grafana \\ -e \u0026#34;GF_SECURITY_ADMIN_PASSWORD=admin\u0026#34; \\ -v \u0026#34;/volume1/docker/monitor/grafana/grafana.ini:/etc/grafana/grafana.ini\u0026#34; \\ -v \u0026#34;/volume1/docker/monitor/grafana/data/:/var/lib/grafana\u0026#34; \\ $(cat /etc/hosts |grep -Ev \u0026#34;^$|[#;]\u0026#34; | awk -F \u0026#39; \u0026#39; \u0026#39;{if(NR\u0026gt;2){print \u0026#34;--add-host \u0026#34;$2\u0026#34;:\u0026#34;$1}}\u0026#39;) \\ grafana/grafana grafana 5、界面展示 1、添加数据源 2、导入大屏展示 ID： Linux：8919 群辉：14284 14364\n","date":"2023-11-28T15:17:01Z","image":"https://www.ownit.top/title_pic/14.jpg","permalink":"https://www.ownit.top/p/202311281517/","title":"使用Prometheus监控Synology（群辉）"},{"content":"1、引言 在当今数字化时代，数据是企业和组织的核心资产。为了确保数据的安全性和可恢复性，备份是至关重 要的。然而，手动备份数据可能会繁琐且容易出错，特别是在面对大规模和分布式的数据存储情况下。幸运的是，Ansible作为一种自动化工具，能够帮助我们轻松实现核心数据的集中式自动备份。本文将深入探讨如何使用Ansible来自动备份核心数据，确保数据安全无忧。\n2、背景 公司有一些核心数据需要备份，比如，gitlab，wiki，MySQL，MongoDB等等数据库。\n为了防止数据丢失或损坏，我们需要定期备份GitLab数据。备份的内容通常包括Git存储库、配置文件、数据库等\n之前方案：\n在每台相关的机器上备份，任务分散，数据不容易管理，存储\n现在方案：\n采用一台机器，设置定时任务，把备份数据归档到同一地方存储，统一管理\n使用Ansible自动备份GitLab具有以下优点：\n提高效率：使用Ansible可以自动备份，无需手动执行备份操作。\n提高可靠性：通过Ansible自动化备份可以减少人为错误，提高备份的可靠性。\n充分利用资源：Ansible可以在多个主机上并行执行任务，充分利用资源提高效率。\n3、方案实现 编写Ansible Playbook：在Playbook中定义需要备份的文件和路径，以及备份的文件存储位置。\n执行Ansible Playbook：通过执行Playbook来自动备份GitLab。\n验证备份：检查备份文件是否已正确存储，并确保文件可用于恢复。\n1、基础环境 1 2 3 4 5 6 7 backupGitlab.yml group_vars hosts [root@ansible-yunwei backupData]# tree . . ├── backupGitlab.yml ├── group_vars │?? └── all └── hosts 创建hosts主机\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [gitlab] 192.168.xx.xx ansible_ssh_user=root ansible_ssh_pass=\u0026#34;xxxx.xx\u0026#34; [confluence] 192.168.xx.xx ansible_ssh_user=root ansible_ssh_pass=\u0026#34;xxxx.xx\u0026#34; [uatmysql] 192.168.xx.xx ansible_ssh_user=root ansible_ssh_pass=\u0026#34;xxx.xx\u0026#34; [uatmongo8449] 192.168.xx.xx ansible_ssh_user=root ansible_ssh_pass=\u0026#34;xxxx.xx\u0026#34; [uatmongo9942] 192.168.xxx.xx ansible_ssh_user=root ansible_ssh_pass=\u0026#34;xxxxx.xx\u0026#34; 定义变量环境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # gitlab备份存放目录 gitlabBackupDir: /data/backup/gitlab # confluence备份存放目录 confluenceBackupDir: /data/backup/confluence # uat环境mysql备份存放目录 uatMysqlBackupDir: /data/backup/uatmysql # uat环境mongo(192.168.84.xx)备份存放目录 uatMongo8449BackupDir: /data/backup/uatmongo8449 # uat环境mongo(192.168.99.xx)备份存放目录 uatMongo9942BackupDir: /data/backup/uatmongo9942 # 备份结果记录文件 statusFile: /data/backup/backupStatus 2、编写Ansible Playbook 获取当前日期，创建备份目录。 生成GitLab备份文件并拉取至本地备份目录。 拉取GitLab配置文件并存储在备份目录中。 删除原始备份文件及旧备份文件。 记录备份完成状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 [root@ansible-yunwei backupData]# cat backupGitlab.yml - name: 备份gitlab hosts: gitlab remote_user: root tasks: - name: 获取当日日期 local_action: shell date +%Y%m%d args: warn: no register: date - name: 本机创建备份目录 local_action: file path={{gitlabBackupDir}}/{{date.stdout}} state=directory owner=root group=root mode=0700 - name: 生成gitlab备份 shell: gitlab-backup create STRATEGY=copy args: warn: no - name: 获取文件名 find: paths=/var/opt/gitlab/backups/ recurse=no patterns=\u0026#39;*.tar\u0026#39; register: backupFile - name: 拉取备份文件 fetch: src={{item.path}} dest={{gitlabBackupDir}}/{{date.stdout}}/ flat=True with_items: - \u0026#39;{{backupFile.files}}\u0026#39; - name: 拉取配置文件 fetch: src={{item}} dest={{gitlabBackupDir}}/{{date.stdout}}/ flat=True with_items: - /etc/gitlab/gitlab.rb - /etc/gitlab/gitlab-secrets.json - name: 删除原始文件 file: path={{item.path}} state=absent with_items: - \u0026#39;{{backupFile.files}}\u0026#39; - name: 查找旧的备份 local_action: find paths={{gitlabBackupDir}}/ recurse=no patterns=\u0026#39;*\u0026#39; age=3d age_stamp=mtime file_type=directory register: backupDir - name: 删除旧的备份 local_action: file path={{item.path}} state=absent with_items: - \u0026#39;{{backupDir.files}}\u0026#39; - name: 记录备份完成状态 local_action: shell if [ ! \u0026#34;`ls -A {{gitlabBackupDir}}/{{date.stdout}}`\u0026#34; = \u0026#34;\u0026#34; ]; then echo \u0026#39;{{date.stdout}}:successful:gitlab\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; else echo \u0026#39;{{date.stdout}}:failed:gitlab\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; fi args: warn: no 3、执行ansible-playbook 1 /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupGitlab.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 4、设置定时任务 1 1 0 * * * /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupGitlab.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 5、执行结果 4、备份其余类型 backupConfluence.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 - name: 备份confluence hosts: confluence remote_user: root tasks: - name: 获取当日日期 local_action: shell date +%Y%m%d args: warn: no register: date - name: 获取用于模式匹配的当日日期 local_action: shell date +%Y_%m_%d args: warn: no register: datePattern - name: 本机创建备份目录 local_action: file path={{confluenceBackupDir}}/{{date.stdout}} state=directory owner=root group=root mode=0700 - name: 获取文件名 find: paths=/var/atlassian/application-data/confluence/backups/ recurse=no patterns=\u0026#39;backup-{{datePattern.stdout}}.zip\u0026#39; register: backupFile - name: 拉取备份文件 fetch: src={{item.path}} dest={{confluenceBackupDir}}/{{date.stdout}}/ flat=True with_items: - \u0026#39;{{backupFile.files}}\u0026#39; - name: 查找旧的备份 local_action: find paths={{confluenceBackupDir}}/ recurse=no patterns=\u0026#39;*\u0026#39; age=3d age_stamp=mtime file_type=directory register: backupDir - name: 删除旧的备份 local_action: file path={{item.path}} state=absent with_items: - \u0026#39;{{backupDir.files}}\u0026#39; - name: 记录备份完成状态 local_action: shell if [ ! \u0026#34;`ls -A {{confluenceBackupDir}}/{{date.stdout}}`\u0026#34; = \u0026#34;\u0026#34; ]; then echo \u0026#39;{{date.stdout}}:successful:confluence\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; else echo \u0026#39;{{date.stdout}}:failed:confluence\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; fi args: warn: no ** backupUatMongo8449.yml**\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 - name: 备份uat环境mongo数据库(192.168.xx.xx) hosts: uatmongo8449 remote_user: root tasks: - name: 获取当日日期 local_action: shell date +%Y%m%d args: warn: no register: date - name: 本机创建备份目录 local_action: file path={{uatMongo8449BackupDir}}/{{date.stdout}} state=directory owner=root group=root mode=0700 - name: 生成备份 shell: mongodump --host \u0026#39;192.168.xx.xx\u0026#39; --port \u0026#39;27017\u0026#39; -u \u0026#39;root\u0026#39; -p \u0026#39;xxx\u0026#39; --authenticationDatabase \u0026#39;admin\u0026#39; --gzip -o /tmp/uatmongo8449-{{date.stdout}} args: warn: no - name: 进行打包 archive: path=/tmp/uatmongo8449-{{date.stdout}} dest=/tmp/uatmongo8449-{{date.stdout}}.tar format=tar remove=True - name: 拉取备份文件 fetch: src=/tmp/uatmongo8449-{{date.stdout}}.tar dest={{uatMongo8449BackupDir}}/{{date.stdout}}/ flat=True - name: 删除原始文件 file: path={{item}} state=absent with_items: - \u0026#39;/tmp/uatmongo8449-{{date.stdout}}.tar\u0026#39; - \u0026#39;/tmp/uatmongo8449-{{date.stdout}}\u0026#39; - name: 查找旧的备份 local_action: find paths={{uatMongo8449BackupDir}}/ recurse=no patterns=\u0026#39;*\u0026#39; age=3d age_stamp=mtime file_type=directory register: backupDir - name: 删除旧的备份 local_action: file path={{item.path}} state=absent with_items: - \u0026#39;{{backupDir.files}}\u0026#39; - name: 记录备份完成状态 local_action: shell if [ ! \u0026#34;`ls -A {{uatMongo8449BackupDir}}/{{date.stdout}}`\u0026#34; = \u0026#34;\u0026#34; ]; then echo \u0026#39;{{date.stdout}}:successful:uatMongo8449\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; else echo \u0026#39;{{date.stdout}}:failed:uatMongo8449\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; fi args: warn: no ** backupUatMysql.yml**\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 - name: 备份uat环境mysql数据库 hosts: uatmysql remote_user: root tasks: - name: 获取当日日期 local_action: shell date +%Y%m%d args: warn: no register: date - name: 本机创建备份目录 local_action: file path={{uatMysqlBackupDir}}/{{date.stdout}} state=directory owner=root group=root mode=0700 - name: 生成备份 shell: mysqldump -h\u0026#39;192.168.1xx.xxx\u0026#39; -u\u0026#39;root\u0026#39; -p\u0026#39;xxxx\u0026#39; --single-transaction --flush-logs --master-data=2 --events --routines --triggers --hex-blob --all-databases | gzip \u0026gt; /tmp/uatmysql-{{date.stdout}}.sql.gz args: warn: no - name: 拉取备份文件 fetch: src=/tmp/uatmysql-{{date.stdout}}.sql.gz dest={{uatMysqlBackupDir}}/{{date.stdout}}/ flat=True - name: 删除原始文件 file: path=/tmp/uatmysql-{{date.stdout}}.sql.gz state=absent - name: 查找旧的备份 local_action: find paths={{uatMysqlBackupDir}}/ recurse=no patterns=\u0026#39;*\u0026#39; age=3d age_stamp=mtime file_type=directory register: backupDir - name: 删除旧的备份 local_action: file path={{item.path}} state=absent with_items: - \u0026#39;{{backupDir.files}}\u0026#39; - name: 记录备份完成状态 local_action: shell if [ ! \u0026#34;`ls -A {{uatMysqlBackupDir}}/{{date.stdout}}`\u0026#34; = \u0026#34;\u0026#34; ]; then echo \u0026#39;{{date.stdout}}:successful:uatMysql\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; else echo \u0026#39;{{date.stdout}}:failed:uatMysql\u0026#39; \u0026gt;\u0026gt; {{statusFile}} ; fi args: warn: no 1 2 3 4 5 6 7 [root@kvm-master backupData]# crontab -l # 备份数据：Gitlab、Confluence、uat环境mysql(192.168.1xx.xx)、uat环境mongo(192.168.xx.xx) (每天执行一次) 1 0 * * * /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupGitlab.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 10 0 * * * /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupConfluence.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 20 0 * * * /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupUatMysql.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 30 0 * * * /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupUatMongo8449.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 40 0 * * * /usr/local/python36/bin/ansible-playbook /opt/ansible/backupData/backupUatMongo9942.yml -i /opt/ansible/backupData/hosts 2\u0026gt;\u0026amp;1 \u0026gt;\u0026gt; /data/backup/backup.log 5、总结 通过上述自动化备份策略，我们可以有效地保护Gitlab数据的安全性和完整性。在实际应用过程中，这种备份策略还可以根据需要进行定制和扩展，以满足不同场景下的需求。希望这篇博客对大家有所帮助！\n","date":"2023-10-20T15:59:21Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/202310201559/","title":"精益求精：使用Ansible集中式自动备份核心数据"},{"content":"prometheus-pushgateway安装 一. Pushgateway简介 Pushgateway为Prometheus整体监控方案的功能组件之一，并做于一个独立的工具存在。它主要用于Prometheus无法直接拿到监控指标的场景，如监控源位于防火墙之后，Prometheus无法穿透防火墙；目标服务没有可抓取监控数据的端点等多种情况。\n在类似场景中，可通过部署Pushgateway的方式解决问题。当部署该组件后，监控源通过主动发送监控数据到Pushgateway，再由Prometheus定时获取信息，实现资源的状态监控。 简单图 ​\n工作流程：\na. 监控源通过Post方式，发送数据到Pushgateway，路径为/metrics。\nb. Prometheus服务端设置任务，定时获取Pushgateway上面的监控指标。\nc. Prometheus拿到监控指标后，根据配置的告警规则，如果匹配将触发告警到Alertmanager；同时，Grafana可配置数据源调用Prometheus数据，做为数据展示。\nd. Alertmanager收到告警后，根据规则转发到对应接收人及接收介质；Grafana方面，用户可登录并根据数据源的监控指标，配置相关的图表展示 。\n二. 安装部署 二进制安装\n下载安装包 1 2 3 cd /usr/local wget https://github.com/prometheus/pushgateway/releases/download/v1.4.3/pushgateway-1.4.3.linux-amd64.tar.gz tar -xf pushgateway-1.4.3.linux-amd64.tar.gz system管理 启动服务，默认端口为9091,可通过\u0026ndash;web.listen-address更改监听端口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 root@bj-1:/usr/local# cat /usr/lib/systemd/system/pushgateway.service [Unit] Description=Prometheus pushgateway Requires=network.target remote-fs.target After=network.target remote-fs.target ? [Service] Type=simple User=root Group=root ExecStart=/usr/local/pushgateway/pushgateway --persistence.file=\u0026#34;/usr/local/pushgateway/data/\u0026#34; --persistence.interval=5m #保存时间5分钟 ExecReload=/bin/kill -HUP $MAINPID KillMode=process Restart=on-failure RestartSec=5s ? [Install] WantedBy=multi-user.target ​\n三.prometheus添加配置 新增job pushgateway\n1 2 3 4 5 6 7 8 vim /usr/local/prometheus/prometheus.yml - job_name: \u0026#39;pushgateway\u0026#39; scrape_interval: 30s honor_labels: true #加上此配置exporter节点上传数据中的一些标签将不会被pushgateway节点的相同标签覆盖 static_configs: - targets: [\u0026#39;10.3.1.11:9091\u0026#39;] labels: instance: pushgateway ‘’查看target状态：\n​\n四. 数据推送Pushgateway pushgateway的数据推送支持两种方式，Prometheus Client SDK推送和API推送。\n​\n1、Client SDK推送 Prometheus本身提供了支持多种语言的SDK，可通过SDK的方式，生成相关的数据，并推送到pushgateway，这也是官方推荐的方案。目前的SDK覆盖语言有官方的​\n1 2 3 4 Go Java or Scala Python Ruby 也有许多第三方的，详情可参见此链接：https://prometheus.io/docs/instrumenting/clientlibs/\n示例:\n本示例以python为例，讲解SDK的使用\n1 2 3 4 5 6 7 8 from prometheus_client import Counter,Gauge,push_to_gateway from prometheus_client.core import CollectorRegistry registry = CollectorRegistry() data1 = Gauge(\u0026#39;gauge_test_metric\u0026#39;,\u0026#39;This is a gauge-test-metric\u0026#39;,[\u0026#39;method\u0026#39;,\u0026#39;path\u0026#39;,\u0026#39;instance\u0026#39;],registry=registry) data1.labels(method=\u0026#39;get\u0026#39;,path=\u0026#39;/aaa\u0026#39;,instance=\u0026#39;instance1\u0026#39;).inc(3) push_to_gateway(\u0026#39;10.12.61.3:9091\u0026#39;, job=\u0026#39;alex-job\u0026#39;,registry=registry) 注解：\n第一、二行代码：引入相关的Prometheus SDK；\n第五行代码：创建相关的指标，类型为Gauge。其中“gauge_test_metric”为指标名称，\u0026lsquo;This is a gauge-test-metric\u0026rsquo;为指标注释，[\u0026lsquo;method\u0026rsquo;,\u0026lsquo;path\u0026rsquo;,\u0026lsquo;instance\u0026rsquo;] 为指标相关的label。\n第六行代码：添加相关的label信息和指标value 值。\n第六行代码：push数据到pushgateway，\u0026lsquo;10.12.61.3:9091\u0026rsquo;为发送地址，job指定该任务名称。\n以上代码产生的指标数据等同如下 ：\n1 2 3 # HELP gauge_test_metric This is a gauge-test-metric # TYPE gauge_test_metric gauge gauge_test_metric{instance=\u0026#34;instance1\u0026#34;,method=\u0026#34;get\u0026#34;,path=\u0026#34;/aaa\u0026#34;} 3.0 2、Post推送Node-expoerter组件数据 安装好node_exporter,此处不多介绍\n传送监控数据到pushgateway节点\n对于传过去的监控项会添加此处定义的标签 job=test instance=10.2.1.11 hostname=ip-10-2-1-11\n1 curl 127.0.0.1:9100/metrics|curl --data-binary @- http://10.3.1.11:9091/metrics/job/test/instance/10.2.1.11/hostname/ip-10-2-1-11 编写脚本\nnode_date.sh\n1 2 3 4 5 6 #!/bin/bash job_name=\u0026#34;Bj\u0026#34; hostname=$(hostname) HOST_IP=$(hostname --all-ip-addresses | awk \u0026#39;{print $1}\u0026#39;) /usr/bin/curl 127.0.0.1:9100/metrics|/usr/bin/curl --data-binary @- http://sanming.f3322.net:9091/metrics/job/$job_name/instance/$HOST_IP/hostname/$hostname crontab定时任务\n1 2 #Ansible: node_date * * * * * /bin/bash /usr/local/node_exporter/node_date.sh 批量给node-exporter添加定时任务\nAnsible剧本\n1 2 3 4 5 6 7 8 9 10 11 root@bj-1:/opt/node_date# cat playbook.yml - hosts: all remote_user: root gather_facts: no tasks: - name: 推送磁盘脚本 copy: src=node_date.sh dest=/usr/local/node_exporter mode=u+x - name: 设置定时任务 cron: name=\u0026#34;node_date\u0026#34; job=\u0026#34;/bin/bash /usr/local/node_exporter/node_date.sh\u0026#34; state=\u0026#34;present\u0026#34; - name: 执行脚本 shell: /bin/bash /usr/local/node_exporter/node_date.sh 删除某个实例的数据：\n1 curl -X DELETE http://10.3.1.11:9091/metrics/job/test/instance/10.2.1.11/hostname/ip-10-2-1-11 ‍\n3、pushgateway脚本示例 (1)TCP连接 pushgateway本身没有任何抓取监控数据的功能，它只能被动地等待数据被推送过来，故需要用户自行编写数据采集脚本。\n例：采集TCP waiting_connection瞬时数量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mkdir -p /app/scripts/pushgateway cat \u0026lt;\u0026lt;EOF \u0026gt;/app/scripts/pushgateway/tcp_waiting_connection.sh #!/bin/bash # 获取hostname，且host不能为localhost instance_name=`hostname -f | cut -d \u0026#39;.\u0026#39; -f 1` if [ $instance_name = \u0026#34;localhost\u0026#34; ];then echo \u0026#34;Must FQDN hostname\u0026#34; exit 1 fi # For waiting connections label=\u0026#34;count_netstat_wait_connetions\u0026#34; count_netstat_wait_connetions=`netstat -an | grep -i wait | wc -l` echo \u0026#34;$label:$count_netstat_wait_connetions\u0026#34; echo \u0026#34;$label $count_netstat_wait_connetions\u0026#34; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name EOF chmod +x /app/scripts/pushgateway/tcp_waiting_connection.sh 1)netstat -an | grep -i wait | wc -l该自定义监控的取值方法\n2)实际上就是将K/V键值对通过POST方式推送给pushgateway，格式如下：\nhttp://localhost:9091/metricspushgateway url\njob/pushgateway数据推送过去的第一个label，即exported_job=\u0026ldquo;pushgateway\u0026rdquo;（类似prometheus.yml中定义的job）\ninstance/$instance_name数据推送过去的第一个label，即exported_instance=\u0026ldquo;deepin-PC\u0026rdquo;\n‍\n2.定时执行脚本\n1 2 3 crontab -e * * * * * /app/scripts/pushgateway/tcp_waiting_connection.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 prometheus默认每15秒从pushgateway获取一次数据，而cron定时任务最小精度是每分钟执行一次，若想没15秒执行一次，则：\n方法1：sleep：定义多条定时任务\n1 2 3 4 * * * * * /app/scripts/pushgateway/tcp_waiting_connection.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 * * * * * * sleep 15; /app/scripts/pushgateway/tcp_waiting_connection.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 * * * * * * sleep 30; /app/scripts/pushgateway/tcp_waiting_connection.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 * * * * * * sleep 45; /app/scripts/pushgateway/tcp_waiting_connection.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 方法2：for循环\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat \u0026lt;\u0026lt;EOF \u0026gt;/app/scripts/pushgateway/tcp_waiting_connection.sh #!/bin/bash time=15 for (( i=0; i\u0026lt;60; i=i+time )); do instance_name=`hostname -f | cut -d \u0026#39;.\u0026#39; -f 1` if [ $instance_name = \u0026#34;localhost\u0026#34; ];then echo \u0026#34;Must FQDN hostname\u0026#34; exit 1 fi label=\u0026#34;count_netstat_wait_connetions\u0026#34; count_netstat_wait_connetions=`netstat -an | grep -i wait | wc -l` echo \u0026#34;$label:$count_netstat_wait_connetions\u0026#34; echo \u0026#34;$label $count_netstat_wait_connetions\u0026#34; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name sleep $time done exit 0 EOF 此时cron定时任务只需要定义一条：\n1 2 3 crontab -e * * * * * /app/scripts/pushgateway/tcp_waiting_connection.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 注：若解释器使用#!/bin/bash，则调试时使用全路径或相对路径或者bash /app/scripts/pushgateway/tcp_waiting_connection.sh执行脚本；若解释器使用#!/bin/sh，则调试时使用sh /app/scripts/pushgateway/tcp_waiting_connection.sh执行脚本，否则出现错误：Syntax error: Bad for loop variable\n3.promethues查看监控值count_netstat_wait_connetions\n4.TCP等待连接数：count_netstat_wait_connetions（通过自定义脚本实现，通过node_exporter也可实现）\n处于各种wait状态的TCP连接（close_wait，time_wait等）也是日常排查负载（网络负载，服务器负载，数据库负载等）的一个重要指标：一般wait类型的TCP过大时，一定说明系统网络负载（流量负载）出现了问题；原因多样（网络问题，访问请求量，DDOS流量，数据库，CPU等都有可能）\n‍\n‍\n1 2 3 4 5 6 7 8 vi count_netstat_wait_connections.sh #!/bin/bash instance_name=`hostname -f | cut -d\u0026#39;.\u0026#39; -f1` #获取本机名，用于后面的的标签 label=\u0026#34;count_netstat_wait_connections\u0026#34; #定义key名 count_netstat_wait_connections=`netstat -an | grep -i wait | wc -l` #获取数据的命令 echo \u0026#34;$label: $count_netstat_wait_connections\u0026#34; echo \u0026#34;$label $count_netstat_wait_connections\u0026#34; | curl --data-binary @- http://server.com:9091/metrics/job/pushgateway_test/instance/$instance_name #这里pushgateway_test就是prometheus主配置文件里job的名字，需要保持一致，这样数据就会推送给这个job。后面的instance则是指定机器名，使用的就是脚本里获取的那个变量值 ‍\n参考文档：\nPrometheus分布式监控\nprometheus-pushgateway安装\nPrometheus监控运维实战十一：Pushgateway\n","date":"2023-10-16T09:04:53Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/202310160904/","title":"Prometheus的Pushgateway快速部署及使用"},{"content":"ch\n技术 后端：flask 可视化：echarts 前端：HTML+JavaScript+css\n大屏布局 大屏拆分 案例项目中大屏可按版块进行拆解，会发现这里大屏主要由标题、折线图、柱状图、地图、滚动图和词云等组成，整体可切分为8个版块，如下： 下方为简单演示： HTML 我们整体布局前，先通过简单的案例了解前端布局实现方法。\n创建一个html文件，这里先调整标题的布局位置，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no\u0026#34;\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;ECharts\u0026lt;/title\u0026gt; \u0026lt;!-- 引入刚刚下载的 ECharts 文件 --\u0026gt; \u0026lt;!-- 引入 jQuery 库 --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.staticfile.org/jquery/3.6.0/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;static/js/echarts.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;title\u0026#34;\u0026gt;机器监控实时跟踪\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;time\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 为 ECharts 准备一个定义了宽高的 DOM --\u0026gt; \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;cpu\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;disk\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;network\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;html\u0026gt; 展示 CSS 在上面添加一些css的样式，划分相关的位置\n1 2 3 4 5 position: absolute; width: 100%; height: 50%; top: 50%; right: 0%; 上面就是划分位置的参数，能够帮我们快速划分好位置。 这段代码是用于对一个元素进行定位的 CSS 样式设置。以下是对每个参数的详细介绍：\nposition: absolute;：将元素的定位类型设置为绝对定位，即相对于其最近的具有定位（非static）的父元素进行定位。 width: 100%;：将元素的宽度设置为父元素的100%。换句话说，元素的宽度将填充其父元素的整个宽度。 height: 50%;：将元素的高度设置为父元素高度的50%。元素将占据其父元素高度的一半。 top: 50%;：将元素的顶部边缘相对于其包含块（通常是最近的已定位祖先元素）的顶部边缘偏移50%。此设置将使元素的中心垂直居中。 right: 0%;：将元素的右侧边缘相对于其包含块的右侧边缘偏移0%。换句话说，元素将紧贴其包含块的右侧边缘。\n根据以上参数设置，这段代码将使元素以绝对定位方式出现在父元素内部。元素的宽度将填满整个父元素的宽度，高度为父元素高度的一半。元素将垂直居中，且右侧紧贴父元素。请注意，此代码片段中未提及其他定位相关属性（如左侧偏移量），因此可能还需要其他样式设置来完整定义元素的位置和大小。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no\u0026#34;\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; \u0026lt;title\u0026gt;ECharts\u0026lt;/title\u0026gt; \u0026lt;!-- 引入刚刚下载的 ECharts 文件 --\u0026gt; \u0026lt;!-- 引入 jQuery 库 --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.staticfile.org/jquery/3.6.0/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;static/js/echarts.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; background: #333; } #title { position: absolute; width: 40%; height: 10%; top: 0%; left: 30%; background: #666666; color: white; font-size: 30px; display: flex; align-items: center; justify-content: center; font-weight: bold; /* 加粗字体 */ border-radius: 10px; /* 增加圆润感 */ box-shadow: 0 0 10px rgba(0, 0, 0, 0.2); /* 增加一些阴影，增加科技感 */ } #main { position: absolute; width: 40%; height: 40%; top: 10%; left: 30%; background: #46a9be; color: white; font-size: 30px; display: flex; align-items: center; justify-content: center; font-weight: bold; /* 加粗字体 */ border-radius: 10px; /* 增加圆润感 */ box-shadow: 0 0 10px rgba(0, 0, 0, 0.2); /* 增加一些阴影，增加科技感 */ } #cpu { position: absolute; width: 30%; height: 40%; top: 10%; left: 0%; background: #48dada; color: white; font-size: 30px; display: flex; align-items: center; justify-content: center; font-weight: bold; /* 加粗字体 */ border-radius: 10px; /* 增加圆润感 */ box-shadow: 0 0 10px rgba(0, 0, 0, 0.2); /* 增加一些阴影，增加科技感 */ } #time { position: absolute; /* width: 30%; */ height: 10%; top: 5%; right: 2%; color: #FFFFFF; font-size: 20px; /* background: green; */ } #disk { position: absolute; width: 30%; height: 40%; top: 10%; right: 0%; background: #48dada; color: white; font-size: 30px; display: flex; align-items: center; justify-content: center; font-weight: bold; /* 加粗字体 */ border-radius: 10px; /* 增加圆润感 */ box-shadow: 0 0 10px rgba(0, 0, 0, 0.2); /* 增加一些阴影，增加科技感 */ } #network { position: absolute; width: 100%; height: 50%; top: 50%; right: 0%; background: #dea594; color: white; font-size: 30px; display: flex; align-items: center; justify-content: center; font-weight: bold; /* 加粗字体 */ border-radius: 10px; /* 增加圆润感 */ box-shadow: 0 0 10px rgba(0, 0, 0, 0.2); /* 增加一些阴影，增加科技感 */ } \u0026lt;/style\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;title\u0026#34;\u0026gt;机器监控实时跟踪\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;time\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 为 ECharts 准备一个定义了宽高的 DOM --\u0026gt; \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;cpu\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;disk\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;network\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;html\u0026gt; JS 时间显示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 function showTime() { var date = new Date(); var year = date.getFullYear(); var month = date.getMonth() + 1; // getMonth() 返回的月份是从0开始的，所以需要+1 var day = date.getDate(); var hour = date.getHours(); var minute = date.getMinutes(); var second = date.getSeconds(); // 使用 \u0026#39;padStart\u0026#39; 函数来确保每个部分都是两位数 month = month.toString().padStart(2, \u0026#39;0\u0026#39;); day = day.toString().padStart(2, \u0026#39;0\u0026#39;); hour = hour.toString().padStart(2, \u0026#39;0\u0026#39;); minute = minute.toString().padStart(2, \u0026#39;0\u0026#39;); second = second.toString().padStart(2, \u0026#39;0\u0026#39;); var formattedDate = year + \u0026#39;年\u0026#39; + month + \u0026#39;月\u0026#39; + day + \u0026#39;日，\u0026#39; + hour + \u0026#39;时\u0026#39; + minute + \u0026#39;分\u0026#39; + second + \u0026#39;秒\u0026#39;; document.getElementById(\u0026#34;time\u0026#34;).innerHTML = formattedDate; } setInterval(showTime, 1000); 这段代码是用 JavaScript 实现一个实时显示当前时间的页面特效,原理： 首先定义了一个 showTime() 函数，它通过创建一个新的 Date 对象来获取当前时间，并从中提取年、月、日、小时、分钟和秒的值。然后使用 padStart() 函数来确保月、日、小时、分钟和秒都是两位数（如果前缀不足，则在前面添加 \u0026lsquo;0\u0026rsquo;）。 接下来，将各个提取的时间值整合为一个字符串 formattedDate，其中各个部分之间添加了一些中文字符作为分隔符。 最后，将拼接好的时间字符串赋值给页面上带有 \u0026ldquo;time\u0026rdquo; ID 的元素，并通过调用 setInterval() 函数每隔一秒钟更新一次该元素的内容，从而实现了实时显示当前时间的效果。\n这个例子比较简单，但是通过这个例子可以了解到 JavaScript 中获取和处理时间的基本方法，以及如何使用定时器来定期更新页面内容。\necharts\nhttps://echarts.apache.org/examples/zh/index.html#chart-type-line\n例子：https://echarts.apache.org/examples/zh/editor.html?c=gauge-simple\n我们要使用 这个JS来进行大屏展示 CPU\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 var myChart_CPU = echarts.init(document.getElementById(\u0026#39;cpu\u0026#39;)); // 指定图表的配置项和数据 var option_cpu = { tooltip: { formatter: \u0026#39;{a} \u0026lt;br/\u0026gt;{b} : {c}%\u0026#39; }, series: [ { name: \u0026#39;CPU\u0026#39;, type: \u0026#39;gauge\u0026#39;, progress: { show: true }, detail: { valueAnimation: true, formatter: \u0026#39;{value}\u0026#39; }, data: [ { value: \u0026#39;{{ cpu_percent }} %\u0026#39;, name: \u0026#39;CPU使用率\u0026#39; } ] } ] }; // 定义函数，发送 Ajax 请求获取内存使用率数据 function getCPUData() { // 使用原生 JavaScript 发送 Ajax 请求 // var xhr = new XMLHttpRequest(); // xhr.onreadystatechange = function () { // if (xhr.readyState === 4 \u0026amp;\u0026amp; xhr.status === 200) { // var cpuPercent = JSON.parse(xhr.responseText).cpu_percent; // updateChart_cpu(cpuPercent); // 调用更新图表的函数 // } // }; // xhr.open(\u0026#39;GET\u0026#39;, \u0026#39;/get_cpu_data\u0026#39;, true); // xhr.send(); // 使用 jQuery 发送 Ajax 请求 $.ajax({ url: \u0026#39;/get_cpu_data\u0026#39;, dataType: \u0026#39;json\u0026#39;, type: \u0026#39;GET\u0026#39;, success: function (data) { var cpuPercent = data.cpu_percent; updateChart_cpu(cpuPercent); // 调用更新图表的函数 }, error: function (xhr, status, error) { console.log(\u0026#39;获取数据失败\u0026#39;); } }); } // 定义函数，更新图表数据并重新渲染图表 function updateChart_cpu(cpuPercent) { option_cpu.series[0].data[0].value = cpuPercent; myChart_CPU.setOption(option_cpu); } // 使用刚指定的配置项和数据显示图表。 myChart_CPU.setOption(option_cpu); // 使用 setInterval 定时刷新数据 setInterval(getCPUData, 5000); // 每5秒刷新一次数据 这个只是前端的JS请求后端的接口获取到数据才会显示（/get_cpu_data）\n1 2 3 4 5 6 @app.route(\u0026#39;/get_cpu_data\u0026#39;) def get_cpu_data(): # 获取机器的 CPU 使用率 cpu_percent = psutil.cpu_percent() print(cpu_percent) return jsonify(cpu_percent=cpu_percent) 进击版（JS和CSS剥离） CSS 在static 目录 创建 CSS文件目录 cpu.css\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #cpu { position: absolute; width: 30%; height: 40%; top: 10%; left: 0%; background: #48dada; color: white; font-size: 30px; display: flex; align-items: center; justify-content: center; font-weight: bold; /* 加粗字体 */ border-radius: 10px; /* 增加圆润感 */ box-shadow: 0 0 10px rgba(0, 0, 0, 0.2); /* 增加一些阴影，增加科技感 */ } 在html页面引入相关的css\n1 \u0026lt;link href=\u0026#34;../static/css/main.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; JS cpu.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 var myChart_CPU = echarts.init(document.getElementById(\u0026#39;cpu\u0026#39;)); // 指定图表的配置项和数据 var option_cpu = { tooltip: { formatter: \u0026#39;{a} \u0026lt;br/\u0026gt;{b} : {c}%\u0026#39; }, series: [ { name: \u0026#39;CPU\u0026#39;, type: \u0026#39;gauge\u0026#39;, progress: { show: true }, detail: { valueAnimation: true, formatter: \u0026#39;{value}\u0026#39; }, data: [ { value: \u0026#39;0\u0026#39;, name: \u0026#39;CPU使用率\u0026#39; } ] } ] }; // 使用刚指定的配置项和数据显示图表。 myChart_CPU.setOption(option_cpu); controller.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 function getCPUData() { // 使用原生 JavaScript 发送 Ajax 请求 // var xhr = new XMLHttpRequest(); // xhr.onreadystatechange = function () { // if (xhr.readyState === 4 \u0026amp;\u0026amp; xhr.status === 200) { // var cpuPercent = JSON.parse(xhr.responseText).cpu_percent; // updateChart_cpu(cpuPercent); // 调用更新图表的函数 // } // }; // xhr.open(\u0026#39;GET\u0026#39;, \u0026#39;/get_cpu_data\u0026#39;, true); // xhr.send(); // 使用 jQuery 发送 Ajax 请求 $.ajax({ url: \u0026#39;/get_cpu_data\u0026#39;, dataType: \u0026#39;json\u0026#39;, type: \u0026#39;GET\u0026#39;, success: function (data) { var cpuPercent = data.cpu_percent; updateChart_cpu(cpuPercent); // 调用更新图表的函数 }, error: function (xhr, status, error) { console.log(\u0026#39;获取数据失败\u0026#39;); } }); } // 定义函数，更新图表数据并重新渲染图表 function updateChart_cpu(cpuPercent) { option_cpu.series[0].data[0].value = cpuPercent; myChart_CPU.setOption(option_cpu); } getCPUData(); setInterval(getCPUData, 5000); 在html页面进行引入，切记，controller.js 最好一个引入\n1 2 \u0026lt;script src=\u0026#34;../static/js/cpu.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;../static/js/controller.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 正常显示\n后续如果继续深入研究，后端框架可以换成高性能的tornado或者功能更强大的Django，可视化的组件可以换成pyecharts，前端可以使用vue，react框架等。还可以考虑加入sqlite数据库或连接db数据库，打造成一个更完整的项目。\n参考文档： https://www.cnblogs.com/hugboy/p/15427793.html https://zhuanlan.zhihu.com/p/584796840 https://github.com/xiaokai1996/big_screen/blob/master/big_screen%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md https://blog.csdn.net/dwhyxjfm/article/details/127946379\n","date":"2023-09-22T16:13:03Z","image":"https://www.ownit.top/title_pic/27.jpg","permalink":"https://www.ownit.top/p/202309221613/","title":"Flask配合Echarts写一个动态可视化大屏"},{"content":"引言 在开发分布式系统时，调试是一个重要但复杂的环节。开发者通常需要跨越多个服务、模块和线程来追踪和解决问题。在没有远程调试的情况下，许多开发者会在代码中添加各种日志语句，然后重新部署和上线来调试。这种方法不仅费时，而且可能引入额外的错误或问题。\n有时候，在本地环境中调试时没有发现问题，但当代码被打包并部署到测试环境时，却会出现一堆莫名其妙的问题。这可能是因为不同的环境、配置或服务之间的交互导致了问题的出现。\n幸运的是，有一种强大的工具可以帮助解决这些问题，那就是远程调试。通过远程调试，开发者可以在远程服务器上直接对Java应用程序进行调试，就像在本地环境中一样。这样，就可以避免重新部署和上线的耗时过程，并且可以实时查看和修改变量的值、查看堆栈信息等。\n本文将介绍如何使用IntelliJ IDEA的远程调试功能来高效调试分布式系统。此外，我们还将探讨一些常见的调试问题和解决方法，帮助你更好地应对分布式系统开发中的挑战。\n1、准备工作 1、确保你的IDEA已经安装并配置好，且已安装了Java开发工具包（JDK）。 确保你的Java应用程序已经部署到远程服务器上，并且可以通过远程访问进行调试。\n2、启动调试应用 使用特定JVM参数运行服务端代码 要让远程服务器运行的代码支持远程调试，则启动的时候必须加上特定的JVM参数，这些参数是：\n1 2 3 4 5 6 -Xdebug -Xrunjdwp:transport=dt_socket,suspend=n,server=y,address=${debug_port} 或者 -Xdebug -Xrunjdwp:transport=dt_socket,suspend=n,server=y,address=127.0.0.1:5555 将address设置为127.0.0.1:5555，表示将调试端口限制为本地访问，远程无法访问， 设置参数应该注意 其中的${debug_port}是用户自定义的，为debug端口，本例以5005端口为例。 如果只是临时调试，在端口号前面不要加上限制访问的IP地址，调试完成之后，将上述JVM参数去除掉之后重新发布下，防范开放远程调试端口可能带来的安全风险。\n1 java -Dspring.config.location=application-pre.yml -Xdebug -Xrunjdwp:transport=dt_socket,address=5005,server=y,suspend=n -jar fubaodai-app-1.0-SNAPSHOT.jar 3、配置远程调试 在IDEA中，打开\u0026quot;Run/Debug Configurations\u0026quot;对话框。 在对话框中，点击\u0026quot;+\u0026ldquo;按钮，选择\u0026quot;Remote\u0026rdquo;。 在弹出的对话框中，填写远程服务器的连接信息，包括服务器IP地址、端口号、调试协议等。 点击\u0026quot;OK\u0026quot;按钮保存配置。 或者 Remote 4、本地IDEA启动debug模式 **要求：**双方代码一致，否则远程调试无法启动； 本地启动刚刚配置的 Remote Server，正常时会看到日志:\n1 Connected to the target VM, address: \u0026#39;xxx:5005\u0026#39;, transport: \u0026#39;socket\u0026#39; 5、设置断点，开始调试 本地 IDEA 代码中设置断点 浏览器或手机 HTTP 访问服务器 IDEA 即可在断点暂停并跟踪 6、远程调试的问题和注意事项 确保远程服务器的Java版本和IDEA的Java版本一致。 确保远程服务器的防火墙设置允许IDEA的调试连接通过。 确保远程服务器上已经安装了所需的调试工具（如jdb）。 在远程调试时，需要注意保护敏感信息，如密码、密钥等。 在修改代码时，需要同步远程服务器上的代码，以免出现版本不一致的问题。 ","date":"2023-09-06T15:14:42Z","image":"https://www.ownit.top/title_pic/25.jpg","permalink":"https://www.ownit.top/p/202309061514/","title":"IntelliJ IDEA远程调试：使用IDEA Remote Debug进行高效调试的指南"},{"content":"引言 Jumpserver是一款强大的堡垒机系统，可以有效管理和控制企业内部服务器的访问权限，提高网络安全性。本文将介绍如何使用Python编程语言，结合Jumpserver提供的API接口，实现对跳板机的管理和操作。\n1、什么是Jumpserver？ Jumpserver是一种堡垒机系统，它提供了一种安全且集中的方式来管理和控制用户对服务器的访问权限。Jumpserver可以帮助企业实现统一认证、审计日志记录、权限管理等功能，从而提高网络安全性。\n2、Jumpserver提供的API接口 Jumpserver提供了一组强大的API接口，可以实现对跳板机的管理和操作。这些API包括获取服务器列表、认证访问权限、执行命令和文件传输等功能，可以通过HTTP请求进行调用。\n目前使用的版本：v3.6.2 不同步的版本，API 接口有变化\nhttps://docs.jumpserver.org/zh/v3/dev/rest_api/ 3、Python中的HTTP请求库 在Python中，我们可以使用第三方的HTTP请求库，如requests库或http.client库，来发送HTTP请求并获取响应。这些库提供了简洁的接口，方便我们与Jumpserver的API进行交互。\nrequests库： requests是一个功能强大、简单易用的第三方HTTP请求库，广泛应用于Python开发中。它提供了简洁的API，使得发送HTTP请求变得非常简单。使用requests库，我们可以发送各种类型的HTTP请求（GET、POST、PUT等），设置请求头、请求参数、请求体等，并能够获得服务器响应的状态码、内容等信息。\n示例代码：\n1 2 3 4 5 6 7 8 9 10 import requests # 发送GET请求并获取响应 response = requests.get(\u0026#39;https://api.example.com/users\u0026#39;) # 获取响应内容 content = response.text # 获取响应状态码 status_code = response.status_code 4、使用Python调用Jumpserver API进行认证 在使用Jumpserver的API之前，我们需要先进行认证。通常，Jumpserver会提供一个登录接口，我们可以使用用户名和密码进行登录，并获取到访问令牌（Access Token）。在后续的API请求中，我们需要将该访问令牌作为认证凭证。 https://docs.jumpserver.org/zh/v3/dev/rest_api/#12 推荐使用：Access Key方式 或者 Private Token 1 2 3 4 5 6 7 8 docker exec -it jms_core /bin/bash cd /opt/jumpserver/apps python manage.py shell from users.models import User u = User.objects.get(username=\u0026#39;admin\u0026#39;) u.create_private_token() 已经存在 private_token，可以直接获取即可 u.private_token 此处我才用双层验证方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 KeyID = \u0026#39;77e76d19-141c-xxx-8d2b-xxxx\u0026#39; SecretID = \u0026#39;a04817bc-0bb1-439f-baa2-xxxx\u0026#39; gmt_form = \u0026#39;%a, %d %b %Y %H:%M:%S GMT\u0026#39; ptivate_token = \u0026#39;xxxxxxxxxx\u0026#39; headers = { \u0026#39;accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, # \u0026#39;X-CSRFToken\u0026#39;: \u0026#39;eoLo2AVcQK5X1JQ392JHCzjZ8wPCWZJFJao5O9ObH8zQwtiPhGBzaOnNKjaENShf\u0026#39;, \u0026#34;Authorization\u0026#34;: \u0026#39;Token \u0026#39; + ptivate_token, \u0026#39;X-JMS-ORG\u0026#39;: \u0026#39;00000000-0000-0000-0000-000000000002\u0026#39;, \u0026#39;Date\u0026#39;: datetime.datetime.utcnow().strftime(gmt_form) } # 认证 def get_auth(KeyID, SecretID): \u0026#34;\u0026#34;\u0026#34; 认证 :param KeyID: The key ID :param SecretID: The secret ID :return: \u0026#34;\u0026#34;\u0026#34; signature_headers = [\u0026#39;(request-target)\u0026#39;, \u0026#39;accept\u0026#39;, \u0026#39;date\u0026#39;] auth = HTTPSignatureAuth(key_id=KeyID, secret=SecretID, algorithm=\u0026#39;hmac-sha256\u0026#39;, headers=signature_headers) return auth auth = get_auth(KeyID, SecretID) 5、Jumpserver接口自动调用 获取所有用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 获取所有用户 def get_user_all(): \u0026#34;\u0026#34;\u0026#34; 获取所有用户 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#39;/api/v1/users/users/\u0026#39; response = requests.get(url, auth=auth, headers=headers) user_list = json.loads(response.text) count = 0 for i in user_list: count += 1 print(i) print(count) 获取监控指标\n1 2 3 4 5 6 7 8 9 10 # 获取监控指标 def get_prometheus_metric(): \u0026#34;\u0026#34;\u0026#34; 获取监控指标 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/prometheus/metrics/\u0026#34; response = requests.get(url, headers=headers, auth=auth) print(response.text) return response.text 获取所有资产节点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 获取所有资产节点 def get_node_all(): \u0026#34;\u0026#34;\u0026#34; 获取所有资产节点 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/nodes/\u0026#34; response = requests.get(url, headers=headers, auth=auth) node_list = json.loads(response.text) count = 0 for i in node_list: count += 1 print(i) print(count) return response.json() 查看当前token（即admin）的所有资产\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def get_asset_all(): \u0026#34;\u0026#34;\u0026#34; 查看当前token（即admin）的所有资产 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/assets/\u0026#34; response = requests.get(url, headers=headers, auth=auth) node_list = json.loads(response.text) count = 0 for i in node_list: count += 1 print(i) print(count) return response.json() 创建资产节点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def assets_nodes_create(node_name): \u0026#34;\u0026#34;\u0026#34; 创建资产节点 :param node_name: :return: \u0026#34;\u0026#34;\u0026#34; node_data = { \u0026#34;value\u0026#34;: node_name } url = jms_url + \u0026#34;/api/v1/assets/nodes/\u0026#34; node_info = get_node_info(node_name) if node_info: # 根据node_name去查询，如果查到了说明已经有了。 print(\u0026#34;{name}已存在, id: {id}\u0026#34;.format(name=node_name, id=node_info[0][\u0026#34;id\u0026#34;])) else: data = json.dumps(node_data) resp = requests.post(url, headers=headers, data=data) return resp.json() 根据ip获取资产信息\n1 2 3 4 5 6 7 8 9 10 11 12 def get_assets_list_by_ip(ip): \u0026#34;\u0026#34;\u0026#34; 根据ip获取资产信息 :param ip: :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/assets/\u0026#34; response = requests.get(url, headers=headers, params={ \u0026#34;ip\u0026#34;: ip }) print(response.json()) return response.json() 查看资产节点信息\n1 2 3 4 5 6 7 8 9 10 11 12 def get_node_info(node_name): \u0026#34;\u0026#34;\u0026#34; 查看资产节点信息 :param node_name: 节点名称 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/nodes/\u0026#34; response = requests.get(url, auth=auth, headers=headers, params={ \u0026#34;value\u0026#34;: node_name }) print(response.text) return response.json() 创建资产机器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 创建资产机器 def asset_create(ip, hostname, node_id, comment): \u0026#34;\u0026#34;\u0026#34; 创建资产机器 :param ip: ip地址 :param hostname: 主机名 :param node_id: 节点id :return: 返回创建的资产信息 \u0026#34;\u0026#34;\u0026#34; asset_Data = { \u0026#34;name\u0026#34;: hostname, \u0026#34;address\u0026#34;: ip, \u0026#34;platform\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;protocols\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;port\u0026#34;: 22 }], \u0026#34;is_active\u0026#34;: True, \u0026#34;nodes\u0026#34;: [node_id], \u0026#34;comment\u0026#34;: comment, \u0026#34;accounts\u0026#34;: [{ # 账号模板id \u0026#34;template\u0026#34;: \u0026#34;60b11033-a6e1-467d-8388-68a0e64134ff\u0026#34;, }] } url = jms_url + \u0026#34;/api/v1/assets/hosts/\u0026#34; print(url) data = json.dumps(asset_Data) print(data) response = requests.post(url, auth=auth, headers=headers, data=data) print(response.text) 运行创建服务器资产\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 运行创建服务器资产 def run_create_assets(node_name, project_name, ip, comment): \u0026#34;\u0026#34;\u0026#34; 运行创建服务器资产 :param node_name: 节点名称 :param project_name: 机器名称 :param ip: ip地址 :param comment: 备注 :return: \u0026#34;\u0026#34;\u0026#34; # 节点id，无节点时创建节点 node_info = get_node_info(node_name) # 如果len(node_info) == 0 说明没有节点，需要创建节点 if len(node_info) == 0: # 创建节点 node_id = assets_nodes_create(node_name) print(node_id) else: # 获取节点id node_id = node_info[0][\u0026#34;id\u0026#34;] print(node_id) # 管理用户 id hostname = \u0026#34;{ip}_{project_name}\u0026#34;.format(ip=ip, project_name=project_name) # 查IP,创建资产 ip_info = get_assets_list_by_ip(ip) if ip_info: print(\u0026#34;%s 已存在，nodes: %s\u0026#34; % (ip_info[0][\u0026#34;address\u0026#34;], ip_info[0][\u0026#34;nodes\u0026#34;])) else: asset_create(ip, hostname, node_id, comment) 获取组织信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def get_org_info(): \u0026#34;\u0026#34;\u0026#34; 获取组织信息 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/orgs/orgs/\u0026#34; response = requests.get(url, headers=headers) org_list = response.text print(org_list) for i in org_list.split(\u0026#34;id\u0026#34;): print(i) return response.json() 6、完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 # -*- coding: utf-8 -*- # @Time : 2023/8/29 14:21 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : jms_add.py # @Software: PyCharm import requests, datetime, json from httpsig.requests_auth import HTTPSignatureAuth KeyID = \u0026#39;77e76d19-141c-4545--xxx\u0026#39; SecretID = \u0026#39;a04817bc-0bb1-439f-baa2-xxxx\u0026#39; gmt_form = \u0026#39;%a, %d %b %Y %H:%M:%S GMT\u0026#39; ptivate_token = \u0026#39;xxxxx\u0026#39; headers = { \u0026#39;accept\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, # \u0026#39;X-CSRFToken\u0026#39;: \u0026#39;eoLo2AVcQK5X1JQ392JHCzjZ8wPCWZJFJao5O9ObH8zQwtiPhGBzaOnNKjaENShf\u0026#39;, \u0026#34;Authorization\u0026#34;: \u0026#39;Token \u0026#39; + ptivate_token, \u0026#39;X-JMS-ORG\u0026#39;: \u0026#39;00000000-0000-0000-0000-000000000002\u0026#39;, \u0026#39;Date\u0026#39;: datetime.datetime.utcnow().strftime(gmt_form) } # 认证 def get_auth(KeyID, SecretID): \u0026#34;\u0026#34;\u0026#34; 认证 :param KeyID: The key ID :param SecretID: The secret ID :return: \u0026#34;\u0026#34;\u0026#34; signature_headers = [\u0026#39;(request-target)\u0026#39;, \u0026#39;accept\u0026#39;, \u0026#39;date\u0026#39;] auth = HTTPSignatureAuth(key_id=KeyID, secret=SecretID, algorithm=\u0026#39;hmac-sha256\u0026#39;, headers=signature_headers) return auth auth = get_auth(KeyID, SecretID) # 获取所有用户 def get_user_all(): \u0026#34;\u0026#34;\u0026#34; 获取所有用户 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#39;/api/v1/users/users/\u0026#39; response = requests.get(url, auth=auth, headers=headers) user_list = json.loads(response.text) count = 0 for i in user_list: count += 1 print(i) print(count) # 获取监控指标 def get_prometheus_metric(): \u0026#34;\u0026#34;\u0026#34; 获取监控指标 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/prometheus/metrics/\u0026#34; response = requests.get(url, headers=headers, auth=auth) print(response.text) return response.text # 获取所有资产节点 def get_node_all(): \u0026#34;\u0026#34;\u0026#34; 获取所有资产节点 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/nodes/\u0026#34; response = requests.get(url, headers=headers, auth=auth) node_list = json.loads(response.text) count = 0 for i in node_list: count += 1 print(i) print(count) return response.json() # 查看当前token（即admin）的所有资产 def get_asset_all(): \u0026#34;\u0026#34;\u0026#34; 查看当前token（即admin）的所有资产 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/assets/\u0026#34; response = requests.get(url, headers=headers, auth=auth) node_list = json.loads(response.text) count = 0 for i in node_list: count += 1 print(i) print(count) return response.json() ################################################################################################### # 创建资产节点 def assets_nodes_create(node_name): \u0026#34;\u0026#34;\u0026#34; 创建资产节点 :param node_name: :return: \u0026#34;\u0026#34;\u0026#34; node_data = { \u0026#34;value\u0026#34;: node_name } url = jms_url + \u0026#34;/api/v1/assets/nodes/\u0026#34; node_info = get_node_info(node_name) if node_info: # 根据node_name去查询，如果查到了说明已经有了。 print(\u0026#34;{name}已存在, id: {id}\u0026#34;.format(name=node_name, id=node_info[0][\u0026#34;id\u0026#34;])) else: data = json.dumps(node_data) resp = requests.post(url, headers=headers, data=data) return resp.json() # def get_assets_list_by_ip(ip): \u0026#34;\u0026#34;\u0026#34; 根据ip获取资产信息 :param ip: :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/assets/\u0026#34; response = requests.get(url, headers=headers, params={ \u0026#34;ip\u0026#34;: ip }) print(response.json()) return response.json() # 查看资产节点信息 def get_node_info(node_name): \u0026#34;\u0026#34;\u0026#34; 查看资产节点信息 :param node_name: 节点名称 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/assets/nodes/\u0026#34; response = requests.get(url, auth=auth, headers=headers, params={ \u0026#34;value\u0026#34;: node_name }) print(response.text) return response.json() # 创建资产机器 def asset_create(ip, hostname, node_id, comment): \u0026#34;\u0026#34;\u0026#34; 创建资产机器 :param ip: ip地址 :param hostname: 主机名 :param node_id: 节点id :return: 返回创建的资产信息 \u0026#34;\u0026#34;\u0026#34; asset_Data = { \u0026#34;name\u0026#34;: hostname, \u0026#34;address\u0026#34;: ip, \u0026#34;platform\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;protocols\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;port\u0026#34;: 22 }], \u0026#34;is_active\u0026#34;: True, \u0026#34;nodes\u0026#34;: [node_id], \u0026#34;comment\u0026#34;: comment, \u0026#34;accounts\u0026#34;: [{ # 账号模板id \u0026#34;template\u0026#34;: \u0026#34;60b11033-a6e1-467d-8388-68a0e64134ff\u0026#34;, }] } url = jms_url + \u0026#34;/api/v1/assets/hosts/\u0026#34; print(url) data = json.dumps(asset_Data) print(data) response = requests.post(url, auth=auth, headers=headers, data=data) print(response.text) # 运行创建服务器资产 def run_create_assets(node_name, project_name, ip, comment): \u0026#34;\u0026#34;\u0026#34; 运行创建服务器资产 :param node_name: 节点名称 :param project_name: 机器名称 :param ip: ip地址 :param comment: 备注 :return: \u0026#34;\u0026#34;\u0026#34; # 节点id，无节点时创建节点 node_info = get_node_info(node_name) # 如果len(node_info) == 0 说明没有节点，需要创建节点 if len(node_info) == 0: # 创建节点 node_id = assets_nodes_create(node_name) print(node_id) else: # 获取节点id node_id = node_info[0][\u0026#34;id\u0026#34;] print(node_id) # 管理用户 id hostname = \u0026#34;{ip}_{project_name}\u0026#34;.format(ip=ip, project_name=project_name) # 查IP,创建资产 ip_info = get_assets_list_by_ip(ip) if ip_info: print(\u0026#34;%s 已存在，nodes: %s\u0026#34; % (ip_info[0][\u0026#34;address\u0026#34;], ip_info[0][\u0026#34;nodes\u0026#34;])) else: asset_create(ip, hostname, node_id, comment) def get_org_info(): \u0026#34;\u0026#34;\u0026#34; 获取组织信息 :return: \u0026#34;\u0026#34;\u0026#34; url = jms_url + \u0026#34;/api/v1/orgs/orgs/\u0026#34; response = requests.get(url, headers=headers) org_list = response.text print(org_list) for i in org_list.split(\u0026#34;id\u0026#34;): print(i) return response.json() if __name__ == \u0026#39;__main__\u0026#39;: jms_url = \u0026#39;https://jms.xxx.top\u0026#39; username = \u0026#39;admin\u0026#39; password = \u0026#39;xxxxxx\u0026#39; # 获取token # token = get_token(jms_url, username, password) # 创建资产节点 # assets_nodes_create(\u0026#34;k8s\u0026#34;) # 根据ip获取资产信息 # get_assets_list_by_ip(\u0026#34;192.168.11.10\u0026#34;) # 查看资产节点信息 # get_node_info(\u0026#34;k8s\u0026#34;) # 创建资产调用 # node_id = [\u0026#34;e8641c37-93e3-450e-aaf8-64d5baa69753\u0026#34;] # get_node_info(\u0026#34;k8s\u0026#34;) # asset_create(ip, hostname, node_id) # 运行创建服务器资产 # run_create_assets(\u0026#34;test\u0026#34;, \u0026#34;风控\u0026#34;, \u0026#34;192.168.11.10\u0026#34;, \u0026#34;测试\u0026#34;) # 获取组织信息 # get_org_info() # 获取所有用户 get_user_all() ","date":"2023-09-05T11:44:32Z","image":"https://www.ownit.top/title_pic/55.jpg","permalink":"https://www.ownit.top/p/202309051144/","title":"Python调用Jumpserver的Api接口增删改查"},{"content":"Kubernetes API使用 1、 API是什么？ API（Application Programming Interface，应用程序接口）： 是一些预先定义的接口（如函数、HTTP接口），或指软件系统不同组成部分衔接的约定。 用来提供应用程序与开发人员基于某软件或硬件得以访问的一组例程，而又无需访问源码，或理解内部工作机制的细节。\nK8s也提供API接口，提供这个接口的是管理节点的apiserver组件，apiserver服务负责提供HTTP API，以便用户、其他组件相互通信。\n有两种方式可以操作K8s中的资源：\nHTTP API：https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/ 客户端库： https://kubernetes.io/zh/docs/reference/using-api/client-libraries/ 2、K8s认证方式 K8s支持三种客户端身份认证：\nHTTPS 证书认证：基于CA证书签名的数字证书认证\nHTTP Token认证：通过一个Token来识别用户\nHTTP Base认证：用户名+密码的方式认证\nHTTPS证书认证（kubeconfig）：\n1 2 3 4 5 6 7 import os from kubernetes import client, config config.load_kube_config(file_path) # 指定kubeconfig配置文件 apps_api = client.AppsV1Api() # 资源接口类实例化 for dp in apps_api.list_deployment_for_all_namespaces().items: print(dp) HTTP Token认证（ServiceAccount）：\n1 2 3 4 5 6 7 8 from kubernetes import client, config configuration = client.Configuration() configuration.host = \u0026#34;https://192.168.31.61:6443\u0026#34; # APISERVER地址 configuration.ssl_ca_cert=\u0026#34;ca.crt\u0026#34; # CA证书 configuration.verify_ssl = True # 启用证书验证 configuration.api_key = {\u0026#34;authorization\u0026#34;: \u0026#34;Bearer \u0026#34; + token} # 指定Token字符串 client.Configuration.set_default(configuration) apps_api = client.AppsV1Api() 获取Token字符串：创建service account并绑定默认cluster-admin管理员集群角色：\n1 2 3 4 5 6 # 创建用户 $ kubectl create serviceaccount dashboard-admin -n kube-system # 用户授权 $ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin # 获取用户Token $ kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 其他常用资源接口类实例化：\n1 2 3 4 core_api = client.CoreV1Api() # namespace,pod,service,pv,pvc apps_api = client.AppsV1Api() # deployment networking_api = client.NetworkingV1beta1Api() # ingress storage_api = client.StorageV1Api() # storage_class Python调用 https://kubernetes.io/zh/docs/reference/using-api/client-libraries/ 相关代码示例：https://github.com/kubernetes-client/python/tree/master/examples\n1、安装插件模块 1 pip install kubernetes k8s.yml 文件则是 /root/.kube/config\n2、deployment服务 增加 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import os from kubernetes import client, config config.load_kube_config(\u0026#34;k8s.yml\u0026#34;) # 指定kubeconfig配置文件 apps_api = client.AppsV1Api() # 资源接口类实例化 core_api = client.CoreV1Api() # namespace,pod,service,pv,pvc def deploy_create(): \u0026#34;\u0026#34;\u0026#34; 创建一个deployment,命名空间默认为default，如果要创建其他命名空间的，需要先创建命名空间 :return: \u0026#34;\u0026#34;\u0026#34; namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; replicas = 3 labels = {\u0026#39;a\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;2\u0026#39;} # 不区分数据类型，都要加引号 image = \u0026#34;nginx\u0026#34; body = client.V1Deployment( api_version=\u0026#34;apps/v1\u0026#34;, kind=\u0026#34;Deployment\u0026#34;, metadata=client.V1ObjectMeta(name=name), spec=client.V1DeploymentSpec( replicas=replicas, selector={\u0026#39;matchLabels\u0026#39;: labels}, template=client.V1PodTemplateSpec( metadata=client.V1ObjectMeta(labels=labels), spec=client.V1PodSpec( containers=[client.V1Container( name=\u0026#34;web\u0026#34;, image=image )] ) ), ) ) try: apps_api.create_namespaced_deployment(namespace=namespace, body=body) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 400: print(e) print(\u0026#34;格式错误\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: deploy_create() 查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import os from kubernetes import client, config config.load_kube_config(\u0026#34;k8s.yml\u0026#34;) # 指定kubeconfig配置文件 apps_api = client.AppsV1Api() # 资源接口类实例化 core_api = client.CoreV1Api() # namespace,pod,service,pv,pvc def deploy_get(): \u0026#34;\u0026#34;\u0026#34; 查询一个deployment :return: \u0026#34;\u0026#34;\u0026#34; namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; try: resp = apps_api.read_namespaced_deployment(namespace=namespace, name=name) print(resp) print(\u0026#34;查询成功\u0026#34;) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 404: print(\u0026#34;没找到\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: deploy_get() 修改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import os from kubernetes import client, config config.load_kube_config(\u0026#34;k8s.yml\u0026#34;) # 指定kubeconfig配置文件 apps_api = client.AppsV1Api() # 资源接口类实例化 core_api = client.CoreV1Api() # namespace,pod,service,pv,pvc def deploy_update(): namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; replicas = 6 labels = {\u0026#39;a\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;2\u0026#39;} # 不区分数据类型，都要加引号 image = \u0026#34;nginx\u0026#34; body = client.V1Deployment( api_version=\u0026#34;apps/v1\u0026#34;, kind=\u0026#34;Deployment\u0026#34;, metadata=client.V1ObjectMeta(name=name), spec=client.V1DeploymentSpec( replicas=replicas, selector={\u0026#39;matchLabels\u0026#39;: labels}, template=client.V1PodTemplateSpec( metadata=client.V1ObjectMeta(labels=labels), spec=client.V1PodSpec( containers=[client.V1Container( name=\u0026#34;web\u0026#34;, image=image )] ) ), ) ) try: apps_api.patch_namespaced_deployment(namespace=namespace, name=name, body=body) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 400: print(e) print(\u0026#34;格式错误\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) elif status == 404: print(\u0026#34;没找到\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: deploy_update() 删除 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import os from kubernetes import client, config config.load_kube_config(\u0026#34;k8s.yml\u0026#34;) # 指定kubeconfig配置文件 apps_api = client.AppsV1Api() # 资源接口类实例化 core_api = client.CoreV1Api() # namespace,pod,service,pv,pvc # 删除一个deployment def deploy_delete(): namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; body = client.V1DeleteOptions() try: apps_api.delete_namespaced_deployment(namespace=namespace, name=name, body=body) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 404: print(\u0026#34;没找到\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) elif status == 409: print(\u0026#34;冲突\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: deploy_delete() 3、SVC服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def svc_get(): \u0026#34;\u0026#34;\u0026#34; 查询一个service :return: \u0026#34;\u0026#34;\u0026#34; # 查询 for svc in core_api.list_namespaced_service(namespace=\u0026#34;default\u0026#34;).items: print(svc.metadata.name) def svc_delete(): \u0026#34;\u0026#34;\u0026#34; 删除一个service :return: \u0026#34;\u0026#34;\u0026#34; namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; body = client.V1DeleteOptions() try: core_api.delete_namespaced_service(namespace=namespace, name=name, body=body) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 404: print(\u0026#34;没找到\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) elif status == 409: print(\u0026#34;冲突\u0026#34;) def svc_create(): \u0026#34;\u0026#34;\u0026#34; 创建一个service,命名空间默认为default，如果要创建其他命名空间的，需要先创建命名空间 :return: \u0026#34;\u0026#34;\u0026#34; namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; selector = {\u0026#39;a\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;2\u0026#39;} # 不区分数据类型，都要加引号 port = 80 target_port = 80 type = \u0026#34;NodePort\u0026#34; body = client.V1Service( api_version=\u0026#34;v1\u0026#34;, kind=\u0026#34;Service\u0026#34;, metadata=client.V1ObjectMeta( name=name ), spec=client.V1ServiceSpec( selector=selector, ports=[client.V1ServicePort( port=port, target_port=target_port )], type=type ) ) try: core_api.create_namespaced_service(namespace=namespace, body=body) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 400: print(e) print(\u0026#34;格式错误\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) def svc_update(): \u0026#34;\u0026#34;\u0026#34; 更新一个service :return: \u0026#34;\u0026#34;\u0026#34; namespace = \u0026#34;default\u0026#34; name = \u0026#34;api-test\u0026#34; body = client.V1Service( api_version=\u0026#34;v1\u0026#34;, kind=\u0026#34;Service\u0026#34;, metadata=client.V1ObjectMeta( name=name ), spec=client.V1ServiceSpec( selector={\u0026#39;a\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;b\u0026#39;: \u0026#39;2\u0026#39;}, ports=[client.V1ServicePort( port=80, target_port=8080 )], type=\u0026#34;NodePort\u0026#34; ) ) try: core_api.patch_namespaced_service(namespace=namespace, name=name, body=body) except Exception as e: status = getattr(e, \u0026#34;status\u0026#34;) if status == 400: print(e) print(\u0026#34;格式错误\u0026#34;) elif status == 403: print(\u0026#34;没权限\u0026#34;) ","date":"2023-08-31T18:41:32Z","image":"https://www.ownit.top/title_pic/43.jpg","permalink":"https://www.ownit.top/p/202308311841/","title":"掌握Kubernetes API：释放容器编排的潜力"},{"content":"一、JupyerHub jupyter notebook 是一个非常有用的工具，我们可以在浏览器中任意编辑调试我们的python代码，并且支持markdown 语法，可以说是科研利器。但是这种情况适合个人使用，也就是jupyter notebook以我们自己的主机作为服务器，然后我们用自己的浏览器编辑自己本机的python代码。\n最近公司搭建了业务模型的服务器，每个人都有一个用户可以使用GPU资源，但是每次写代码要在本地调试好了然后再ssh提交到服务器运行，如果有问题，还要再在本地更改然后再次提交，非常的麻烦。为了解决这个烦恼，我们在GPU服务器上搭建了jupyterhub, 它和notebook不同之处在于它是一个hub，哈哈，也就是notebook的服务器，把它装在服务器上，然后大家可以通过局域网在浏览器上进行python代码的编辑和调试。\njupyterhub 和 jupyter notebook一样是python的一个包，可以通过pip安装，也可以通过 conda安装，在服务器端安装就可以供大家使用。\n二、环境 确保你的服务器环境满足以下要求：\n一台运行支持的操作系统（如 Ubuntu、CentOS 等）的服务器 安装了 Python 和 pip 足够的系统资源（CPU、内存、磁盘空间） 1、Python环境 安装 Miniconda 是管理 Python 环境以及安装 Python 包的一个方便工具。 以下是在 CentOS 上安装 Miniconda 的步骤：\n下载 Miniconda 安装包： 在终端中执行以下命令，下载适用于 CentOS 的 Miniconda 安装包（64 位）：\n1 2 3 4 5 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 或 curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 运行安装脚本： 运行下载的脚本来安装 Miniconda。首先，给脚本执行权限：\n1 chmod +x Miniconda3-latest-Linux-x86_64.sh 然后运行安装脚本：\n1 ./Miniconda3-latest-Linux-x86_64.sh 按照提示，阅读许可协议并同意。\n配置 Miniconda： 安装过程中会提示你是否将 Miniconda 添加到 shell 的 PATH 中。选择 \u0026ldquo;yes\u0026rdquo;，这将允许你在终端中直接使用 conda 命令。\n重启终端： 安装完成后，为了使配置生效，关闭当前终端窗口，然后重新打开一个新的终端。\n5.测试安装：\n在新的终端中，运行以下命令来验证 conda 是否已成功安装：\n1 2 root@bt:/home/fengkong# conda --version conda 23.5.0 安装插件补全 1 conda install -c conda-forge conda-bash-completion 备注：\n1 2 3 4 5 6 7 3、配置环境变量：.bashrc 如果选择了init会自己配置 export PATH=~/miniconda3/bin:$PATH 3.5、卸载miniconda 找到miniconda3的文件夹，使用rm命令将它删除： 然后，用vim命令进入.bashrc文件，将conda的语句用#注释掉 最后，重新激活一下source .bashrc就可以了。 2、Jupyter Lab 安装 JupyterLab 非常简单，只需一行命令即可：\n1 2 3 4 conda 安装 [Miniconda] conda install -c conda-forge jupyterlab or pip 安装 pip install jupyterlab 注册环境 1 2 conda create -n python3_7 python=3.7 ipykernel or 创建后 conda install ipykernel 环境添加到ipykernel （添加的是当前环境 需要先activate到需要环境） 1 python -m ipykernel install --user --name python3_7 --display-name \u0026#34;python3_7\u0026#34; 查看，删除注册到内核的环境 1 2 3 jupyter kernelspec list 查看 jupyter kernelspec remove python3_7 删除 conda remove -n py36 --all 删环境 配置文件 1 2 3 4 5 6 7 8 9 10 11 jupyter lab --generate-config 生成配置 c.ServerApp.ip = \u0026#39;*\u0026#39; 所有人访问 c.LabApp.open_browser = False 不再默认打开浏览器 jupyter lab password 修改密码 conda config --set auto_activate_base false 禁止自启环境 打开这个`jupyterhub_config.py` c.Spawner.default_url = \u0026#39;/lab\u0026#39; c.JupyterHub.ip = \u0026#39;0.0.0.0\u0026#39; 安装插件 1 conda install nodejs 装中文 1 pip install jupyterlab-language-pack-zh-CN 启动 1 nohup jupyterhub -f /opt/jupyterhub/jupyterhub_config.py \u0026amp; 3、关联系统的其它认证用户 用Root账户启动，才能关联系统的其它认证用户\n平台环境基于jupyterhub+conda构建，默认环境是ubuntu账户下的conda环境，请勿用作开发环境， 请创建并使用自己的linux账户后，再自建环境使用\n环境注册命令\n环境名称 {python版本}{具体功用(看是否专用项目)}{使用人} 示例：python38_{xx模型}_long\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; 注册环境：在创建环境的同时添加ipykernel核心 conda create -n python38_xxx python=3.8 ipykernel \u0026gt; 环境添加到ipykernel（操作前必须先激活进入对应环境） conda activate python38_xxx python -m ipykernel install --user --name python38_xxx --display-name \u0026#34;python38_xxx\u0026#34; 内核注册异常处理 1、手动执行内核添加命令 如上: python -m ipykernel install --user --name python38_xxx --display-name \u0026#34;python38_xxx\u0026#34; 2、重启面板，File-\u0026gt;HubControlPanel-\u0026gt;stopMyServer\u0026amp;startMyServer 3、选择内核时可选中always start the preferred kernel，设置为默认内核环境 导入windows环境包\n1 2 3 4 5 6 7 8 9 10 pip list --format=freeze \u0026gt; requirements.txt 进入到需要安装的环境 conda activate python38_xxx 删除requirements.txt文件多余的包(conda自带或windows特有 类似：conda,clyent,distribute,pip,jupyter,setuptools,wheel .. 安装过程失败或卡住不动的包，建议删除后继再单独安装 pip导入 python -m pip install -r requirements.txt ","date":"2023-08-17T15:05:02Z","image":"https://www.ownit.top/title_pic/45.jpg","permalink":"https://www.ownit.top/p/202308171505/","title":"JupyterHub实战应用"},{"content":"1、背景 在多语言开发环境中，我们经常需要进行跨语言的操作。有时，我们可能会在Python环境下需要使用Java的库或者功能。这个博客将展示如何在Python中调用Java的JavaParser库来解析Java源代码。\n2、需求 在许多软件开发场景中，我们可能需要解析Java源代码以获取其结构信息。比如说，我们可能希望知道源代码中定义了哪些类和方法。虽然Python库javalang可以在一定程度上满足我们的需求，但它不能产生我们想要的JSON格式的输出，该格式以类似于 [{\u0026ldquo;code\u0026rdquo;: \u0026ldquo;xxxx\u0026rdquo;}, {\u0026ldquo;code\u0026rdquo;: \u0026ldquo;xxxx\u0026rdquo;}, {\u0026ldquo;code\u0026rdquo;: \u0026ldquo;xxxx\u0026rdquo;}] 的形式，列出了源代码中的每个类和方法。 为了解决这个问题，我们可以使用Java的JavaParser库。以下是一个示例，展示了如何使用JavaParser来解析Java源代码，并生成我们想要的JSON格式的输出。\n3、JavaParser概述 JavaParser是一个Java库，可以用于解析Java源代码并生成抽象语法树（AST）。通过使用JavaParser，我们可以轻松地获取Java源代码的结构信息，比如类定义，方法定义等。\n4、创建Java解析器应用 首先，我们需要创建一个新的Java类，我们将其命名为JavaFileParser。在JavaFileParser类中，我们将定义一个主方法，该方法将接受一个命令行参数，该参数指定了要解析的Java源文件的路径。\n在主方法中，我们首先检查提供的文件路径是否指向一个实际存在的文件。如果文件不存在，我们将打印一条错误消息并退出。如果文件存在，我们将创建一个JavaParser实例，并使用它来解析源文件。\n解析结果将被封装在一个Optional对象中。如果解析成功，我们可以通过调用Optional的get方法来获取CompilationUnit对象。CompilationUnit对象表示了Java源文件的顶级结构。\n接下来，我们创建一个JSONArray实例，然后遍历CompilationUnit中的所有类和接口。对于每个类或接口，我们创建一个JSONObject实例，并将类或接口的名称和源代码添加到这个JSONObject中。然后，我们将这个JSONObject添加到JSONArray中。\n接着，我们遍历每个类或接口中的所有方法。对于每个方法，我们同样创建一个JSONObject实例，并将方法的名称和源代码添加到这个JSONObject中。然后，我们将这个JSONObject添加到JSONArray中。\n最后，我们将JSONArray转换为字符串，并打印到控制台。\n如果解析失败，我们将打印一条错误消息。 pom.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.12\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.fujfu\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;java-file\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;java-file\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;java-file\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.javaparser\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javaparser-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.25.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.vaadin.external.google\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;android-json\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.20131108.vaadin1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; JavaFileParser\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 package com.fujfu.javafile; import com.github.javaparser.JavaParser; import com.github.javaparser.ParseResult; import com.github.javaparser.ast.CompilationUnit; import com.github.javaparser.ast.body.ClassOrInterfaceDeclaration; import com.github.javaparser.ast.body.MethodDeclaration; import org.json.JSONArray; import org.json.JSONException; import org.json.JSONObject; import java.io.File; import java.io.IOException; import java.util.Optional; public class JavaFileParser { public static void main(String[] args) throws IOException { if (args.length \u0026lt; 1) { System.out.println(\u0026#34;Please provide the file path as an argument.\u0026#34;); return; } String filePath = args[0]; File file = new File(filePath); if (!file.exists()) { System.out.println(\u0026#34;File does not exist: \u0026#34; + filePath); return; } JavaParser javaParser = new JavaParser(); ParseResult\u0026lt;CompilationUnit\u0026gt; parse = javaParser.parse(file); Optional\u0026lt;CompilationUnit\u0026gt; optionalCompilationUnit = parse.getResult(); if (optionalCompilationUnit.isPresent()) { CompilationUnit compilationUnit = optionalCompilationUnit.get(); JSONArray jsonArray = new JSONArray(); // 遍历所有的类 compilationUnit.findAll(ClassOrInterfaceDeclaration.class).forEach(c -\u0026gt; { JSONObject classJson = new JSONObject(); try { classJson.put(\u0026#34;name\u0026#34;, c.getName().asString()); } catch (JSONException e) { e.printStackTrace(); } try { classJson.put(\u0026#34;code\u0026#34;, c.toString()); } catch (JSONException e) { e.printStackTrace(); } jsonArray.put(classJson); // 遍历类中的所有方法 c.findAll(MethodDeclaration.class).forEach(m -\u0026gt; { JSONObject methodJson = new JSONObject(); try { methodJson.put(\u0026#34;name\u0026#34;, m.getName().asString()); } catch (JSONException e) { e.printStackTrace(); } try { methodJson.put(\u0026#34;code\u0026#34;, m.toString()); } catch (JSONException e) { e.printStackTrace(); } jsonArray.put(methodJson); }); }); System.out.println(jsonArray.toString()); //jsonArray.toString() 循环 // for (int i = 0; i \u0026lt; jsonArray.length(); i++) { // JSONObject jsonObject = null; // try { // jsonObject = jsonArray.getJSONObject(i); // } catch (JSONException e) { // e.printStackTrace(); // } // try { // System.out.println(\u0026#34;Name: \u0026#34; + jsonObject.getString(\u0026#34;name\u0026#34;)); // } catch (JSONException e) { // e.printStackTrace(); // } //// try { //// System.out.println(\u0026#34;Code: \u0026#34; + jsonObject.getString(\u0026#34;code\u0026#34;)); //// } catch (JSONException e) { //// e.printStackTrace(); //// } // } } else { System.out.println(\u0026#34;Failed to parse the file.\u0026#34;); } } } 5、Java单文件打包成Jar 6、在Python中调用Java程序 首先，你需要一个用Java编写的程序，这个程序需要调用JavaParser库来解析Java源代码，并将解析结果转换为JSON格式输出。我们假设你已经有了这样的Java程序，并且你已经将它打包为名为javafilejson.jar的JAR文件。\n然后，在Python中，我们将使用subprocess模块来调用这个JAR文件。subprocess模块允许我们从Python代码中执行外部命令，我们将使用它来运行java -jar javafilejson.jar {file_path}命令，其中{file_path}是你希望解析的Java源文件的路径。\n命令的执行结果将被捕获并存储在output变量中。如果命令成功执行，我们将打印一条成功信息，然后解析output变量的内容，并以Python字典的形式打印出来。如果命令执行失败，我们将打印一条错误信息。\n下面是具体的Python代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json import subprocess def execute_java_command(file_path): command = f\u0026#39;java -jar javafilejson.jar {file_path}\u0026#39; process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True) output, error = process.communicate() if process.returncode == 0: print(\u0026#34;命令执行成功！\u0026#34;) print(\u0026#34;输出：\u0026#34;) all_data = output.decode(\u0026#34;gbk\u0026#34;) lst = json.loads(all_data) for i in lst: print(i[\u0026#34;name\u0026#34;], i[\u0026#34;code\u0026#34;]) else: print(\u0026#34;命令执行失败！\u0026#34;) print(\u0026#34;错误信息：\u0026#34;) print(error) # 调用函数示例 file_path = \u0026#34;LoanInfoController.java\u0026#34; execute_java_command(file_path) 7、 总结 这就是如何在Python环境下调用Java的JavaParser库来解析Java源代码的方法。这种跨语言的解决方案不仅能够扩大我们的工具箱，还能够帮助我们更好地理解源代码的结构，并在需要的时候对其进行修改。\nJavaParser官方文档：https://javaparser.org/ Python subprocess模块官方文档：https://docs.python.org/3/library/subprocess.html\n以上就是我们关于\u0026quot;跨语言代码解析：利用Python调用Java的JavaParser解析Java源代码\u0026quot;的博客。希望这篇博客对你在跨语言开发过程中有所帮助。\n","date":"2023-07-13T12:10:36Z","image":"https://www.ownit.top/title_pic/70.jpg","permalink":"https://www.ownit.top/p/202307131210/","title":"实现跨语言互动：如何在Python中调用Java的JavaParser库解析Java源代码"},{"content":"简介 学习如何使用 Python 的 retrying 库来处理在程序运行过程中可能出现的各种异常和错误。 retrying 是一种简单、易于使用的重试机制，帮助我们处理由网络问题或其他暂时性错误引起的失败。在很多情况下，简单的重试可能就是解决问题的最好方式。通过本篇博客，你将了解到如何在 Python 中使用 retrying。\n安装retrying库 要安装 retrying 库，我们可以使用 pip 命令：\n1 pip install retrying retrying的功能 一般装饰器api 特定的停止条件（限制尝试次数） 特定的等待条件（每次尝试之间的指数增长的时间等待） 自定义的异常进行尝试 自定义的异常进行尝试返回结果 最简单的一个使用方法是无论有任何异常出现，都会一直重新调用一个函数、方法，直到返回一个值 基础使用 让我们从一个简单的例子开始。假设我们有一个经常会失败的函数，我们希望在这个函数失败时进行重试。以下是如何使用 retrying 库实现这个目标：\n1 2 3 4 5 6 7 8 9 10 11 from retrying import retry @retry def make_trouble(): print(\u0026#34;Trying...\u0026#34;) raise Exception(\u0026#34;Exception!\u0026#34;) try: make_trouble() except Exception: print(\u0026#34;Failed, even with retrying.\u0026#34;) 在这个例子中，我们使用了 retry 修饰器（decorator）。如果 make_trouble 函数引发异常，retry 会捕获这个异常并重试函数。如果在尝试一定次数后仍然失败，那么异常将会被抛出。 自定义重试 在默认情况下，retry 会在每次失败后立即重试，直到成功为止。然而，在很多情况下，我们可能希望自定义重试的行为。retrying 库提供了一些参数，让我们能够进行自定义：\nstop_max_attempt_number：最大重试次数。 stop_max_delay：最大延迟毫秒数。 wait_fixed：每次重试之间的固定等待时间（毫秒）。 wait_random_min，wait_random_max：每次重试之间的随机等待时间（毫秒）。 1 2 3 4 5 6 7 8 9 10 11 from retrying import retry @retry(stop_max_attempt_number=3, wait_fixed=2000) def make_trouble(): print(\u0026#34;Trying...\u0026#34;) raise Exception(\u0026#34;Exception!\u0026#34;) try: make_trouble() except Exception: print(\u0026#34;Failed, even with retrying.\u0026#34;) 在这个例子中，make_trouble 函数会最多尝试3次，每次尝试之间等待2秒。 更复杂的重试条件 除了自定义重试次数和等待时间，我们还可以自定义重试的条件。例如，我们可能只希望在特定的异常出现时进行重试。这可以通过在 retry 修饰器中添加一个 retry_on_exception 函数来实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from retrying import retry def retry_if_io_error(exception): \u0026#34;\u0026#34;\u0026#34;Return True if we should retry (in this case when it\u0026#39;s an IOError), False otherwise\u0026#34;\u0026#34;\u0026#34; return isinstance(exception, IOError) @retry(retry_on_exception=retry_if_io_error,stop_max_attempt_number=3, wait_fixed=2000) def might_io_error(): print(\u0026#34;Trying...\u0026#34;) raise IOError(\u0026#34;IO Error!\u0026#34;) try: might_io_error() except Exception: print(\u0026#34;Failed, even with retrying.\u0026#34;) 在上面的代码中，我们定义了一个函数 retry_if_io_error，只有当异常是 IOError 类型时，我们才进行重试。 结束 使用 retrying 库，我们可以很容易地在 Python 中实现错误重试机制。这在处理网络请求、数据库操作或任何可能因暂时性问题失败的操作时都非常有用。希望你已经对如何使用 retrying 有了一个基本的了解，并能在你的 Python 项目中找到它的用武之地。\n参考文档 https://juejin.cn/post/7108202816665026573\n","date":"2023-07-12T09:27:00Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202307120927/","title":"Python错误处理的艺术：使用retrying库实现高效重试机制"},{"content":"引言 在开发Web应用程序时，我们经常需要对某些功能或API进行速率限制，以防止滥用和保护服务器资源。Python的ratelimit库提供了一种简单且强大的方法来实现速率限制。本文将介绍如何使用ratelimit库在Python应用程序中实现速率限制功能。\n什么是速率限制？ 速率限制是一种限制某个操作或功能的调用频率的方法。它可以防止恶意用户或程序对系统造成过大的负载或滥用系统资源。速率限制通常通过设置每秒或每分钟允许的最大请求数来实现。\nratelimit库简介 ratelimit是一个Python库，它提供了速率限制的功能。它基于令牌桶算法，允许您以简洁而灵活的方式对函数或方法进行速率限制。\nratelimit 提供的装饰器，可以控制被装饰的函数在某个周期内被调用的次数不超过一个阈值，尽管作者本意是限制那些访问web API 的函数的调用次数，但你可以推而广之，所有不能频繁调用的函数都可以用这个装饰器来修饰。\n项目的github地址: https://github.com/tomasbasham/ratelimit\nratelimit库 实例 @sleep_and_retry @limits(calls=2, period=10)\n@sleep_and_retry 是一个 Python 装饰器，用于在函数调用达到限制后自动进行重试，并在重试之前进行休眠。它通常与限流库（如 ratelimit）一起使用，以确保函数在限制条件下被正确执行。\n@limits(calls=2, period=10) 是 ratelimit 库中的另一个装饰器，用于设置函数的限制条件。在这个例子中，它指定了函数在 10 秒内最多可以被调用 2 次。\n当函数被装饰时，@sleep_and_retry 会在函数调用超过限制时自动进行休眠并重试。具体的行为如下：\n当函数第一次被调用时，它会立即执行。 如果函数在 10 秒内已经被调用了 2 次，它将抛出一个异常，指示超过了限制。 如果函数调用达到了限制，@sleep_and_retry 会在下一次调用之前休眠一段时间，以确保函数在下一个计算周期开始之前被重试。 休眠时间的计算基于上一次函数调用的时间和当前时间之间的差值。\n要开始使用ratelimit库，首先需要安装它。可以使用pip命令进行安装：\n1 pip install ratelimit 使用ratelimit库进行速率限制：\n下面是一个示例，演示如何在Python函数上应用速率限制：\n1 2 3 4 5 6 7 8 from ratelimit import limits, sleep_and_retry # 设置速率限制为每秒最多两次调用 @sleep_and_retry @limits(calls=2, period=1) def my_function(): # 在此处添加您的功能代码 pass 在上面的示例中，@limits(calls=2, period=1)装饰器定义了一个速率限制规则，允许每秒最多调用两次my_function函数。@sleep_and_retry装饰器可确保当超出速率限制时，函数将休眠并重试。\n根据您的需求，您可以根据实际情况调整calls和period参数的值。\n日常使用 函数限制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from ratelimit import limits, sleep_and_retry import time # 设置每分钟最多调用次数为 3 @limits(calls=3, period=60) @sleep_and_retry def my_function(): # 在这里编写你的函数逻辑 print(\u0026#34;函数被调用\u0026#34;) # 测试函数的调用 for _ in range(10): try: my_function() except Exception as e: print(\u0026#34;调用次数超过限制，等待1分钟后继续\u0026#34;) time.sleep(60) my_function() 1 2 3 4 5 6 7 8 9 10 11 from ratelimit import limits, sleep_and_retry @sleep_and_retry @limits(calls=1, period=10) def call_api(): print(\u0026#39;call api\u0026#39;) while True: call_api() 高并发线程池 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from ratelimit import limits, sleep_and_retry from concurrent.futures import ThreadPoolExecutor # 设置每分钟最多调用次数为 3 @sleep_and_retry @limits(calls=3, period=10) def my_function(): # 在这里编写你的函数逻辑 print(\u0026#34;函数被调用\u0026#34;) # 创建 ThreadPoolExecutor 对象 executor = ThreadPoolExecutor(max_workers=5) # 测试函数的调用 for _ in range(100): executor.submit(my_function) 参考文档： python开源项目解读—ratelimit，限制函数单位时间内被调用次数 Python Django 配置使用django-ratelimit限制网站接口访问频率\n","date":"2023-07-04T11:04:27Z","image":"https://www.ownit.top/title_pic/66.jpg","permalink":"https://www.ownit.top/p/202307041104/","title":"限制速度，释放潜力：Python中的ratelimit库解密"},{"content":"一 、背景 在某些情况下，我们需要快速了解哪些项目包含特定的配置，例如使用了fastjson库或数据库的连接配置。然而，在GitLab上逐个代码仓库进行搜索是非常耗时的。为了提高效率，我们开发了一个Python脚本工具，用于实现全局搜索功能。\n该脚本能够在GitLab上对所有项目进行全局搜索，帮助我们快速确定哪些项目使用了特定的配置。无论项目数量是几个、几十个还是上百个，这个脚本都能够节省大量的时间和精力。\n使用这个工具，我们可以根据关键词（如库名、配置项等）进行搜索，并获取包含该关键词的项目列表。脚本会自动遍历所有项目，并返回符合条件的项目链接，方便我们进一步查看和分析。\n无论是对大型团队还是个人开发者，这个工具都能提供便利。它能够高效地帮助我们了解项目中的配置使用情况，加快问题排查和代码审查的速度。\n如果你的项目数量较少，只有几个项目，并且能够轻松在GitLab上逐个搜索，那么这个脚本可能并不适用于你。但是，当你面对数十甚至上百个项目时，这个脚本将成为你的得力助手，提供高效的全局搜索能力。\n通过这个GitLab全局搜索文本工具，我们能够更快地发现项目中的配置使用情况，优化代码结构，提高开发效率，以及更好地管理和维护项目。\n二、原理流程 用户输入GitLab的URL和访问令牌。 系统使用输入的URL和访问令牌与GitLab建立连接。 系统调用GitLab模块，获取GitLab中的所有项目ID。 对于每个项目ID，系统构建搜索接口URL，包括关键字和项目ID作为参数。 系统使用请求库向搜索接口发送HTTP请求，获取搜索结果页面内容。 系统使用XPath解析搜索结果页面内容，提取相关信息。 如果搜索结果存在： 系统判断关键字匹配成功，并将匹配到的项目ID和关键字相关信息存储起来。 可选：系统展示匹配到关键字的项目信息给用户。 继续处理下一个项目ID，重复步骤4-7，直到所有项目ID处理完毕。 系统输出所有匹配到关键字的项目信息。 项目流程结束。 三、python版本与依赖 python:3.8.0 (如果本地已有python3的环境则不需要通过docker) python依赖的module如下：\n1 2 3 4 5 6 7 import json import os import requests from lxml import etree from multiprocessing import Pool import gitlab 四、 操作步骤 1、安装上面的模块 1 2 pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple pip install gitlab -i https://pypi.tuna.tsinghua.edu.cn/simple 2、获取Gitlab的token 要获取GitLab的访问令牌（Token），请按照以下步骤操作：\n登录到GitLab账户。 在右上角的用户菜单中，点击\u0026quot;Settings\u0026quot;选项。 在左侧导航栏中，选择\u0026quot;Access Tokens\u0026quot;选项。 在\u0026quot;Personal Access Tokens\u0026quot;部分，点击\u0026quot;Create a token\u0026quot;按钮。 输入访问令牌的名称和过期日期（可选）。 选择所需的权限范围（API、读取用户信息等）。 点击\u0026quot;Create personal access token\u0026quot;按钮。 访问令牌将被生成，并显示在屏幕上。请务必复制该令牌并妥善保存，因为它将在创建后只显示一次。 注意事项：\n访问令牌具有与您的账户相同的权限，请谨慎保管，并仅将其用于受信任的应用程序或脚本。 如果不再需要访问令牌或想要撤销访问权限，请返回到\u0026quot;Access Tokens\u0026quot;页面，并点击相应的撤销按钮。 请注意，获取访问令牌需要有相应的权限，如果您无法找到或访问\u0026quot;Access Tokens\u0026quot;选项，请联系您的GitLab管理员以获取帮助。 3、获取Gitlab接口的cookie 要获取GitLab接口的Cookie，可以按照以下步骤操作：\n打开浏览器并登录到GitLab账户。 在登录成功后，打开浏览器的开发者工具（通常是按F12键或右键点击页面并选择\u0026quot;检查\u0026quot;或\u0026quot;开发者工具\u0026quot;选项）。 在开发者工具中，切换到\u0026quot;网络\u0026quot;（Network）选项卡。 在浏览器中执行需要获取Cookie的操作，例如访问某个页面或执行某个API请求。 在开发者工具中，您将看到所有网络请求的记录。 在请求列表中，找到与GitLab相关的请求。 点击该请求，在请求详细信息的右侧面板中，可以找到请求头（Headers）的信息。 在请求头中，查找名为\u0026quot;Cookie\u0026quot;的字段。该字段的值即为GitLab接口的Cookie。 请注意，Cookie包含了用户的身份验证信息，因此请妥善保管Cookie，并仅将其用于合法和授权的用途。同时，获取Cookie需要有相应的权限，如果您无法获取Cookie，请联系您的GitLab管理员以获取帮助。 4、Python代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 # -*- coding: utf-8 -*- # @Time : 2023/6/21 14:20 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : git_prod_scan.py # @Software: PyCharm import json import os import requests from lxml import etree from multiprocessing import Pool import gitlab # 需要检索的内容关键字 search_content = \u0026#39;yunkedai\u0026#39; token = \u0026#39;xxxxx-Y9FLN\u0026#39; gitlab_url = \u0026#39;https://gitlab.xxxx.com/\u0026#39; gl = gitlab.Gitlab(gitlab_url, token) private_Cookie = \u0026#39;xxxxxxx\u0026#39; headers = { \u0026#39;Cookie\u0026#39;: private_Cookie } def get_all_projects(): data = [] for g in gl.groups.list(all=True): for p in g.projects.list(all=True): project = gl.projects.get(p.id) item = { \u0026#34;id\u0026#34;: p.id, \u0026#34;group\u0026#34;: g.name, \u0026#34;project\u0026#34;: p.name } data.append(item) # 将数据保存为 JSON 文件 with open(\u0026#39;all_output.json\u0026#39;, \u0026#39;w\u0026#39;) as file: json.dump(data, file, indent=4) def getProject(s): \u0026#34;\u0026#34;\u0026#34; 根据项目 ID 获取项目信息并打印项目链接 \u0026#34;\u0026#34;\u0026#34; url = \u0026#34;%s/search?utf8=\u0026amp;snippets=\u0026amp;scope=\u0026amp;search=%s\u0026amp;project_id=%s\u0026#34; % (gitlab_url, search_content, s) response = requests.get(url, headers=headers) pagehtml = etree.HTML(response.text) try: li_element = pagehtml.xpath(\u0026#39;//li[@data-qa-selector=\u0026#34;code_tab\u0026#34;]\u0026#39;)[0] projecttitle = li_element.xpath(\u0026#39;.//span/text()\u0026#39;)[0] if int(projecttitle) \u0026gt; 0: # 查询项目信息 project = gl.projects.get(s) # 获取项目链接 project_url = project.web_url print(\u0026#34;项目存在关键词：\u0026#34; + project_url) else: return except IndexError: pass def start_project(): pool = Pool(processes=5) # 读取项目数据 with open(\u0026#39;all_output.json\u0026#39;, \u0026#39;r\u0026#39;) as file: data = file.read() data = json.loads(data) # 并发调用 getProject 函数 for i in data: id = i.get(\u0026#39;id\u0026#39;) pool.apply_async(func=getProject, args=(id,)) print(\u0026#39;end\u0026#39;) pool.close() pool.join() print(\u0026#34;================================================================\u0026#34;) print(\u0026#34;所有项目已扫描完毕\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: file_path = \u0026#39;all_output.json\u0026#39; if os.path.isfile(file_path): start_project() else: get_all_projects() start_project() 五、测试效果 我要 获取 所有项目 包含 hire_core 的关键字\n请修改代码层面\n1 search_content = \u0026#39;hire_core\u0026#39; 执行脚本 六、总结 上面的GitLab全局Python代码搜索工具具有以下优势：\n搜索效率高：该工具通过使用GitLab的API接口进行搜索，避免了手动在每个项目中进行搜索的繁琐过程。它能够快速扫描多个项目，从而节省了大量的时间和精力。 全面性：该工具可以在GitLab上进行全局搜索，即同时搜索所有项目，而不仅仅局限于单个项目。这样可以确保没有遗漏任何一个项目，提高了搜索的全面性和准确性。 多线程支持：工具采用了多线程的并发处理方式，可以同时处理多个项目的搜索请求，提高了搜索效率。这意味着可以快速地并发搜索大量的项目，更快地找到符合条件的代码片段。 输出结果清晰：工具会输出符合搜索条件的项目链接，让用户能够直观地查看项目和相关代码。这方便了用户对搜索结果的检查和进一步的处理。 灵活可扩展：工具的代码结构清晰，易于理解和修改。用户可以根据自己的需求进行定制和扩展，例如修改搜索关键字、调整并发线程数等，以适应不同的搜索场景和项目规模。 ","date":"2023-06-21T18:41:14Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/202306211841/","title":"GitPySearch- 全局Python代码搜索工具"},{"content":"1、背景 日志是非常重要的信息资源。它们记录了应用程序的运行状态、错误和异常情况，帮助我们了解系统的健康状况以及发现潜在的问题。为了高效地管理和分析日志数据，许多组织采用了Elasticsearch、Logstash和Kibana（ELK）堆栈作为日志收集和分析的解决方案。\n开发一个实时监控和告警脚本，专门用于监控ELK平台中的错误日志，并及时发送告警通知给相关人员。该系统将通过扫描Elasticsearch中的日志数据，筛选出等级为ERROR的错误日志，并根据预设的告警规则进行处理。\n2、目的 使用Python从Elasticsearch中查询特定级别为ERROR的错误日志，并通过钉钉机器人实现告警聚合和发送，以提高错误日志的处理效率和及时响应能力。\n为什么开发这个脚本？ 因为目前我们这边没有监控日志的信息，出现问题不能及时发现 和预知 优势 1、消息进行聚合，每个项目的多条告警信息，汇总一条发送。突破钉钉机器人每分钟只能发送20条的限制 2、告警信息you太多的重复，进行去重处理，添加告警次数发送。防止被钉钉限流 3、原理 使用Python的Elasticsearch库连接到Elasticsearch集群。 构建Elasticsearch查询DSL（领域专用语言），过滤出级别为ERROR的日志记录。 执行查询并获取结果。 对查询结果进行聚合，统计每个项目的错误次数。 根据聚合结果，生成告警消息的Markdown格式内容。 使用钉钉机器人发送告警消息到指定的钉钉群。 4、流程 导入必要的Python库，包括elasticsearch和requests。 创建Elasticsearch连接，指定Elasticsearch集群的主机和端口。 构建Elasticsearch查询DSL，设置查询条件为日志级别为ERROR。 执行查询，获取查询结果。 对查询结果进行处理，聚合每个项目的错误次数。 根据聚合结果生成告警消息的Markdown内容。 使用钉钉机器人API发送告警消息到指定的钉钉群。 5、实现代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 # -*- coding: utf-8 -*- # @Time : 2023/6/17 18:11 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : all_es.py # @Software: PyCharm from collections import Counter from datetime import datetime, timedelta import requests from elasticsearch import Elasticsearch from monitor.es_ding import send_pretty_message # Elasticsearch客户端实例 es = Elasticsearch(hosts=[\u0026#39;http://172.18.xxx.xxxx:9200\u0026#39;], http_auth=(\u0026#39;elastic\u0026#39;, \u0026#39;xxxxx\u0026#39;), sniff_on_start=True, # 连接前测试 sniff_on_connection_fail=True, # 节点无响应时刷新节点 sniff_timeout=300, # 设置超时时间 headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}) def format_timestamp(timestamp): \u0026#34;\u0026#34;\u0026#34;格式化时间为Elasticsearch接受的字符串格式\u0026#34;\u0026#34;\u0026#34; return timestamp.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) def search_errors(): \u0026#34;\u0026#34;\u0026#34;执行查询，获取错误日志数据\u0026#34;\u0026#34;\u0026#34; current_time = datetime.now() one_minute_ago = current_time - timedelta(minutes=10) current_time_str = format_timestamp(current_time) one_minute_ago_str = format_timestamp(one_minute_ago) index = \u0026#39;app-prod-*\u0026#39; # 替换为实际的索引名称 query = { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: one_minute_ago_str, \u0026#34;lt\u0026#34;: current_time_str, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;, \u0026#34;time_zone\u0026#34;: \u0026#34;+08:00\u0026#34; } } }, { \u0026#34;match\u0026#34;: { \u0026#34;loglevel\u0026#34;: \u0026#34;ERROR\u0026#34; #匹配项目错误等级 } }, { \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;projectname\u0026#34;: \u0026#34;fox-data-spiderman\u0026#34; # 需要屏蔽的项目 } } ] } } ] } }, \u0026#34;_source\u0026#34;: [ ## 输出的字段 \u0026#34;date\u0026#34;, \u0026#34;projectname\u0026#34;, \u0026#34;threadname\u0026#34;, \u0026#34;msg\u0026#34; ], \u0026#34;from\u0026#34;: 0, \u0026#34;size\u0026#34;: 10000, # 返回查询的条数 } result = es.search(index=index, body=query) total_documents = result[\u0026#34;hits\u0026#34;][\u0026#34;total\u0026#34;][\u0026#34;value\u0026#34;] print(f\u0026#34;总共匹配到 {total_documents} 条文档\u0026#34;) result = result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;] all_result = [] for i in result: all_result.append(i[\u0026#39;_source\u0026#39;]) msg_counter = Counter(d[\u0026#39;msg\u0026#39;] for d in all_result if \u0026#39;msg\u0026#39; in d) results = [] for d in all_result: if \u0026#39;msg\u0026#39; in d and d[\u0026#39;msg\u0026#39;] in msg_counter: count = msg_counter[d[\u0026#39;msg\u0026#39;]] del msg_counter[d[\u0026#39;msg\u0026#39;]] d[\u0026#39;count\u0026#39;] = count d[\u0026#39;msg\u0026#39;] = d[\u0026#39;msg\u0026#39;][:100] + (\u0026#39;...\u0026#39; if len(d[\u0026#39;msg\u0026#39;]) \u0026gt; 100 else \u0026#39;\u0026#39;) results.append(d) return results def aggregate_errors(results): \u0026#34;\u0026#34;\u0026#34;按项目名称聚合错误日志\u0026#34;\u0026#34;\u0026#34; aggregated_data = {} for d in results: projectname = d.get(\u0026#39;projectname\u0026#39;) if projectname: if projectname not in aggregated_data: aggregated_data[projectname] = [] aggregated_data[projectname].append({\u0026#39;date\u0026#39;: d.get(\u0026#39;date\u0026#39;), \u0026#39;msg\u0026#39;: d.get(\u0026#39;msg\u0026#39;), \u0026#39;count\u0026#39;: d.get(\u0026#39;count\u0026#39;)}) return aggregated_data def generate_summary(projectname, messages): \u0026#34;\u0026#34;\u0026#34;生成Markdown格式的消息摘要\u0026#34;\u0026#34;\u0026#34; markdown_text = f\u0026#39;### {projectname} \\n\\n\u0026#39; for message in messages: markdown_text += f\u0026#34;**时间：** {message[\u0026#39;date\u0026#39;]}\\n\\n\u0026#34; markdown_text += f\u0026#34;**告警次数：** \u0026lt;font color=\u0026#39;red\u0026#39;\u0026gt;\u0026lt;b\u0026gt;{message[\u0026#39;count\u0026#39;]}\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\\n\\n\u0026#34; markdown_text += f\u0026#34;{message[\u0026#39;msg\u0026#39;]}\\n\\n---\\n\\n\u0026#34; return markdown_text def send_message_summary(projectname, messages): \u0026#34;\u0026#34;\u0026#34;发送摘要消息给钉钉机器人\u0026#34;\u0026#34;\u0026#34; summary = generate_summary(projectname, messages) data = { \u0026#39;msgtype\u0026#39;: \u0026#39;markdown\u0026#39;, \u0026#39;markdown\u0026#39;: { \u0026#39;title\u0026#39;: f\u0026#39;{projectname}消息告警\u0026#39;, \u0026#39;text\u0026#39;: summary } } webhook_url = \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxxxxxxx\u0026#39; # 替换为实际的Webhook URL response = requests.post(webhook_url, json=data) if response.status_code == 200: print(\u0026#39;消息发送成功\u0026#39;) else: print(\u0026#39;消息发送失败\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: errors = search_errors() aggregated_errors = aggregate_errors(errors) for projectname, messages in aggregated_errors.items(): print(f\u0026#34;{projectname}:\u0026#34;) print(messages) 6、Crontab添加定时任务 也可以用采用：Jenkins与GitLab的定时任务工作流程 https://blog.csdn.net/heian_99/article/details/131164591?spm=1001.2014.3001.5501\n1 2 #日志 */2 * * * * cd /python_app/elasticsearch; /opt/anaconda3/envs/py38/bin/python -u es_monitor.py \u0026gt;\u0026gt; es_error_info.log 2\u0026gt;\u0026amp;1 该定时任务的含义是每隔2分钟执行一次指定目录下的 es_monitor.py 脚本，并将输出信息追加到 es_error_info.log 文件中。这样可以定期监控 Elasticsearch 的错误日志，并记录相关信息以便后续查看和分析。\n7、总结 本博客，为我们构建了一个完整的应用日志监控和告警系统，通过ELK技术栈和钉钉机器人的结合，使得我们能够及时发现和处理应用中的错误，提高了团队的工作效率和系统的稳定性。\n","date":"2023-06-17T18:26:30Z","image":"https://www.ownit.top/title_pic/10.jpg","permalink":"https://www.ownit.top/p/202306171826/","title":"提高错误日志处理效率！使用Python和钉钉机器人实现自动告警聚合"},{"content":"简介： Jenkins是一个流行的开源自动化服务器，而GitLab是一个强大的代码托管和协作平台。通过结合Jenkins和GitLab，我们可以建立一个强大的定时任务自动化工作流程，实现代码拉取、构建、测试和部署的自动化。本篇博客将介绍如何使用Jenkins与GitLab相结合，构建一个完整的定时任务自动化流程。\n背景 目前我们的代码存放在gitlab中，有版本控制比较方便开发。现在有个Crontab的仓库存放，日常的定时任务脚本。 如果放在Linux的crontab上，代码更新完毕 需要及时拉取才会 执行新的代码，比较麻烦。这边就采用jenkins的crontab来执行定时任务\n优势 Jenkins可以满足您提到的核心需求，并提供以下功能：\n审计执行日志：Jenkins记录每个任务的执行日志，并提供可视化界面来查看和审计任务的执行历史。您可以随时查看任务的执行输出、错误信息以及任何产生的警告或异常。\n任务超时强制kill：Jenkins允许您为每个任务设置超时时间。如果任务在指定的时间内未完成，Jenkins会强制终止任务的执行，以防止无限循环或资源占用等问题。\n任务的超时重试：Jenkins提供了内置的插件，如Pipeline和Job DSL，可以轻松地编写自定义的任务流程，并支持任务的超时重试。您可以在任务失败或超时后自动触发重试，并设置重试次数和时间间隔，以确保任务能够成功完成。\n错误重试和自定义次数：Jenkins提供了灵活的错误处理机制，您可以在任务执行失败时自定义处理逻辑。您可以设置任务在遇到错误时自动触发重试，并指定重试次数和重试间隔，以便应对临时性的错误和问题。\nJenkins作为一个开源的自动化服务器，具有广泛的社区支持和丰富的插件生态系统，可以满足各种复杂的任务调度和自动化需求。通过适当的配置和使用插件，您可以实现高度定制化的任务调度和执行流程，满足您特定的需求，并确保任务的可靠执行和错误处理。\nGitlab创建仓库 登录GitLab：打开您的Web浏览器，输入GitLab的URL，并使用您的GitLab账户登录。\n导航到项目页面：登录后，您将看到GitLab的仪表板或项目列表。点击页面顶部的“+”按钮或导航栏上的“New project”按钮。\n创建新项目：在新项目页面上，您需要提供以下信息来创建新的仓库：\n项目路径：指定项目的路径，也就是项目在GitLab上的URL地址。它将用于在仓库的URL中唯一标识您的项目。 可见级别：选择您希望谁能访问您的项目。您可以选择公开（Public）、私有（Private）或内部（Internal）可见性级别。 项目名称：为您的项目指定一个易于识别的名称。 描述（可选）：提供关于项目的简要描述，以便其他人了解项目的目的和内容。 初始化仓库：选择是否要初始化一个空的仓库，或者您可以选择从现有的代码库导入代码。 完成设置：在提供必要信息后，单击页面下方的“Create project”按钮。GitLab将根据您提供的信息创建新的仓库，并将您重定向到新仓库的页面。\n配置仓库（可选）：在新仓库页面上，您可以进一步配置仓库的设置，例如添加项目成员、设置分支保护规则、启用CI/CD等。这些设置可以根据您的具体需求进行调整。 添加测试脚本\nmonitor.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 #!/bin/bash echo \u0026#34;=======================\u0026#34; echo $(date) #!/bin/bash # @Author: danqing # @Description: Host Daily Check Script # beseem CentOS6.X CentOS7.X # echo \u0026#34;Host Daily Check Script\u0026#34; [ $(id -u) -gt 0 ] \u0026amp;\u0026amp; echo \u0026#34;请用root用户执行此脚本！\u0026#34; \u0026amp;\u0026amp; exit 1 if [ ! -d /root/check_log ];then mkdir /root/check_log echo \u0026#34;/root/check_log检查日志存放目录创建成功\u0026#34; else echo \u0026#34;/root/check_log检查日志存放目录已存在\u0026#34; fi function getSystem(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;############################ 系统信息检查 ############################\u0026#34; Default_LANG=${LANG} OS=$(uname -o) Release=$(cat /etc/redhat-release 2\u0026gt;/dev/null) Kernel=$(uname -r) Hostname=$(uname -n) Nowdate=$(date +\u0026#39;%F %T\u0026#39;) LastReboot=$(who -b | awk \u0026#39;{print $3,$4}\u0026#39;) uptime=$(uptime | sed \u0026#39;s/.*up \\([^,]*\\), .*/\\1/\u0026#39;) echo \u0026#34; 语言环境: $Default_LANG\u0026#34; echo \u0026#34; 系统: $OS\u0026#34; echo \u0026#34; 发行版本: $Release\u0026#34; echo \u0026#34; 内核: $Kernel\u0026#34; echo \u0026#34; 主机名: $Hostname\u0026#34; echo \u0026#34; 当前时间: $Nowdate\u0026#34; echo \u0026#34; 最后启动: $LastReboot\u0026#34; echo \u0026#34; 运行时间: $uptime\u0026#34; } function getCpu(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;############################ CPU检查 ############################\u0026#34; Physical_CPUs=$(grep \u0026#34;physical id\u0026#34; /proc/cpuinfo| sort | uniq | wc -l) Virt_CPUs=$(grep \u0026#34;processor\u0026#34; /proc/cpuinfo | wc -l) CPU_Kernels=$(grep \u0026#34;cores\u0026#34; /proc/cpuinfo|uniq| awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39;) CPU_Type=$(grep \u0026#34;model name\u0026#34; /proc/cpuinfo | awk -F \u0026#39;: \u0026#39; \u0026#39;{print $2}\u0026#39; | sort | uniq) CPU_Hz=$(cat /proc/cpuinfo | grep \u0026#34;cpu MHz\u0026#34; | uniq | awk -F\u0026#39;:\u0026#39; \u0026#39;{sub(/ /,\u0026#34;\u0026#34;,$2);printf \u0026#34;%s MHz\\n\u0026#34;,$2}\u0026#39;) CPU_Arch=$(uname -m) CPU_Usage=$(cat /proc/loadavg | awk \u0026#39;{print $1}\u0026#39;) echo \u0026#34;物理CPU个数: $Physical_CPUs\u0026#34; echo \u0026#34;逻辑CPU个数: $Virt_CPUs\u0026#34; echo \u0026#34;每CPU核心数: $CPU_Kernels\u0026#34; echo \u0026#34;CPU型号: $CPU_Type\u0026#34; echo \u0026#34;CPU频率: $CPU_Hz\u0026#34; echo \u0026#34;CPU架构: $CPU_Arch\u0026#34; echo \u0026#34;CPU使用率: ${CPU_Usage}%\u0026#34; } function getMemory(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;############################ 内存检查 ############################\u0026#34; Memory_Used=$(awk \u0026#39;/MemTotal/{total=$2}/MemFree/{free=$2}END{print (total-free)/1024/1024}\u0026#39; /proc/meminfo) Memory_Total=$(awk \u0026#39;/MemTotal/{total=$2}END{print (total)/1024/1024}\u0026#39; /proc/meminfo) # kb的换算是1000 kB的换算是1024 Memory_Usage=$(awk \u0026#39;/MemTotal/{total=$2}/MemFree/{free=$2}END{print (total-free)/total*100}\u0026#39; /proc/meminfo) echo \u0026#34;已使用内存/全部内存: ${Memory_Used}GB/${Memory_Total}GB\u0026#34; echo \u0026#34;内存使用率: ${Memory_Usage}%\u0026#34; } function getDisk(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;############################ 硬盘检查 ############################\u0026#34; Disk_Count=$(lsblk |awk \u0026#39;/disk/{print $1}\u0026#39;|wc -l) echo \u0026#34;硬盘数量: ${Disk_Count}个\u0026#34; echo \u0026#34;硬盘分区情况: \u0026#34; echo \u0026#34;`df -hTP | sort |grep -E \u0026#34;/sd|/mapper\u0026#34; |awk \u0026#39;{print ($1 \u0026#34;\\t\\n\u0026#34; \u0026#34; 文件系统\u0026#34;$2 \u0026#34; 合计\u0026#34;$3 \u0026#34; 已用\u0026#34;$4 \u0026#34; 剩余\u0026#34;$5 \u0026#34; 使用率\u0026#34;$6 \u0026#34; 挂载点\u0026#34;$7)}\u0026#39;`\u0026#34; # -P, --portability 使用 POSIX 输出格式,方便shell过滤处理 smartctl -V \u0026gt;\u0026amp;/dev/null if [ $? -eq 0 ]; then echo \u0026#34;smartctl工具已安装,可以进行硬盘健康检测: \u0026#34; for i in $(lsblk |awk \u0026#39;/disk/{print $1}\u0026#39;) do echo \u0026#34;硬盘\u0026#34;$i `smartctl -H /dev/$i |grep -Ei \u0026#34;OK|PASSED|FAILED|Failure|Failed\u0026#34;` done else echo \u0026#34;smartctl工具未安装,无法进行硬盘健康检测\u0026#34; fi # \u0026#34;\\n磁盘IO信息:$(iotop -bon 1 \u0026amp;\u0026gt;/dev/null || echo \u0026#39;iotop 未安装信息获取失败\u0026#39;)\u0026#34; } function getNetwork(){ echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;############################ 网络检查 ############################\u0026#34; Network_Device=$(cat /proc/net/dev | awk \u0026#39;NR\u0026gt;2 \u0026amp;\u0026amp; $1 !~/lo/ {sub(/:/,\u0026#34;\u0026#34;);print $1}\u0026#39;) for i in $Network_Device do #echo \u0026#34;网卡：$i 状态: $(ip link show $Network_Device | awk \u0026#39;NR==1{print $9}\u0026#39;) RX: $(ethtool -g $Network_Device | grep \u0026#34;RX:\u0026#34; | tail -1 | awk \u0026#39;{print $2}\u0026#39;) TX: $(ethtool -g $Network_Device | grep \u0026#34;TX:\u0026#34; | tail -1 | awk \u0026#39;{print $2}\u0026#39;)\u0026#34; # rx是接收(receive),tx是发送(transport) Mac_Info=$(ip link | egrep -v \u0026#34;lo\u0026#34; | grep link | awk \u0026#39;{print $2}\u0026#39;) echo \u0026#34;MAC地址: $Mac_Info\u0026#34; Private_Ip=$(ip addr | awk \u0026#39;/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \u0026#34;\\\\1\u0026#34;, \u0026#34;g\u0026#34;, $2)}\u0026#39;) echo \u0026#34;IP地址: $Private_Ip\u0026#34; # Public_Ip=$(curl ifconfig.me -s) # echo \u0026#34;公网IP地址: $Public_Ip\u0026#34; Gateway=$(ip route | grep default | awk \u0026#39;{print $3}\u0026#39;) # echo \u0026#34;网关地址: $Gateway\u0026#34; # Dns_Config=$(grep nameserver /etc/resolv.conf| grep -v \u0026#34;#\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) # echo \u0026#34;DNS地址: $Dns_Config\u0026#34; echo \u0026#34;网关连接情况: $(ping -c 4 -i 0.5 -W 3 $Gateway \u0026amp;\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#39;正常通信\u0026#39; || echo \u0026#39;无法通信\u0026#39;)\u0026#34; echo \u0026#34;外网连接情况: $(ping -c 4 -i 0.5 -W 3 baidu.com \u0026amp;\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#39;正常通信\u0026#39; || echo \u0026#39;无法通信\u0026#39;)\u0026#34; # 发送4次请求包，每次间隔0.5秒，最长等待时间为3秒 done Listen_Port=$(ss -tuln | grep LISTEN | awk \u0026#39;{print $5}\u0026#39; | awk -F: \u0026#39;{print $2$4}\u0026#39; | sort |uniq -d | tr \u0026#39;\\n\u0026#39; \u0026#39;,\u0026#39; | sed \u0026#39;s/,$//\u0026#39;) echo \u0026#34;系统运行的端口: $Listen_Port\u0026#34; } function check(){ echo \u0026#34;Host Daily Check Script\u0026#34; getSystem getCpu getMemory getDisk getNetwork } RESULTFILE=\u0026#34;/root/check_log/check-`date +%Y%m%d`.txt\u0026#34; check \u0026gt; $RESULTFILE echo \u0026#34;检查结果：$RESULTFILE\u0026#34; cat $RESULTFILE Jenkins配置Gitlab的认证凭证 在Jenkins中配置GitLab的认证凭证，您可以按照以下步骤进行操作：\n登录Jenkins：打开您的Web浏览器，输入Jenkins的URL，并使用管理员账户登录到Jenkins控制台。\n导航到凭据页面：在Jenkins控制台的主页上，点击左侧导航栏中的\u0026quot;凭据\u0026quot;（Credentials）选项。 创建新的凭据：在凭据页面，点击\u0026quot;系统\u0026quot;（System）下方的\u0026quot;全局凭据\u0026quot;（Global credentials）选项。\n添加凭据：在全局凭据页面，点击\u0026quot;添加凭据\u0026quot;（Add Credentials）按钮。\n选择凭据类型：在\u0026quot;添加凭据\u0026quot;页面，选择GitLab凭据的类型。根据您的GitLab配置，可以选择不同的类型，例如\u0026quot;Username with password\u0026quot;（用户名和密码）或\u0026quot;SSH Username with private key\u0026quot;（SSH用户名和私钥）等。 填写凭据信息：根据您选择的凭据类型，填写相关的凭据信息。例如，如果选择\u0026quot;Username with password\u0026quot;，则需要提供GitLab的用户名和密码；如果选择\u0026quot;SSH Username with private key\u0026quot;，则需要提供GitLab的SSH用户名和私钥等信息。 保存凭据：完成填写凭据信息后，点击页面下方的\u0026quot;保存\u0026quot;（Save）按钮，将凭据保存到Jenkins中。\n配置Jenkins项目：在Jenkins中的项目配置中，找到与GitLab相关的配置部分，例如构建触发器或源码管理。在这些配置中，您将看到\u0026quot;凭据\u0026quot;（Credentials）的选项。\n选择凭据：在相关配置中，找到\u0026quot;凭据\u0026quot;（Credentials）选项，并选择您之前保存的GitLab凭据。\n保存配置：完成凭据选择后，确保保存项目的配置。\n完成上述步骤后，Jenkins将能够使用您配置的GitLab凭据进行与GitLab的交互，例如拉取代码、触发构建等操作。这样，您就可以通过Jenkins与GitLab进行无缝集成，并实现持续集成和持续交付的自动化流程。\nJenkins设置定时任务 登录Jenkins：使用管理员账户登录到Jenkins控制台。\n创建新项目：在Jenkins控制台主页上，点击\u0026quot;新建任务\u0026quot;（New Item）按钮。\n输入项目名称：为新项目指定一个易于识别的名称，并选择自由风格的软件项目（FreeStyle Project）类型。 配置源码管理：在项目配置页面的\u0026quot;源码管理\u0026quot;（Source Code Management）部分，选择Git作为源码管理工具，并填写GitLab仓库的URL。\n配置凭据：如果您之前在Jenkins中配置了GitLab的认证凭证，请在源码管理部分选择相应的凭据，以便Jenkins能够访问GitLab仓库。 配置构建触发器：在项目配置页面的\u0026quot;构建触发器\u0026quot;（Build Triggers）部分，选择\u0026quot;定时构建\u0026quot;（Build periodically）选项。\n设置定时任务：在\u0026quot;定时构建\u0026quot;选项中，使用Cron表达式指定定期执行任务的时间。例如，要每天的上午9点执行任务，可以使用\u0026quot;0 9 * * *\u0026ldquo;的Cron表达式。 配置构建步骤：在项目配置页面的\u0026quot;构建\u0026rdquo;（Build）部分，配置需要执行的构建步骤。这可以是构建脚本、测试命令、部署操作或任何其他您希望Jenkins执行的任务。 保存配置：完成配置后，确保保存项目的配置。\nJenkins将按照您设置的定时任务，定期拉取GitLab仓库中的代码，并执行您配置的构建步骤。您可以在Jenkins控制台的项目页面上查看每次构建的执行日志和结果，以及任何产生的警告或错误。\nJenkins定时任务执行 时间 查询日志 ","date":"2023-06-12T11:20:03Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202306121120/","title":"实现无间断的自动化：Jenkins与GitLab的定时任务工作流程"},{"content":"方案一 基于LVM快照进行备份切换 介绍: MySQL数据库本身并不支持快照功能(sqlServer支持) 因此快照备份是指通过文件系统支持的快照功能对数据库进行备份 备份的前提是将所有数据库文件放在同一文件分区中，然后对该分区进行快照操作\nLVM是LINUX系统下对磁盘分区进行管理的一种机制,LVM使用写时复制(copy-on-write)的技术来创建快照——例如，当创建一个快照时，仅复制原始卷中数据的元数据(meta data 注：data block)，并不会有数据的物理操作，因此 快照的创建过程是非常快的. 以下是关于pvcreate、vgcreate和lvcreate命令的解释和它们在逻辑卷管理过程中的关系图：\npvcreate：pvcreate命令用于创建物理卷（Physical Volume，PV）。物理卷是硬盘或分区的逻辑卷管理（LVM）组件，用于存储数据。pvcreate命令会将指定的硬盘或分区标记为物理卷，并准备其用于逻辑卷的创建。\nvgcreate：vgcreate命令用于创建卷组（Volume Group，VG）。卷组是物理卷的逻辑分组，它允许将多个物理卷组织在一起以形成一个大容量的存储池。vgcreate命令会将多个物理卷组合成一个卷组，并为该卷组分配一个唯一的名称。\nlvcreate：lvcreate命令用于创建逻辑卷（Logical Volume，LV）。逻辑卷是卷组中的逻辑存储单元，类似于传统的分区。逻辑卷可以根据需要调整大小，并用于创建文件系统或挂载其他文件系统。lvcreate命令会在指定的卷组中创建一个逻辑卷，并为其分配大小和名称。\n下面是它们之间的关系图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 +------------------------+ | pvcreate | | (创建物理卷，PV) | +-----------|------------+ | v +------------------------+ | vgcreate | | (创建卷组，VG) | +-----------|------------+ | v +------------------------+ | lvcreate | | (创建逻辑卷，LV) | +------------------------+ 实战分盘 假设您要使用一块10TB的磁盘来创建逻辑卷管理（LVM）结构，以下是使用pvcreate、vgcreate和lvcreate的命令和解释：\npvcreate：创建物理卷（Physical Volume，PV） 命令：pvcreate /dev/sdX 解释：使用pvcreate命令创建物理卷时，将/dev/sdX替换为您要使用的实际磁盘设备路径，例如/dev/sda。该命令会将磁盘设备标记为物理卷，以便用于逻辑卷的创建。 vgcreate：创建卷组（Volume Group，VG） 命令：vgcreate vg_name /dev/sdX 解释：使用vgcreate命令创建卷组时，将vg_name替换为您想要为卷组指定的名称，/dev/sdX为已创建物理卷的磁盘设备路径。该命令将一个或多个物理卷组合成一个卷组，以便在其中创建逻辑卷。 lvcreate：创建逻辑卷（Logical Volume，LV） 命令：lvcreate -L size -n lv_name vg_name 解释：使用lvcreate命令创建逻辑卷时，可以使用以下参数： -L size：指定逻辑卷的大小，可以使用单位（如G、M、T）来表示大小。 例如，-L 5G表示创建一个大小为5GB的逻辑卷。 -n lv_name：指定逻辑卷的名称，替换lv_name为您想要为逻辑卷指定的名称。 vg_name：指定逻辑卷所属的卷组名称。 1 2 3 pvcreate /dev/sdX vgcreate vg_name /dev/sdX lvcreate -L 5T -n lv_name vg_name 当快照创建完成，原始卷上 有写操作时，快照会跟踪原始卷块的改变，将要改变的数据在改变之前 复制到快照预留的空间里，因此这个原理的实现叫做写时复制\n先决条件和配置 所有的InnoDB文件(InnoDB的表空间文件和InnoDB的事务日志)必须是在单个逻辑卷(分区); 你需要绝对的时间点一致性，LVM不能为多于一个卷做某个时间点一致的快照。(这是LVM的一个限制；其他一些系统没有这个问题。) 必须在卷组中有足够的空闲空间来创建快照。需要多少取决于负载。当配置系统时，应该留一些未分配的空间以便后面做快照。 优缺点: 优点：\n几乎是热备 (创建快照前把表上锁，创建完后立即释放) 支持所有存储引擎 备份速度快 无需使用昂贵的商业软件(它是操作系统级别的) 缺点：\n无法预计服务停止时间 数据如果分布在多个卷上比较麻烦 (针对存储级别而言) 步骤 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 ## 创建lvm分区 # 创建LVM逻辑卷组 vgcreate mysql /dev/sdb # 创建LV逻辑卷 lvcreate -n lv_mysql -L 4G mysql # 格式化lvm分区 mkfs.xfs /dev/mysql/lv_mysql ## 将当前数据迁移到逻辑卷上(因为只有逻辑卷上的数据才可以进行快照) # 停止数据库 systemctl stop mysqld # 备份所有数据文件到指定的目录文件下 cd /var/lib/mysql tar czf /backup/mysql/mysql.tar.gz * # 挂载逻辑卷到当前mysql的数据目录下 mount /dev/mysql/lv_mysql /var/lib/mysql # 将备份的数据解压到数据目录下 tar xf /backup/mysql/mysql.tar.gz -C /var/lib/mysql # 修改权限再重新启动 chown mysql.mysql /var/lib/mysql systemctl start mysqld ## 快照备份数据库 # 给数据库加读锁,防止快照时插入数据 mysql\u0026gt;flush table with read lock; # 给mysql的数据库所在的逻辑卷创建快照 lvcreate -n lv_mysql_s -L 500M -s /dev/mysql/lv_mysql # 解锁 mysql\u0026gt; unlock tables; # 将逻辑卷挂载到目录里,这里创建一个临时目录 mkdir /mnt/mysql mount -o nouuid /dev/mysql/lv_mysql_s /mnt/mysql/ ## 测试快照备份,此处是直接切换了datadir的路径,也可以将快照数据保存,恢复时直接用快照数据替换原路径数据 # 修改mysql datadir路径 vim /etc/my.cnf datadir=/mnt/mysql # 重启mysql systemctl restart mysqld ## 其他一些查看信息的命令 # 查看磁盘分区 lsblk # 查看挂载信息的命令 df -h # 查看逻辑分区信息命令 lvdisplay 方案二 基于检测binlog来进行数据同步: 介绍: 利用canal进行基于MySQL数据库增量日志(binlog)解析，提供增量数据订阅和消费\n原理: 1 2 3 4 canal 工作原理 1. canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议 2. MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal ) 3. canal 解析 binary log 对象(原始为 byte 流) 官网示意图: 优缺点: 1 2 3 4 5 6 优点: 1. 相比于快照备份,不需要对源数据库重启或加锁.可以直接在代码层面实现 3. 与业务完全解耦,面向数据的,方便扩展,后续可以接入多个存储目的地 4. 可以对数据进行处理,可以分别根据DELETE,UPDATE,INSERT分别做不同的操作，并且可以根据biglog时间戳来更准确处理数据 缺点: 1. 需要单独维护`canal服务`和`客户端`.存在开发与维护成本.并且还需要占用服务器IO,内存,带宽资源 关键步骤: 确认已开启mysql的binlog,并且选择ROW(行)模式 show variables like \u0026rsquo;log_bin\u0026rsquo;; 安装canal(github直接可以下载),下载后根据QuickStart配置参数,运行 编写客户端代码(不局限于java)\n与canal建立长连接 循环获取数据，如果获取到则进行处理 数据处理，可以接入多个目的地 两种方案针对报表业务的解决方案 lvm快照备份 将备份切换过程编写为脚本，利用linux中corn定时执行脚本\n缺点：\n因为要锁表甚至重启，可能会影响到生产环境。 本身对linux不是很通透，可能会出现一些预期之外的问题。\nbiglog检测增量 当天结束整点时，canal客户端根据超过整点后的数据的时间来新建第二天的表，同时将旧表数据复制到新表。这样可以确保每新建的一张表最多包含不大于当天的数据。\n缺点：\n因为表数据是翻倍再递增的，所以数据量比起之前要大很多 新建表名影响到之前报表业务，之前报表也需要同步修改一下表名 补充：\n理想上来讲,应该是不会存在丢失消息,并且在当日结束后几秒内完成当日数据的隔离\n参考:\nhttps://blog.csdn.net/qingsong3333/article/details/77418238 https://help.aliyun.com/document_detail/131141.html https://github.com/alibaba/canal 《高性能MySQL(第3版)》\n","date":"2023-06-01T10:53:20Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202306011053/","title":"“实时数据同步：构建高效的 MySQL 数据同步方案“"},{"content":"原理 在现代的容器化环境中，节点资源的管理是一个重要的任务。特别是对于内存资源的管理，它直接影响着容器应用的性能和可用性。在 Kubernetes 中，我们可以利用自动调度和控制的机制来实现对节点内存的有效管理。本文将介绍一种基于 Bash 脚本的节点内存管理方案，并探讨其原理、优势、缺点以及部署和应用趋势。\n背景 注意到一个节点的内存使用率非常高，一些 Pods 在这个节点上无法正常运行并不断重启。经过仔细研究，发现是因为这个节点的内存资源不足，导致一些高内存需求的 Pods 无法正常运行。您决定实施一种解决方案，以确保这些高内存需求的 Pods 不会被调度到这个节点上，直到您能够通过扩容或升级硬件等方式来解决这个问题。 方案 为了实现这个解决方案，您决定使用 Kubernetes 中的污点（Taint）和容忍度（Toleration）机制。您将首先添加一个污点到这个节点，以标识它当前无法容纳高内存需求的 Pods。然后，您将在这些 Pods 的 YAML 文件中添加容忍度字段，以允许它们在具有更充足内存资源的其他节点上运行。最后，您将设置 SchedulingDisabled 标志，以确保后续的 Pods 不会被调度到这个节点上，直到您解决了该节点的内存问题。\n脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash # 功能：控制节点内存 # 如果节点内存占用率超过 90%，则禁止在该节点上调度 Pod # 如果节点内存占用率低于平均值且平均值小于 90%，则允许在该节点上调度 Pod # 使用 kubectl top 命令获取节点的内存占用率，并忽略掉一些特定的节点 data=$(kubectl top node | grep -v \u0026#34;MEMORY\u0026#34; | grep -v \u0026#34;cn-shenzhen\u0026#34; | sed \u0026#34;s/%//g\u0026#34;) # 计算所有节点的内存占用率的平均值 avg=$(echo \u0026#34;$data\u0026#34; | awk \u0026#39;{ sum += $NF } END { print sum / NR }\u0026#39; | awk -F. \u0026#39;{ print $1 }\u0026#39;) # 逐行读取每个节点的信息（节点名称和内存占用率） echo \u0026#34;$data\u0026#34; | awk \u0026#39;{ print $1, $NF }\u0026#39; | while read line do # 获取节点名称和内存占用率 n=$(echo $line | awk \u0026#39;{ print $1 }\u0026#39;) m=$(echo $line | awk \u0026#39;{ print $2 }\u0026#39;) # 如果内存占用率超过 90%，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod if [ \u0026#34;$m\u0026#34; -ge \u0026#34;90\u0026#34; ];then if kubectl get node | grep $n | grep -q \u0026#34;SchedulingDisabled\u0026#34;;then continue else kubectl cordon $n fi # 如果内存占用率低于平均值且平均值小于 90%，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod elif [ \u0026#34;$m\u0026#34; -lt \u0026#34;$avg\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$avg\u0026#34; -lt \u0026#34;90\u0026#34; ];then if kubectl get node | grep $n | grep -q \u0026#34;SchedulingDisabled\u0026#34;;then kubectl uncordon $n fi fi done 升级版(动态) 它实现了以下功能：\n使用 kubectl top 命令获取节点的内存占用率，并忽略掉特定节点（例如 \u0026ldquo;MEMORY\u0026rdquo; 和 \u0026ldquo;cn-shenzhen\u0026rdquo;）的数据。 计算所有节点的内存占用率的平均值。 遍历每个节点的内存占用率，并根据阈值动态调整节点的调度状态。 如果某个节点的内存占用率超过 90%，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod（使用 kubectl cordon 命令）。 如果某个节点的内存占用率低于下限阈值，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod（使用 kubectl uncordon 命令）。 这样，脚本会根据节点的内存占用率动态调整节点的调度状态，以确保每台机器资源使用率基本一样，并在内存占用率达到 90% 时禁止调度 Pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #/********************************************************** # * Author : 南宫乘风 # * Email : 1794748404@qq.com # * Last modified : 2023-07-31 16:21 # * Filename : k8s_auto_mem.sh # * Description : 根据节点的内存占用率动态调整节点的调度状态，以确保每台机器资源使用率基本一样，并在内存占用率达到 90% 时禁止调度 Pod。 # * *******************************************************/ #/bin/bash # 日志函数，接收时间、日志级别和日志内容作为参数 log() { local timestamp=$(date +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) local level=$1 local message=$2 echo \u0026#34;[$timestamp] [$level] $message\u0026#34; } log \u0026#34;info\u0026#34; \u0026#34;开始执行脚本\u0026#34; # 使用 kubectl top 命令获取节点的内存占用率，并忽略掉一些特定的节点 data=$(kubectl top node | grep -v \u0026#34;MEMORY\u0026#34; | grep -v \u0026#34;cn-shenzhen\u0026#34; | sed \u0026#34;s/%//g\u0026#34;) # 计算所有节点的内存占用率的平均值 avg=$(echo \u0026#34;$data\u0026#34; | awk \u0026#39;{ sum += $NF } END { print sum / NR }\u0026#39; | awk -F. \u0026#39;{ print $1 }\u0026#39;) # 逐行读取每个节点的信息（节点名称和内存占用率） echo \u0026#34;$data\u0026#34; | awk \u0026#39;{ print $1, $NF }\u0026#39; | while read line do # 获取节点名称和内存占用率 n=$(echo $line | awk \u0026#39;{ print $1 }\u0026#39;) m=$(echo $line | awk \u0026#39;{ print $2 }\u0026#39;) # 输出节点名称和内存占用率日志 log \u0026#34;info\u0026#34; \u0026#34;节点 $n 的内存占用率为 $m%\u0026#34; # 如果内存占用率超过90%，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod if [ \u0026#34;$m\u0026#34; -ge 90 ];then if kubectl get node $n | grep -q \u0026#34;SchedulingDisabled\u0026#34;;then log \u0026#34;info\u0026#34; \u0026#34;节点 $n 内存占用率超过90%，已经禁止调度 Pod\u0026#34; else log \u0026#34;info\u0026#34; \u0026#34;节点 $n 内存占用率超过90%，禁止调度 Pod\u0026#34; kubectl cordon $n fi else # 根据平均阈值动态调整阈值范围（比如将平均阈值的上限和下限设置为平均值的上下10%） threshold=$(echo \u0026#34;$avg * 0.1\u0026#34; | bc) upper_threshold=$(printf \u0026#34;%.0f\u0026#34; $(echo \u0026#34;$avg + $threshold\u0026#34; | bc)) lower_threshold=$(printf \u0026#34;%.0f\u0026#34; $(echo \u0026#34;$avg - $threshold\u0026#34; | bc)) # 如果内存占用率超过上限阈值，并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod if [ \u0026#34;$m\u0026#34; -ge \u0026#34;$upper_threshold\u0026#34; ];then if kubectl get node $n | grep -q \u0026#34;SchedulingDisabled\u0026#34;;then log \u0026#34;info\u0026#34; \u0026#34;节点 $n 内存占用率过高，已经禁止调度 Pod\u0026#34; else log \u0026#34;info\u0026#34; \u0026#34;节点 $n 内存占用率过高，禁止调度 Pod\u0026#34; kubectl cordon $n fi # 如果内存占用率低于下限阈值，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod elif [ \u0026#34;$m\u0026#34; -lt \u0026#34;$lower_threshold\u0026#34; ];then if kubectl get node $n | grep -q \u0026#34;SchedulingDisabled\u0026#34;;then log \u0026#34;info\u0026#34; \u0026#34;节点 $n 内存占用率较低，允许调度 Pod\u0026#34; kubectl uncordon $n fi fi fi done log \u0026#34;info\u0026#34; \u0026#34;脚本执行完成\u0026#34; 原理 该脚本的原理是通过获取节点的内存占用率，并根据预设的阈值进行判断和操作。具体流程如下：\n使用 kubectl top 命令获取节点的内存占用率，并忽略掉特定的节点。 计算所有节点的内存占用率的平均值。 逐行读取每个节点的信息，包括节点名称和内存占用率。 如果节点内存占用率超过设定的阈值（例如 90%），并且该节点没有被设置为不可调度状态，则禁止在该节点上调度 Pod。 如果节点内存占用率低于平均值且平均值小于阈值，并且该节点已被设置为不可调度状态，则允许在该节点上调度 Pod。 通过这样的原理，我们可以在集群中实现对节点内存的动态管理，确保节点资源的合理利用和容器应用的稳定运行。\n优势 自动化管理： 该脚本实现了自动化的节点内存管理，无需手动干预，减轻了运维人员的负担。 实时监测：通过定期执行脚本，可以实时监测节点的内存占用情况，及时做出调整，提高了容器应用的性能和可用性。 智能决策：根据设定的阈值和平均值，脚本能够智能地决策是否禁止或允许在节点上调度 Pod，确保资源的合理分配。\n缺点 依赖性： 该脚本依赖于 Kubernetes 命令行工具 kubectl 和集群的配置，因此需要保证环境的正确配置和可用性。 单一维度： 该脚本仅基于节点的内存占用率进行管理，没有考虑其他资源（如 CPU、存储）的情况，因此在综合资源管理方面还有待完善。\n","date":"2023-05-31T14:13:57Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/202305311413/","title":"当节点内存管理遇上 Kubernetes：自动调度与控制"},{"content":"前言 同步/异步的概念：\n同步是指完成事务的逻辑，先执行第一个事务，如果阻塞了，会一直等待，直到这个事务完成，再执行第二个事务，顺序执行 异步是和同步相对的，异步是指在处理调用这个事务的之后，不会等待这个事务的处理结果，直接处理第二个事务去了，通过状态、通知、回调来通知调用者处理结果。\nasyncio是python3.4版本引入到标准库 python3.5又加入了async/await特性。\n背景 因为业务需求需要写一个接口，然后返回数据。但是这个接口需要执行程序，需要1分钟左右。 1、当接口需要执行长时间的程序时，浏览器必须等待程序运行结束并返回结果才能响应请求。 2、如果程序执行时间过长，浏览器会一直等待，这将影响用户的体验，因为用户需要等待很长时间才能得到响应。 3、改造成异步执行可以避免这种情况，因为异步执行可以使得程序不需要等待长时间的 IO 操作完成，而是让程序在进行这些操作时可以进行其他的计算任务，从而提高程序的效率和响应速度，从而提高用户体验。\n原理 当我们使用 Python 进行异步编程时，使用异步调用可以提高程序的效率。异步调用是指程序在执行过程中可以在某些操作等待的时候切换到其他任务，从而提高程序的并发性能，简单来说就是通过利用 I/O 等待时间提高程序的执行效率。在 Python 中，通常我们使用 asyncio 库来实现异步调用。\n介绍在 Python 中使用 asyncio 实现异步调用的一些常见操作和技巧\n1. 使用 async/await 使用 async/await 是使用 asyncio 库的首选方法。使用这种方法可以把异步代码看作是顺序执行的代码，让代码更加易于编写和阅读。以下是使用 async/await 进行异步调用的一个简单示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio async def my_coroutine(): print(\u0026#39;开始执行协程\u0026#39;) await asyncio.sleep(1) print(\u0026#39;协程执行完毕\u0026#39;) async def main(): print(\u0026#39;开始执行主程序\u0026#39;) await asyncio.gather( my_coroutine(), my_coroutine(), my_coroutine() ) print(\u0026#39;主程序执行完毕\u0026#39;) asyncio.run(main()) 在上面的示例中，我们定义了一个 名为 my_coroutine 的协程，其中使用了 await asyncio.sleep(1) 来模拟等待 1 秒钟的操作。在主程序中，我们使用 asyncio.gather() 函数并行执行了 3 个 my_coroutine 协程，并等待它们全部执行完后输出了主程序执行完毕的提示。注意，在 Python 中使用 asyncio 进行异步编程，程序的入口点必须是一个协程，这个协程通常被称为主程序。\n2. 使用 asyncio.ensure_future() 等价于 await asyncio.ensure_future() 函数可以将协程或者一个 Future 对象封装成另一个 Future 对象，并触发其执行。相比于 Awaitable 协议，Future 有更广泛的用途，Future 可以由标准库中的其他库或者第三方库引入，可以在未来的时间点以其他方式进行操作。\n以下是使用 asyncio.ensure_future() 函数并发执行多个协程的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import asyncio async def my_coroutine(i): print(f\u0026#39;协程 {i} 开始执行\u0026#39;) await asyncio.sleep(1) print(f\u0026#39;协程 {i} 执行完毕\u0026#39;) async def main(): tasks = [] for i in range(3): task = asyncio.ensure_future(my_coroutine(i)) tasks.append(task) await asyncio.gather(*tasks) print(\u0026#39;主程序执行完毕\u0026#39;) asyncio.run(main()) ·同步代码：\n1 2 3 4 5 6 7 8 9 10 11 import time def hello(): time.sleep(1) def run(): for i in range(5): hello() print(\u0026#39;Hello World:%s\u0026#39; % time.time()) # 任何伟大的代码都是从Hello World 开始的！ if __name__ == \u0026#39;__main__\u0026#39;: run() ·异步代码\n1 2 3 4 5 6 7 8 9 10 11 12 import time import asyncio # 定义异步函数 async def hello(): await asyncio.sleep(1) print(\u0026#39;Hello World:%s\u0026#39; % time.time()) if __name__ ==\u0026#39;__main__\u0026#39;: loop = asyncio.get_event_loop() tasks = [hello() for i in range(5)] loop.run_until_complete(asyncio.wait(tasks)) async def 用来定义异步函数，await 表示当前协程任务等待睡眠时间，允许其他任务运行。然后获得一个事件循环，主线程调用asyncio.get_event_loop()时会创建事件循环，你需要把异步的任务丢给这个循环的run_until_complete()方法，事件循环会安排协同程序的执行。\n3. 使用 asyncio.Queue asyncio.Queue 类是一个基于协程的生产者-消费者队列，它可以让生产者和消费者通过异步编程方式交换数据。以下是使用 asyncio.Queue 实现生产者-消费者模式的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import asyncio import random async def producer(queue): while True: print(\u0026#39;生产者正在生产数据...\u0026#39;) data = random.randint(1, 100) await queue.put(data) await asyncio.sleep(1) async def consumer(queue): while True: print(\u0026#39;消费者正在等待数据...\u0026#39;) data = await queue.get() print(f\u0026#39;消费者消费了数据: {data}\u0026#39;) async def main(): queue = asyncio.Queue() producer_task = asyncio.ensure_future(producer(queue)) consumer_task = asyncio.ensure_future(consumer(queue)) await asyncio.gather(producer_task, consumer_task) asyncio.run(main()) 在上面的示例中，我们定义了一个生产者协程和一个消费者协程，生产者负责往队列中生产数据，消费者则负责从队列中消费数据。在主程序中，我们使用 asyncio.Queue() 创建了一个队列，然后使用 asyncio.ensure_future() 来创建了生产者和消费者 task，最后使用 asyncio.gather() 同时执行了这两个 task。\n使用 asyncio.Queue 不仅可以实现生产者-消费者模式，还可以实现多任务并发处理场景，例如使用多个生产者往队列中添加数据，使用多个消费者从队列中取数据进行处理等。\n综上所述，Python 中的异步调用是通过 asyncio 库来实现的，通过 async/await 语法或者 asyncio.ensure_future() 函数可以创建协程对象，并使用 asyncio.gather() 函数并行执行这些协程，从而实现异步调用的目的。此外，asyncio.Queue 类可以用于实现生产者-消费者模式等多种异步编程场景。\n异步数据返回 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 from flask import Flask, jsonify, request import time import threading app = Flask(__name__) # Dictionary to store task status and results tasks = {} def perform_task(task_id): # Simulating a time-consuming task time.sleep(5) # Update task status and result tasks[task_id][\u0026#39;status\u0026#39;] = \u0026#39;completed\u0026#39; tasks[task_id][\u0026#39;result\u0026#39;] = \u0026#39;Task completed successfully\u0026#39; @app.route(\u0026#39;/task\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def create_task(): # Generate a unique task ID task_id = str(len(tasks) + 1) # Create a new task with initial status as \u0026#39;processing\u0026#39; tasks[task_id] = {\u0026#39;status\u0026#39;: \u0026#39;processing\u0026#39;, \u0026#39;result\u0026#39;: None} # Start a new thread to perform the task asynchronously thread = threading.Thread(target=perform_task, args=(task_id,)) thread.start() return jsonify({\u0026#39;task_id\u0026#39;: task_id, \u0026#39;status\u0026#39;: \u0026#39;processing\u0026#39;}) @app.route(\u0026#39;/task/\u0026lt;task_id\u0026gt;\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_task_status(task_id): if task_id in tasks: task_status = tasks[task_id][\u0026#39;status\u0026#39;] task_result = tasks[task_id][\u0026#39;result\u0026#39;] if task_status == \u0026#39;completed\u0026#39;: # Task completed, return the result return jsonify({\u0026#39;status\u0026#39;: task_status, \u0026#39;result\u0026#39;: task_result}) else: # Task still processing, return the status return jsonify({\u0026#39;status\u0026#39;: task_status}) else: # Invalid task ID return jsonify({\u0026#39;error\u0026#39;: \u0026#39;Invalid task ID\u0026#39;}), 404 if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) 在访问接口task时，会生成一个信息存入到{}，然后异步执行任务。网页会自动跳转到/task/id 等待数据返回。 其实也可以使用Flask 的Celery 更加方便 简单。\n参考文档： https://juejin.cn/post/6992116138398187533 https://www.cnblogs.com/shenh/p/9090586.html https://www.cnblogs.com/shenh/p/15401891.html\n实战代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from flask import Flask, jsonify, request, make_response from testrunner import LabourRunner app = Flask(__name__) app.config[\u0026#34;SEND_FILE_MAX_AGE_DEFAULT\u0026#34;] = 300 app.config[\u0026#34;timeout\u0026#34;] = 60 @app.errorhandler(400) def par_err(error): return make_response(jsonify({\u0026#39;code\u0026#39;: \u0026#39;400\u0026#39;, \u0026#39;msg\u0026#39;: \u0026#39;请求参数不合法\u0026#39;}), 400) @app.errorhandler(404) def page_not_found(error): return make_response(jsonify({\u0026#39;code\u0026#39;: \u0026#39;404\u0026#39;, \u0026#39;msg\u0026#39;: \u0026#39;请求参数不合法\u0026#39;}), 404) @app.route(\u0026#39;/actuator/health\u0026#39;, methods=[\u0026#39;GET\u0026#39;, \u0026#39;HEAD\u0026#39;]) def health(): return jsonify({\u0026#39;online\u0026#39;: True}) @app.route(\u0026#39;/api/autotest\u0026#39;) def autotest(): dir_path = request.args.get(\u0026#39;dir_path\u0026#39;) print(dir_path) # TODO: 在这里添加对 path 和 file 参数的处理逻辑 # ... if not dir_path: # return make_response(jsonify({\u0026#39;code\u0026#39;: \u0026#39;404\u0026#39;, \u0026#39;msg\u0026#39;: \u0026#39;请求参数不合法\u0026#39;}), 404) return \u0026#34;请传入数据\u0026#34; s = LabourRunner() dest_name = s.runner(dir_path) file_url = \u0026#34;https://uat-xxx.xxxx.com/static/\u0026#34; + dest_name result = {\u0026#39;path\u0026#39;: dir_path, \u0026#39;dest_name\u0026#39;: file_url} return result if __name__ == \u0026#34;__main__\u0026#34;: app.run(host=\u0026#34;0.0.0.0\u0026#34;, port=5000, debug=True) 实战代码异步优化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 import threading import time from flask import Flask, jsonify, request, make_response, url_for, redirect from testrunner import LabourRunner app = Flask(__name__) app.config[\u0026#34;SEND_FILE_MAX_AGE_DEFAULT\u0026#34;] = 300 app.config[\u0026#34;timeout\u0026#34;] = 60 tasks = {} @app.errorhandler(400) def par_err(error): return make_response(jsonify({\u0026#39;code\u0026#39;: \u0026#39;400\u0026#39;, \u0026#39;msg\u0026#39;: \u0026#39;请求参数不合法\u0026#39;}), 400) @app.errorhandler(404) def page_not_found(error): return make_response(jsonify({\u0026#39;code\u0026#39;: \u0026#39;404\u0026#39;, \u0026#39;msg\u0026#39;: \u0026#39;请求参数不合法\u0026#39;}), 404) @app.route(\u0026#39;/actuator/health\u0026#39;, methods=[\u0026#39;GET\u0026#39;, \u0026#39;HEAD\u0026#39;]) def health(): return jsonify({\u0026#39;online\u0026#39;: True}) def perform_task(task_id, dir_path): # Simulating a time-consuming task s = LabourRunner() dest_name = s.runner(dir_path) file_url = \u0026#34;https://uat-xxxx.xxxx.com/static/\u0026#34; + dest_name # Update task status and result tasks[task_id][\u0026#39;status\u0026#39;] = file_url tasks[task_id][\u0026#39;result\u0026#39;] = dir_path @app.route(\u0026#39;/api/autotest\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def autotest(): task_id = str(len(tasks) + 1) tasks[task_id] = {\u0026#39;status\u0026#39;: \u0026#39;processing\u0026#39;, \u0026#39;result\u0026#39;: None} dir_path = request.args.get(\u0026#39;dir_path\u0026#39;) # TODO: 在这里添加对 path 和 file 参数的处理逻辑 # ... if not dir_path: error_msg = {\u0026#39;code\u0026#39;: 400, \u0026#39;msg\u0026#39;: \u0026#39;请求参数不合法或者文件夹不存在\u0026#39;} return jsonify(error_msg), 400 # return \u0026#34;请传入数据\u0026#34; thread = threading.Thread(target=perform_task, args=(task_id, dir_path,)) thread.start() return redirect(url_for(\u0026#39;get_task_status\u0026#39;, task_id=task_id)) @app.route(\u0026#39;/api/autotest/\u0026lt;task_id\u0026gt;\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_task_status(task_id): if task_id in tasks: task_status = tasks[task_id][\u0026#39;status\u0026#39;] task_result = tasks[task_id][\u0026#39;result\u0026#39;] if task_status == \u0026#39;completed\u0026#39;: # Task completed, return the result return jsonify({\u0026#39;status\u0026#39;: task_status, \u0026#39;result\u0026#39;: task_result}) else: # Task still processing, return the status return jsonify({\u0026#39;status\u0026#39;: task_status}) else: # Invalid task ID return jsonify({\u0026#39;error\u0026#39;: \u0026#39;Invalid task ID\u0026#39;}), 404 if __name__ == \u0026#34;__main__\u0026#34;: app.run(host=\u0026#34;0.0.0.0\u0026#34;, port=5000, debug=True) ","date":"2023-05-19T15:58:13Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/202305191558/","title":"当Python遇上异步编程：实现高效、快速的程序运行！"},{"content":"项目背景 随着钉钉应用的不断普及和企业数字化程度的提高，越来越多的企业需要开发钉钉接口来完成内部业务流程的自动化和优化。而Flask框架，则是一个轻量级的Python web框架，具有快速开发和灵活性的优势，是钉钉接口开发的理想选择。\n简介 本博客将介绍如何使用Flask框架开发钉钉接口模版。通过本篇博客的学习，您将能够实现企业自定义机器人（Custom Bot）的基本功能，包括接收和发送消息，回复消息模版等。同时，我们也会提供完整的代码和相关技术文档，方便您在实际工作中快速实现自己的钉钉接口需求。\n大致流程 环境搭建：安装Flask和钉钉SDK，并进行配置。 接口开发：编写接收和处理钉钉请求的API，并进行路由配置。 消息处理：解析钉钉请求的消息体，进行自定义消息的构建和发送。 模版回复：使用Jinja2模版引擎生成回复消息，并返回给钉钉服务端。 运行测试：运行Flask应用，并使用Postman等工具进行测试和调试。 以上仅是大致流程，具体实现步骤和技术细节会在博客中逐一详解。\n步骤流程 1、项目依赖 requirements.txt\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 certifi==2023.5.7 charset-normalizer==3.1.0 click==8.1.3 colorama==0.4.6 DingtalkChatbot==1.5.7 docopt==0.6.2 Flask==2.2.5 idna==3.4 importlib-metadata==6.6.0 itsdangerous==2.1.2 Jinja2==3.1.2 MarkupSafe==2.1.2 pipreqs==0.4.13 requests==2.30.0 typing_extensions==4.5.0 urllib3==1.26.5 Werkzeug==2.2.3 yarg==0.1.9 zipp==3.15.0 2、工具类 utils/send_ding.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # -*- coding: utf-8 -*- # @Time : 2023/5/13 19:16 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : send_ding.py # @Software: PyCharm # -*- coding: utf-8 -*- # @Time : 2023/5/13 13:44 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : send_dingding.py # @Software: PyCharm \u0026#34;\u0026#34;\u0026#34; 发送钉钉的api接口消息验证 \u0026#34;\u0026#34;\u0026#34; # https://oapi.dingtalk.com/robot/send?access_token=xxx # 加密 xxxx import time import hmac import hashlib import base64 import urllib.parse import requests secret = \u0026#39;xxxx 加密秘钥\u0026#39; secret_enc = secret.encode(\u0026#39;utf-8\u0026#39;) access_token = \u0026#39;token\u0026#39; def generate_timestamp(): \u0026#34;\u0026#34;\u0026#34;获取当前时间戳\u0026#34;\u0026#34;\u0026#34; return str(round(time.time() * 1000)) def generate_sign(secret_enc, timestamp): \u0026#34;\u0026#34;\u0026#34;生成签名\u0026#34;\u0026#34;\u0026#34; string_to_sign = \u0026#39;{}\\n{}\u0026#39;.format(timestamp, secret) string_to_sign_enc = string_to_sign.encode(\u0026#39;utf-8\u0026#39;) hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.parse.quote_plus(base64.b64encode(hmac_code)) return sign def main(): timestamp = generate_timestamp() sign = generate_sign(secret_enc, timestamp) # 构造请求 URL url = \u0026#39;https://oapi.dingtalk.com/robot/send?access_token={}\u0026amp;timestamp={}\u0026amp;sign={}\u0026#39;.format(access_token, timestamp, sign) content = \u0026#39;Hello, World!\u0026#39; # 构造请求 headers 和 body headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} payload = {\u0026#39;msgtype\u0026#39;: \u0026#39;text\u0026#39;, \u0026#39;text\u0026#39;: {\u0026#39;content\u0026#39;: content}} # 发送请求并返回响应结果 response = requests.post(url, json=payload, headers=headers) print(response.text) if __name__ == \u0026#39;__main__\u0026#39;: main() 3、Flask接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 # -*- coding: utf-8 -*- # @Time : 2023/5/13 19:15 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : app.py # @Software: PyCharm import datetime import json from time import strftime from utils.send_ding import access_token, generate_timestamp, generate_sign, secret_enc from dingtalkchatbot.chatbot import DingtalkChatbot import datetime from flask import Flask, render_template, request, jsonify app = Flask(__name__) def generate_text_info(url, title, context, author): t = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) text = (\u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#008000\\\u0026#39;\u0026gt;\u0026lt;b\u0026gt;[巡检]\u0026lt;/b\u0026gt; \u0026lt;/font\u0026gt;\u0026lt;b\u0026gt;每天巡检\u0026lt;/b\u0026gt;\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\\n\\n --- \\n\\n\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#708090\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;详情：\u0026lt;/b\u0026gt; [点击查看详情](%s)\u0026lt;/font\u0026gt; \\n\\n \u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#778899\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;巡检标题：\u0026lt;/b\u0026gt; %s\u0026lt;/font\u0026gt; \\n\\n \u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#708090\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;巡检类容：\u0026lt;/b\u0026gt; %s\u0026lt;/font\u0026gt; \\n\\n \u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#708090\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;相关人员：\u0026lt;/b\u0026gt; %s\u0026lt;/font\u0026gt;\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\\n\\n --- \\n\\n\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;b\u0026gt;播报时间：\u0026lt;/b\u0026gt; %s\u0026#39;\u0026#39;\u0026#39;) % (url, title, context, author, t) return text def generate_text_error(url, title, context, author): t = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) text = (\u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#FF0000\\\u0026#39;\u0026gt;\u0026lt;b\u0026gt;[巡检]\u0026lt;/b\u0026gt; \u0026lt;/font\u0026gt;\u0026lt;b\u0026gt;每天巡检异常\u0026lt;/b\u0026gt;\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\\n\\n --- \\n\\n\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#708090\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;详情：\u0026lt;/b\u0026gt; [点击查看详情](%s)\u0026lt;/font\u0026gt; \\n\\n \u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#778899\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;巡检标题：\u0026lt;/b\u0026gt; %s\u0026lt;/font\u0026gt; \\n\\n \u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#708090\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;巡检类容：\u0026lt;/b\u0026gt; %s\u0026lt;/font\u0026gt; \\n\\n \u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;font color=\\\u0026#39;#708090\\\u0026#39; size=2\u0026gt;\u0026lt;b\u0026gt;相关人员：\u0026lt;/b\u0026gt; %s\u0026lt;/font\u0026gt;\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\\n\\n --- \\n\\n\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39;\u0026lt;b\u0026gt;播报时间：\u0026lt;/b\u0026gt; %s\u0026#39;\u0026#39;\u0026#39;) % (url, title, context, author, t) return text def send_dingtalk_msg(title, wehook, text): ddrobot = DingtalkChatbot(wehook) ret = ddrobot.send_markdown(title, text=text, is_at_all=False) return ret def dingtalk_robot(url, title, content, author, level): timestamp = generate_timestamp() sign = generate_sign(secret_enc, timestamp) # 构造请求 URL wehook = \u0026#39;https://oapi.dingtalk.com/robot/send?access_token={}\u0026amp;timestamp={}\u0026amp;sign={}\u0026#39;.format(access_token, timestamp, sign) # wehook = \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=paste_your_token_from_dingtalk\u0026#39; if level == \u0026#34;info\u0026#34;: text = generate_text_info(url, title, content, author) else: text = generate_text_error(url, title, content, author) result = send_dingtalk_msg(title, wehook, text) return result message = { \u0026#39;msg\u0026#39;: \u0026#39;请按照上面格式 post 请求参数(source（认证） title（标题） content（内容） 必选参数)\u0026#39;, \u0026#39;data\u0026#39;: {\u0026#39;source\u0026#39;: \u0026#39;heian\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;rds日志拉去\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;rds日志拉去正常\u0026#39;, \u0026#39;level\u0026#39;: \u0026#39;info|error (默认: info)\u0026#39;, \u0026#39;author\u0026#39;: \u0026#39;南宫乘风（默认: 巡检机器人）\u0026#39;} } @app.route(\u0026#39;/ding/send\u0026#39;, methods=[\u0026#39;POST\u0026#39;, \u0026#34;GET\u0026#34;]) def send(): if request.method == \u0026#39;GET\u0026#39;: return jsonify(message) else: json_data = request.get_json() source = json_data.get(\u0026#39;source\u0026#39;) title = json_data.get(\u0026#39;title\u0026#39;) content = json_data.get(\u0026#39;content\u0026#39;) level = json_data.get(\u0026#39;level\u0026#39;, \u0026#39;info\u0026#39;) author = json_data.get(\u0026#39;author\u0026#39;, \u0026#39;巡检机器人\u0026#39;) if not source or not title or not content: return jsonify(message) if source != \u0026#39;heian\u0026#39;: return \u0026#34;认证失败,请联系运维人员！\u0026#34; else: # 在这里处理传入的参数 url = \u0026#39;http://127.0.0.1:5000/\u0026#39; result = dingtalk_robot(url, title, content, author, level) # print(result) if result[\u0026#39;errcode\u0026#39;] == 0: print(\u0026#39;消息发送成功！\u0026#39;) return \u0026#34;消息发送成功！\u0026#34; else: print(\u0026#39;消息发送失败：\u0026#39;, result[\u0026#39;errmsg\u0026#39;]) return f\u0026#34;消息发送失败, {result[\u0026#39;errmsg\u0026#39;]}\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) 项目测试 1、启动Flask 2、访问接口 http://127.0.0.1:5000/ding/send\nGET请求 POST请求 参数：\n1 2 3 4 5 6 7 { \u0026#34;author\u0026#34;: \u0026#34;南宫乘风（默认: 巡检机器人）\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;rds日志拉去正常\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;info|error (默认: info)\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;heian\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;rds日志拉去\u0026#34; } 传入JSON数据不对 ","date":"2023-05-13T21:07:25Z","image":"https://www.ownit.top/title_pic/16.jpg","permalink":"https://www.ownit.top/p/202305132107/","title":"Flask轻松构建钉钉接口模版，实现自动化流程优化"},{"content":"1、项目背景 随着云原生技术的不断发展，容器化应用已成为企业构建云原生架构的重要方式。而随着集群规模不断扩大，跨主机通信的需求也越来越重要。在 Kubernetes 集群中，Pod 是最小的调度和管理单位，而网络也是 Kubernetes 中最重要的组成部分。为了满足跨主机通信的需求，社区提供了各种各样的解决方案，其中 Calico 是一种利用 BGP 协议解决 Pod 与外部网络通信的解决方案。\n2、简介 Calico 是一个开源的容器网络解决方案，可以通过使用 BGP 协议来管理容器网络。与传统的基于 VXLAN 的解决方案相比，使用 Calico 可以避免网络数据包的封装和解封过程，提高了网络传输的效率和吞吐量。在 Kubernetes 集群中，Calico 可以用来打通 Pod 和局域网的网络，从而实现跨主机通信。\n3、背景 现状 集群内pod\u0026amp;node可以通过pod ip直接进行访问，容器访问虚拟机没有问题，但是虚拟机不能访问容器，尤其是通过consul注册的服务，必须打通网络后才可以互相调用 目标 打通pod和虚拟机的网络，使虚拟机可以访问pod ip 官方文档：https://docs.projectcalico.org/archive/v3.8/networking/bgp 4、使用Calico打通Pod网络 1、[Kubernetes Master节点]安装calico控制命令calicoctl\n1 2 3 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.8.9/calicoctl chmod +x calicoctl mv calicoctl /usr/bin/calicoctl 2、 [calicoctl安装节点]添加calico配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mkdir /etc/calico cat \u0026gt; /etc/calico/calicoctl.cfg \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \u0026#34;kubernetes\u0026#34; kubeconfig: \u0026#34;/root/.kube/config\u0026#34; EOF # 测试 calicoctl version Client Version: v3.8.9 Git commit: 0991d2fb Cluster Version: v3.8.9 # 出现此行代表配置正确 Cluster Type: k8s,bgp,kdd # 出现此行代表配置正确 3、 [calicoctl安装节点]配置集群路由反射器，node节点与master节点对等、master节点彼此对等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # 在本环境下将kubernetes master节点作为反射器使用 # 查看节点信息 [root@master1 node]# kubectl get node NAME STATUS ROLES AGE VERSION master Ready control-plane,master 3h19m v1.22.4 node1 Ready \u0026lt;none\u0026gt; 3h16m v1.22.4 node2 Ready \u0026lt;none\u0026gt; 3h15m v1.22.4 # 导出Master节点配置(多个导出) calicoctl get node k8s-test-master-1.fjf --export -o yaml \u0026gt; k8s-test-master-1.yml calicoctl get node k8s-test-master-2.fjf --export -o yaml \u0026gt; k8s-test-master-2.yml calicoctl get node k8s-test-master-3.fjf --export -o yaml \u0026gt; k8s-test-master-3.yml calicoctl get node master --export -o yaml \u0026gt; k8s-test-master-1.yml # 在3个Master节点配置中添加以下配置用于标识该节点为反射器 metadata: ...... labels: ...... i-am-a-route-reflector: true ...... spec: bgp: ...... routeReflectorClusterID: 224.0.0.1 # 更新节点配置 calicoctl apply -f k8s-test-master-1.yml # 其他节点与反射器对等 calicoctl apply -f - \u0026lt;\u0026lt;EOF kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: peer-to-rrs spec: nodeSelector: \u0026#34;!has(i-am-a-route-reflector)\u0026#34; peerSelector: has(i-am-a-route-reflector) EOF # 反射器彼此对等 calicoctl apply -f - \u0026lt;\u0026lt;EOF kind: BGPPeer apiVersion: projectcalico.org/v3 metadata: name: rr-mesh spec: nodeSelector: has(i-am-a-route-reflector) peerSelector: has(i-am-a-route-reflector) EOF 4、 [calicoctl安装节点]配置Master节点与核心交换机对等\n1 2 3 4 5 6 7 8 9 10 11 12 calicoctl apply -f - \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: rr-border spec: nodeSelector: has(i-am-a-route-reflector) peerIP: 192.168.83.1 asNumber: 64512 EOF # peerIP: 核心交换机IP # asNumber: 用于和核心交换机对等的ID 5、 [192.168.83.1,设备型号：cisco 3650]配置核心交换与Master(反射器)节点对等，这一步需要在对端BGP设备上操作，这里是用核心交换机\n1 2 3 4 5 router bgp 64512 bgp router-id 192.168.83.1 neighbor 192.168.83.36 remote-as 64512 neighbor 192.168.83.49 remote-as 64512 neighbor 192.168.83.54 remote-as 64512 6、查看BGP 对等状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 calicoctl node status # INFO字段全部为Established 即为正常 Calico process is running. IPv4 BGP status +---------------+---------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +---------------+---------------+-------+----------+-------------+ | 192.168.83.1 | node specific | up | 06:38:55 | Established | | 192.168.83.54 | node specific | up | 06:38:55 | Established | | 192.168.83.22 | node specific | up | 06:38:55 | Established | | 192.168.83.37 | node specific | up | 06:38:55 | Established | | 192.168.83.49 | node specific | up | 06:38:55 | Established | | 192.168.83.52 | node specific | up | 06:38:55 | Established | +---------------+---------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 7、测试，使用其他网段，如192.168.82.0/24的虚拟机ping 某一个pod ip，能正常通信即代表成功\n1 2 3 4 5 6 7 8 9 10 [dev][root@spring-boot-demo1-192.168.82.85 ~]# ping -c 3 172.15.190.2 PING 172.15.190.2 (172.15.190.2) 56(84) bytes of data. 64 bytes from 172.15.190.2: icmp_seq=1 ttl=62 time=0.677 ms 64 bytes from 172.15.190.2: icmp_seq=2 ttl=62 time=0.543 ms 64 bytes from 172.15.190.2: icmp_seq=3 ttl=62 time=0.549 ms --- 172.15.190.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2000ms rtt min/avg/max/mdev = 0.543/0.589/0.677/0.067 ms [dev][root@spring-boot-demo1-192.168.82.85 ~]# 5、使用Calico打通Svc网络 现状 一般情况下，Kuberntes集群暴露服务的方式有Ingress、NodePort、HostNetwork，这几种方式用在生产环境下是没有问题的，安全性和稳定性有保障。但是在内部开发环境下，使用起来就有诸多不便，开发希望可以直接访问自己的服务，但是Pod IP又是随机变化的，这个时候我们就可以使用SVC IP 或者SVC Name进行访问 目标 打通SVC网络，使开发本地可以通过SVC IP 或 SVC Name访问集群服务 官方文档：https://docs.projectcalico.org/archive/v3.8/networking/service-advertisement 注意：前提是已经用BGP打通了Pod网络或已经建立了BGP对等才可以继续进行 1、 [Kubernetes Master]确定SVC网络信息 1 2 3 4 [root@master1 node]# kubectl cluster-info dump|grep -i \u0026#34;service-cluster-ip-range\u0026#34; \u0026#34;--service-cluster-ip-range=172.16.0.0/16\u0026#34;, \u0026#34;--service-cluster-ip-range=172.16.0.0/16\u0026#34;, 2、 [Kubernetes Master]启用SVC网络广播\n1 2 [root@master1 ~]# kubectl patch ds -n kube-system calico-node --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;template\u0026#34;: {\u0026#34;spec\u0026#34;: {\u0026#34;containers\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;calico-node\u0026#34;, \u0026#34;env\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;CALICO_ADVERTISE_CLUSTER_IPS\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;172.16.0.0/16\u0026#34;}]}]}}}}\u0026#39; daemonset.apps/calico-node patched 3、测试 正常情况下启用BGP广播后，3分钟内核心交换即可接收到路由信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # 找到集群DNS服务进行测试 kubectl get svc kube-dns -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 172.16.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 3d21h # 找一个Pod IP在集群外进行解析测试，如果可以解析到结果说明SVC网络已经打通 [dev][root@spring-boot-demo1-192.168.82.85 ~]# dig -x 172.15.190.2 @172.16.0.10 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.9.4-RedHat-9.9.4-61.el7_5.1 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -x 172.15.190.2 @172.16.0.10 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 23212 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;2.190.15.172.in-addr.arpa. IN PTR ;; ANSWER SECTION: 2.190.15.172.in-addr.arpa. 30 IN PTR 172-15-190-2.ingress-nginx.ingress-nginx.svc.k8s-test.fjf. # 可以正常解析到主机记录 ;; Query time: 3 msec ;; SERVER: 172.16.0.10#53(172.16.0.10) ;; WHEN: Fri Jul 09 15:26:55 CST 2021 ;; MSG SIZE rcvd: 150 ","date":"2023-05-11T21:39:12Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/202305112139/","title":"Calico的BGP打通Kubernetes网络和局域网"},{"content":"背景 在爬虫应用开发中，常常需要批量下载图片，并对图片进行去重处理。Python 是一种非常流行的编程语言，也是开发爬虫应用的首选，本文将介绍如何使用 Python 下载图片，并对下载的图片进行去重处理。\n内容 首先，我们需要使用 Python 中的 Requests 库来下载图片，并使用 OS 库来创建保存图片的文件夹。下载图片后，我们可以使用 hashlib 库对图片的内容做哈希处理，并将处理后的哈希值作为图片的唯一识别标志，以便进行去重处理。在对图片进行去重处理时，我们需要将下载的图片与已有的图片进行比对，可以使用字典或集合等数据结构来存储已有图片的哈希值，以便查找和比对。在所有的图片下载完成后，我们可以将下载的图片的文件名或哈希值保存到本地文本文件中，以备后续查看或处理。 一些好看的动漫api接口：https://blog.csdn.net/likepoems/article/details/123924270\nhttps://img.r10086.com/\n代码 1、爬取图片代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # -*- coding: utf-8 -*- # @Time : 2023/3/30 13:56 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : main.py # @Software: PyCharm import os import requests from time import sleep # https://img.r10086.com/ # https://blog.csdn.net/likepoems/article/details/123924270 def download_images(dir_path, file_prefix, num_images): \u0026#34;\u0026#34;\u0026#34; 循环访问接口并保存图片到指定目录 dir_path：图片保存的目录 file_prefix：保存的文件名前缀 num_images：需要下载的图片数量 \u0026#34;\u0026#34;\u0026#34; if not os.path.exists(dir_path): os.makedirs(dir_path) # 设置请求头 headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \u0026#39; \u0026#39;Chrome/58.0.3029.110 Safari/537.36 \u0026#39; } for i in range(num_images): response = requests.get(\u0026#39;https://api.r10086.com/img-api.php?type=原神横屏系列1\u0026#39;, headers=headers) if response.status_code == 200: # 构造文件名 file_name = os.path.join(dir_path, f\u0026#39;{file_prefix}_{i}.jpg\u0026#39;) # 保存图片到本地文件 with open(file_name, \u0026#39;wb\u0026#39;) as f: f.write(response.content) print(file_name + \u0026#34; 下载完成\u0026#34;) else: print(f\u0026#39;获取图片失败，状态码：{response.status_code}\u0026#39;) sleep(1) # 示例 if __name__ == \u0026#39;__main__\u0026#39;: dir_path = \u0026#39;dongman\u0026#39; file_prefix = \u0026#39;image\u0026#39; num_images = 1000 download_images(dir_path, file_prefix, num_images) 2、图片去重 原理：MD5 是一种常用的哈希算法，它可以将任意长度的输入（比如一个字符串或者一个文件）转换成一个 128 比特长度的输出，输出值通常表示为一个 32 位的十六进制数字串。而对于任意输入的变化，其产生的输出也会有所不同，因此可以将 MD5 值作为唯一的识别标志来去重。在 Python 中，我们可以使用 hashlib 库中的 md5 函数来生成 MD5 值。\n流程：其具体实现流程如下：\n导入 hashlib 库。 定义与图片相关的 path、filename 和 filesize 等变量，使用 os.path 库中的函数处理路径和文件名。 对图片的二进制数据使用 hashlib.md5() 生成 MD5 值。 将生成的 MD5 值转换为字符串格式，去除无用字符。 使用集合或字典等数据结构存储已有图片的 MD5 值，在遍历待下载的图片时，判断其对应的 MD5 值是否已经存在于集合或字典中，若存在则说明图片已下载过，不再重复下载；否则可以将该图片下载下来，并将其对应的 MD5 值加入到已有图片集合中。 下载图片后，将其文件名或 MD5 值存储到本地文本文件中，便于后续查看或处理。 上述流程基本描述了使用 MD5 值去重的具体实现过程，其中还需结合具体应用场景进行优化和改进。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import os import shutil import hashlib def get_md5(file): \u0026#34;\u0026#34;\u0026#34;计算文件的MD5值\u0026#34;\u0026#34;\u0026#34; if not os.path.isfile(file): return None with open(file, \u0026#39;rb\u0026#39;) as f: md5 = hashlib.md5() md5.update(f.read()) return md5.hexdigest() def find_duplicate_images(dir_path): \u0026#34;\u0026#34;\u0026#34;查找重复图片\u0026#34;\u0026#34;\u0026#34; all_images = [] md5_list = [] delete_list = [] # 遍历整个目录，将所有图片的路径保存到一个列表中 for root, dirs, files in os.walk(dir_path): for file in files: if file.endswith(\u0026#39;.jpg\u0026#39;) or file.endswith(\u0026#39;.png\u0026#39;): all_images.append(os.path.join(root, file)) # 对于每个图片，计算它的MD5值，并将MD5值和路径保存到两个列表中 for image in all_images: md5 = get_md5(image) if md5 is not None: md5_list.append(md5) else: delete_list.append(image) # 判断MD5值列表中是否有重复的值，如果有，则说明该图片是重复图片，将其路径保存到一个删除列表中 for i in range(len(md5_list)): for j in range(i + 1, len(md5_list)): if md5_list[i] == md5_list[j]: delete_list.append(all_images[j]) # 遍历删除列表，将其中的图片移动到目标目录中 if not os.path.exists(target_dir): os.makedirs(target_dir) for image in delete_list: try: shutil.move(image, os.path.join(target_dir, os.path.basename(image))) print(\u0026#39;已移动重复文件：\u0026#39;, image) except Exception as e: print(\u0026#39;移动失败：%s，错误：%s\u0026#39; % (image, str(e))) print(\u0026#39;重复图片搜索完成，共找到%d个重复文件！\u0026#39; % len(delete_list)) # 示例 if __name__ == \u0026#39;__main__\u0026#39;: # 需要移动重复图片的目标目录 # target_dir设置全局变量 global target_dir target_dir = \u0026#39;repeat_image\u0026#39; dir_path = \u0026#39;dongman\u0026#39; find_duplicate_images(dir_path) ","date":"2023-04-13T19:45:42Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/202304131945/","title":"Python实现批量图片下载及去重处理"},{"content":"背景 某公司使用阿里云的 ECS 和 Redis 服务作为其业务支撑，为了及时了解机器的使用情况，领导要求业务部门对所有阿里云机器的平均资源使用率进行统计，并汇总在一个 Excel 表格中，以便领导查看和分析。\n需求 为了满足领导的需求，我们需要实现以下基本需求：\n使用阿里云 Python SDK 获取所有 ECS 和 Redis 实例的 ID 和名称，以及机器的规格和资源使用情况等信息。\n使用阿里云监控服务 ECS、Redis API 获取实例的监控数据，并将其保存为 Python 字典类型。\n根据用户定义的时间范围，计算所有实例的平均资源使用率，并将其汇总到一个 Python 字典中。\n使用 Excel 处理库（例如 openpyxl），创建一个新的 Excel 文件，并在其中创建一个新的 sheet。\n将计算得到的数据写入到 Excel 文件中。在写入时，需要提取出每个实例的 ID、名称、规格、平均 CPU 使用率、平均内存使用率和平均网络带宽使用率等信息，并按照一定的格式写入到 Excel 文件的合适位置。\n最后，保存 Excel 文件并退出程序。用户应当能够方便地查看和使用导出的 Excel 表格，并根据需要进行数据分析和处理。\n通过实现上述需求，业务部门可以较为方便和及时地了解到各个机器的使用情况，快速发现并解决资源使用过度或者资源浪费的问题，提高云计算资源的利用率和企业运营效率。同时，我们也能够充分发挥 Python 和阿里云 API 的强大能力，实现一个业务部门的全新 Python 项目，提升自己的编程能力和阿里云 API 使用经验。\n环境 Python 3.8 pip install alibabacloud_cms20190101==2.0.5\npip install alibabacloud_r_kvstore20150101==2.20.7\n获取所有ECS和Redis 机器的资源使用率\n使用到的api接口\n产品排列表 https://cms.console.aliyun.com/metric-meta/acs_ecs_dashboard/ecs 获取所有机器 https://next.api.aliyun.com/api/Cms/2019-01-01/DescribeMonitoringAgentHosts 获取所有redis https://next.api.aliyun.com/api/R-kvstore/2015-01-01/DescribeInstancesOverview\n资源使用率获取 https://next.api.aliyun.com/api/Cms/2019-01-01/DescribeMetricLast 实例数据获取（ECS和Redis） 可以参考下方的Api接口，我已经封装成函数\n获取所有机器 https://next.api.aliyun.com/api/Cms/2019-01-01/DescribeMonitoringAgentHosts 获取所有redsi https://next.api.aliyun.com/api/R-kvstore/2015-01-01/DescribeInstancesOverview\n如果要使用下方函数，请添加自己的密钥对\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 # -*- coding: utf-8 -*- # @Time : 2023/4/12 9:55 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : ecs_all.py # @Software: PyCharm import json from typing import List from alibabacloud_cms20190101.client import Client as Cms20190101Client from alibabacloud_cms20190101 import models as cms_20190101_models from alibabacloud_r_kvstore20150101.client import Client as R_kvstore20150101Client from alibabacloud_tea_openapi import models as open_api_models from alibabacloud_r_kvstore20150101 import models as r_kvstore_20150101_models from alibabacloud_tea_util import models as util_models from alibabacloud_tea_util.client import Client as UtilClient def create_client_redis( access_key_id: str, access_key_secret: str, ) -\u0026gt; R_kvstore20150101Client: \u0026#34;\u0026#34;\u0026#34; 使用AK\u0026amp;SK初始化账号Client @param access_key_id: @param access_key_secret: @return: Client @throws Exception \u0026#34;\u0026#34;\u0026#34; config = open_api_models.Config( # 必填，您的 AccessKey ID, access_key_id=access_key_id, # 必填，您的 AccessKey Secret, access_key_secret=access_key_secret ) # 访问的域名 config.endpoint = f\u0026#39;r-kvstore.aliyuncs.com\u0026#39; return R_kvstore20150101Client(config) def create_client_ecs( access_key_id: str, access_key_secret: str, ) -\u0026gt; Cms20190101Client: \u0026#34;\u0026#34;\u0026#34; 使用AK\u0026amp;SK初始化账号Client @param access_key_id: @param access_key_secret: @return: Client @throws Exception \u0026#34;\u0026#34;\u0026#34; config = open_api_models.Config( # 必填，您的 AccessKey ID, access_key_id=access_key_id, # 必填，您的 AccessKey Secret, access_key_secret=access_key_secret ) # 访问的域名 config.endpoint = f\u0026#39;metrics.ap-southeast-1.aliyuncs.com\u0026#39; return Cms20190101Client(config) def get_ecs_instances(): \u0026#34;\u0026#34;\u0026#34; 获取阿里云 ECS 实例列表 \u0026#34;\u0026#34;\u0026#34; client = create_client_ecs(\u0026#39;xxxx\u0026#39;, \u0026#39;xxxx\u0026#39;) describe_monitoring_agent_hosts_request = cms_20190101_models.DescribeMonitoringAgentHostsRequest() runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 data = client.describe_monitoring_agent_hosts_with_options(describe_monitoring_agent_hosts_request, runtime) return data except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) def get_redis_instances(): # 工程代码泄露可能会导致AccessKey泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议使用更安全的 STS 方式，更多鉴权访问方式请参见：https://help.aliyun.com/document_detail/378659.html client = create_client_redis(\u0026#39;xxxx\u0026#39;, \u0026#39;xxxx\u0026#39;) describe_instances_overview_request = r_kvstore_20150101_models.DescribeInstancesOverviewRequest( region_id=\u0026#39;cn-shenzhen\u0026#39; ) runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 data = client.describe_instances_overview_with_options(describe_instances_overview_request, runtime) return data except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) if __name__ == \u0026#39;__main__\u0026#39;: ecs_data = get_ecs_instances().to_map() redis_data = get_redis_instances().to_map() # 将 ECS 数据写入 JSON 文件中 with open(\u0026#39;ecs.json\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(json.dumps(ecs_data, indent=4,ensure_ascii=False)) # 将 Redis 数据写入 JSON 文件中 with open(\u0026#39;redis.json\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(json.dumps(redis_data, indent=4,ensure_ascii=False)) print(\u0026#34;数据已经写入本地文件\u0026#34;) ECS实例资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 # -*- coding: utf-8 -*- # @Time : 2023/4/11 17:17 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : ecs_monitor.py # @Software: PyCharm import json import sys from typing import List from alibabacloud_cms20190101.client import Client as Cms20190101Client from alibabacloud_tea_openapi import models as open_api_models from alibabacloud_cms20190101 import models as cms_20190101_models from alibabacloud_tea_util import models as util_models from alibabacloud_tea_util.client import Client as UtilClient from openpyxl import Workbook from openpyxl.utils import get_column_letter class Sample: def __init__(self): pass @staticmethod def create_client( access_key_id: str, access_key_secret: str, ) -\u0026gt; Cms20190101Client: \u0026#34;\u0026#34;\u0026#34; 使用AK\u0026amp;SK初始化账号Client @param access_key_id: @param access_key_secret: @return: Client @throws Exception \u0026#34;\u0026#34;\u0026#34; config = open_api_models.Config( # 必填，您的 AccessKey ID, access_key_id=access_key_id, # 必填，您的 AccessKey Secret, access_key_secret=access_key_secret ) # 访问的域名 config.endpoint = f\u0026#39;metrics.ap-southeast-1.aliyuncs.com\u0026#39; return Cms20190101Client(config) @staticmethod def main(parameter): # 工程代码泄露可能会导致AccessKey泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议使用更安全的 STS 方式，更多鉴权访问方式请参见：https://help.aliyun.com/document_detail/378659.html client = Sample.create_client(\u0026#39;xxxx\u0026#39;, \u0026#39;xxxx\u0026#39;) describe_metric_top_request = cms_20190101_models.DescribeMetricTopRequest( period=\u0026#39;2592000\u0026#39;, namespace=\u0026#39;acs_ecs_dashboard\u0026#39;, metric_name=parameter, orderby=\u0026#39;Average\u0026#39;, start_time=\u0026#39;2023-03-12 00:00:00\u0026#39;, end_time=\u0026#39;2023-04-11 00:00:00\u0026#39;, length=\u0026#39;100\u0026#39;, ) runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 data = client.describe_metric_top_with_options(describe_metric_top_request, runtime) return data except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) def get_host(): global host_list # 打开 host.json 文件 with open(\u0026#39;ecs.json\u0026#39;, \u0026#39;r\u0026#39;) as f: # 读取文件中的 JSON 数据 data = json.load(f) # 输出读取到的数据 host_list = data[\u0026#39;body\u0026#39;][\u0026#34;Hosts\u0026#34;][\u0026#34;Host\u0026#34;] host_list = [{k: v for k, v in d.items() if k not in ( \u0026#39;isAliyunHost\u0026#39;, \u0026#39;NetworkType\u0026#39;, \u0026#39;InstanceTypeFamily\u0026#39;, \u0026#39;Region\u0026#39;, \u0026#39;AliUid\u0026#39;, \u0026#39;SerialNumber\u0026#39;, \u0026#39;EipId\u0026#39;,\u0026#39;EipAddress\u0026#39;)} for d in host_list] print(host_list) return host_list def ecs_dashboard(parameter): data_ecs = Sample.main(parameter) response_dict = data_ecs.body.to_map() data_Datapoints = json.loads(response_dict[\u0026#34;Datapoints\u0026#34;]) # for data in data_Datapoints: # print(data) # print(type(data_Datapoints), data_Datapoints) return data_Datapoints def set_workbook(ece_data_list): data = { \u0026#39;AgentVersion\u0026#39;: \u0026#39;2.1.56\u0026#39;, \u0026#39;HostName\u0026#39;: \u0026#39;xxx\u0026#39;, \u0026#39;InstanceId\u0026#39;: \u0026#39;i-xxxxxxx\u0026#39;, \u0026#39;IpGroup\u0026#39;: \u0026#39;xxxxxxxxxx\u0026#39;, \u0026#39;OperatingSystem\u0026#39;: \u0026#39;Linux\u0026#39;, \u0026#39;CPU_Average\u0026#39;: 4.475, \u0026#39;Memory_Average\u0026#39;: 20.499 } # 新建一个 Workbook 对象 wb = Workbook() # 获取第一个 sheet ws = wb.active # 获取最大行数 max_row = ws.max_row # 设置表头 headers = list(data.keys()) for i, header in enumerate(headers, start=1): ws.cell(row=1, column=i).value = header # 写入数据 for row_data in ece_data_list: # 写入数据 max_row += 1 for i, header in enumerate(row_data.keys(), start=1): column_letter = get_column_letter(i) cell_address = \u0026#39;{}{}\u0026#39;.format(column_letter, max_row) ws[cell_address] = row_data[header] # 保存文件 wb.save(\u0026#39;ECS_服务器资源.xlsx\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: host_list_data = get_host() es_monitor_list_cpu = ecs_dashboard(\u0026#34;CPUUtilization\u0026#34;) es_monitor_list_memory = ecs_dashboard(\u0026#34;memory_usedutilization\u0026#34;) # 匹配CPU # 遍历 list2，匹配 instanceId 并添加 Average 值到 list1 的对应字典中 for d2 in es_monitor_list_cpu: for d1 in host_list_data: if d1[\u0026#39;InstanceId\u0026#39;] == d2[\u0026#39;instanceId\u0026#39;]: d1[\u0026#39;CPU_Average\u0026#39;] = d2[\u0026#39;Average\u0026#39;] # 匹配内存 for d3 in es_monitor_list_memory: for d1 in host_list_data: if d1[\u0026#39;InstanceId\u0026#39;] == d3[\u0026#39;instanceId\u0026#39;]: d1[\u0026#39;Memory_Average\u0026#39;] = d3[\u0026#39;Average\u0026#39;] # 输出更新后的 list1 # lst_sorted = sorted(host_list_data, key=lambda x: x[\u0026#39;Memory_Average\u0026#39;], reverse=True) print(host_list_data) set_workbook(host_list_data) Redis实例资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 # -*- coding: utf-8 -*- # @Time : 2023/4/11 17:17 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : ecs_monitor.py # @Software: PyCharm # 产品排列表 https://cms.console.aliyun.com/metric-meta/acs_ecs_dashboard/ecs?spm=a2c4g.163515.0.0.27c776abvQGocz # 获取所有机器 https://next.api.aliyun.com/api/Cms/2019-01-01/DescribeMonitoringAgentHosts?params={}\u0026amp;tab=DEBUG # 获取所有redsi https://next.api.aliyun.com/api/R-kvstore/2015-01-01/DescribeInstancesOverview?spm=a2c4g.473769.0.i0\u0026amp;lang=PYTHON\u0026amp;params={%22RegionId%22:%22cn-shenzhen%22}\u0026amp;tab=DEBUG import json import sys from typing import List from alibabacloud_cms20190101.client import Client as Cms20190101Client from alibabacloud_tea_openapi import models as open_api_models from alibabacloud_cms20190101 import models as cms_20190101_models from alibabacloud_tea_util import models as util_models from alibabacloud_tea_util.client import Client as UtilClient from openpyxl import Workbook from openpyxl.utils import get_column_letter class Sample: def __init__(self): pass @staticmethod def create_client( access_key_id: str, access_key_secret: str, ) -\u0026gt; Cms20190101Client: \u0026#34;\u0026#34;\u0026#34; 使用AK\u0026amp;SK初始化账号Client @param access_key_id: @param access_key_secret: @return: Client @throws Exception \u0026#34;\u0026#34;\u0026#34; config = open_api_models.Config( # 必填，您的 AccessKey ID, access_key_id=access_key_id, # 必填，您的 AccessKey Secret, access_key_secret=access_key_secret ) # 访问的域名 config.endpoint = f\u0026#39;metrics.ap-southeast-1.aliyuncs.com\u0026#39; return Cms20190101Client(config) @staticmethod def main(parameter): # 工程代码泄露可能会导致AccessKey泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议使用更安全的 STS 方式，更多鉴权访问方式请参见：https://help.aliyun.com/document_detail/378659.html client = Sample.create_client(\u0026#39;xxxx\u0026#39;, \u0026#39;xxx\u0026#39;) describe_metric_top_request = cms_20190101_models.DescribeMetricTopRequest( period=\u0026#39;2592000\u0026#39;, namespace=\u0026#39;acs_kvstore\u0026#39;, metric_name=parameter, orderby=\u0026#39;Average\u0026#39;, start_time=\u0026#39;2023-03-12 00:00:00\u0026#39;, end_time=\u0026#39;2023-04-11 00:00:00\u0026#39;, length=\u0026#39;100\u0026#39;, ) runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 data = client.describe_metric_top_with_options(describe_metric_top_request, runtime) return data except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) def get_host(): global host_list # 打开 host.json 文件 with open(\u0026#39;redis.json\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: # 读取文件中的 JSON 数据 data = json.load(f) # 输出读取到的数据 print(data) host_list = data[\u0026#39;body\u0026#39;][\u0026#34;Instances\u0026#34;] host_list = [{k: v for k, v in d.items() if k not in (\u0026#39;ArchitectureType\u0026#39;, \u0026#39;EngineVersion\u0026#39;, \u0026#39;EndTime\u0026#39;, \u0026#39;ResourceGroupId\u0026#39;, \u0026#39;ZoneId\u0026#39;, \u0026#39;CreateTime\u0026#39;, \u0026#39;VSwitchId\u0026#39;, \u0026#39;InstanceClass\u0026#39;, \u0026#39;ConnectionDomain\u0026#39;, \u0026#39;VpcId\u0026#39;, \u0026#39;ChargeType\u0026#39;, \u0026#39;NetworkType\u0026#39;, \u0026#39;InstanceStatus\u0026#39;, \u0026#39;RegionId\u0026#39;, \u0026#39;InstanceType\u0026#39;, \u0026#39;SecondaryZoneId\u0026#39;)} for d in host_list] return host_list def ecs_dashboard(parameter): data_ecs = Sample.main(parameter) response_dict = data_ecs.body.to_map() data_Datapoints = json.loads(response_dict[\u0026#34;Datapoints\u0026#34;]) return data_Datapoints def set_workbook(host_list_data): data = { \u0026#39;Capacity\u0026#39;: 256, \u0026#39;InstanceId\u0026#39;: \u0026#39;r-xxxxx\u0026#39;, \u0026#39;InstanceName\u0026#39;: \u0026#39;大数据\u0026#39;, \u0026#39;PrivateIp\u0026#39;: \u0026#39;xxxxxxxx\u0026#39;, \u0026#39;Memory_Average\u0026#39;: 15.719 } # 新建一个 Workbook 对象 wb = Workbook() # 获取第一个 sheet ws = wb.active # 获取最大行数 max_row = ws.max_row # 设置表头 headers = list(data.keys()) for i, header in enumerate(headers, start=1): ws.cell(row=1, column=i).value = header # 写入数据 for row_data in host_list_data: # 写入数据 max_row += 1 for i, header in enumerate(row_data.keys(), start=1): column_letter = get_column_letter(i) cell_address = \u0026#39;{}{}\u0026#39;.format(column_letter, max_row) ws[cell_address] = row_data[header] # 保存文件 wb.save(\u0026#39;Redis_服务器资源.xlsx\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: host_list_data = get_host() redis_monitor_list_Memory = ecs_dashboard(\u0026#34;StandardMemoryUsage\u0026#34;) # 匹配CPU # 遍历 list2，匹配 instanceId 并添加 Average 值到 list1 的对应字典中 for d2 in redis_monitor_list_Memory: for d1 in host_list_data: if d1[\u0026#39;InstanceId\u0026#39;] == d2[\u0026#39;instanceId\u0026#39;]: d1[\u0026#39;Memory_Average\u0026#39;] = d2[\u0026#39;Average\u0026#39;] # # 输出更新后的 list1 lst_sorted = sorted(host_list_data, key=lambda x: x[\u0026#39;Memory_Average\u0026#39;], reverse=True) print(host_list_data) set_workbook(lst_sorted) ","date":"2023-04-12T10:53:54Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202304121053/","title":"Python批量导出阿里云ECS和Redis实例的监控数据到Excel"},{"content":"背景 在日常工作中，我们经常需要创建Nginx配置文件的模板，以便在不同的环境中快速部署和配置Nginx服务器。然而，这样的任务通常需要重复性高、耗时长，且容易出错。为了加快这些任务的完成，并提高工作效率，可以使用一些自动化工具来简化Nginx配置文件的生成和管理。\n其中，一种常见的方法是使用基于文本替换的模板引擎，如Jinja2、Mustache等，将Nginx配置文件中的变量替换为实际的值。例如，可以将配置文件中的端口号、域名、SSL证书路径等信息作为变量，在部署时再根据实际情况进行替换，从而快速生成满足需求的Nginx配置文件。此外，还可以使用版本控制工具（如Git）来对Nginx配置文件进行管理，并利用CI/CD工具（如Jenkins）自动构建和部署Nginx服务器。\n通过自动化工具的使用，可以大大提高Nginx服务器的配置效率和准确性，并更好地适应不同环境下的需求。\n1、需求 开发部门 不定期会更新新的项目上线，会用到域名绑定服务器 进行暴露。\n流程 开发部门提出域名订单需求，包括需要绑定的域名和相应的服务器地址。 运维部门在DNS管理控制面板中添加DNS解析记录，将需要绑定的域名解析到相应的服务器IP地址。 运维部门在Nginx配置中创建新的server块，配置要绑定的域名和相应的站点信息，例如文档根目录、日志文件、SSL证书等。 运维部门将Nginx配置文件中的变量和实际的服务器地址进行替换，例如替换$server_name变量为实际要绑定的域名。 运维部门重载或重新启动Nginx服务，使新的配置生效。 最终，域名解析到相应的服务器地址，并由Nginx正确地将请求路由到相应的站点。 2、kubernetes+ingress实战 1、转发kubernetes的ingress kubernetes-cluster.conf\n1 2 3 4 upstream kubernetes-cluster { server 192.168.82.42 weight=5; keepalive 16; } 2、域名配置 ogateway-uat.xxxx.net.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 server { listen 80; server_name ogateway-uat.xxxx.net; rewrite ^/(.*)$ https://$host/$1 permanent; # IP白名单 include /usr/local/openresty/nginx/whitelist/corporation.conf; } server { listen 443 ssl; server_name ogateway-uat.xxxx.net; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/xxx.net.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/xxx.net.key; include ssl.conf; # IP白名单 放开时间0920-0930 include /usr/local/openresty/nginx/whitelist/corporation.conf; location / { proxy_pass http://kubernetes-cluster; include https_proxy.conf; } } 3、反向代理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 server { listen 80; server_name customer-uat.xxxxx.com; # IP白名单 include /usr/local/openresty/nginx/whitelist/corporation.conf; rewrite ^/(.*)$ https://$host/$1 permanent; } server { listen 443 ssl; server_name customer-uat.xxxx.com; # IP白名单 include /usr/local/openresty/nginx/whitelist/corporation.conf; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/xxxx.com.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/xxxxx.com.key; include ssl.conf; location / { proxy_pass http://192.168.102.202; include proxy.conf; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-HTTPS on; add_header Front-End-Https on; } } 4、负载轮询代理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 upstream xxxx-backend-uat { #server 192.168.99.147:1201; server 192.168.82.42; } server { listen 80; server_name inhouse-xxxx-uat.xxxx.com; include /usr/local/openresty/nginx/whitelist/corporation.conf; location / { proxy_pass http://xxxx-backend-uat; include http_proxy.conf; } } server { listen 443 ssl; server_name xxxx-backend-uat.xxxx.com; include /usr/local/openresty/nginx/whitelist/corporation.conf; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/xxxx.com.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/xxxx.com.key; include ssl.conf; location / { proxy_pass http://xxxx-backend-uat; include http_proxy.conf; } } 5、自动化处理 add_nginx.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 #!/bin/bash #日志级别 debug-1, info-2, warn-3, error-4, always-5 LOG_LEVEL=1 #日志名称 job_name=add_nginx #脚本目录 script_dir=/opt #日志文件 LOG_FILE=./${job_name}.log #调试日志 function log_debug() { content=\u0026#34;[DEBUG] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 1 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #信息日志 function log_info() { content=\u0026#34;[INFO] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 2 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #警告日志 function log_warn() { content=\u0026#34;[WARN] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 3 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[33m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #错误日志 function log_err() { content=\u0026#34;[ERROR] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 4 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[31m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #一直都会打印的日志 function log_always() { content=\u0026#34;[ALWAYS] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 5 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } function get_Whitelist() { # 提示用户输入选项 echo \u0026#34;1. 使用白名单\u0026#34; echo \u0026#34;2. 不使用白名单\u0026#34; # 获取用户输入 read -p \u0026#34;请选择要使用白名单配置：\u0026#34; choice_whitelist # 使用case语句根据用户选择设置变量 case $choice_whitelist in 1) use_whitelist=true ;; 2) use_whitelist=false ;; *) echo \u0026#34;错误：请输入正确的选项\u0026#34; \u0026gt;\u0026amp;2 exit 1 ;; esac } function set_whitelist() { if [ \u0026#34;$use_whitelist\u0026#34; = true ]; then log_info \u0026#34;已经设置IP白名单\u0026#34; sed -i -r \u0026#39;s/^(\\s*)#(.*include\\s*\\/usr\\/local\\/openresty\\/nginx\\/whitelist\\/corporation\\.conf;.*)$/\\1\\2/g\u0026#39; \u0026#34;$conf_file\u0026#34; else log_info \u0026#34;不使用IP白名单\u0026#34; fi } function check_nginx() { conf_file=\u0026#34;/chen/company_shell/$realm_name.conf\u0026#34; # 检查Nginx配置文件 if nginx -t \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo \u0026#34;Nginx configuration test passed.\u0026#34; else echo \u0026#34;Nginx configuration test failed. Please check your configuration file.\u0026#34; exit 1 fi } function kubernetes_nginx() { re_301=\u0026#39;rewrite ^/(.*)$ https://$host/$1 permanent;\u0026#39; tld=$(echo \u0026#34;$realm_name\u0026#34; | sed -E \u0026#39;s/.*\\.([^.]+\\.[^.]+)$/\\1/\u0026#39;) cat \u0026gt;$realm_name.conf \u0026lt;\u0026lt;EOF server { listen 80; server_name $realm_name; # IP白名单 #include /usr/local/openresty/nginx/whitelist/corporation.conf; $re_301 } server { listen 443 ssl; server_name $realm_name; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/$tld.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/$tld.key; include ssl.conf; # IP白名单 #include /usr/local/openresty/nginx/whitelist/corporation.conf; location / { proxy_pass http://kubernetes-cluster; include https_proxy.conf; } } EOF } function proxy_nginx() { read -p \u0026#34;设置反向代理（列如：http://172.18.199.115）：\u0026#34; proxy_ip re_301=\u0026#39;rewrite ^/(.*)$ https://$host/$1 permanent;\u0026#39; tld=$(echo \u0026#34;$realm_name\u0026#34; | sed -E \u0026#39;s/.*\\.([^.]+\\.[^.]+)$/\\1/\u0026#39;) cat \u0026gt;$realm_name.conf \u0026lt;\u0026lt;EOF server { listen 80; server_name $realm_name; $re_301 # IP白名单 #include /usr/local/openresty/nginx/whitelist/corporation.conf; } server { listen 443 ssl; server_name $realm_name; # IP白名单 #include /usr/local/openresty/nginx/whitelist/corporation.conf; ssl on; ssl_certificate /usr/local/openresty/nginx/ssl/$tld.crt; ssl_certificate_key /usr/local/openresty/nginx/ssl/$tld.key; include ssl.conf; location / { proxy_pass $proxy_ip; include https_proxy.conf; } } EOF } function upstream_nginx() { pass } # 定义函数：生产域名 function set_kubernetes_nginx() { # TODO: 在这里实现生产域名的操作 log_info \u0026#34;开始配置nginx模板\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; realm_name log_info \u0026#34;域名记录： $realm_name\u0026#34; get_Whitelist kubernetes_nginx # 输出变量值 set_whitelist #配置文件校检 check_nginx } function set_reverse_proxy_nginx() { # TODO: 在这里实现生产域名的操作 log_info \u0026#34;开始配置nginx模板\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; realm_name log_info \u0026#34;域名记录： $realm_name\u0026#34; get_Whitelist proxy_nginx # 输出变量值 set_whitelist #配置文件校检 check_nginx } function set_upstream_proxy_nginx() { # TODO: 在这里实现生产域名的操作 log_info \u0026#34;开始配置nginx模板\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; realm_name log_info \u0026#34;域名记录： $realm_name\u0026#34; get_Whitelist upstream_nginx # 输出变量值 set_whitelist #配置文件校检 check_nginx } # 定义主函数 function main() { # 显示菜单 echo \u0026#34;请选择一个选项：\u0026#34; echo \u0026#34;1. kubernetes接入\u0026#34; echo \u0026#34;2. 反向代理接入\u0026#34; echo \u0026#34;3. 负载均衡接入\u0026#34; # 读取用户输入 read -p \u0026#34;请输您的选择: \u0026#34; choice # 根据用户输入选择对应的操作 case $choice in 1) set_kubernetes_nginx ;; 2) set_reverse_proxy_nginx ;; 3) set_upstream_proxy_nginx ;; *) echo \u0026#34;无效的选项，请重新输入\u0026#34; ;; esac } # 调用主函数 main ","date":"2023-04-10T14:29:25Z","image":"https://www.ownit.top/title_pic/24.jpg","permalink":"https://www.ownit.top/p/202304101429/","title":"Nginx模板自动化"},{"content":"本文将介绍如何在 Spring Boot 中集成阿波罗（Apollo）和 Consul，并使用 Apollo 和 Consul 实现配置管理和服务注册与发现的功能。\n1. 什么是阿波罗 阿波罗是携程开源的分布式配置中心，支持多种编程语言和框架。它提供了一套完整的配置管理解决方案，可以帮助开发者实现配置管理、版本控制、灰度发布等功能。\nApollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。服务端基于 Spring Boot 和 Spring Cloud 开发，打包后可以直接运行，不需要额外安装 Tomcat 等应用容器。\nApollo 支持 4 个维度管理 Key-Value 格式的配置：\napplication (应用) environment (环境) cluster (集群) namespace (命名空间 Namespace 是配置项的集合，类似于一个配置文件的概念) 上图是Apollo配置中心中一个项目的配置首页\n在页面左上方的环境列表模块展示了所有的环境和集群，用户可以随时切换。 页面中央展示了两个namespace(application和FX.apollo)的配置信息，默认按照表格模式展示、编辑。用户也可以切换到文本模式，以文件形式查看、编辑。 页面上可以方便地进行发布、回滚、灰度、授权、查看更改历史和发布历史等操作。 1.1 集成 Apollo 的原理 Spring Boot 集成阿波罗可以通过引入 apollo-client 客户端库，并在 Spring Boot 应用程序中配置连接信息和获取配置信息来实现。具体流程如下：\nSpring Boot 应用程序启动时，先加载 bootstrap.yml 或者 bootstrap.properties 文件中的配置信息。\n在 bootstrap.yml 或者 bootstrap.properties 中配置阿波罗的连接信息（例如，阿波罗的地址、应用程序名称等）。\n在 Spring Boot 应用程序中注入 @Value 注解中指定的阿波罗配置项的值，即可使用阿波罗管理的配置信息。\n如果阿波罗的配置信息发生改变，Spring Boot 应用程序会自动从阿波罗更新最新的配置信息，并重新加载应用程序的配置。\n1.2. 集成 Apollo 的优势 集成阿波罗可以带来以下优势：\n管理多个环境的配置：阿波罗提供了环境切换和灰度发布的功能，可以轻松管理多个环境（例如，开发环境、测试环境、生产环境等）的配置信息。\n实时更新配置：阿波罗支持实时更新配置信息，可以在不重启应用程序的情况下动态更新配置信息。\n版本控制：阿波罗提供了版本控制的功能，可以记录每个配置项的历史版本，方便回滚和恢复数据。\n集成多种框架：阿波罗支持多种编程语言和框架，可以轻松集成到各种应用程序中。\n2、SpringBoot实战集成Apollo 1、引入依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.4\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.springboot.demo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;web-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;web-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;web-demo\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;spring-cloud.version\u0026gt;Finchley.RELEASE\u0026lt;/spring-cloud.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-consul-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.ctrip.framework.apollo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apollo-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2020.0.3\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;/project\u0026gt; 2、增加配置文件 1 2 3 4 5 6 apollo: bootstrap: enabled: true eagerLoad: enabled: true namespaces: application,tech.java.consul,tech.java.logback 3、Apollo配置（公用配置，引用） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 spring.cloud.consul.host = localhost spring.cloud.consul.port = 8500 spring.cloud.consul.discovery.prefer-ip-address = true spring.cloud.consul.discovery.instance-id = ${spring.application.name}-${POD_IP:localhost}-${server.port} consul.datacenter = uat spring.cloud.consul.discovery.healthCheckPath = ${management.context-path}/health consul.cluster = uat management.context-path = /actuator consul.version = 1.5.3 log.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} %level | [%t] %logger{36} [%L] | %msg%n log.dir = /app/logs/app 4、Apollo项目配置 上面为公共的配置引用，可以被多个项目使用\n接下来创建一个单独的Apollo项目。\n创建一个单独的 Apollo 项目可以分为以下几个步骤：\n登录阿波罗控制台：在浏览器中输入 https://config.xxx.com 地址，使用阿波罗管理员账号登录阿波罗控制台。\n创建新项目：在阿波罗控制台中，点击左侧导航栏中的“AppList”，然后点击“Create App”按钮，在弹出的对话框中填写应用程序名称、所属集群、所属命名空间等信息，并点击“Create”按钮创建新项目。\n添加配置项：进入新创建的项目页面后，点击右侧的“Namespace List”标签页，然后选择需要添加配置项的命名空间。在命名空间页面中，点击“Add Item”按钮，填写配置项的 Key 和 Value，并选择该配置项所属的环境（如 dev、test、prod 等）和版本号。\n下载客户端库：在阿波罗控制台中，点击右上角的“Portal”菜单，进入开发者门户网站。在网站中，选择对应的编程语言和框架，然后下载对应的客户端库（例如，Java + Spring Boot 应用程序需要下载 apollo-client 客户端库）。\n集成客户端库：将下载好的客户端库引入应用程序的依赖中，然后在应用程序中添加连接阿波罗的配置信息，并使用客户端库从阿波罗获取配置信息。例如，在 Spring Boot 应用程序中，可以在 bootstrap.yml 文件中添加以下配置信息：\n1 2 3 4 5 6 7 8 9 10 yaml spring: application: name: your-application-name profiles: active: dev apollo: meta: http://apollo-config-server-url bootstrap: enabled: true 其中，your-application-name 是应用程序的名称，dev 是配置环境的名称。http://apollo-config-server-url 是阿波罗的配置服务 URL。\n6.启动应用程序：将集成了 Apollo 的应用程序打包并启动，通过日志和控制台输出可以查看到应用程序从阿波罗获取配置信息的情况。\n以上就是创建一个单独的 Apollo 项目的流程，需要注意的是在实际应用中还需根据具体情况进行调整和优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 name = qqqqqqqqqqqqqqqqqqqqqqqqqqqqq server.port = 8081 spring.application.name = fqdemo4 spring.profiles.active = uat spring.cloud.consul.host = 192.168.102.20 spring.cloud.consul.port = 8500 spring.cloud.enabled = true spring.cloud.consul.discovery.enabled = true spring.cloud.consul.discovery.hostname = 127.0.0.1 spring.cloud.consul.discovery.register = true spring.cloud.consul.discovery.deregister = true spring.cloud.consul.discovery.prefer-ip-address = true spring.cloud.consul.discovery.instance-id = ${spring.application.name} spring.cloud.consul.discovery.service-name = ${spring.application.name} spring.cloud.consul.discovery.health-check-url = http://${spring.cloud.consul.discovery.hostname}:${server.port}/ 5、SpringBoot代码 WebDemoApplication 启动类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package com.springboot.demo.webdemo; import com.ctrip.framework.apollo.spring.annotation.EnableApolloConfig; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController @EnableDiscoveryClient @EnableApolloConfig public class WebDemoApplication { public static void main(String[] args) { SpringApplication.run(WebDemoApplication.class, args); } #apollo的变量获取 @GetMapping(\u0026#34;/\u0026#34;) @Value(\u0026#34;${name}\u0026#34;) public String hello(@RequestParam(value = \u0026#34;name\u0026#34;, defaultValue = \u0026#34;${name}\u0026#34;) String name) { return String.format(\u0026#34;Hello %s!\u0026#34;, name); } } HelloWorldController类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.springboot.demo.webdemo.controller; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; import java.util.HashMap; @RestController @RequestMapping(\u0026#34;/hello\u0026#34;) public class HelloWorldController { @GetMapping public String hello(){ return \u0026#34;hello SpringBoot 搭建成功啦\u0026#34;; } } EmployeeController 类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package com.springboot.demo.webdemo.controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import java.util.HashMap; @RestController @RequestMapping(\u0026#34;/employee\u0026#34;) public class EmployeeController { @GetMapping public HashMap\u0026lt;String, String\u0026gt; index(){ HashMap\u0026lt;String, String\u0026gt; hashmap = new HashMap\u0026lt;String, String\u0026gt;(); hashmap.put(\u0026#34;姓名\u0026#34;, \u0026#34;王二\u0026#34;); hashmap.put(\u0026#34;年龄\u0026#34;, \u0026#34;27\u0026#34;); hashmap.put(\u0026#34;工龄\u0026#34;, \u0026#34;6\u0026#34;); return hashmap; } } 6、启动测试 1 -Dapp.id=156 -Dapollo.meta=http://config.uat.bdata.api.fjf -Denv=uat -javaagent:pinpoint-agent-2.3.3/pinpoint-bootstrap.jar -Dpinpoint.applicationName=fanqiang.uat name变量 修改name变量，无需重启项目\n测试两个接口 Consul已经自动注册上 本文介绍了如何在 Spring Boot 中集成阿波罗，并详细说明了其原理和优势。通过使用阿波罗，可以轻松管理多个环境的配置信息，实现配置管理、版本控制、灰度发布等功能。同时，本文还给出了集成阿波罗的最佳实践，希望能够对开发者和运维在实际项目中使用阿波罗提供一些参考和帮助。\n参考文档：\nhttps://www.cnblogs.com/mrhelloworld/p/apollo1.html\nspring cloud apollo 配置中心 - 掘金\nSpring Boot 集成 Apollo 配置中心，真香、真强大！_51CTO博客_apollo集成springcloud\n","date":"2023-03-27T10:18:30Z","image":"https://www.ownit.top/title_pic/75.jpg","permalink":"https://www.ownit.top/p/202303271018/","title":"SpringBoot集成Apollo和自动注册Consul"},{"content":"1、背景 阿里云是一个全球领先的云计算服务提供商，其域名解析服务可以帮助用户将域名映射到IP地址，从而使得网站可以被访问。\n2、需求 日常使用阿里云域名解析服务需要登录阿里云账号进行操作，但是这样手工操作费时费力。因此，我们需要使用Python编写程序来实现域名解析功能，以便快速方便地完成域名解析任务。具体实现过程为：通过调用阿里云API接口获取用户授权，然后使用Python的DNS解析库对域名进行解析，并将解析结果存储到本地或者发送到指定邮箱。这样就可以省去手动登录阿里云的步骤，大大提高了工作效率。\n3、Python2自动域名解析 Python环境：Python 2.7.5\n阿里云的SDK：\naliyun-python-sdk-alidns (2.6.20)\naliyun-python-sdk-core (2.13.30)\n自动域名解析是一种高效的方式，可以帮助我们快速地将域名映射到对应的IP地址。在CentOS机器上，由于Python环境版本较老，因此需要注意对代码进行适配，以确保程序正常运行。\n在Python 2环境下，一些语法和库函数可能已经过时或不再支持，因此在编写代码时应该仔细检查，并根据需要进行相应的修改和替代。这样才能够确保程序的稳定运行和正确性。\n因为Centos机器默认安装Python2.0的环境，我们就是用2.0的语法进行书写\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #!/usr/bin/env python #coding=utf-8 # 导入模块 import os # 用于和操作系统交互 import sys # 用于获取命令行参数 import time # 用于处理时间相关的操作 import json # 用于解析和生成 JSON 格式的数据 # 导入阿里云 SDK 相关模块 from aliyunsdkcore.client import AcsClient from aliyunsdkcore.acs_exception.exceptions import ClientException from aliyunsdkcore.acs_exception.exceptions import ServerException from aliyunsdkalidns.request.v20150109.AddDomainRecordRequest import AddDomainRecordRequest # 创建一个 AcsClient 实例，用于调用阿里云 API client = AcsClient(\u0026#39;xxxx\u0026#39;, \u0026#39;xxxx\u0026#39;, \u0026#39;cn-xxxx\u0026#39;) # 获取命令行参数，并判断是否满足要求 v = sys.argv if len(v) \u0026lt; 2: print(\u0026#34;域名或记录值不存在\u0026#34;) else: m = v[1] value = v[2] domain = m.split(\u0026#34;.xxx.com\u0026#34;)[0] # 提取二级域名 # 创建一个阿里云 API 调用请求对象 request = AddDomainRecordRequest() request.set_accept_format(\u0026#39;json\u0026#39;) # 设置该请求需要传递的参数 request.set_DomainName(\u0026#34;xxx.com\u0026#34;) # 将 xxx.com 替换为你自己的一级域名 request.set_RR(domain) # 设置二级域名 if len(value) \u0026lt;= 15: request.set_Type(\u0026#34;A\u0026#34;) # 如果记录值不超过 15 个字符，则为 A 记录 else: request.set_Type(\u0026#34;CNAME\u0026#34;) # 否则为 CNAME 记录 request.set_Value(value) # 设置记录值 # 发送 API 请求，并处理异常 try: response = client.do_action_with_exception(request) print(response) except Exception as e: print(e) 使用方法\n1 2 3 ./addDNS-xxx.py fyhm.xxxx.net 需要解析IP for i in `cat dns.list`;do ./addDNS-xx.py $i IP;done 已经存在\n4、shell界面化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 #!/bin/bash #日志级别 debug-1, info-2, warn-3, error-4, always-5 LOG_LEVEL=1 #日志名称 job_name=dns_name #脚本目录 script_dir=/opt #日志文件 LOG_FILE=./${job_name}.log #调试日志 function log_debug() { content=\u0026#34;[DEBUG] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 1 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #信息日志 function log_info() { content=\u0026#34;[INFO] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 2 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #警告日志 function log_warn() { content=\u0026#34;[WARN] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 3 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[33m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #错误日志 function log_err() { content=\u0026#34;[ERROR] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 4 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[31m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } #一直都会打印的日志 function log_always() { content=\u0026#34;[ALWAYS] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $@\u0026#34; [ $LOG_LEVEL -le 5 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32m\u0026#34; ${content} \u0026#34;\\033[0m\u0026#34; } # 定义函数：增加解析记录 function add_dns_record() { local domain=$1 local ip=$2 local script=$3 # 校验域名是否符合要求 if [[ $domain =~ $4 ]]; then log_info \u0026#34;$domain 效验通过 正常\u0026#34; else log_err \u0026#34;不包含 $4，请检查域名是否符合\u0026#34; exit 1 fi # 增加解析记录 log_info \u0026#34;开始增加 $domain 解析\u0026#34; $script $domain $ip \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if [ $? != \u0026#34;0\u0026#34; ]; then log_err \u0026#34;$domain 解析已经存在，请检查\u0026#34; exit 1 fi log_info \u0026#34;$domain 添加解析完毕\u0026#34; } # 定义函数：生产域名 function prod_fujfu() { # TODO: 在这里实现生产域名的操作 log_info \u0026#34;您选择了 prod_fujfu\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; choice2 add_dns_record $choice2 xxxxxxxx $script_dir/addDNS.py \u0026#34;xxxx.com\u0026#34; } # 定义函数：测试域名 function uat_fujfu() { # TODO: 在这里实现测试域名的操作 log_info \u0026#34;您选择了 uat_fujfu\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; choice2 add_dns_record $choice2 xxx.xxx.xxx.xxx $script_dir/addDNS.py \u0026#34;xxxx.com\u0026#34; } # 定义函数：uat 域名 function prod_xxxx() { # TODO: 在这里实现生产域名的操作 log_info \u0026#34;您选择了 prod_xxxx\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; choice2 add_dns_record $choice2 xxx.xxx.xxx.xxx $script_dir/addDNS-xxxx.py \u0026#34;xxxx.net\u0026#34; } # 定义函数：dev域名 function uat_xxxx() { # TODO: 在这里实现dev域名的操作 log_info \u0026#34;您选择了 uat_xxxx\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; choice2 add_dns_record $choice2 xxx.xxx.xxx.xxx $script_dir/addDNS-xxxx.py \u0026#34;xxxx.net\u0026#34; } # 定义函数：uat 域名 function prod_mumugz() { # TODO: 在这里实现生产域名的操作 log_info \u0026#34;您选择了 prod_mumugz\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; choice2 add_dns_record $choice2 xxx.xxx.xxx.xxx $script_dir/addDNS-mumugz.py \u0026#34;xxxx.com\u0026#34; } # 定义函数：dev域名 function uat_mumugz() { # TODO: 在这里实现dev域名的操作 log_info \u0026#34;您选择了 uat_mumugz\u0026#34; read -p \u0026#34;请输您的域名: \u0026#34; choice2 add_dns_record $choice2 xxx.xxx.xxx.xxx $script_dir/addDNS-mumugz.py \u0026#34;xxxx.com\u0026#34; } # 定义主函数 function main() { # 显示菜单 echo \u0026#34;请选择一个选项：\u0026#34; echo \u0026#34;1. 生产 xxxx.com\u0026#34; echo \u0026#34;2. 测试 xxxx.com\u0026#34; echo \u0026#34;3. 生产 xxxx.net\u0026#34; echo \u0026#34;4. 测试 xxxx.net\u0026#34; echo \u0026#34;5. 生产 xxxx.com\u0026#34; echo \u0026#34;6. 测试 xxxx.com\u0026#34; # 读取用户输入 read -p \u0026#34;请输您的选择: \u0026#34; choice # 根据用户输入选择对应的操作 case $choice in 1) prod_fujfu ;; 2) uat_fujfu ;; 3) prod_xxxx ;; 4) uat_xxxx ;; 5) prod_mumugz ;; 6) uat_mumugz ;; *) echo \u0026#34;无效的选项，请重新输入\u0026#34; ;; esac } # 调用主函数 main ","date":"2023-03-21T10:45:13Z","image":"https://www.ownit.top/title_pic/16.jpg","permalink":"https://www.ownit.top/p/202303211045/","title":"Python域名解析"},{"content":"Prometheus 和 Grafana 监控 Consul 简介 Consul是一款常用的服务发现和配置管理工具，可以很好地管理和发现分布式系统中的服务和实例。而Prometheus是一款常用的开源监控和告警系统，可以监控各种不同的系统组件并进行告警和分析。本文将介绍如何使用Prometheus监控Consul服务端，以便更好地管理和分析Consul集群的运行情况。\n安装Conusl服务端 Conusl版本：Consul v1.5.3\n1、首先从官方网站（https://www.consul.io/downloads.html）下载适用于您的操作系统的二进制文件。\n2、解压缩下载的文件。例如，在Linux中，您可以使用以下命令解压缩：\n1 unzip consul_1.x.x_linux_amd64.zip 3、将解压缩的二进制文件移动到一个目录中，例如/usr/local/bin​：\n1 sudo mv consul /usr/local/bin/ 4、创建一个配置文件来配置Consul服务端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mkdir -p /etc/consul/ \u0026amp;\u0026amp; cd /etc/consul/ [root@bt consul]# cat consul.hcl data_dir = \u0026#34;/var/lib/consul\u0026#34; log_level = \u0026#34;WARN\u0026#34; enable_syslog = true server = true bootstrap = true ui = true datacenter = \u0026#34;ownit\u0026#34; check_update_interval = \u0026#34;0s\u0026#34; bind_addr = \u0026#34;192.168.102.20\u0026#34; client_addr = \u0026#34;192.168.102.20\u0026#34; telemetry { prometheus_retention_time = \u0026#34;24h\u0026#34; #Consul自带的监控 disable_hostname = true } 该配置文件指定Consul服务端应该使用的数据中心、是否是服务器节点（server）、启动集群时期望的服务器数量（bootstrap_expect）、数据存储目录（data_dir）、日志级别（log_level）、是否启用syslog（enable_syslog）、是否启用UI（ui）以及HTTP地址（addresses.http）。 5、启动Consul服务端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 vim /usr/lib/systemd/system/consul.service [Unit] Description=consul-server After=network-online.target [Service] User=consul PIDFile=/var/lib/consul/consul-server.pid ExecStart=/usr/bin/consul agent -config-dir /etc/consul -pid-file /var/lib/consul/consul-server.pid -log-file /var/log/consul/consul-server.log ExecReload=/bin/kill -s SIGHUP $MAINPID ExecStop=/bin/kill -INT $MAINPID [Install] WantedBy=multi-user.target systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart consul Prometheus架构 Prometheus监控系统由以下几个主要组件组成：\nPrometheus Server：用于存储和查询监控指标数据。 Exporter：用于将不同的应用程序、系统组件或服务的监控指标数据转换为Prometheus可以处理的格式。 Alertmanager：用于接收来自Prometheus Server的告警信息，并进行处理和发送告警通知。 方法一、Exporter监控Consul服务端 要将Consul服务端的监控指标数据导入到Prometheus中，可以使用Consul的官方Exporter或第三方Exporter。这些Exporter将Consul的运行指标数据暴露为Prometheus可以处理的格式。下面以官方Exporter为例进行介绍。\n安装Consul Exporter 可以通过以下命令来下载Consul Exporter：\n1 wget https://github.com/prometheus/consul_exporter/releases/download/v0.6.0/consul_exporter-0.6.0.linux-amd64.tar.gz 配置Consul Exporter 需要创建一个systemd服务文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 vim /usr/lib/systemd/system/consul_exporter.service [Unit] Description=Consul Exporter After=network.target [Service] Type=simple #User=conusl ExecStart=/usr/local/consul_exporter/consul_exporter --consul.server=192.168.102.20:8500 Restart=always [Install] WantedBy=multi-user.target systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart consul_exporter systemctl status consul_exporter 访问地址： http://192.168.102.20:9107/metrics\n3、配置prometheus文件 1 2 3 4 5 - job_name: \u0026#39;consul\u0026#39; scrape_interval: 10s scrape_timeout: 10s static_configs: - targets: [\u0026#39;192.168.102.20:9107\u0026#39;] 4、grafana导入模板 https://hellowoodes.oss-cn-beijing.aliyuncs.com/picture/custom-consul-grafana-dashboard.json\nhttps://grafana.com/grafana/dashboards/12049-consul-exporter-dashboard/\n‍\n方法二、Consul服务端自带监控 1、配置Consul 1 2 3 4 telemetry { prometheus_retention_time = \u0026#34;24h\u0026#34; #Consul自带的监控 disable_hostname = true } 2、prometheus配置 1 2 3 4 5 6 7 8 9 10 11 - job_name: consul-server honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: \u0026#39;/v1/agent/metrics\u0026#39; scheme: http params: format: [\u0026#34;prometheus\u0026#34;] static_configs: - targets: - 192.168.102.20:8500 3、grafana配置 https://grafana.com/grafana/dashboards/2351-consul/\n点击左侧+​按钮，选择 import​，输入Dashboard ID为 10642​，选择Prometheus为刚才添加的数据源，点击Import后即可看到监控面板\n","date":"2023-03-17T10:57:56Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/202303171057/","title":"Prometheus 和 Grafana 监控 Consul服务端"},{"content":"原理 Spring Boot是一个基于Spring框架的快速开发应用程序的框架，其提供了许多开箱即用的组件和自动配置选项，可以帮助开发人员快速构建高效且功能强大的应用程序。Consul是一种服务发现和配置工具，它可以管理和发现服务，还提供了一些高级功能，例如健康检查、负载均衡、故障转移等。\n在本场景中，我们使用Spring Boot框架创建了消费者和提供者两个模块，并将它们注册到Consul集群中。当消费者需要访问提供者时，它将从Consul中获取提供者的IP地址和端口，并通过这些信息建立连接来进行通信。通过这种方式，我们可以实现基于Consul的服务发现和调用。\n优势 使用Spring Boot和Consul的组合有以下几个优势：\n快速开发：Spring Boot提供了丰富的自动配置选项，可以大大加快开发速度。同时，Consul提供了集成式的服务发现和配置，可以使得服务注册和调用变得更加简单。\n高可用性：Consul可以自动检测服务的健康状态，并在发现异常时自动进行故障转移，保证服务的高可用性。\n负载均衡：Consul可以根据服务的负载情况进行负载均衡，确保请求可以平均分布到不同的提供者上，从而提高整个系统的性能和可靠性。\n可扩展性：使用Consul可以很容易地扩展服务，只需要将新的服务注册到Consul集群中即可。\n缺点 使用Spring Boot和Consul的组合也有以下一些缺点：\n复杂性：使用Consul需要一些配置和管理工作，需要花费一定的精力去理解和掌握其工作原理。\n依赖性：使用Consul需要依赖于第三方工具，这可能会导致一些依赖性问题。\n安全性：使用Consul需要考虑一些安全性问题，例如如何保护服务的访问权限等。\n流程 使用Spring Boot和Consul的组合实现服务注册和发现的流程大致如下：\n消费者启动时向Consul注册中心发送注册请求，并将自己的服务信息注册到Consul中心。 提供者启动时也向Consul注册中心发送注册请求，并将自己的服务信息注册到Consul中心。 消费者需要调用提供者时，从Consul注册中心获取提供者的IP地址和端口。 消费者根据提供者的IP地址和端口建立连接，并进行通信。 Consul可以根据服务的负载情况进行负载均衡，确保请求可以平均分布到不同的提供者上 部署 使用Spring Boot和Consul的组合实现服务注册和发现的部署可以分为以下几个步骤：\n安装Consul集群：首先需要在服务器上安装和配置Consul集群。可以从Consul官网下载安装包，并按照官方文档进行安装和配置。\n创建提供者服务：使用Spring Boot框架创建提供者服务，并将其注册到Consul集群中。\n创建消费者服务：同样使用Spring Boot框架创建消费者服务，并将其注册到Consul集群中。\n测试服务：通过调用消费者服务来测试整个系统的功能和性能。\n代码：GitHub - nangongchengfeng/consul-item: SpringBoot 开发多个模块，使用consul进行通信\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 HTTP GET http: //172.20.28.82:8080/actuator/health: 200 Output: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;consul\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;leader\u0026#34;: \u0026#34;192.168.102.20:8300\u0026#34;, \u0026#34;services\u0026#34;: { \u0026#34;ConsumerServer\u0026#34;: [], \u0026#34;ProviderServer\u0026#34;: [], \u0026#34;consul\u0026#34;: [], \u0026#34;heian\u0026#34;: [], \u0026#34;maple\u0026#34;: [], \u0026#34;nginx\u0026#34;: [] } } }, \u0026#34;discoveryComposite\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;components\u0026#34;: { \u0026#34;discoveryClient\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;services\u0026#34;: [\u0026#34;ConsumerServer\u0026#34;, \u0026#34;ProviderServer\u0026#34;, \u0026#34;consul\u0026#34;, \u0026#34;heian\u0026#34;, \u0026#34;maple\u0026#34;, \u0026#34;nginx\u0026#34;] } } } }, \u0026#34;diskSpace\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;total\u0026#34;: 76454166528, \u0026#34;free\u0026#34;: 63628378112, \u0026#34;threshold\u0026#34;: 10485760, \u0026#34;exists\u0026#34;: true } }, \u0026#34;livenessState\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34; }, \u0026#34;ping\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34; }, \u0026#34;readinessState\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34; }, \u0026#34;refreshScope\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;UP\u0026#34; } }, \u0026#34;groups\u0026#34;: [\u0026#34;liveness\u0026#34;, \u0026#34;readiness\u0026#34;] } 架构 使用Spring Boot和Consul的组合实现服务注册和发现的架构可以分为以下几个层次：\n应用层：应用层包括提供者和消费者两个服务，它们使用Spring Boot框架实现。\n服务发现层：服务发现层由Consul实现，它负责管理和发现服务，同时提供健康检查、负载均衡等功能。\n通信层：通信层负责提供者和消费者之间的通信，可以使用不同的通信协议和框架实现。\n数据层：数据层负责提供数据存储和访问服务，可以使用各种数据库和数据存储技术实现。\n应用趋势 随着微服务架构的发展，服务注册和发现成为了越来越重要的一环。使用Spring Boot和Consul的组合实现服务注册和发现可以提高系统的可靠性和可扩展性，同时也可以提高开发效率和用户体验。因此，预计该技术组合在未来会得到更广泛的应用。\n","date":"2023-03-16T16:12:05Z","image":"https://www.ownit.top/title_pic/70.jpg","permalink":"https://www.ownit.top/p/202303161612/","title":"使用Spring Boot和Consul实现高可用的服务注册与发现"},{"content":"Consul是什么 Consul是一个基于HTTP的服务发现工具，用于配置和管理系统和服务之间的依赖关系。它提供了一个简单的方式来注册、发现和配置服务，并包括健康检查、负载均衡和故障转移等功能。\nConsul是一种分布式系统，用于在大型分布式系统中实现服务发现和配置共享。它使用Raft协议来实现强一致性，保证了在多个节点之间进行的数据更新的原子性。\nConsul客户端可以将它们自己的服务信息注册到Consul服务器。这样，Consul服务器将自动成为所有服务的服务目录。此外，Consul还提供了一个健康检查机制，以确保已注册的服务仍然可以正常工作。\n当一个客户端想要调用一个服务时，它可以从Consul服务器获取服务的网络位置信息。然后，客户端将使用此信息建立与服务的网络连接。\nConsul通过HTTP API公开其服务发现和配置共享功能，这使得使用Consul与其他系统相当容易，如Kubernetes等容器编排工具。\n总之，Consul的分布式注册原理是通过服务端和客户端之间的交互来实现服务发现和健康检查。客户端将服务信息注册到Consul服务器，然后从Consul服务器获取服务信息并建立网络连接。Consul使用Raft协议确保强一致性，并提供HTTP API进行访问。\nConsul原理 Consul采用的是集中式的注册中心架构，其中包含一个服务发现组件和一个KV存储组件。\n服务发现组件：Consul中的服务发现是通过一组Agent和Server实现的。Agent是每个主机上运行的代理，用于与服务交互并报告本地运行状况。Server是负责在Consul集群中运行Raft算法的节点，以实现高可用性。\n当服务启动时，它会向本地的Consul Agent发送一个服务注册请求，Agent会将服务的元数据信息（服务名、地址、端口等）注册到Consul的KV存储中。服务消费者可以通过向Agent发送服务发现请求来获取服务的地址和端口信息，然后向该地址发送请求以调用服务。\nKV存储组件：Consul中的KV存储组件用于存储键值对信息，可以用于存储任何数据。在服务注册和发现中，服务元数据信息就是存储在KV存储中的。\n当服务启动时，它会将自己的元数据信息存储到Consul的KV存储中，同时也会定期发送心跳请求告诉Consul服务还在运行。当服务终止时，它会发送一个注销请求告诉Consul将服务从KV存储中删除。\n服务消费者可以从Consul的KV存储中获取服务的元数据信息，然后使用这些信息来调用服务。同时，服务消费者也可以监听KV存储中元数据信息的变化，当服务的元数据信息发生变化时，它会自动更新本地缓存。\n总体来说，Consul的服务注册和发现过程如下：\n服务启动时，将自己的元数据信息注册到Consul的KV存储中，同时向本地的Consul Agent发送心跳请求告诉Consul服务还在运行。 服务消费者向Consul Agent发送服务发现请求，Agent从KV存储中获取服务的元数据信息并返回给服务消费者。 服务消费者使用获取到的服务元数据信息来调用服务。 当服务终止时，它会发送一个注销请求告诉Consul将服务从KV存储中删除。 启动 Consul 集群 1 2 3 4 5 6 7 8 9 10 11 #启动第1个Server节点，集群要求要有3个Server，将容器8500端口映射到主机8900端口，同时开启管理界面 docker run -d --name=consul1 -p 8900:8500 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=true --bootstrap-expect=3 --client=0.0.0.0 -ui #启动第2个Server节点，并加入集群 docker run -d --name=consul2 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=true --client=0.0.0.0 --join 172.17.0.2 #启动第3个Server节点，并加入集群 docker run -d --name=consul3 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=true --client=0.0.0.0 --join 172.17.0.2 #启动第4个Client节点，并加入集群 docker run -d --name=consul4 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=false --client=0.0.0.0 --join 172.17.0.2 第 1 个启动容器的 IP 一般是 172.17.0.2，后边启动的几个容器 IP 会排着来：172.17.0.3、172.17.0.4、172.17.0.5。\n这些 Consul 节点在 Docker 的容器内是互通的，他们通过桥接的模式通信。但是如果主机要访问容器内的网络，需要做端口映射。\n在启动第一个容器时，将 Consul 的 8500 端口映射到了主机的 8900 端口，这样就可以方便的通过主机的浏览器查看集群信息。\nkubernetes集成Consul Kubernetes中的一个Pod中可以包含多个容器，这些容器可以协同工作，形成一个完整的服务。在这个例子中，一个Pod中包含了三个容器：Consul客户端容器、Java应用容器和Filebeat容器。\nConsul客户端容器是用来向Kubernetes集群外的Consul服务器注册服务的。Consul客户端可以通过API或DNS接口与Consul服务器进行通信，将服务注册信息发送给Consul服务器，包括服务的名称、IP地址、端口号等元数据信息。\nJava应用容器是用来运行业务应用程序的。它会通过Consul客户端容器中的API或DNS接口查询服务的信息，包括IP地址和端口号，以便能够与其它服务通信。\nFilebeat容器则是用来收集Java应用程序的日志文件，并将其发送到指定的日志服务器进行处理和存储。\n下面是大致的架构图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 +---------------------+ | Consul Server | | (集群外的Consul服务器) | +---------------------+ | | HTTP/DNS API | +---------------------+ | Consul Client | | (Pod中的容器) | +---------------------+ / \\ / \\ / \\ HTTP/DNS API | HTTP API +-------+ | +--------+ | Java | | | File- | | App |\u0026lt;------+------\u0026gt;| beat | | | | | +-------+ +--------+ 在这个架构图中，Java应用容器和Filebeat容器都与Consul客户端容器通过HTTP/DNS API进行通信。Java应用容器使用Consul客户端容器提供的服务注册信息与其它服务进行通信，Filebeat容器则通过HTTP API将收集的日志文件发送到指定的日志服务器。Consul客户端容器通过HTTP API将服务注册信息发送给Kubernetes集群外的Consul服务器，以便其它服务可以发现和使用它们。\n以下是 Java 应用通过 Consul 客户端注册到 Consul 服务端的详细步骤流程：\nPod 中的 Consul 客户端启动，并向 Consul 集群发现服务请求。\nConsul 客户端会向 Consul 服务端的任一节点发出注册请求，告知它所在的服务名称、IP 地址、端口号、标签等信息，以及该服务的健康检查规则。\nConsul 服务端接收到注册请求后，会将该服务信息存储在本地的 KV 存储中，并将该服务的状态标记为“不健康”。\nConsul 客户端会根据所注册的健康检查规则，定期向 Consul 服务端发送健康检查请求，并根据响应结果更新本地的服务状态。\n当该服务的状态为“健康”时，Consul 客户端会将该服务的状态信息存储在本地的缓存中，并向本地的 Java 应用提供服务发现和负载均衡功能。\nJava 应用通过调用 Consul 客户端提供的 API，获取所需服务的 IP 地址和端口号，并通过这些信息与目标服务建立连接。\n如果需要对该服务进行修改或注销，Java 应用可以向 Consul 客户端发送对应的请求。\nConsul 客户端接收到请求后，会将请求转发给 Consul 服务端，并更新本地的缓存。\nConsul 服务端接收到请求后，会更新该服务在本地 KV 存储中的信息，并将该服务的状态标记为“不健康”。\nConsul 客户端会根据所注册的健康检查规则，定期向 Consul 服务端发送健康检查请求，并根据响应结果更新本地的服务状态。\n总体架构图如下：\n在该架构中，Consul 客户端和 Java 应用都运行在 Pod 中，其中 Consul 客户端负责服务注册和服务发现，Java 应用则通过 Consul 客户端获取所需服务的信息。同时，Filebeat 用于收集应用的日志数据，并将其发送至目标日志存储系统。\nSpringBoot注册Consul SpringBoot可以通过使用Consul的Java客户端库来实现与Consul的集成，该库提供了丰富的API，包括服务注册、服务发现、KV存储等功能。下面是SpringBoot注册Consul的大致步骤和原理：\n引入Consul客户端库 在SpringBoot项目的pom.xml中引入Consul客户端库的依赖，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.4\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.springboot.demo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;web-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;web-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;web-demo\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;spring-cloud.version\u0026gt;Finchley.RELEASE\u0026lt;/spring-cloud.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-consul-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2020.0.3\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;/project\u0026gt; 2、配置Consul连接信息\n在SpringBoot项目的application.yml文件中配置Consul的连接信息，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 server: port: 1000 spring: application: name: heian cloud: consul: enabled: true host: 192.168.102.20 # 注册中心地址 port: 8500 # 注册中心地址的端口 discovery: hostname: 192.168.96.19 # 本地ip instance-id: ${spring.application.name}:${spring.cloud.consul.discovery.hostname}:${server.port} #health-check-path: ${server.servlet.context-path}/actuator/health # 如果配置了context-path就配置，没有就不配 health-check-interval: 15s # 每隔15s做一次健康检查 register: true register-health-check: true service-name: ${spring.application.name} # 放开端口 management: endpoints: web: exposure: include: \u0026#34;*\u0026#34; endpoint: health: show-details: always 3、实现服务注册\n启动类\n1 2 3 4 5 6 7 8 9 10 @EnableDiscoveryClient @SpringBootApplication public class WebDemoApplication { public static void main(String[] args) { SpringApplication.run(WebDemoApplication.class, args); } } 控制类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package com.springboot.demo.webdemo.controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import java.util.HashMap; @RestController @RequestMapping(\u0026#34;/employee\u0026#34;) public class EmployeeController { @GetMapping public HashMap\u0026lt;String, String\u0026gt; index(){ HashMap\u0026lt;String, String\u0026gt; hashmap = new HashMap\u0026lt;String, String\u0026gt;(); hashmap.put(\u0026#34;姓名\u0026#34;, \u0026#34;王二\u0026#34;); hashmap.put(\u0026#34;年龄\u0026#34;, \u0026#34;27\u0026#34;); hashmap.put(\u0026#34;工龄\u0026#34;, \u0026#34;6\u0026#34;); return hashmap; } } 总结 Spring Boot应用可以通过向Consul注册自身来实现服务发现和治理，使得其他服务可以在Consul中发现并调用它。\n文档：\nhttps://www.cnblogs.com/qingyunye/p/12932493.html\n一篇文章了解Consul服务发现实现原理 | Harries Blog™\n","date":"2023-03-15T19:28:27Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/202303151928/","title":"SpringBoot（微服务）注册分布式Consul"},{"content":"背景：阿里云的ECS服务器因为阿里云升级插件，导致安全防护程序重启，产生不同的端口。导致低自动发现注册的端口 大量报警。\n解决：杀掉关于因为非业务 变更的端口检测的触发器。\n相关文档：\nZabbix监控之主机端口监控自动发现\nzabbix监控端口原理 一个个去添加listen监控tcp的话不现实啊，还是也搞自动发现吧\n分割下来也是2步啊\n第一步脚本丢zabbix-agent下产生自定义键值\n第二步不就是zabbix-server添加自动发现绑定这个键值咯\n什么是安骑士Agent插件？\nAgent 插件_云安全中心（安骑士）-阿里云帮助中心\n解决思路 1、根据zabbix的api 获取的token\n2、根据token获取到问题主机的触发器id\n3、根据触发器id 删除相关的触发器，\n4、消停大面积的告警\nzabbix相关的API文档 可以查询官方文档或者博客\nhttps://www.cnblogs.com/rxysg/p/15700912.html Python调用Zabbix API接口批量修改（禁用/启用）触发器trigger_啥是比亚的技术博客_51CTO博客\n1、获取zabbix的token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # -*- coding: utf-8 -*- # @Time : 2023/2/17 16:45 # @Author : 南宫乘风 # @File : zabbix_trigger.py # @Software: PyCharm import json import os import requests url = \u0026#34;http://ip/zabbix/api_jsonrpc.php\u0026#34; # 此处域名修改为相应的地址 headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json-rpc\u0026#39; } tokens = \u0026#39;97553b7342457602a0a6452f0058c0ed\u0026#39; def token_get(): # 根据账号密码获取token data = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;user.login\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;Admin\u0026#34;, # zabbix管理员用户名 \u0026#34;password\u0026#34;: \u0026#34;密码\u0026#34; # 账户密码 }, \u0026#34;auth\u0026#34;: None, \u0026#34;id\u0026#34;: 1 } json_data = json.dumps(data) req = requests.post(url, data=json_data, headers=headers) js_req = req.json() print(js_req[\u0026#39;result\u0026#39;]) return js_req[\u0026#39;result\u0026#39;] 2、获取zabbix有问题主机触发器的id 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def hosts_get(token): # 获取有问题主机的触发器id # data = { # \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, # \u0026#34;method\u0026#34;: \u0026#34;host.get\u0026#34;, # \u0026#34;params\u0026#34;: { # \u0026#34;output\u0026#34;: [\u0026#34;hostid\u0026#34;, \u0026#34;name\u0026#34;], # \u0026#34;filter\u0026#34;: { # # 筛选条件 # \u0026#34;value\u0026#34;: 1, # value值为1表示有问题 # \u0026#34;status\u0026#34;: 0 # status为0表示已启用的trigger # }, # }, # # \u0026#34;auth\u0026#34;: token, # \u0026#34;id\u0026#34;: 1 # } data = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;trigger.get\u0026#34;, \u0026#34;params\u0026#34;: { # output表示输出结果包含参数有哪些 \u0026#34;output\u0026#34;: [ \u0026#34;triggerid\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;status\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;priority\u0026#34;, \u0026#34;lastchange\u0026#34;, \u0026#34;recovery_mode\u0026#34;, \u0026#34;hosts\u0026#34;, \u0026#34;state\u0026#34;, ], \u0026#34;selectHosts\u0026#34;: \u0026#34;hosts\u0026#34;, # 需包含主机ID信息，以便于根据主机ID查询主机信息 \u0026#34;selectItems\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;filter\u0026#34;: { # 筛选条件 \u0026#34;value\u0026#34;: 1, # value值为1表示有问题 \u0026#34;status\u0026#34;: 0 # status为0表示已启用的trigger }, }, \u0026#34;auth\u0026#34;: token, # 这里的auth就是登录后获取的 \u0026#39;id\u0026#39;: \u0026#39;1\u0026#39; # 这个id可以随意 } json_data = json.dumps(data) req = requests.post(url, data=json_data, headers=headers) js_req = req.json() print(len(js_req[\u0026#39;result\u0026#39;]), js_req[\u0026#39;result\u0026#39;]) id_list = [] #判断 有问题的地自动发现的端口 for item in js_req[\u0026#39;result\u0026#39;]: if \u0026#39;PROCESS\u0026#39; in item[\u0026#39;description\u0026#39;]: id_list.append(item[\u0026#39;triggerid\u0026#39;]) print(len(id_list), id_list) return js_req[\u0026#39;result\u0026#39;] 3、删除触发器的ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def del_trigger(id): id_one = [] ids = id_one.append(str(id)) values = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;trigger.delete\u0026#34;, \u0026#34;params\u0026#34;: id_one, # 触发器id \u0026#34;auth\u0026#34;: tokens, \u0026#34;id\u0026#34;: 1 } json_data = json.dumps(values) req = requests.post(url, data=json_data, headers=headers) js_req = req.json() print(js_req) # return js_req[\u0026#39;result\u0026#39;] 完正代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 # -*- coding: utf-8 -*- # @Time : 2023/2/17 16:45 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : zabbix_trigger.py # @Software: PyCharm import json import os import requests url = \u0026#34;http://ip/zabbix/api_jsonrpc.php\u0026#34; # 此处域名修改为相应的地址 headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json-rpc\u0026#39; } tokens = \u0026#39;97553b7342457602a0a6452f0058c0ed\u0026#39; def token_get(): # 根据账号密码获取token data = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;user.login\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;Admin\u0026#34;, # zabbix管理员用户名 \u0026#34;password\u0026#34;: \u0026#34;密码\u0026#34; # 账户密码 }, \u0026#34;auth\u0026#34;: None, \u0026#34;id\u0026#34;: 1 } json_data = json.dumps(data) req = requests.post(url, data=json_data, headers=headers) js_req = req.json() print(js_req[\u0026#39;result\u0026#39;]) return js_req[\u0026#39;result\u0026#39;] def hosts_get(token): # 获取所有主机信息 # data = { # \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, # \u0026#34;method\u0026#34;: \u0026#34;host.get\u0026#34;, # \u0026#34;params\u0026#34;: { # \u0026#34;output\u0026#34;: [\u0026#34;hostid\u0026#34;, \u0026#34;name\u0026#34;], # \u0026#34;filter\u0026#34;: { # # 筛选条件 # \u0026#34;value\u0026#34;: 1, # value值为1表示有问题 # \u0026#34;status\u0026#34;: 0 # status为0表示已启用的trigger # }, # }, # # \u0026#34;auth\u0026#34;: token, # \u0026#34;id\u0026#34;: 1 # } data = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;trigger.get\u0026#34;, \u0026#34;params\u0026#34;: { # output表示输出结果包含参数有哪些 \u0026#34;output\u0026#34;: [ \u0026#34;triggerid\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;status\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;priority\u0026#34;, \u0026#34;lastchange\u0026#34;, \u0026#34;recovery_mode\u0026#34;, \u0026#34;hosts\u0026#34;, \u0026#34;state\u0026#34;, ], \u0026#34;selectHosts\u0026#34;: \u0026#34;hosts\u0026#34;, # 需包含主机ID信息，以便于根据主机ID查询主机信息 \u0026#34;selectItems\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;filter\u0026#34;: { # 筛选条件 \u0026#34;value\u0026#34;: 1, # value值为1表示有问题 \u0026#34;status\u0026#34;: 0 # status为0表示已启用的trigger }, }, \u0026#34;auth\u0026#34;: token, # 这里的auth就是登录后获取的 \u0026#39;id\u0026#39;: \u0026#39;1\u0026#39; # 这个id可以随意 } json_data = json.dumps(data) req = requests.post(url, data=json_data, headers=headers) js_req = req.json() print(len(js_req[\u0026#39;result\u0026#39;]), js_req[\u0026#39;result\u0026#39;]) id_list = [] for item in js_req[\u0026#39;result\u0026#39;]: if \u0026#39;PROCESS\u0026#39; in item[\u0026#39;description\u0026#39;]: id_list.append(item[\u0026#39;triggerid\u0026#39;]) print(len(id_list), id_list) return js_req[\u0026#39;result\u0026#39;] #这边我做了个调试，如果想直接一次运行成功，建议自己改动 启动是的代码 id_lists = [\u0026#39;21284\u0026#39;, \u0026#39;21244\u0026#39;, \u0026#39;21249\u0026#39;, \u0026#39;21275\u0026#39;, \u0026#39;21264\u0026#39;, \u0026#39;21278\u0026#39;, \u0026#39;21262\u0026#39;, \u0026#39;21263\u0026#39;, \u0026#39;21266\u0026#39;, \u0026#39;21270\u0026#39;, \u0026#39;21272\u0026#39;, \u0026#39;21276\u0026#39;, \u0026#39;21277\u0026#39;, \u0026#39;21279\u0026#39;, \u0026#39;21267\u0026#39;, \u0026#39;21269\u0026#39;, \u0026#39;21254\u0026#39;, \u0026#39;21282\u0026#39;, \u0026#39;21287\u0026#39;, \u0026#39;21268\u0026#39;, \u0026#39;21273\u0026#39;, \u0026#39;21274\u0026#39;, \u0026#39;21285\u0026#39;, \u0026#39;21289\u0026#39;, \u0026#39;21283\u0026#39;, \u0026#39;21286\u0026#39;, \u0026#39;21290\u0026#39;, \u0026#39;21251\u0026#39;, \u0026#39;21250\u0026#39;, \u0026#39;21243\u0026#39;] def del_trigger(id): id_one = [] ids = id_one.append(str(id)) values = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;trigger.delete\u0026#34;, \u0026#34;params\u0026#34;: id_one, # 触发器id \u0026#34;auth\u0026#34;: tokens, \u0026#34;id\u0026#34;: 1 } json_data = json.dumps(values) req = requests.post(url, data=json_data, headers=headers) js_req = req.json() print(js_req) # return js_req[\u0026#39;result\u0026#39;] for i in id_lists: del_trigger(i) ","date":"2023-02-19T11:06:57Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/202302191106/","title":"Python获取zabbix问题触发器"},{"content":"CKA题目 每次还原初始化快照后，开机后等5分钟，ssh登录node01（11.0.1.112）检查一下所有的pod，确保都为Running，再开始练习。\n1 kubectl get pod -A 1、权限控制RBAC 问题 1 2 3 4 5 6 7 8 9 10 11 12 13 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Context 为部署流水线创建一个新的ClusterRole并将其绑定到范围为特定的 namespace 的特定ServiceAccount。 Task 创建一个名为deployment-clusterrole且仅允许创建以下资源类型的新ClusterRole： Deployment StatefulSet DaemonSet 在现有的 namespace app-team1中创建一个名为cicd-token的新 ServiceAccount。 限于 namespace app-team1中，将新的ClusterRole deployment-clusterrole绑定到新的 ServiceAccount cicd-token。 考点：RBAC授权模型的理解。\n没必要参考网址，使用-h帮助更方便。\nkubectl create clusterrole -h\nkubectl create rolebinding -h\n解答 1 2 3 4 5 6 7 8 kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulsets,daemonsets kubectl -n app-team1 create serviceaccount cicd-token # 题目中写了“限于namespace app-team1中”，则创建rolebinding。没有写的话，则创建clusterrolebinding。 kubectl -n app-team1 create rolebinding cicd-token-rolebinding --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token # rolebinding后面的名字cicd-token-rolebinding随便起的，因为题目中没有要求，如果题目中有要求，就不能随便起了。 检查（考试时，可以不检查的） kubectl -n app-team1 describe rolebinding cicd-token-rolebinding ‍\n2、查看pod的CPU 问题 1 2 3 4 5 6 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 通过 pod label name=cpu-loader，找到运行时占用大量 CPU 的 pod， 并将占用 CPU 最高的 pod 名称写入文件 /opt/KUTR000401/KUTR00401.txt（已存在）。 考点：kubectl top --l 命令的使用\n没必要参考网址，使用-h帮助更方便。\nkubectl top pod -h\n‍\n解答 1 2 3 4 5 6 7 8 9 10 11 12 13 考试时务必执行，切换集群。模拟环境中不需要执行。 # kubectl config use-context k8s 开始操作 # 查看pod名称 -A是所有namespace kubectl top pod -l name=cpu-loader --sort-by=cpu -A # 将cpu占用最多的pod的name写入/opt/test1.txt文件 echo \u0026#34;查出来的Pod Name\u0026#34; \u0026gt; /opt/KUTR000401/KUTR00401.txt 检查 cat /opt/KUTR000401/KUTR00401.txt 3、配置网络策略 NetworkPolicy ‍\n问题 1 2 3 4 5 6 7 8 9 10 设置配置环境： [candidate@node-1] $ kubectl config use-context hk8s Task 在现有的namespace my-app中创建一个名为allow-port-from-namespace的新NetworkPolicy。 确保新的NetworkPolicy允许namespace echo中的Pods连接到namespace my-app中的Pods的9000端口。 进一步确保新的NetworkPolicy： 不允许对没有在监听 端口9000的Pods的访问 不允许非来自 namespace echo中的Pods的访问 双重否定就是肯定，所以最后两句话的意思就是：\n仅允许端口为9000的pod方法。\n仅允许echo命名空间中的pod访问。\n考点：NetworkPolicy 的创建\n解答 参考官方文档，拷贝yaml文件内容，并修改。\n这里要注意，模拟题和真题截图里都有提到echo这个namespace，但是和真题的截图比较，你会发现，两个echo出现的位置不同，一个作为访问者，一个作为\n被访问者。\n所以不要搞混了。他们其实只是个名字而已，叫什么都无所谓，但要分清访问和被访问。\npodSelector：每个 NetworkPolicy 都包括一个 podSelector​， 它对该策略所适用的一组 Pod 进行选择。示例中的策略选择带有 \u0026quot; role (作用) =db\u0026quot; 标签的 Pod。 空的 podSelector​ 选择名字空间下的所有 Pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 开始操作 查看所有ns的标签label kubectl get ns --show-labels 如果访问者的namespace没有标签label，则需要手动打一个。如果有一个独特的标签label，则也可以直接使用。 kubectl label ns echo project=echo 编写一个yaml文件 vim networkpolicy.yaml #注意 :set paste，防止yaml文件空格错序。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: my-app spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: project: echo ports: - protocol: TCP port: 9000 检查\n1 kubectl describe networkpolicy -n my-app 4、暴露服务 service 问题 1 2 3 4 5 6 7 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 请重新配置现有的deployment front-end 以及添加名为http的端口规范来公开现有容器 nginx 的端口80/tcp。 创建一个名为front-end-svc的新service，以公开容器端口http。 配置此service，以通过各个Pod所在的节点上的 NodePort 来公开他们。 ‍\n解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # kubectl config use-context k8s 开始操作 检查deployment信息，并记录SELECTOR的Lable标签，这里是app=front-end kubectl get deployment front-end -o wide 参考官方文档，按照需要edit deployment，添加端口信息 kubectl edit deployment front-end #注意 :set paste，防止yaml文件空格错序。 spec: containers: - image: vicuu/nginx:hello imagePullPolicy: IfNotPresent name: nginx #找到此位置。下文会简单说明一下yaml文件的格式，不懂yaml格式的，往下看。 ports: #添加这4行 - name: http containerPort: 80 protocol: TCP 暴露对应端口 kubectl expose deployment front-end --type=NodePort --port=80 --target-port=80 --name=front-end-svc #注意考试中需要创建的是NodePort，还是ClusterIP。如果是ClusterIP，则应为--type=ClusterIP #--port是service的端口号，--target-port是deployment里pod的容器的端口号。 暴露服务后，检查一下service的selector标签是否正确，这个要与deployment的selector标签一致的。 kubectl get svc front-end-svc -o wide kubectl get deployment front-end -o wide 5、创建 Ingress 问题 1 2 3 4 5 6 7 8 9 10 11 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 如下创建一个新的nginx Ingress 资源： 名称: ping Namespace: ing-internal 使用服务端口 5678在路径 /hello 上公开服务 hello 可以使用以下命令检查服务 hello的可用性，该命令应返回 hello： curl -kL \u0026lt;INTERNAL_IP\u0026gt;/hello 解答 ‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 vim ingress.yaml #注意 :set paste，防止yaml文件空格错序。 apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller name: nginx-example annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; spec: controller: k8s.io/ingress-nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ping namespace: front-end annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx-example #这里调用上面新建的ingressClassName 为nginx-example rules: - http: paths: - path: /hello pathType: Prefix backend: service: name: hello port: number: 80 ‍\n最后curl检查\n通过get ingress 查看ingress 的内外IP，然后通过提供的 curl 测试 ingress 是否正确。\n‍\n另一种方法\n1 kubectl create ingress ping --rule=/hello=hello:5678 --class=nginx --annotation=nginx.ingress.kubernetes.io/rewrite-target=/ -n ing-internal ‍\n6、扩容 deployment副本数量 问题 1 2 3 4 5 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 将 deployment presentation 扩展至 4个 pods kubectl scale deployment -h\n解答 1 2 3 4 5 6 先检查一下现有的pod数量（可不检查） kubectl get deployments presentation -o wide kubectl get pod -l app=presentation 扩展成4个 kubectl scale deployment presentation --replicas=4 7、调度 pod 到指定节点 问题 1 2 3 4 5 6 7 8 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 按如下要求调度一个 pod： 名称：nginx-kusc00401 Image：nginx Node selector：disk=ssd 考点：nodeSelect属性的使用\n解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 开始操作 先检查一个是否有这个pod，应该是没有创建的，所以需要创建 kubectl get pod -A|grep nginx-kusc00401 # 确保node有这个labels，考试时，检查一下就行，应该已经提前设置好了labels。 kubectl get nodes --show-labels|grep \u0026#39;disk=ssd\u0026#39; node02 添加标签 kubectl label nodes node02 disk=ssd cat pod_nginx_ssd.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: env: app spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disk: ssd kubectl get pod nginx -o wide 8、查看可用节点数量 问题 1 2 3 4 5 [candidate@node-1] $ kubectl config use-context k8s Task 检查有多少 nodes 已准备就绪（不包括被打上 Taint：NoSchedule 的节点）， 并将数量写入 /opt/KUSC00402/kusc00402.txt 考点：检查节点角色标签，状态属性，污点属性的使用\nkubectl -h\n解答 1 2 3 4 5 6 7 # grep的-i 是忽略大小写，grep -v是排除在外，grep -c是统计查出来的条数。 kubectl describe nodes | grep -i Taints | grep -vc NoSchedule echo \u0026#34;查出来的数字\u0026#34; \u0026gt; /opt/KUSC00402/kusc00402.txt 其实你直接使用如下命令，就能一眼看出来，几个没有的。 kubectl describe nodes | grep -i Taints 9、创建多容器的 pod 问题 1 2 3 4 5 6 7 8 9 10 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 按如下要求调度一个Pod： 名称：kucc8 app containers: 2 container 名称/images： ⚫ nginx ⚫ consul 解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # kubectl config use-context k8s 可以参考上一题“调度pod到指定节点”的yaml配置文件。 开始操作 vim pod-kucc.yaml #注意 :set paste，防止yaml文件空格错序。 apiVersion: v1 kind: Pod metadata: name: nginx-consul labels: env: app spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent - name: consul image: consul imagePullPolicy: IfNotPresent kubectl get pod nginx-consul 10、创建 PV 问题 1 2 3 Task 创建名为 app-config 的 persistent volume，容量为 1Gi，访问模式为 ReadWriteMany。 volume 类型为 hostPath，位于 /srv/app-config 考点：hostPath类型的pv\n解答 https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 直接从官方复制合适的案例，修改参数，然后设置 hostPath 为 /srv/app-config 即可。 开始操作 vim pv.yaml #注意 :set paste，防止yaml文件空格错序。 [root@bt cka]# cat pv_config_app.yaml apiVersion: v1 kind: PersistentVolume metadata: name: app-config spec: capacity: storage: 1Gi accessModes: - ReadWriteMany hostPath: path: \u0026#34;/srv/app-config\u0026#34; kubectl get pv # RWX是ReadWriteMany，RWO是ReadWriteOnce。 11、创建 PVC 问题 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Task 创建一个新的PersistentVolumeClaim： 名称: pv-volume Class: csi-hostpath-sc 容量: 10Mi 创建一个新的Pod，来将PersistentVolumeClaim作为volume进行挂载： 名称：web-server Image：nginx:1.16 挂载路径：/usr/share/nginx/html 配置新的Pod，以对volume具有ReadWriteOnce权限。 最后，使用kubectl edit或kubectl patch将 PersistentVolumeClaim的容量扩展为70Mi，并记录此更改。 考点：pvc的创建class属性的使用，–record记录变更\n解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 根据官方文档复制一个PVC配置，修改参数，不确定的地方就是用 kubectl 的 explain 帮助。 开始操作 vim pvc.yaml #注意 :set paste，防止yaml文件空格错序。 [root@bt cka]# cat pvc_storgae.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-volume spec: storageClassName: nfs-client accessModes: - ReadWriteOnce resources: requests: storage: 10Mi 创建PVC kubectl apply -f pvc.yaml 检查 kubectl get pvc vim pvc-pod.yaml #注意 :set paste，防止yaml文件空格错序 apiVersion: v1 kind: Pod metadata: name: web-server spec: volumes: - name: task-pv-storage #绿色的两个name要一样。 persistentVolumeClaim: claimName: pv-volume #这个要使用上面创建的pvc名字 containers: - name: nginx image: nginx:1.16 volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: task-pv-storage #绿色的两个name要一样。 创建 kubectl apply -f pvc-pod.yaml 检查 kubectl get pod web-server 改大小，并记录过程。 # 将storage: 10Mi改为storage: 70Mi （模拟环境里会报错，下面有解释。） # 注意是修改上面的spec:里面的storage: kubectl edit pvc pv-volume --record #模拟环境是nfs存储，操作时，会有报错忽略即可。考试时用的动态存储，不会报错的。 模拟环境使用的是nfs做后端存储，是不支持动态扩容pvc的（:wq保存退出时，会报错）。 所以最后一步修改为70Mi，只是操作一下即可。换成ceph做后端存储，可以，但是集群资源太少，无法做ceph。 ‍\n12、查看 pod 日志 问题 1 2 3 4 5 6 [candidate@node-1] $ kubectl config use-context k8s Task 监控 pod foo 的日志并： 提取与错误 RLIMIT_NOFILE相对应的日志行 将这些日志行写入 /opt/KUTR00101/foo 考点：kubectl logs命令\n解答 1 2 3 4 kubectl logs foo | grep \u0026#34;RLIMIT_NOFILE\u0026#34; \u0026gt; /opt/KUTR00101/foo 检查 cat /opt/KUTR00101/foo 13、使用 sidecar 代理容器日志 问题 1 2 3 4 5 6 7 8 9 10 11 12 [candidate@node-1] $ kubectl config use-context k8s Context 将一个现有的 Pod 集成到 Kubernetes 的内置日志记录体系结构中（例如 kubectl logs）。 添加 streaming sidecar 容器是实现此要求的一种好方法。 Task 使用busybox Image来将名为sidecar的sidecar容器添加到现有的Pod 11-factor-app中。 新的sidecar容器必须运行以下命令： /bin/sh -c tail -n+1 -f /var/log/11-factor-app.log 使用挂载在/var/log的Volume，使日志文件11-factor-app.log 可用于sidecar 容器。 除了添加所需要的volume mount以外，请勿更改现有容器的规格。 考题翻译成白话，就是：\n添加一个名为sidecar 的边车容器(使用busybox 镜像)，加到已有的 pod 11-factor-app 中。\n确保 sidecar 容器能够输出 /var/log/11-factor-app.log 的信息。\n使用 volume 挂载 /var/log 目录，确保 sidecar 能访问 11-factor-app.log 文件\n考点：pod两个容器共享存储卷\nhttps://kubernetes.io/zh-cn/docs/concepts/cluster-administration/logging/\n解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 通过 kubectl get pod -o yaml 的方法备份原始 pod 信息，删除旧的pod 11-factor-app copy 一份新 yaml 文件，添加 一个名称为 sidecar 的容器 新建 emptyDir 的卷，确保两个容器都挂载了 /var/log 目录 新建含有 sidecar 的 pod，并通过 kubectl logs 验证 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: leagcy-app image: busybox args: - /bin/sh - -c - \u0026gt; i=0; while true; do echo \u0026#34;$i: $(date)\u0026#34; \u0026gt;\u0026gt; /var/log/legacy-app.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: sidecar image: busybox args: [/bin/sh, -c, \u0026#39;tail -n+1 -F /var/log/legacy-app.log\u0026#39;] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} 验证： kubectl exec counter-log -c sidecar -- tail -f /var/log/legacy-app.log 14、升级集群 问题 1 2 3 4 5 6 7 8 9 10 11 12 Task 现有的Kubernetes 集群正在运行版本1.25.1。仅将master节点上的所有 Kubernetes控制平面和节点组件升级到版本1.25.2。 确保在升级之前 drain master节点，并在升级后 uncordon master节点。 可以使用以下命令，通过ssh连接到master节点： ssh master01 可以使用以下命令，在该master节点上获取更高权限： sudo -i 另外，在主节点上升级kubelet和kubectl。 请不要升级工作节点，etcd，container 管理器，CNI插件， DNS服务或任何其他插件。 （注意，考试敲命令时，注意要升级的版本，根据题目要求输入具体的升级版本！！！）\n考点：如何离线主机，并升级控制面板和升级节点\n解答 ‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 1、切换环境 kubectl config use-context mk8s 2、配置 #升级kueadm kubectl cordon master01 kubectl drain mk8s-master-0 --ignore=daemonsets 连接master机器 提权 ssh mk8s-master-0 sudo -i 升级kubeadm apt-get update apt-cache show kubeadm|grep 1.25.2 apt install kubeadm=1.25.2 -y kubeadm upgrade plan #这里可以先查下的：apt-cache show|grep kubeadm #kubeadm upgrade install v1.20.1 #。。。淦，这个写错了。。。。。。。是apply，并且要加上--ectd-ugrade=false。。。。。。题目要求不升级 etcd； 注意下这2个版本号写法的区别。。。。 kubeadm upgrade apply v1.20.1 --etcd-upgrade=false #升级kubelt 和 kubectl apt install kubelet=1.20.1-00 kubectl=1.20.1-00 -y systemctl restart kubelet #这里要重启下kubelt的，切记。。。 exit exit kubectl uncordon mk8s-master-0 3、验证 kubectl get node -owide kubectl --version kubelet --version 15、备份还原 etcd 问题 1 2 3 4 5 6 7 8 9 10 11 12 13 此项目无需更改配置环境。但是，在执行此项目之前，请确保您已返回初始节点。 [candidate@master01] $ exit #注意，这个之前是在master01上，所以要exit退到node01，如果已经是node01了，就不要再exit了。 Task 首先，为运行在https://11.0.1.111:2379 上的现有 etcd 实例创建快照并将快照保存到 /var/lib/backup/etcd-snapshot.db （注意，真实考试中，这里写的是https://127.0.0.1:2379） 为给定实例创建快照预计能在几秒钟内完成。 如果该操作似乎挂起，则命令可能有问题。用 CTRL + C 来取消操作，然后重试。 然后还原位于/data/backup/etcd-snapshot-previous.db的现有先前快照。 提供了以下TLS证书和密钥，以通过etcdctl连接到服务器。 CA 证书: /opt/KUIN00601/ca.crt 客户端证书: /opt/KUIN00601/etcd-client.crt 客户端密钥: /opt/KUIN00601/etcd-client.key 考点：etcd的备份和还原命令\n解答 1 2 3 4 5 6 7 8 9 # 备份 ETCDCTL_API=3 etcdctl snapshot save /data/backup/etcd-snapshot.db --endpoints=https://127.0.0.1:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key # 恢复 systemctl stop etcd systemctl cat etcd # 确认下数据目录（--data-dir值） mv /var/lib/etcd /var/lib/etcd.bak ETCDCTL_API=3 etcdctl snapshot restore /data/backup/etcd-snapshot-previous.db --data-dir=/var/lib/etcd chown -R etcd:etcd /var/lib/etcd systemctl start etcd ‍\n‍\n16、排查集群中故障节点 问题 1 2 3 4 5 6 7 名为node02的Kubernetes worker node处于NotReady状态。 调查发生这种情况的原因，并采取相应的措施将node恢复为Ready状态，确保所做的任何更改永久生效。 可以使用以下命令，通过ssh连接到node02节点： ssh node02 可以使用以下命令，在该节点上获取更高权限： sudo -i 解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 过 get nodes 查看异常节点，登录节点查看 kubelet 等组件的 status 并判断原因。 真实考试时，这个异常节点的kubelet服务没有启动导致的，就这么简单。 执行初始化这道题的脚本a.sh，模拟node02异常。 （考试时，不需要执行的！考试时切到这道题的集群后，那个node就是异常的。） 1、切换环境 kubectl config use-context wk8s #考试时切到这道题的集群后，那个 node 就是异常的。 真实考试时，这个异常节点的 kubelet 服务没有启动导致的，就这么简单。 2、配置 kubectl get node #查看Not Ready的node节点 ssh wk8s-node-0 sudo -i systemctl status kubelet systemctl start kubelet systemctl enable kubelet exit #退出root用户 exit #退出故障节点 3、验证 kubectl get node #jounarlctl -u kubelet 查看kubelet日志 17、节点维护 问题 1 2 Task 将名为 node02 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。 考点：cordon和drain 命令的使用\n解答 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1、切换环境 kubectl config use-context ek8s 2、配置 kubectl cordon ek8s-node-1 #设置次节点为不可调度s kubectl drain ek8s-node-1 --ignore-daemonsets #设置次节点为不可调度，并且排空次节点 #如果上面命令报错就加上一个 --delete-local-data --force kubectl drain ek8s-node-1 --ignore-daemonsets --delete-local-data --force kubectl drain ek8s-node-1 --ignore-daemonsets --delete-emptydir-data --force 3、验证 kubectl get node ","date":"2023-01-31T18:10:28Z","image":"https://www.ownit.top/title_pic/63.jpg","permalink":"https://www.ownit.top/p/202301311810/","title":"Kubernetes考试题（CKA）"},{"content":"每周都有免费游戏 - Epic Games 近期看到Epic在送游戏，目前每周都会有活动白嫖。\n身为白嫖党，肯定要操作一下。\n游戏列表：Epic Games Store 每周免费游戏（331） | indienova GameDB 游戏库\n大致思路：\n1、根据网站，获取可 “ 白嫖 ”的游戏\n2、处理相关信息，组成文本\n3、发送到微信上，让我们知道。\n1、查询网站 下面网页，会发布最新免费，可白嫖的游戏。我们爬取这些信息，进行判断\n游戏列表：Epic Games Store 每周免费游戏（331） | indienova GameDB 游戏库\n2、代码编写 1.我们爬取网页的数据\n2.获取网页中所有游戏的信息\n3.判断游戏信息是最新编辑的\n4.汇总信息进行发送到微信\n相关实例代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 # -*- coding: utf-8 -*- # @Time : 2022/12/29 16:33 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : epic_all.py # @Software: PyCharm import json import re import time import requests from bs4 import BeautifulSoup def get_url_info(): url = \u0026#39;https://indienova.com/gamedb/list/121/p/1\u0026#39; headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Safari/537.36 Edg/90.0.818.41\u0026#39;} res = requests.get(url, headers=headers).text return res def check_epci_info(game_info_list): list_content = [] for i in game_info_list: game_time = i[\u0026#39;game_time\u0026#39;] if \u0026#39;小时前\u0026#39; in game_time[0]: content = f\u0026#34;中文名称：{i[\u0026#39;game_zh\u0026#39;]} \u0026lt;br/\u0026gt;\u0026#34; \\ f\u0026#34;英文名称：{i[\u0026#39;game_en\u0026#39;]} \u0026lt;br/\u0026gt;\u0026#34; \\ f\u0026#34;领取时间：{i[\u0026#39;game_start\u0026#39;]} \u0026lt;br/\u0026gt;\u0026#34; \\ f\u0026#34;发布时间：{i[\u0026#39;game_time\u0026#39;]}\u0026lt;br/\u0026gt;\u0026lt;br/\u0026gt;\u0026#34; list_content.append(content) return list_content def parse_web_page(res): soup = BeautifulSoup(res, \u0026#34;html.parser\u0026#34;) res.encode(\u0026#39;UTF-8\u0026#39;).decode(\u0026#39;UTF-8\u0026#39;) div_class = soup.find(name=\u0026#39;div\u0026#39;, attrs={\u0026#34;id\u0026#34;: \u0026#34;portfolioList\u0026#34;}) # print(div_class[0]) game_name = div_class.find_all(name=\u0026#39;div\u0026#39;, attrs={\u0026#34;class\u0026#34;: \u0026#34;col-xs-12 col-sm-6 col-md-4 user-game-list-item\u0026#34;}) list_game = str(game_name).split(\u0026#39;\u0026lt;div class=\u0026#34;col-xs-12 col-sm-6 col-md-4 user-game-list-item\u0026#34;\u0026gt;\u0026#39;) game_info_list = [] for i in list_game[1:]: dict_info = {} # print(\u0026#39;----------------------------------------------------------------------------------\u0026#39;) game = BeautifulSoup(i, \u0026#34;html.parser\u0026#34;) game_all_info = game.find(name=\u0026#39;h4\u0026#39;) game_name_zh = game_all_info.find_all(name=\u0026#39;a\u0026#39;) game_name_en = game_all_info.find_all(name=\u0026#39;small\u0026#39;) game_name_zh = re.findall(r\u0026#39;\u0026gt;(.+?)\u0026lt;\u0026#39;, str(game_name_zh)) game_name_en = re.findall(r\u0026#39;\u0026gt;(.+?)\u0026lt;\u0026#39;, str(game_name_en)) # print(game_name_zh, game_name_en) game_start_end = game.find(name=\u0026#39;p\u0026#39;, attrs={\u0026#34;class\u0026#34;: \u0026#34;intro\u0026#34;}) game_start_end_new = game_start_end.find_all(name=\u0026#39;span\u0026#39;) game_edit_time = game.find(name=\u0026#39;p\u0026#39;, attrs={\u0026#34;class\u0026#34;: \u0026#34;text-date\u0026#34;}) game_edit_time_new = game_edit_time.find_all(name=\u0026#39;small\u0026#39;) game_edit_time_new = str(game_edit_time_new).replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#34;\\n\u0026#34;, \u0026#34; \u0026#34;) game_start_end_new = re.findall(r\u0026#39;\u0026gt;(.+?)\u0026lt;\u0026#39;, str(game_start_end_new)) game_edit_time_new = re.findall(r\u0026#39;\u0026gt;(.+?)\u0026lt;\u0026#39;, str(game_edit_time_new)) dict_info[\u0026#34;game_zh\u0026#34;] = game_name_zh dict_info[\u0026#34;game_en\u0026#34;] = game_name_en dict_info[\u0026#34;game_start\u0026#34;] = game_start_end_new dict_info[\u0026#34;game_time\u0026#34;] = game_edit_time_new game_info_list.append(dict_info) # print(game_start_end_new,game_edit_time_new) return game_info_list def send_to_epic_message(list_content): content = \u0026#39;\u0026#39;.join(list_content) + \u0026#39;\\nhttps://indienova.com/gamedb/list/121/p/1\u0026#39; token = \u0026#39;tokenxxxxxxxxxxxxxxxxxxxx\u0026#39; day_time = time.strftime(\u0026#39;%Y-%m-%d\u0026#39;, time.localtime(time.time())) title = f\u0026#39;Epic免费游戏-{day_time}\u0026#39; # 改成你要的标题内容 error_time = time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, time.localtime(time.time())) # 获取格式化的时间 url = \u0026#39;http://www.pushplus.plus/send\u0026#39; data = { \u0026#34;token\u0026#34;: token, # 密钥 \u0026#34;title\u0026#34;: title, # 标题 \u0026#34;content\u0026#34;: content, # 发送的信息内容，这里我们是json数组 \u0026#34;template\u0026#34;: \u0026#34;markdown\u0026#34; # 数据类型为json } body = json.dumps(data).encode(encoding=\u0026#39;utf-8\u0026#39;) headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} request_result = requests.post(url, data=body, headers=headers) # print(request_result) # \u0026lt;Response [200]\u0026gt; if __name__ == \u0026#39;__main__\u0026#39;: res = get_url_info() game_info_list = parse_web_page(res) list_content = check_epci_info(game_info_list) send_to_epic_message(list_content) 3、发送平台（pushplus） 微信公众号关注\npushplus(推送加)-微信消息推送平台\n获取token进行配置即可。\n4、定时任务 我们要把脚本部署到Linux操作环境 上。\n首先记得安装依赖。pip install 模块\n定时任务，每天10点运行一次。\n1 0 10 * * * /usr/bin/python3 /opt/epic_send.py 5、增加 喜加一 的资讯通知 网站为steam free game，steam free promotion - Steam Stats\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # -*- coding: utf-8 -*- # @Time : 2022/12/29 15:51 # @Author : 南宫乘风 # @Email : 1794748404@qq.com # @File : epic.py # @Software: PyCharm import json import time import requests from bs4 import BeautifulSoup def get_free(): url = \u0026#39;https://steamstats.cn/xi\u0026#39; headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Safari/537.36 Edg/90.0.818.41\u0026#39;} r = requests.get(url, headers=headers) r.raise_for_status() r.encoding = r.apparent_encoding soup = BeautifulSoup(r.text, \u0026#34;html.parser\u0026#34;) text = \u0026#34;今日喜加一 \u0026lt;br\u0026gt;\u0026#34; + \u0026#39;https://steamstats.cn/xi\u0026#39; + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; tbody = soup.find(\u0026#39;tbody\u0026#39;) tr = tbody.find_all(\u0026#39;tr\u0026#39;) i = 1 for tr in tr: td = tr.find_all(\u0026#39;td\u0026#39;) name = td[1].string.strip().replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;) gametype = td[2].string.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;) start = td[3].string.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;) end = td[4].string.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;) time = td[5].string.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;) oringin = td[6].find(\u0026#39;span\u0026#39;).string.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\r\u0026#39;, \u0026#39;\u0026#39;) text = text + \u0026#34;序号：\u0026#34; + str( i) + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; + \u0026#34;游戏名称：\u0026#34; + name + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; + \u0026#34;DLC/game：\u0026#34; + gametype + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; + \u0026#34;开始时间：\u0026#34; + start + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; + \u0026#34;结束时间：\u0026#34; + end + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; + \u0026#34;是否永久：\u0026#34; + time + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; + \u0026#34;平台：\u0026#34; + oringin + \u0026#39;\u0026lt;br\u0026gt;\u0026#39; # print(text) i++ return text def send_to_epic_message(text_info): content = \u0026#39;\u0026#39;.join(text_info) token = \u0026#39;xxxxxxxxxxxxxxxxx\u0026#39; day_time = time.strftime(\u0026#39;%Y-%m-%d\u0026#39;, time.localtime(time.time())) title = f\u0026#39;喜加一 免费游戏-{day_time}\u0026#39; # 改成你要的标题内容 error_time = time.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;, time.localtime(time.time())) # 获取格式化的时间 url = \u0026#39;http://www.pushplus.plus/send\u0026#39; data = { \u0026#34;token\u0026#34;: token, # 密钥 \u0026#34;title\u0026#34;: title, # 标题 \u0026#34;content\u0026#34;: content, # 发送的信息内容，这里我们是json数组 \u0026#34;template\u0026#34;: \u0026#34;markdown\u0026#34; # 数据类型为json } body = json.dumps(data).encode(encoding=\u0026#39;utf-8\u0026#39;) headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} request_result = requests.post(url, data=body, headers=headers) if __name__ == \u0026#34;__main__\u0026#34;: game_info = get_free() if len(game_info) \u0026gt; 40: send_to_epic_message(get_free()) 参考文档：python获取steam/epic喜加一信息并自动发送到微信 - 知乎\n","date":"2022-12-30T14:30:16Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202212301430/","title":"Python通知Epic白嫖游戏信息"},{"content":"K8S部署Apollo配置中心 参考文档: https://github.com/apolloconfig/apollo/tree/v1.8.0\n1 2 3 [K8S部署apollo配置中心](https://www.cnblogs.com/Fengyinyong/p/14903725.html) [apollo官网文档](https://www.apolloconfig.com/#/zh/README) 1、错误问题记录 在k8s里面部署时也遇到了同样的一些问题，在此记录下：\n提示exec user process caused \u0026quot;no such file or directory\u0026quot;\n由于将Windows文件拷贝到CentOS里面导致的换行符不对，解决方案是在CentOS虚机内git clone 当前仓储 构建的docker镜像启动失败\n同上一个问题，从Windows拷贝过去的文件权限不正确导致的shell脚本执行失败，解决方案同上或者chmod +x 一个环境的admin server由于健康检查不过导致不断重启\n数据库帐号填错了导致无法登录数据库引发的问题 2、介绍 Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。\n服务端基于Spring Boot和Spring Cloud开发，打包后可以直接运行，不需要额外安装Tomcat等应用容器。\nJava客户端不依赖任何框架，能够运行于所有Java运行时环境，同时对Spring/Spring Boot环境也有较好的支持。\n.Net客户端不依赖任何框架，能够运行于所有.Net运行时环境。\n‍\n说明：最近在用K8S部署微服务，而微服务的配置文件众多，需要一个配置中心来处理配置文件。于是采用apollo来作为配置中心。本实例介绍了如何采用K8S部署高可用的apollo集群。\n‍\n3、环境配置 mysql5.7.39\n注意：mysql的版本一定要大于5.7，不然导入数据库sql文件会报错的\n因为实验方便，给root远程登录权限（切忌，生产环境，禁用root，单独数据库配置单独账号）\n数据库名称 IP 作用 远程账号 DevApolloConfigDB 192.168.102.20 DEV环境的配置 root ProdApolloConfigDB 192.168.102.20 PRO环境的配置 root ApolloPortalDB 192.168.102.20 WEB界面管理 root ‍\n安装Apollo版本：1.8.0（注意：sql文件和jar版本一定要对应）\n（目前官方给4套环境部署安装，我这边采用PRO和DEV环境）\n已经构建成功的Kubernetes集群，\n1 2 3 4 5 [root@bt nginx]# kubectl get node NAME STATUS ROLES AGE VERSION master01 Ready control-plane,master 2d16h v1.23.0 node01 Ready \u0026lt;none\u0026gt; 2d16h v1.23.0 node02 Ready \u0026lt;none\u0026gt; 2d16h v1.23.0 4、Apollo安装 ‍\n1、克隆Apollo代码 官方地址：https://github.com/apolloconfig/apollo\n在Centos上克隆（因为字符原因，所以要在Linux克隆，千万不要win上下载上传，容易出问题）\n1 2 3 4 5 6 7 8 9 10 git clone https://github.com/apolloconfig/apollo.git cd apollo #查看版本， git tag #切换分支 git checkout v1.8.0 #创建新的分支，切换 git checkout -b heian 2、初始化Mysql数据库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [root@bt db]# pwd /opt/apollo/scripts/apollo-on-kubernetes/db [root@bt db]# ls config-db-dev config-db-prod config-db-test-alpha config-db-test-beta portal-db [root@bt db]# tree . . ├── config-db-dev │?? └── apolloconfigdb.sql ├── config-db-prod │?? └── apolloconfigdb.sql ├── config-db-test-alpha │?? └── apolloconfigdb.sql ├── config-db-test-beta │?? └── apolloconfigdb.sql └── portal-db └── apolloportaldb.sql 大胆执行，数据库名称官方定义过，不会冲突 执行dev环境 cd config-db-dev/ [root@bt config-db-dev]# mysql -uroot -p \u0026lt; apolloconfigdb.sql Enter password: 执行pro环境 cd config-db-pro/ [root@bt config-db-dev]# mysql -uroot -p \u0026lt; apolloconfigdb.sql Enter password: 执行portal环境 cd portal-db/ [root@bt config-db-dev]# mysql -uroot -p \u0026lt; apolloportaldb.sql 3、下载jar版本 ‍\n官方下载地址：https://github.com/apolloconfig/apollo/releases/tag/v1.8.0\n1 2 3 4 cd /opt/apollo wget https://github.com/apolloconfig/apollo/releases/download/v1.8.0/apollo-adminservice-1.8.0-github.zip wget https://github.com/apolloconfig/apollo/releases/download/v1.8.0/apollo-configservice-1.8.0-github.zip wget https://github.com/apolloconfig/apollo/releases/download/v1.8.0/apollo-portal-1.8.0-github.zip 进行解压，需要其中的jar\n1 2 3 4 5 6 7 8 ​ 解压 apollo-portal-1.8.0-github.zip ​ 获取 apollo-portal-1.8.0.jar, 重命名为 apollo-portal.jar, 放到 scripts/apollo-on-kubernetes/apollo-portal-server ​ 解压 apollo-adminservice-1.8.0-github.zip ​ 获取 apollo-adminservice-1.8.0.jar, 重命名为 apollo-adminservice.jar, 放到 scripts/apollo-on-kubernetes/apollo-admin-server ​ 解压 apollo-configservice-1.8.0-github.zip ​ 获取 apollo-configservice-1.8.0.jar, 重命名为 apollo-configservice.jar, 放到 scripts/apollo-on-kubernetes/apollo-config-se 4、构建Docker镜像 1、采用K8S部署apollo时，需要用到多个镜像。这些镜像，需要自己构建\n2、如果麻烦，可以去官方镜像 https://hub.docker.com/u/apolloconfig\n1 2 3 4 5 6 docker pull apolloconfig/apollo-configservice:1.8.0 docker pull apolloconfig/apollo-adminservice:1.8.0 docker pull apolloconfig/apollo-portal:1.8.0 还需要一个alpine-bash-3.8 做初始化镜像操作 docker pull zgadocker/alpine-bash-3.8-image:latest 测试这个不行，就按照下方自己打包 alpine-bash-3.8-image 1 2 3 [root@bt apollo-on-kubernetes]# cd alpine-bash-3.8-image [root@bt alpine-bash-3.8-image]# docker build -t harbor.ownit.top/ownit/alpine-bash:3.8 . [root@bt alpine-bash-3.8-image]# docker push harbor.ownit.top/ownit/alpine-bash:3.8 apollo-config-server 1 2 3 [root@bt apollo-on-kubernetes]# cd apollo-config-server [root@bt apollo-config-server]# docker build -t harbor.ownit.top/ownit/apollo-configservice:1.8.0 . [root@bt apollo-config-server]# docker push harbor.ownit.top/ownit/apollo-configservice:1.8.0 apollo-admin-server 1 2 3 [root@bt apollo-on-kubernetes]# cd apollo-admin-server [root@bt apollo-admin-server]# docker build -t harbor.ownit.top/ownit/apollo-adminservice:1.8.0 . [root@bt apollo-admin-server]# docker push harbor.ownit.top/ownit/apollo-adminservice:1.8.0 apollo-portal-server 1 2 3 [root@bt apollo-on-kubernetes]# cd apollo-portal-server [root@bt apollo-portal-server]# docker build -t harbor.ownit.top/ownit/apollo-portal:1.8.0 . [root@bt apollo-portal-server]# docker push harbor.ownit.top/ownit/apollo-portal:1.8.0 验证 5、配置Yaml文件 环境 文件名称 执行顺序 dev service-apollo-config-server-dev.yaml 1 dev service-apollo-admin-server-dev.yaml 2 pro service-apollo-config-server-prod.yaml 3 pro service-apollo-admin-server-prod.yaml 4 portal service-apollo-portal-server.yaml 5 1、service-apollo-config-server-dev.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 --- # configmap for apollo-config-server-dev kind: ConfigMap apiVersion: v1 metadata: namespace: sre name: configmap-apollo-config-server-dev data: application-github.properties: | spring.datasource.url = jdbc:mysql://192.168.102.20:3306/DevApolloConfigDB?characterEncoding=utf8\u0026amp;serverTimezone=Asia/Shanghai spring.datasource.username = root spring.datasource.password = 123456 eureka.service.url = http://statefulset-apollo-config-server-dev-0.service-apollo-meta-server-dev:8080/eureka/,http://statefulset-apollo-config-server-dev-1.service-apollo-meta-server-dev:8080/eureka/,http://statefulset-apollo-config-server-dev-2.service-apollo-meta-server-dev:8080/eureka/ --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-meta-server-dev labels: app: service-apollo-meta-server-dev spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: pod-apollo-config-server-dev type: ClusterIP clusterIP: None sessionAffinity: ClientIP --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-config-server-dev labels: app: service-apollo-config-server-dev spec: ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 30002 selector: app: pod-apollo-config-server-dev type: NodePort sessionAffinity: ClientIP --- kind: StatefulSet apiVersion: apps/v1 metadata: namespace: sre name: statefulset-apollo-config-server-dev labels: app: statefulset-apollo-config-server-dev spec: serviceName: service-apollo-meta-server-dev replicas: 1 selector: matchLabels: app: pod-apollo-config-server-dev updateStrategy: type: RollingUpdate template: metadata: labels: app: pod-apollo-config-server-dev spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - pod-apollo-config-server-dev topologyKey: kubernetes.io/hostname volumes: - name: volume-configmap-apollo-config-server-dev configMap: name: configmap-apollo-config-server-dev items: - key: application-github.properties path: application-github.properties containers: - image: harbor.ownit.top/ownit/apollo-configservice:1.8.0 securityContext: privileged: true imagePullPolicy: IfNotPresent name: container-apollo-config-server-dev ports: - protocol: TCP containerPort: 8080 volumeMounts: - name: volume-configmap-apollo-config-server-dev mountPath: /apollo-config-server/config/application-github.properties subPath: application-github.properties env: - name: APOLLO_CONFIG_SERVICE_NAME value: \u0026#34;service-apollo-config-server-dev.sre\u0026#34; readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 120 periodSeconds: 10 dnsPolicy: ClusterFirst restartPolicy: Always 2、service-apollo-admin-server-dev.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 --- # configmap for apollo-admin-server-dev kind: ConfigMap apiVersion: v1 metadata: namespace: sre name: configmap-apollo-admin-server-dev data: application-github.properties: | spring.datasource.url = jdbc:mysql://192.168.102.20:3306/DevApolloConfigDB?characterEncoding=utf8\u0026amp;serverTimezone=Asia/Shanghai spring.datasource.username = root spring.datasource.password = 123456 eureka.service.url = http://statefulset-apollo-config-server-dev-0.service-apollo-meta-server-dev:8080/eureka/,http://statefulset-apollo-config-server-dev-1.service-apollo-meta-server-dev:8080/eureka/,http://statefulset-apollo-config-server-dev-2.service-apollo-meta-server-dev:8080/eureka/ --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-admin-server-dev labels: app: service-apollo-admin-server-dev spec: ports: - protocol: TCP port: 8090 targetPort: 8090 selector: app: pod-apollo-admin-server-dev type: ClusterIP sessionAffinity: ClientIP --- kind: Deployment apiVersion: apps/v1 metadata: namespace: sre name: deployment-apollo-admin-server-dev labels: app: deployment-apollo-admin-server-dev spec: replicas: 1 selector: matchLabels: app: pod-apollo-admin-server-dev strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: pod-apollo-admin-server-dev spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - pod-apollo-admin-server-dev topologyKey: kubernetes.io/hostname volumes: - name: volume-configmap-apollo-admin-server-dev configMap: name: configmap-apollo-admin-server-dev items: - key: application-github.properties path: application-github.properties initContainers: - image: harbor.ownit.top/ownit/alpine-bash:3.8 name: check-service-apollo-config-server-dev command: [\u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;curl --connect-timeout 2 --max-time 5 --retry 60 --retry-delay 1 --retry-max-time 120 service-apollo-config-server-dev.sre:8080\u0026#34;] containers: - image: harbor.ownit.top/ownit/apollo-adminservice:1.8.0 securityContext: privileged: true imagePullPolicy: IfNotPresent name: container-apollo-admin-server-dev ports: - protocol: TCP containerPort: 8090 volumeMounts: - name: volume-configmap-apollo-admin-server-dev mountPath: /apollo-admin-server/config/application-github.properties subPath: application-github.properties env: - name: APOLLO_ADMIN_SERVICE_NAME value: \u0026#34;service-apollo-admin-server-dev.sre\u0026#34; readinessProbe: tcpSocket: port: 8090 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: tcpSocket: port: 8090 initialDelaySeconds: 120 periodSeconds: 10 dnsPolicy: ClusterFirst restartPolicy: Always 3、service-apollo-config-server-prod.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 --- # configmap for apollo-config-server-prod kind: ConfigMap apiVersion: v1 metadata: namespace: sre name: configmap-apollo-config-server-prod data: application-github.properties: | spring.datasource.url = jdbc:mysql://192.168.102.20:3306/ProdApolloConfigDB?characterEncoding=utf8\u0026amp;serverTimezone=Asia/Shanghai spring.datasource.username = root spring.datasource.password = 123456 eureka.service.url = http://statefulset-apollo-config-server-prod-0.service-apollo-meta-server-prod:8080/eureka/,http://statefulset-apollo-config-server-prod-1.service-apollo-meta-server-prod:8080/eureka/,http://statefulset-apollo-config-server-prod-2.service-apollo-meta-server-prod:8080/eureka/ --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-meta-server-prod labels: app: service-apollo-meta-server-prod spec: ports: - protocol: TCP port: 8080 targetPort: 8080 selector: app: pod-apollo-config-server-prod type: ClusterIP clusterIP: None sessionAffinity: ClientIP --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-config-server-prod labels: app: service-apollo-config-server-prod spec: ports: - protocol: TCP port: 8080 targetPort: 8080 nodePort: 30005 selector: app: pod-apollo-config-server-prod type: NodePort sessionAffinity: ClientIP --- kind: StatefulSet apiVersion: apps/v1 metadata: namespace: sre name: statefulset-apollo-config-server-prod labels: app: statefulset-apollo-config-server-prod spec: serviceName: service-apollo-meta-server-prod replicas: 1 selector: matchLabels: app: pod-apollo-config-server-prod updateStrategy: type: RollingUpdate template: metadata: labels: app: pod-apollo-config-server-prod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - pod-apollo-config-server-prod topologyKey: kubernetes.io/hostname volumes: - name: volume-configmap-apollo-config-server-prod configMap: name: configmap-apollo-config-server-prod items: - key: application-github.properties path: application-github.properties containers: - image: harbor.ownit.top/ownit/apollo-configservice:1.8.0 securityContext: privileged: true imagePullPolicy: Always name: container-apollo-config-server-prod ports: - protocol: TCP containerPort: 8080 volumeMounts: - name: volume-configmap-apollo-config-server-prod mountPath: /apollo-config-server/config/application-github.properties subPath: application-github.properties env: - name: APOLLO_CONFIG_SERVICE_NAME value: \u0026#34;service-apollo-config-server-prod.sre\u0026#34; readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 120 periodSeconds: 10 dnsPolicy: ClusterFirst restartPolicy: Always 3、service-apollo-admin-server-prod.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 --- # configmap for apollo-admin-server-prod kind: ConfigMap apiVersion: v1 metadata: namespace: sre name: configmap-apollo-admin-server-prod data: application-github.properties: | spring.datasource.url = jdbc:mysql://192.168.102.20:3306/ProdApolloConfigDB?characterEncoding=utf8\u0026amp;serverTimezone=Asia/Shanghai spring.datasource.username = root spring.datasource.password = 123456 eureka.service.url = http://statefulset-apollo-config-server-prod-0.service-apollo-meta-server-prod:8080/eureka/,http://statefulset-apollo-config-server-prod-1.service-apollo-meta-server-prod:8080/eureka/,http://statefulset-apollo-config-server-prod-2.service-apollo-meta-server-prod:8080/eureka/ --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-admin-server-prod labels: app: service-apollo-admin-server-prod spec: ports: - protocol: TCP port: 8090 targetPort: 8090 selector: app: pod-apollo-admin-server-prod type: ClusterIP sessionAffinity: ClientIP --- kind: Deployment apiVersion: apps/v1 metadata: namespace: sre name: deployment-apollo-admin-server-prod labels: app: deployment-apollo-admin-server-prod spec: replicas: 1 selector: matchLabels: app: pod-apollo-admin-server-prod strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: pod-apollo-admin-server-prod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - pod-apollo-admin-server-prod topologyKey: kubernetes.io/hostname volumes: - name: volume-configmap-apollo-admin-server-prod configMap: name: configmap-apollo-admin-server-prod items: - key: application-github.properties path: application-github.properties initContainers: - image: harbor.ownit.top/ownit/alpine-bash:3.8 name: check-service-apollo-config-server-prod command: [\u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;curl --connect-timeout 2 --max-time 5 --retry 50 --retry-delay 1 --retry-max-time 120 service-apollo-config-server-prod.sre:8080\u0026#34;] containers: - image: harbor.ownit.top/ownit/apollo-adminservice:1.8.0 securityContext: privileged: true imagePullPolicy: IfNotPresent name: container-apollo-admin-server-prod ports: - protocol: TCP containerPort: 8090 volumeMounts: - name: volume-configmap-apollo-admin-server-prod mountPath: /apollo-admin-server/config/application-github.properties subPath: application-github.properties env: - name: APOLLO_ADMIN_SERVICE_NAME value: \u0026#34;service-apollo-admin-server-prod.sre\u0026#34; readinessProbe: tcpSocket: port: 8090 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: tcpSocket: port: 8090 initialDelaySeconds: 120 periodSeconds: 10 dnsPolicy: ClusterFirst restartPolicy: Always 5、service-apollo-portal-server.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 --- # 为外部 mysql 服务设置 service kind: Service apiVersion: v1 metadata: namespace: sre name: service-mysql-for-portal-server labels: app: service-mysql-for-portal-server spec: ports: - protocol: TCP port: 3306 targetPort: 3306 type: ClusterIP sessionAffinity: None --- kind: Endpoints apiVersion: v1 metadata: namespace: sre name: service-mysql-for-portal-server subsets: - addresses: # 更改为你的 mysql addresses, 例如 1.1.1.1 - ip: 192.168.102.20 ports: - protocol: TCP port: 3306 --- # configmap for apollo-portal-server kind: ConfigMap apiVersion: v1 metadata: namespace: sre name: configmap-apollo-portal-server data: application-github.properties: | spring.datasource.url = jdbc:mysql://192.168.102.20:3306/ApolloPortalDB?characterEncoding=utf8 # mysql username spring.datasource.username = root # mysql password spring.datasource.password = 123456 apollo-env.properties: | dev.meta=http://service-apollo-config-server-dev.sre:8080 fat.meta=http://service-apollo-config-server-test-alpha.sre:8080 uat.meta=http://service-apollo-config-server-test-beta.sre:8080 pro.meta=http://service-apollo-config-server-prod.sre:8080 --- kind: Service apiVersion: v1 metadata: namespace: sre name: service-apollo-portal-server labels: app: service-apollo-portal-server spec: ports: - protocol: TCP port: 8070 targetPort: 8070 nodePort: 30001 selector: app: pod-apollo-portal-server type: NodePort # portal session 保持 sessionAffinity: ClientIP --- kind: Deployment apiVersion: apps/v1 metadata: namespace: sre name: deployment-apollo-portal-server labels: app: deployment-apollo-portal-server spec: # 3 个实例 replicas: 1 selector: matchLabels: app: pod-apollo-portal-server strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: pod-apollo-portal-server spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - pod-apollo-portal-server topologyKey: kubernetes.io/hostname volumes: - name: volume-configmap-apollo-portal-server configMap: name: configmap-apollo-portal-server items: - key: application-github.properties path: application-github.properties - key: apollo-env.properties path: apollo-env.properties initContainers: # 确保 admin-service 正常提供服务 - image: harbor.ownit.top/ownit/alpine-bash:3.8 name: check-service-apollo-admin-server-dev command: [\u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;curl --connect-timeout 2 --max-time 5 --retry 60 --retry-delay 1 --retry-max-time 120 service-apollo-admin-server-dev.sre:8090\u0026#34;] # - image: harbor.ownit.top/ownit/alpine-bash:3.8 # name: check-service-apollo-admin-server-alpha # command: [\u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;curl --connect-timeout 2 --max-time 5 --retry 60 --retry-delay 1 --retry-max-time 120 service-apollo-admin-server-test-alpha.sre:8090\u0026#34;] # - image: harbor.ownit.top/ownit/alpine-bash:3.8 # name: check-service-apollo-admin-server-beta # command: [\u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;curl --connect-timeout 2 --max-time 5 --retry 60 --retry-delay 1 --retry-max-time 120 service-apollo-admin-server-test-beta.sre:8090\u0026#34;] - image: harbor.ownit.top/ownit/alpine-bash:3.8 name: check-service-apollo-admin-server-prod command: [\u0026#39;bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#34;curl --connect-timeout 2 --max-time 5 --retry 60 --retry-delay 1 --retry-max-time 120 service-apollo-admin-server-prod.sre:8090\u0026#34;] containers: - image: harbor.ownit.top/ownit/apollo-portal:1.8.0 # 更改为你的 docker registry 下的 image securityContext: privileged: true imagePullPolicy: IfNotPresent name: container-apollo-portal-server ports: - protocol: TCP containerPort: 8070 volumeMounts: - name: volume-configmap-apollo-portal-server mountPath: /apollo-portal-server/config/application-github.properties subPath: application-github.properties - name: volume-configmap-apollo-portal-server mountPath: /apollo-portal-server/config/apollo-env.properties subPath: apollo-env.properties env: - name: APOLLO_PORTAL_SERVICE_NAME value: \u0026#34;service-apollo-portal-server.sre\u0026#34; readinessProbe: tcpSocket: port: 8070 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: tcpSocket: port: 8070 # 120s 内, server 未启动则重启 container initialDelaySeconds: 120 periodSeconds: 15 dnsPolicy: ClusterFirst restartPolicy: Always 6、执行Yaml文件 1 2 3 4 5 6 7 8 9 10 # create namespace kubectl create namespace sre dev环境 kubectl apply -f apollo-env-dev/service-apollo-config-server-dev.yaml --record kubectl apply -f apollo-env-dev/service-apollo-admin-server-dev.yaml --record pro环境 kubectl apply -f apollo-env-prod/service-apollo-config-server-prod.yaml --record kubectl apply -f apollo-env-prod/service-apollo-admin-server-prod.yaml --record 7、配置ingress域名 apollo-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: sre name: service-apollo-portal-server annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;0\u0026#34; nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;600\u0026#34; nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;600\u0026#34; kubernetes.io/tls-acme: \u0026#34;true\u0026#34; cert-manager.io/cluster-issuer: \u0026#34;example-issuer\u0026#34; labels: app: service-apollo-portal-server spec: rules: - host: apollo.ownit.top http: paths: - pathType: Prefix path: / backend: service: name: service-apollo-portal-server port: number: 8070 8、配置nginx代理 ‍\nnginx配置\napollo.ownit.top.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 server { listen 80; server_name apollo.ownit.top; rewrite ^/(.*)$ https://$host/$1 permanent; # IP白名单 include /opt/nginx/whitelist/corporation.conf; } server { listen 443 ssl; server_name apollo.ownit.top; # IP白名单 include /opt/nginx/whitelist/corporation.conf; #ssl on; ssl_certificate /opt/nginx/ssl/ownit.top.crt; ssl_certificate_key /opt/nginx/ssl/ownit.top.key; include ssl.conf; location / { proxy_pass http://kubernetes-cluster; #转发到k8s的ingress 80端口 include https_proxy.conf; } access_log /www/wwwlogs/apollo.ownit.top.log; error_log /www/wwwlogs/apollo.ownit.top.error.log; } [root@bt nginx]# nginx -t nginx: the configuration file /www/server/nginx/conf/nginx.conf syntax is ok nginx: configuration file /www/server/nginx/conf/nginx.conf test is successful [root@bt nginx]# nginx -s reload kubernetes-cluster.conf 转发\n1 2 3 4 5 [root@bt nginx]# cat kubernetes-cluster.conf upstream kubernetes-cluster { server 192.168.102.40 weight=5; keepalive 16; } 9、配置powerdns域名解析 10、测试验证 https://apollo.ownit.top/\n默认账号/密码：apollo / admin\n如果没有ingress，则可使用nodeport访问\nk8s节点ip+30001 端口即可访问\nhttp://k8s节点ip:30001/\n5、Apollo配置使用 1、添加超级管理员用户 1、管理员用户选择 管理员工具-\u0026gt;系统参数\n2、填入系统内置Key：superAdmin 并点击查询，在value部分添加要加入人的用户id(LDAP用户ID为准，可通过wiki后台用户列表查询)后保存即可\n2、新增部门：organizations 1 [{\u0026#34;orgId\u0026#34;:\u0026#34;kaifa\u0026#34;,\u0026#34;orgName\u0026#34;:\u0026#34;开发部门\u0026#34;},{\u0026#34;orgId\u0026#34;:\u0026#34;yunwei\u0026#34;,\u0026#34;orgName\u0026#34;:\u0026#34;运维部门\u0026#34;},{\u0026#34;orgId\u0026#34;:\u0026#34;ceshi\u0026#34;,\u0026#34;orgName\u0026#34;:\u0026#34;测试部门\u0026#34;}] 3、新增环境：apollo.portal.envs apollo.portal.envs\n6、namespace管理 私有namespace 1、添加namespace Namespace作为配置的分类，可当成一个配置文件。\n以添加rocketmq配置为例，添加\u0026quot;spring-rocketmq”Namespace配置rocketmq相关信息。\n1、添加项目私有Namespace:spring-rocketmq\n进入项目首页，点击左下脚的”添加Namespace”，共包括两项:关联公共Namespace和创建Namespace ,这里选择\u0026quot;创建Namespace\u0026quot;\n2、填写详细的信息\n有两种选择，一种是public（所有项目的）；另一种是private（当前项目的）；如下图所示\n3、提交过后会自动跳转到下面的页面（操控权限）\n4、不操作返回首页\n2、为namespace操作配置 可以按之前的方法进行操作，也可以通过下面的内容进行批量操作\n如：\n1 2 rocketmq.name-server = 127.0.0.0:9876 rocketmq.producer.group = PID_ACCOUNT 发布\n‍\n3、获取namespace里面的数据 修改代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class GetConfigTest { public static void main(String[] args) { while (true){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } // Config appConfig = ConfigService.getAppConfig(); //读取指定的namespace下的配置信息 Config config = ConfigService.getConfig(\u0026#34;spring-rocketmq\u0026#34;); //获取配置信息,第一个参数:配置的key，第二个参数:默认值 String value = config.getProperty(\u0026#34;rocketmq.producer.group\u0026#34;, null); System.out.printf(\u0026#34;现在：%s, sms.enable: %s%n\u0026#34;, new Date().toString(),value); } } } 运行结果\n公共配置 1、添加公共的namespace 在项目开发中，有一些配置可能是通用的，我们可以通过把这些通用的配置放到公共的Namespace中，这样其他项目要使用时可以直接添加需要的Namespace\n新建common-template项目 2、新建namespace\n3、添加配置信息\n2、前往其他的项目去关联第一步的namespace ‍\n3、修改公共配置 修改server.servlet.context-path为：/account-service\n4、使用代码读取配置信息 注意：如果项目不一样的话，就需要去将之前在运行里面的环境进行修改，如下\n修改代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class GetConfigTest { public static void main(String[] args) { while (true){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } // Config appConfig = ConfigService.getAppConfig(); //读取指定的namespace下的配置信息 Config config = ConfigService.getConfig(\u0026#34;TEST1.spring-boot-http\u0026#34;); //获取配置信息,第一个参数:配置的key，第二个参数:默认值 String value = config.getProperty(\u0026#34;server.servlet.context-path\u0026#34;, null); System.out.printf(\u0026#34;现在：%s, sms.enable: %s%n\u0026#34;, new Date().toString(),value); } } } ","date":"2022-12-29T10:03:56Z","image":"https://www.ownit.top/title_pic/21.jpg","permalink":"https://www.ownit.top/p/202212291003/","title":"K8S部署Apollo配置中心"},{"content":"环境 操作系统 ： CentOS7.3.1611_x64\nPython 版本 : 3.6.8\nApollo源码地址：\nGitHub - apolloconfig/apollo: Apollo is a reliable configuration management system suitable for microservice configuration management scenarios.\n访问Apollo使用这个库：\nGitHub - filamoon/pyapollo: Python client for Ctrip\u0026rsquo;s Apollo.\n不要使用pypi提供的apollo库（是一个编辑器的库）。\n其实真正需要的也就一个文件（pyapollo.py）：\npyExamples/pyapollo.py at master · mike-zhang/pyExamples · GitHub\n安装pyapollos 1 pip install pyapollos 注意注意注意：这个pyapollo库使用时会报链接超时，所以不要使用了 这里是一个别人修改后的库, github传送门，直接复制这个代码创建一个类使用就好了。\n参考文档：https://blog.csdn.net/weixin_44809381/article/details/123072829\n使用示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import time import pyapollos a = pyapollos.ApolloClient(app_id=\u0026#34;movie-project\u0026#34;, config_server_url=\u0026#34;https://config-server-dev.ownit.top\u0026#34;, cluster=\u0026#39;default\u0026#39;, timeout=10) #如果是关联空间的值，必须使用namespace ，指定空间名称 c = a.get_value(\u0026#34;ops.appDomain\u0026#34;, namespace=\u0026#34;yunwei.common-public-config\u0026#34;) print(c) #测试值 var code = \u0026#34;cb224f2d-8ec8-4d66-85cc-072fb64ea58d\u0026#34; 生产实战实例 配置环境变量：\nAPOLLO_CONFIG_URL 为 apollo的 获取值的地址：config-server-dev.ownit.top\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import os from pyapollo import ApolloClient client = ApolloClient(app_id=\u0026#34;spider-customer\u0026#34;, cluster=\u0026#34;default\u0026#34;, config_server_url=\u0026#39;http://\u0026#39; + os.environ.get(\u0026#39;APOLLO_CONFIG_URL\u0026#39;)) project_name = client.get_value(\u0026#39;project.name\u0026#39;) # 服务 server_port = int(client.get_value(\u0026#39;server.port\u0026#39;)) env = client.get_value(\u0026#39;env\u0026#39;) # redis redis_host = client.get_value(\u0026#39;redis.host\u0026#39;) redis_port = int(client.get_value(\u0026#39;redis.port\u0026#39;)) redis_db = int(client.get_value(\u0026#39;redis.db\u0026#39;)) redis_password = client.get_value(\u0026#39;redis.password\u0026#39;) # mysql mysql_host = client.get_value(\u0026#39;mysql.host\u0026#39;) mysql_port = int(client.get_value(\u0026#39;mysql.port\u0026#39;)) mysql_user = client.get_value(\u0026#39;mysql.user\u0026#39;) mysql_password = client.get_value(\u0026#39;mysql.password\u0026#39;) mysql_database = client.get_value(\u0026#39;mysql.database\u0026#39;) # 钉钉机器人 log_robot_token = client.get_value(\u0026#39;log_robot_token\u0026#39;) log_robot_secret = client.get_value(\u0026#39;log_robot_secret\u0026#39;) # oss oss_endpoint = client.get_value(\u0026#39;oss.ossEndpoint\u0026#39;) oss_accessKeyId = client.get_value(\u0026#39;oss.ossAccessKeyId\u0026#39;) oss_accessKeySecret = client.get_value(\u0026#39;oss.ossAccessKeySecret\u0026#39;) oss_bucketName = client.get_value(\u0026#39;oss.bucketName\u0026#39;) 二、调用API接口获取配置 1，通过带缓存的Http接口从Apollo读取配置 该接口会从缓存中获取配置，适合频率较高的配置拉取请求，如简单的每30秒轮询一次配置。\n由于缓存最多会有一秒的延时，所以如果需要配合配置推送通知实现实时更新配置的话，请参考通过不带缓存的Http接口从Apollo读取配置\nHttp接口说明 URL: {config_server_url}/configfiles/json/{appId}/{clusterName}/{namespaceName}?ip={clientIp}\nMethod: GET\n参数说明：\n参数名 是否必须 参数值 备注 config_server_url 是 Apollo配置服务的地址 appId 是 应用的appId clusterName 是 集群名 一般情况下传入 default 即可。 如果希望配置按集群划分，可以参考集群独立配置说明做相关配置，然后在这里填入对应的集群名。 namespaceName 是 Namespace的名字 如果没有新建过Namespace的话，传入application即可。 如果创建了Namespace，并且需要使用该Namespace的配置，则传入对应的Namespace名字。需要注意的是对于properties类型的namespace，只需要传入namespace的名字即可，如application。对于其它类型的namespace，需要传入namespace的名字加上后缀名，如datasources.json ip 否 应用部署的机器ip 这个参数是可选的，用来实现灰度发布。 如果不想传这个参数，请注意URL中从?号开始的query parameters整个都不要出现。 2，通过不带缓存的Http接口从Apollo读取配置 该接口会直接从数据库中获取配置，可以配合配置推送通知实现实时更新配置。\nHttp接口说明 URL: {config_server_url}/configs/{appId}/{clusterName}/{namespaceName}?releaseKey={releaseKey}\u0026amp;ip={clientIp}\nMethod: GET\n参数说明：\n参数名 是否必须 参数值 备注 config_server_url 是 Apollo配置服务的地址 appId 是 应用的appId clusterName 是 集群名 一般情况下传入 default 即可。 如果希望配置按集群划分，可以参考集群独立配置说明做相关配置，然后在这里填入对应的集群名。 namespaceName 是 Namespace的名字 如果没有新建过Namespace的话，传入application即可。 如果创建了Namespace，并且需要使用该Namespace的配置，则传入对应的Namespace名字。需要注意的是对于properties类型的namespace，只需要传入namespace的名字即可，如application。对于其它类型的namespace，需要传入namespace的名字加上后缀名，如datasources.json releaseKey 否 上一次的releaseKey 将上一次返回对象中的releaseKey传入即可，用来给服务端比较版本，如果版本比下来没有变化，则服务端直接返回304以节省流量和运算 ip 否 应用部署的机器ip 这个参数是可选的，用来实现灰度发布。 ","date":"2022-12-29T09:39:33Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/202212290939/","title":"Python访问Apollo获取配置"},{"content":"简介：使用阿里云的RDS数据库，开启DAS的数据库治理服务。会产生大量的审计日志。\n我们有2T的审计日志数据，保留180天，每小时收费空间：0.008元/GB/小时\n计算下来：2x1024x 24x 30 x 0.008 =11796 元\n解决：打算数据量存储30天，以前的审计日志，可以使用阿里云的API 调用，下载，归档报错。如果出现问题，可以及时定位。保留周期更长。费用更少。\n阿里云API接口：查询实例的SQL审计日志 (aliyun.com)\n目前使用这个API接口拉取，基本相关信息已经存在。\n高级一点可以采用 一下API拉取日志\n按照访问来源统计全量请求数据的API接口_数据库自治服务-阿里云帮助中心\nGetFullRequestStatResultByInstanceId - 按照SQL ID异步统计全量请求数据 (aliyun.com)\n环境准备 1、Python 3.8 的环境\n2、安装阿里云的sdk\nSDK 包名称alibabacloud_rds20140815\nSDK 版本2.1.2\nSDK 包管理平台pypi\nSDK 安装命令\n1 pip install alibabacloud_rds20140815==2.1.2 提示仓库同步可能会有延迟，如果遇到版本不存在的情况，请稍后再试或使用上一个版本\n阿里云 OpenAPI 开发者门户 (aliyun.com)\n基础代码 此代码是阿里云自动生成的\n一个执行，一个异步执行。选择其中一个即可。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # -*- coding: utf-8 -*- # This file is auto-generated, don\u0026#39;t edit it. Thanks. import sys from typing import List from alibabacloud_rds20140815.client import Client as Rds20140815Client from alibabacloud_tea_openapi import models as open_api_models from alibabacloud_rds20140815 import models as rds_20140815_models from alibabacloud_tea_util import models as util_models from alibabacloud_tea_util.client import Client as UtilClient class Sample: def __init__(self): pass @staticmethod def create_client( access_key_id: str, access_key_secret: str, ) -\u0026gt; Rds20140815Client: \u0026#34;\u0026#34;\u0026#34; 使用AK\u0026amp;SK初始化账号Client @param access_key_id: @param access_key_secret: @return: Client @throws Exception \u0026#34;\u0026#34;\u0026#34; config = open_api_models.Config( # 必填，您的 AccessKey ID, access_key_id=access_key_id, # 必填，您的 AccessKey Secret, access_key_secret=access_key_secret ) # 访问的域名 config.endpoint = f\u0026#39;rds.aliyuncs.com\u0026#39; return Rds20140815Client(config) @staticmethod def main( args: List[str], ) -\u0026gt; None: # 工程代码泄露可能会导致AccessKey泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议使用更安全的 STS 方式，更多鉴权访问方式请参见：https://help.aliyun.com/document_detail/378659.html client = Sample.create_client(\u0026#39;accessKeyId\u0026#39;, \u0026#39;accessKeySecret\u0026#39;) describe_sqllog_records_request = rds_20140815_models.DescribeSQLLogRecordsRequest( dbinstance_id=\u0026#39;xxxxxx\u0026#39;, start_time=\u0026#39;2022-11-18T00:00:00Z\u0026#39;, end_time=\u0026#39;2022-11-19T00:00:00Z\u0026#39;, page_size=100, page_number=1 ) runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 client.describe_sqllog_records_with_options(describe_sqllog_records_request, runtime) except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) @staticmethod async def main_async( args: List[str], ) -\u0026gt; None: # 工程代码泄露可能会导致AccessKey泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议使用更安全的 STS 方式，更多鉴权访问方式请参见：https://help.aliyun.com/document_detail/378659.html client = Sample.create_client(\u0026#39;accessKeyId\u0026#39;, \u0026#39;accessKeySecret\u0026#39;) describe_sqllog_records_request = rds_20140815_models.DescribeSQLLogRecordsRequest( dbinstance_id=\u0026#39;xxxxxx\u0026#39;, start_time=\u0026#39;2022-11-18T00:00:00Z\u0026#39;, end_time=\u0026#39;2022-11-19T00:00:00Z\u0026#39;, page_size=100, page_number=1 ) runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 await client.describe_sqllog_records_with_options_async(describe_sqllog_records_request, runtime) except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) if __name__ == \u0026#39;__main__\u0026#39;: Sample.main(sys.argv[1:]) 2、根据小时拉去代码 因为数据量很大，我们准备根据 **小时 ** 拉去日志。\n代码介绍：\n根据每小时拉去文件，进行按时间保存，拉去一天的量完成后 会钉钉通知\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 import datetime import json import os import sys from time import sleep import requests from alibabacloud_rds20140815.client import Client as Rds20140815Client from alibabacloud_tea_openapi import models as open_api_models from alibabacloud_rds20140815 import models as rds_20140815_models from alibabacloud_tea_util import models as util_models from alibabacloud_tea_util.client import Client as UtilClient import copy class Sample: def __init__(self): pass @staticmethod def create_client( access_key_id: str, access_key_secret: str, ) -\u0026gt; Rds20140815Client: \u0026#34;\u0026#34;\u0026#34; 使用AK\u0026amp;SK初始化账号Client @param access_key_id: @param access_key_secret: @return: Client @throws Exception \u0026#34;\u0026#34;\u0026#34; config = open_api_models.Config( # 必填，您的 AccessKey ID, access_key_id=access_key_id, # 必填，您的 AccessKey Secret, access_key_secret=access_key_secret ) # 访问的域名 config.endpoint = f\u0026#39;rds.aliyuncs.com\u0026#39; return Rds20140815Client(config) @staticmethod def main(page_number=1, startTime=None, endTime=None): # 工程代码泄露可能会导致AccessKey泄露，并威胁账号下所有资源的安全性。以下代码示例仅供参考，建议使用更安全的 STS 方式，更多鉴权访问方式请参见：https://help.aliyun.com/document_detail/378659.html client = Sample.create_client(\u0026#39;xxxxxxxxxxxxxx\u0026#39;, \u0026#39;xxxxxxxxxxxxxxxxx\u0026#39;) describe_sqllog_records_request = rds_20140815_models.DescribeSQLLogRecordsRequest( #数据库实例ID dbinstance_id=\u0026#39;rm-xxxxxxxxxxxxxxxxx\u0026#39;, end_time=endTime, start_time=startTime, page_size=100, page_number=page_number ) runtime = util_models.RuntimeOptions() try: # 复制代码运行请自行打印 API 的返回值 data = client.describe_sqllog_records_with_options(describe_sqllog_records_request, runtime) return data except Exception as error: # 如有需要，请打印 error UtilClient.assert_as_string(error.message) def msg(text): json_text = { \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;11111\u0026#34; ], \u0026#34;isAtAll\u0026#34;: False }, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: text } } print(requests.post(api_url, json.dumps(json_text), headers=headers).content) if __name__ == \u0026#39;__main__\u0026#39;: startTime = \u0026#39;2022-11-19T00:00:00Z\u0026#39; for i in range(24): endTime = (datetime.datetime.strptime(startTime, \u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) + datetime.timedelta( hours=1)).strftime(\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) rds_time = datetime.datetime.strptime(startTime, \u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) rds_time_file_log = rds_time.strftime(\u0026#34;%Y-%m-%d_%H\u0026#34;) # print(rds_time_file) rds_file = rds_time.strftime(\u0026#34;%Y-%m-%d\u0026#34;) # print(rds_time_file) folder = os.path.join(os.path.abspath(os.path.dirname(__file__)), rds_file) print(folder) log_path = os.path.exists(folder) if not log_path: os.makedirs(folder) print(startTime, endTime) rds = Sample.main(startTime=startTime, endTime=endTime) # 总条数 rds_num = rds.body.total_record_count # 页数 rds_page = rds.body.page_number # page的num数量 pag_num_max = 100 # 每页返回的数值 page_record_count = rds.body.page_record_count # 总页数 page_num_sum = int(rds_num / pag_num_max) + 1 rds_log = rds.body.items.to_map() for i in range(page_num_sum + 1): print(i + 1) num = i + 1 rds_one = Sample.main(page_number=num, startTime=startTime, endTime=endTime) page_record_count_one = rds_one.body.page_record_count if page_record_count_one != 0: rds_log_one = rds_one.body.items.to_map() # 获取rds 的真实日志数据 for v in rds_log_one[\u0026#39;SQLRecord\u0026#39;]: with open(folder + \u0026#39;/\u0026#39; + \u0026#39;rds_\u0026#39; + rds_time_file_log + \u0026#39;.log\u0026#39;, \u0026#39;a\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(f\u0026#34;{str(v)} \\n\u0026#34;) if (i + 1) % 2000 == 0: sleep(10) f.close() startTime = endTime # 参数days=1（天+1） 可以换成 minutes=1（分钟+1）、seconds=1（秒+1） token = \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; text = \u0026#34;python拉去数据\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json;charset=utf-8\u0026#39;} api_url = \u0026#34;https://oapi.dingtalk.com/robot/send?access_token=%s\u0026#34; % token msg(\u0026#39;RDS数据拉去-完成告警\u0026#39;) ","date":"2022-12-19T10:44:22Z","image":"https://www.ownit.top/title_pic/07.jpg","permalink":"https://www.ownit.top/p/202212191044/","title":"Python本地下载-实例的SQL审计日志"},{"content":"在查看gitlab CI作业时，发现意外报错\n1 2 3 4 5 6 重新初始化现存的 Git 版本库于 /home/gitlab-runner/builds/nzfEHD8s/0/devops/dig/.git/ fatal: git fetch-pack: expected shallow list fatal: The remote end hung up unexpectedly Cleaning up project directory and file based variables 00:00 ERROR: Job failed: exit status 1 报错时由于git版本引起的，查看git版本\n1 2 [root@bt dig]# git version git version 1.8.3.1 使用yum list | grep git，yum默认阿里云源里面最新的版本就是1.18.3 我们需要升级git 的版本\n1、安装源 1 yum install http://opensource.wandisco.com/centos/7/git/x86_64/wandisco-git-release-7-2.noarch.rpm 2、更新git软件 1 yum install git -y 3、检查版本测试\n1 2 [root@bt dig]# git --version git version 2.31.1 版本已经更新最2.31了，再次执行gitlab CI作业，报错已经解决了。\n","date":"2022-12-01T11:59:37Z","image":"https://www.ownit.top/title_pic/63.jpg","permalink":"https://www.ownit.top/p/202212011159/","title":"Git引起的 gitlab-runner 报错"},{"content":"目录\n1、Repositories\n1、配置路径\n2、注册快照存储库\n2、查看注册的库\n3、创建快照\n1、为全部索引创建快照\n2、为指定索引创建快照\n4、查看备份完成的列表\n5、删除快照\n6、从快照恢复\n1、恢复指定索引\n2、恢复所有索引（除.开头的系统索引）\n3、恢复所有索引（包含.开头的系统索引）\n4、将快照恢复到Indexing Service实例中。\n7、查看快照恢复信息\n1、查看快照中指定索引的恢复状态\n2、查看集群中的所有索引的恢复信息\n8、删除正在进行快照恢复的索引\n2、备份脚本\n1、Elasticsearch索引备份\n2、Elasticsearch索引清理\n快照是增量的，可以包含在多个ES版本中创建的索引。如果在一个快照中的任何索引时在不兼容的ES版本中创建的，你将不能恢复该快照。\n在升级前备份数据的时候，如果快照中的索引是在与你升级版本不兼容的ES版本中创建的，那么这些快照将不能被恢复。\n如果你的情况是需要恢复一个与你当前运行的集群版本不兼容的索引快照，你可以先恢复到最新的兼容版本中，然后在当前版本中使用 reindex-from-remote 重建索引。远程Reindexing只能在源索引source为enabled的情况下进行。获取并重建数据索引的时间可能比简单恢复快照要长的多。如果你的数据量较大，建议先用一部分数据测试远程reindex，以便了解需要花费的时间\n1、Repositories 必须先注册一个快照仓库，然后才能进行快照和恢复操作。建议为每个主版本创建一个新快照仓库。有效的仓库设置取决于仓库类型。\n如果多个集群注册同一个快照仓库，只有一个集群可以对仓库进行写访问，其他所有集群应该设置该仓库为 readonly 模式。\n跨主版本时快照格式可能会改变，所以不同版本的集群写同一个快照仓库，某个版本写的快照可能对其他版本不可见，仓库快照也存在问题。ES不支持仓库对所有集群设置为readonly，其中一个集群和不同主版本的多个集群一起工作。\n关于备份可以分为备份数据，备份集群配置，备份安全配置\n关于还原可以分为还原数据，还原安全配置\n1、配置路径 大致分为以下几步（如果您的集群启用了安全功能，则在备份数据时必须授权快照API调用）\n修改elasticsearch.yml添加快照存储位置配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 cat elasticsearch.yml node.name: node-1 discovery.seed_hosts: [\u0026#34;172.18.199.93\u0026#34;] cluster.initial_master_nodes: node-1 network.host: 0.0.0.0 path.data: /data02/elasticsearch/data path.logs: /data02/elasticsearch/logs xpack.security.enabled: true xpack.license.self_generated.type: basic xpack.security.transport.ssl.enabled: true ​ xpack.sql.enabled: false http.cors.enabled: true http.cors.allow-credentials: true http.cors.allow-origin: \u0026#34;/.*/\u0026#34; http.cors.allow-headers: WWW-Authenticate,X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization ​ # 避免发生OOM，发生OOM对集群影响很大的,揉合 request 和 fielddata 断路器保证两者组合起来不会使用超过堆内存的 70%。 indices.breaker.total.limit: 80% # 有了这个设置，最久未使用（LRU）的 fielddata 会被回收为新数据腾出空间 indices.fielddata.cache.size: 10% # # # fielddata 断路器默认设置堆的 作为 fielddata 大小的上限。 indices.breaker.fielddata.limit: 60% # # #request 断路器估算需要完成其他请求部分的结构大小，例如创建一个聚合桶，默认限制是堆内存的 40%。 indices.breaker.request.limit: 40% # # #最久未使用（LRU）的 fielddata 会被回收为新数据腾出空间 必须要添加的配置 indices.breaker.total.use_real_memory: false ####副本数设置唯一 #索引调优 # indices.memory.index_buffer_size: 20% indices.memory.min_index_buffer_size: 96mb ​ # Search pool thread_pool.search.size: 5 ​ ​ discovery.zen.fd.ping_timeout: 120s discovery.zen.fd.ping_retries: 6 discovery.zen.fd.ping_interval: 30s ​ #仓库 path.repo: [\u0026#34;/data/es_snapshot\u0026#34;] 给备份目录权限\n1 chown -R es:es /data/es_snapshot 2、注册快照存储库 快照可以存储在本地或远程存储库中。远程存储库可以驻留在 Amazon S3、HDFS、Microsoft Azure、Google Cloud Storage 和存储库插件支持的其他平台上。\n除了location 参数外，还可以通过max_snapshot_bytes_per_sec和max_restore_bytes_per_sec 来限制备份和恢复时的速度\n查看快照仓库。\n1 GET _snapshot 1 2 3 4 5 6 7 curl -XPUT \u0026#39;http://192.168.18.15:9200/_snapshot/backup（仓库名称）\u0026#39; -d \u0026#39;{ \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/data/es_snapshot\u0026#34;, \u0026#34;compress\u0026#34;: true #压缩 } }\u0026#39; 或者在Kibana console中输入如下的命令进行注册也可以\n1 2 3 4 5 6 7 PUT _snapshot/backup { \u0026#34;type\u0026#34;: \u0026#34;fs\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;location\u0026#34;: \u0026#34;/data/es_snapshot\u0026#34;, \u0026#34;compress\u0026#34;: true } 2、查看注册的库 GET _snapshot/\n1 GET _snapshot/ 3、创建快照 自建Elasticsearch中创建一个快照，用来备份您需要迁移的索引数据。创建快照时，默认会备份所有打开的索引。如果您不想备份系统索引，例如以**.kibana**、.security、.monitoring等开头的索引，可在创建快照时指定需要备份的索引\n注意： 建议您不要备份系统索引，因为系统索引会占用较大空间。\n1、为全部索引创建快照 1 PUT _snapshot/my_backup/snapshot_1 以上命令会为所有打开的索引创建名称为snapshot_1的快照，并保存到my_backup仓库中。该命令会立刻返回，并在后台执行备份任务。如果您希望任务执行完成后再返回，可通过添加wait_for_completion实现。该参数会阻塞调用直到备份完成，如果是大型快照，需要很长时间才能返回。\n1 PUT _snapshot/my_backup/snapshot_1?wait_for_completion=true 说明\n一个仓库可以包含多个快照，每个快照中可以包含所有、部分或单个索引的备份数据。\n第一次创建快照时，系统会备份所有的数据，后续所有的快照仅备份已存快照和新快照之间的增量数据。随着快照的不断进行，备份也在增量的添加和删除。这意味着后续备份会相当快速，因为它们只传输很小的数据量。\n2、为指定索引创建快照 系统默认会备份所有打开的索引。如果您在使用Kibana，并且考虑到磁盘空间大小因素，不需要把所有诊断相关的.kibana索引都备份起来，那么可以在创建快照时，指定需要备份的索引。\n1 2 3 4 PUT _snapshot/my_backup/snapshot_2 { \u0026#34;indices\u0026#34;: \u0026#34;index_1,index_2\u0026#34; } 4、查看备份完成的列表 1 2 3 4 5 6 7 8 查看所有快照信息 GET _snapshot/my_backup/_all 根据快照名查看指定快照的信息 GET _snapshot/my_backup/snapshot_3 使用_status API查看指定快照的信息 GET _snapshot/my_backup/snapshot_3/_status 5、删除快照 删除指定的快照。如果该快照正在进行，执行以下命令，系统会中断快照进程并删除仓库中创建到一半的快照。\n1 DELETE _snapshot/my_backup/snapshot_3 注意 删除快照请使用DELETE API，而不能使用其他机制删除（例如手动删除可能会造成备份严重损坏）。因为快照是增量的，很多快照可能依赖于之前的备份数据。DELETE API能够过滤出还在被其他快照使用的数据，只删除不再被使用的备份数据。\n6、从快照恢复 注意\n建议不要恢复.开头的系统索引，此操作可能会导致Kibana访问失败。\n如果集群中存在与待恢复索引同名的索引，需要提前删除或者关闭该同名索引后再恢复，否则恢复失败。\n如果需要跨地域恢复集群快照，需要先将原地域OSS中的快照数据迁移到目标地域的OSS中，再恢复到目标地域的Elasticsearch集群中。OSS间迁移的具体操作，请参见阿里云OSS之间迁移教程。\n1、恢复指定索引 如果您需要在不替换现有数据的前提下，恢复旧版本的数据来验证内容，或者进行其他处理，可恢复指定的索引，并重命名该索引。\n1 2 3 4 5 6 POST /_snapshot/my_backup/snapshot_1/_restore { \u0026#34;indices\u0026#34;: \u0026#34;index_1\u0026#34;, \u0026#34;rename_pattern\u0026#34;: \u0026#34;index_(.+)\u0026#34;, \u0026#34;rename_replacement\u0026#34;: \u0026#34;restored_index_$1\u0026#34; } 参数 说明 indices 只恢复index_1索引，忽略快照中的其他索引。 rename_pattern 查找正在恢复的索引，该索引名称需要与提供的模板匹配。 rename_replacement 重命名查找到的索引。 2、恢复所有索引（除.开头的系统索引） 1 2 POST _snapshot/my_backup/snapshot_1/_restore {\u0026#34;indices\u0026#34;:\u0026#34;*,-.monitoring*,-.security*,-.kibana*\u0026#34;,\u0026#34;ignore_unavailable\u0026#34;:\u0026#34;true\u0026#34;} 3、恢复所有索引（包含.开头的系统索引） 1 POST _snapshot/my_backup/snapshot_1/_restore 假设snapshot_1中包含5个索引，那么这5个索引都会被恢复到集群中。\n_restore API会立刻返回，恢复进程会在后台进行。如果您希望调用阻塞直到恢复完成，可以添加wait_for_completion参数。 1 POST _snapshot/my_backup/snapshot_1/_restore?wait_for_completion=true 4、将快照恢复到Indexing Service实例中。 例如将my_backup仓库中，snapshot_1快照中的index_1索引数据恢复到Indexing Service实例中，示例如下。\n** 说明** 实际使用时，您需要将对应信息替换为您实际的信息。\n1 2 3 4 5 6 7 POST /_snapshot/my_backup/snapshot_1/_restore { \u0026#34;indices\u0026#34;: \u0026#34;index_1\u0026#34;, \u0026#34;ignore_index_settings\u0026#34;: [ \u0026#34;index.apack.cube.following_index\u0026#34; ] } 7、查看快照恢复信息 您可以通过_recovery API来监控快照恢复的状态、进度等信息。\n1、查看快照中指定索引的恢复状态 1 GET restored_index_3/_recovery 2、查看集群中的所有索引的恢复信息 说明 获取的恢复信息可能包含跟您的恢复进程无关的其他分片的恢复信息。\n1 GET /_recovery/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 { \u0026#34;restored_index_3\u0026#34; : { \u0026#34;shards\u0026#34; : [ { \u0026#34;id\u0026#34; : 0, \u0026#34;type\u0026#34; : \u0026#34;snapshot\u0026#34;, \u0026#34;stage\u0026#34; : \u0026#34;index\u0026#34;, \u0026#34;primary\u0026#34; : true, \u0026#34;start_time\u0026#34; : \u0026#34;2014-02-24T12:15:59.716\u0026#34;, \u0026#34;stop_time\u0026#34; : 0, \u0026#34;total_time_in_millis\u0026#34; : 175576, \u0026#34;source\u0026#34; : { \u0026#34;repository\u0026#34; : \u0026#34;my_backup\u0026#34;, \u0026#34;snapshot\u0026#34; : \u0026#34;snapshot_3\u0026#34;, \u0026#34;index\u0026#34; : \u0026#34;restored_index_3\u0026#34; }, \u0026#34;target\u0026#34; : { \u0026#34;id\u0026#34; : \u0026#34;ryqJ5lO5S4-lSFbGnt****\u0026#34;, \u0026#34;hostname\u0026#34; : \u0026#34;my.fqdn\u0026#34;, \u0026#34;ip\u0026#34; : \u0026#34;10.0.**.**\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;my_es_node\u0026#34; }, \u0026#34;index\u0026#34; : { \u0026#34;files\u0026#34; : { \u0026#34;total\u0026#34; : 73, \u0026#34;reused\u0026#34; : 0, \u0026#34;recovered\u0026#34; : 69, \u0026#34;percent\u0026#34; : \u0026#34;94.5%\u0026#34; }, \u0026#34;bytes\u0026#34; : { \u0026#34;total\u0026#34; : 79063092, \u0026#34;reused\u0026#34; : 0, \u0026#34;recovered\u0026#34; : 68891939, \u0026#34;percent\u0026#34; : \u0026#34;87.1%\u0026#34; }, \u0026#34;total_time_in_millis\u0026#34; : 0 }, \u0026#34;translog\u0026#34; : { \u0026#34;recovered\u0026#34; : 0, \u0026#34;total_time_in_millis\u0026#34; : 0 }, \u0026#34;start\u0026#34; : { \u0026#34;check_index_time\u0026#34; : 0, \u0026#34;total_time_in_millis\u0026#34; : 0 } } ] } } 输出结果会展示所有恢复中的索引，并列出这些索引中的所有分片。同时每个分片中会展示启动和停止时间、持续时间、恢复百分比、传输字节数等统计值。部分参数说明如下。\n参数 说明 type 恢复的类型。snapshot表示这个分片是在从一个快照恢复。 source 待恢复的快照和仓库。 percent 恢复的进度。**94.5%**表示对应分片已经恢复了94.5%的数据。 8、删除正在进行快照恢复的索引 通过DELETE命令删除正在恢复的索引，取消恢复操作。\n​\n1 DELETE /restored_index_3 2、备份脚本 ​\n1、Elasticsearch索引备份 1 2 #Elasticsearch索引备份 1 3 * * * /bin/bash -x /opt/shell-code/es_backup.sh \u0026gt; /opt/shell-code/logs/es_backup.log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 #!/bin/bash #/********************************************************** # * Author : 南宫乘风 # * Email : 1794748404@qq.com # * Last modified : 2022-11-03 23:43 # * Filename : 01_ping_check_isalive.sh # * Description : # * *******************************************************/ #/bin/bash ################################### #日志级别 debug-1, info-2, warn-3, error-4, always-5 LOG_LEVEL=3 user=$(whoami) #日志文件 LOG_FILE=./Elasticsearch.log #调试日志 function log_debug() { content=\u0026#34;[DEBUG] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; [ $LOG_LEVEL -le 1 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE } #信息日志 function log_info() { content=\u0026#34;[INFO] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; [ $LOG_LEVEL -le 2 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE } #警告日志 function log_warn() { content=\u0026#34;[WARN] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; [ $LOG_LEVEL -le 3 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE } #错误日志 function log_err() { content=\u0026#34;[ERROR] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; [ $LOG_LEVEL -le 4 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE } #一直都会打印的日志 function log_always() { content=\u0026#34;[ALWAYS] $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) $0 $user msg : $@\u0026#34; [ $LOG_LEVEL -le 5 ] \u0026amp;\u0026amp; echo $content \u0026gt;\u0026gt;$LOG_FILE } # 执行快照备份 #times=\u0026#34;2022.09.08 2022.09.09 2022.09.10 2022.09.11 2022.09.12 2022.09.13 2022.09.14 2022.09.15 2022.09.16 2022.09.17 2022.09.18 2022.09.19 2022.09.20 2022.09.21 2022.09.22 2022.09.23 2022.09.24 2022.09.25 2022.09.26 2022.09.27 2022.09.28 2022.09.29 2022.09.30 2022.10.01 2022.10.02 2022.10.03 2022.10.04 2022.10.05 2022.10.06 2022.10.07 2022.10.08 2022.10.09 2022.10.10 2022.10.11 2022.10.12 2022.10.13 2022.10.14 2022.10.15 2022.10.16 2022.10.17 2022.10.18 2022.10.19 2022.10.20 2022.10.21 2022.10.22 2022.10.23 2022.10.24 2022.10.25 2022.10.26 2022.10.27 2022.10.28 2022.10.29 2022.10.30 2022.10.31 2022.11.01 2022.11.02 2022.11.03 2022.11.04 2022.11.05 2022.11.06 2022.11.07 2022.11.08 2022.11.09 \u0026#34; time=$(date +\u0026#34;%Y.%m.%d\u0026#34; -d \u0026#34;-1day\u0026#34;) host=\u0026#39;http://elasticsearch-log.prod.server.fjf:9200\u0026#39; username=\u0026#39;elastic\u0026#39; password=\u0026#39;4reQiUCLB7meNU8VoUsm\u0026#39; #仓库名称 STORE_NAME=\u0026#34;backup\u0026#34; #curl的绝对路径 CURL_CMD=\u0026#34;/usr/bin/curl\u0026#34; #告警信息 webhook=\u0026#39;https://oapi.dingtalk.com/robot/send?access_token=810921774dac36151d8939b6ea68c90df667f50e1313xxxxxxxxxxxx\u0026#39; cluster=\u0026#39;ES备份告警\u0026#39; function SendMsgToDingding() { curl $webhook -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#34; { \u0026#39;msgtype\u0026#39;: \u0026#39;text\u0026#39;, \u0026#39;text\u0026#39;: { \u0026#39;content\u0026#39;: \u0026#39;集群名称：$cluster\\n告警信息： $1 已经备份完成 \\n\u0026#39; }, \u0026#39;at\u0026#39;: { \u0026#39;isAtAll\u0026#39;: true } }\u0026#34; } list_groups=\u0026#34;labor-prod-${time},app-prod-${time},fk-prod-${time}\u0026#34; curl -s -u elastic:4reQixxxxxVoUsm -sXPUT http://exxxxxxxxxxxxxxxxxxxerver.fjf:9200/_snapshot/backup/log_es_${time}?wait_for_completion=true -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39;{ \u0026#34;indices\u0026#34;: \u0026#34;\u0026#39;${list_groups}\u0026#39;\u0026#34; }\u0026#39; if [[ $? -eq 0 ]]; then SendMsgToDingding $list_groups log_always \u0026#34; $list_groups 已经备份成功\u0026#34; else log_err \u0026#34;$list_groups 备份失败，请查看\u0026#34; fi 2、Elasticsearch索引清理 1 2 # Elasticsearch索引清理 1 3 * * * bash /opt/shell-code/es_index.sh 2\u0026gt;\u0026amp;1 \u0026gt; /usr/local/script/es_index.log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #author : 南宫乘风 # * Email : 1794748404@qq.com # * Last modified : 2022-11-03 23:43 # * Filename : 01_ping_check_isalive.sh # * Description : # * *******************************************************/ #/bin/bash ################################### #!/bin/bash host=\u0026#39;elasticxxxxxxxxxxxxxxxx.fjf\u0026#39; username=\u0026#39;elastic\u0026#39; password=\u0026#39;4reQiUxxxxxxxxxxxxxx\u0026#39; ########################################### 如下用于清理旧的索引数据 ########################################## # 清除旧索引 function purgeOldIndexes() { for i in $INDEXES; do for index in `curl -u $username:$password -s http://$host:9200/_cat/indices |grep -P \u0026#34;$i\u0026#34; |awk \u0026#39;{print $3}\u0026#39; |sort -r | tail -n +${NUM}` do echo ${index} curl -u $username:$password -s -XDELETE http://$host:9200/${index} done done } # 不同索引的日志保留时长要求不同， NUM=11 INDEXES=\u0026#34;monitoring-es syslog zipkin-prod:dependency zipkin-prod:span \u0026#34; purgeOldIndexes sleep 10 NUM=31 INDEXES=\u0026#34;logstash-fk-prod logstash-_doc logstash-prod \u0026#34; purgeOldIndexes sleep 10 NUM=60 INDEXES=\u0026#34;app-prod fk-prod \u0026#34; purgeOldIndexes sleep 10 NUM=91 INDEXES=\u0026#34;labor-prod\u0026#34; purgeOldIndexes 文档：通过OSS将自建Elasticsearch数据迁移至阿里云\n自动备份与恢复\n手动备份与恢复\n备份你的集群\nElasticsearch的Snapshot and Restore（快照备份与恢复）\nSnapshot And Restore\n自动备份与恢复\n","date":"2022-11-13T09:22:34Z","image":"https://www.ownit.top/title_pic/51.jpg","permalink":"https://www.ownit.top/p/202211130922/","title":"Elasticsearch快照备份"},{"content":"1、Gitlab-runner ​GitLab Runner是一个开源项目，用于运行您的作业并将结果发送回GitLab。它与GitLab CI结合使用，GitLab CI是GitLab随附的用于协调作业的开源持续集成服务。​\n要求 GitLab Runner是用Go编写的，可以作为一个二进制文件运行，不需要特定于语言的要求。它旨在在GNU / Linux，macOS和Windows操作系统上运行。只要您可以在其他操作系统上编译Go二进制文件，其他操作系统就可能会运行。\n如果要使用Docker，请安装最新版本。GitLab Runner需要最少的Docker v1.13.0。\nGitLab Runner版本应与GitLab版本同步。尽管较旧的Runner仍可以使用较新的GitLab版本，反之亦然，但在某些情况下，如果版本存在差异，则功能可能不可用或无法正常工作。在次要版本更新之间可以保证向后兼容性，但是请注意，GitLab的次要版本更新会引入新功能，这些新功能将要求Runner在同一次要版本上使用。\n特点 允许运行：\n同时执行多个作业。\n对多个服务器（甚至每个项目）使用多个令牌。\n限制每个令牌的并行作业数。\n可以运行作业：\n在本地。\n使用Docker容器。\n使用Docker容器并通过SSH执行作业。\n使用Docker容器在不同的云和虚拟化管理程序上自动缩放。\n连接到远程SSH服务器。\n用Go编写并以单个二进制文件的形式分发，而没有其他要求。\n支持Bash，Windows Batch和Windows PowerShell。\n在GNU / Linux，macOS和Windows（几乎可以在任何可以运行Docker的地方）上运行。\n允许自定义作业运行环境。\n自动重新加载配置，无需重启。\n易于使用的设置，并支持Docker，Docker-SSH，Parallels或SSH运行环境。\n启用Docker容器的缓存。\n易于安装，可作为GNU / Linux，macOS和Windows的服务。\n嵌入式Prometheus指标HTTP服务器。\n裁判工作者监视Prometheus度量标准和其他特定于工作的数据并将其传递给GitLab。\n2、GitLab Runner安装 可以在GNU / Linux，macOS，FreeBSD和Windows上​安装和使用GitLab Runner 。您可以使用Docker安装它，手动下载二进制文件，也可以使用GitLab提供的rpm / deb软件包的存储库。\n1. 使用GItLab官方仓库安装 重点掌握CentOS系统的安装方式\n重点掌握Ubuntu系统的安装方式\n我们提供Debian，Ubuntu，Mint，RHEL，Fedora和CentOS当前受支持版本的软件包。\nDistribution Version End of Life date Debian stretch approx. 2022 Debian jessie June 2020 Ubuntu bionic April 2023 Ubuntu xenial April 2021 Mint sonya approx. 2021 Mint serena approx. 2021 Mint sarah approx. 2021 RHEL/CentOS 7 June 2024 RHEL/CentOS 6 November 2020 Fedora 29 approx. November 2019 Add GitLab’s official repository: 添加官方仓库\n1 2 3 4 5 # For Debian/Ubuntu/Mint curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash # For RHEL/CentOS/Fedora curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh | sudo bash Install the latest version of GitLab Runner: 安装最新版本\n1 2 3 4 5 # For Debian/Ubuntu/Mint sudo apt-get install gitlab-runner # For RHEL/CentOS/Fedora sudo yum install gitlab-runner To install a specific version of GitLab Runner: 安装指定版本\n1 2 3 4 5 6 7 # for DEB based systems apt-cache madison gitlab-runner sudo apt-get install gitlab-runner=10.0.0 # for RPM based systems yum list gitlab-runner --showduplicates | sort -r sudo yum install gitlab-runner-10.0.0-1 更新runner\n1 2 3 4 5 6 7 # For Debian/Ubuntu/Mint sudo apt-get update sudo apt-get install gitlab-runner # For RHEL/CentOS/Fedora sudo yum update sudo yum install gitlab-runner 2. Linux上手动安装GitLab Runner 如果您不能使用​deb / rpm存储库安装GitLab Runner，或者您的GNU / Linux操作系统不在支持的版本中，则可以使用以下一种方法手动安装它，这是最后的选择。\n通过deb或rpm软件包 下载软件包\n在​https://gitlab-runner-downloads.s3.amazonaws.com/latest/index.html上找到最新的文件名和选项 。\n选择一个版本并下载二进制文件，如文档所述，该文件用于​下载任何其他标记的 GitLab Runner发行版。\n例如，对于Debian或Ubuntu：\n1 2 3 4 5 curl -LJO https://gitlab-runner-downloads.s3.amazonaws.com/latest/deb/gitlab-runner_\u0026lt;arch\u0026gt;.deb dpkg -i gitlab-runner_\u0026lt;arch\u0026gt;.deb dpkg -i gitlab-runner_\u0026lt;arch\u0026gt;.deb 例如，对于CentOS或Red Hat Enterprise Linux：\n​\n1 2 3 4 5 curl -LJO https://gitlab-runner-downloads.s3.amazonaws.com/latest/rpm/gitlab-runner_\u0026lt;arch\u0026gt;.rpm rpm -i gitlab-runner_\u0026lt;arch\u0026gt;.rpm rpm -Uvh gitlab-runner_\u0026lt;arch\u0026gt;.rpm 使用二进制文件(官方推荐) 参考地址： https://docs.gitlab.com/12.6/runner/install/bleeding-edge.html#download-any-other-tagged-release\n下载指定版本： 将上面URL中的latest切换为 v12.6。\n​\n1 2 3 4 5 6 7 8 9 10 11 # Linux x86-64 sudo curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 # Linux x86 sudo curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-386 # Linux arm sudo curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-arm # Linux arm64 sudo curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-arm64 添加执行权限\n1 sudo chmod +x /usr/local/bin/gitlab-runner 创建一个gitlab用户\n1 sudo useradd --comment \u0026#39;GitLab Runner\u0026#39; --create-home gitlab-runner --shell /bin/bash 安装并作为服务运行\n1 2 sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner sudo gitlab-runner start 更新\n1 2 3 4 5 6 7 8 9 10 11 #停止服务 sudo gitlab-runner stop #下载新版本二进制包 sudo curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 #赋予执行权限 sudo chmod +x /usr/local/bin/gitlab-runner #启动服务 sudo gitlab-runner start 3. 在容器中运行GitLab Runner 1 docker run --rm -t -id -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 3、GitLab Runner注册 **参考链接：**​使用GitLab CI运行GitLab Runner并执行Pipeline\n大概过程： 获取runner token -\u0026gt; 进行注册\nGitLabRunner 类型 shared ： 运行整个平台项目的作业（gitlab）\ngroup： 运行特定group下的所有项目的作业（group）\nspecific: 运行指定的项目作业（project）\nlocked： 无法运行项目作业\npaused： 不会运行作业\n获取runner token 获取shared类型runnertoken\n进入系统设置 -\u0026gt; Runners\n获取group类型的runnertoken\n进入group -\u0026gt; Settings -\u0026gt; CI/CD -\u0026gt; Runners -\u0026gt; Group Runners\n获取specific类型的runnertoken\n进入具体的项目 -\u0026gt; Settings -\u0026gt; CI/CD -\u0026gt; Runners -\u0026gt; Specific Runners\n进行注册 方式1： 启动容器交互式注册 1 docker run --rm -t -i -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 register 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Runtime platform arch=amd64 os=linux pid=6 revision=ac8e767a version=12.6.0 Running in system-mode. Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): #gitlab地址 http://192.168.1.105 Please enter the gitlab-ci token for this runner: #gitlab-runner的token 4tutaeWWL3srNEcmHs1s Please enter the gitlab-ci description for this runner: #gitlab-runner的描述 [00e4f023b5ae]: devops-service-runner Please enter the gitlab-ci tags for this runner (comma separated): #gitlab-runner的 标签 build Registering runner... succeeded runner=4tutaeWW Please enter the executor: parallels, virtualbox, docker-ssh+machine, kubernetes, docker+machine, custom, docker, docker-ssh, shell, ssh: #gitlab-runner的运行的方式 shell Runner registered successfully. Feel free to start it, but if it\u0026#39;s running already the config should be automatically reloaded! 方式2：直接注册 1 2 3 4 5 6 7 8 9 10 11 docker run --rm -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 register \\ --non-interactive \\ --executor \u0026#34;docker\u0026#34; \\ --docker-image alpine:latest \\ --url \u0026#34;http://192.168.1.200:30088/\u0026#34; \\ --registration-token \u0026#34;JRzzw2j1Ji6aBjwvkxAv\u0026#34; \\ --description \u0026#34;docker-runner\u0026#34; \\ --tag-list \u0026#34;docker,aws\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --locked=\u0026#34;false\u0026#34; \\ --access-level=\u0026#34;not_protected\u0026#34; 1 2 3 4 5 6 7 8 9 10 docker run -it --rm -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 register \\ --non-interactive \\ --executor \u0026#34;shell\u0026#34; \\ --url \u0026#34;http://gitlab.devops.com\u0026#34; \\ --registration-token \u0026#34;FZwnmCacrXfk37yZzfNJ\u0026#34; \\ --description \u0026#34;devops-runner\u0026#34; \\ --tag-list \u0026#34;build,deploy\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --locked=\u0026#34;false\u0026#34; \\ --access-level=\u0026#34;not_protected\u0026#34; 1 2 3 4 5 6 7 8 9 10 docker run -itd --rm -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 register \\ --non-interactive \\ --executor \u0026#34;shell\u0026#34; \\ --url \u0026#34;http://192.168.1.200:30088/\u0026#34; \\ --registration-token \u0026#34;JRzzw2j1Ji6aBjwvkxAv\u0026#34; \\ --description \u0026#34;devops-runner\u0026#34; \\ --tag-list \u0026#34;build,deploy\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --locked=\u0026#34;false\u0026#34; \\ --access-level=\u0026#34;not_protected\u0026#34; 测试环境使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #uat dev fat 环境 gitlab-runner register \\ --non-interactive \\ --executor \u0026#34;shell\u0026#34; \\ --url \u0026#34;http://172.17.20.20:8200/\u0026#34; \\ --registration-token \u0026#34;HH5ey1cfWis5UsByz1hs\u0026#34; \\ --description \u0026#34;devops-runner\u0026#34; \\ --tag-list \u0026#34;dev,fat,uat\u0026#34; \\ --run-untagged=\u0026#34;true\u0026#34; \\ --locked=\u0026#34;false\u0026#34; \\ --access-level=\u0026#34;not_protected\u0026#34; # prod 环境 gitlab-runner register \\ --non-interactive \\ --executor \u0026#34;shell\u0026#34; \\ --url \u0026#34;https://gitlab.ownit.top/\u0026#34; \\ --registration-token \u0026#34;uExfDik6eNLMzEzpnmD8\u0026#34; \\ --description \u0026#34;prod\u0026#34; \\ --tag-list \u0026#34;prod\u0026#34; \\ --run-untagged=\u0026#34;false\u0026#34; \\ --locked=\u0026#34;false\u0026#34; \\ --access-level=\u0026#34;not_protected\u0026#34; 其他变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -c value, --config value 指定配置文件 --template-config value 指定模板配置文件 --tag-list value 指定runner的标签列表，逗号分隔 -n, --non-interactive 无交互进行runner注册 --leave-runner 如果注册失败，不用删除runner -r value, --registration-token value runner的注册token --run-untagged 注册运行未加标签的构建，默认当标签列表为空时值为true --locked 锁定runner 默认true --access-level value 设置访问等级 not_protected or ref_protected; 默认 not_protected --maximum-timeout value 为作业设置最大运行超时时间 默认零 单位秒 --paused 设置runner为 paused,默认 \u0026#39;false\u0026#39; --name value, --description value Runner 名称 --limit value 程序处理的最大构建数量default: \u0026#34;0\u0026#34; --output-limit value 最大的构建大小单位kb default: \u0026#34;0\u0026#34; --request-concurrency value 作业请求的最大并发数 default: \u0026#34;0\u0026#34; -u value, --url value GitlabCI服务器地址 -t value, --token value GitlabCI服务器token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 root@82948f0a37e3:/# gitlab-runner register --help Runtime platform arch=amd64 os=linux pid=168 revision=ac8e767a version=12.6.0 NAME: gitlab-runner register - register a new runner USAGE: gitlab-runner register [command options] [arguments...] OPTIONS: --tls-ca-file value File containing the certificates to verify the peer when using HTTPS [$CI_SERVER_TLS_CA_FILE] --tls-cert-file value File containing certificate for TLS client auth when using HTTPS [$CI_SERVER_TLS_CERT_FILE] --tls-key-file value File containing private key for TLS client auth when using HTTPS [$CI_SERVER_TLS_KEY_FILE] --executor value 选择执行器，例如shell，docker --builds-dir value 设置构建存储目录 --cache-dir value 设置构建缓存目录 --clone-url value 覆盖默认通过git克隆的URL --env value 注入自定义环境变量以构建环境 --pre-clone-script value 在提取代码之前执行的特定于运行程序的命令脚本 --pre-build-script value 特定于运行程序的命令脚本，在提取代码之后，在构建执行之前执行 --post-build-script value 特定于运行程序的命令脚本，在提取代码后以及在构建执行后立即执行 --debug-trace-disabled 设置为true时，Runner将禁用使用CI_DEBUG_TRACE功能的可能性 --shell value 选择 bash, cmd or powershell [$RUNNER_SHELL] --custom_build_dir-enabled 启用作业特定的构建目录[$CUSTOM_BUILD_DIR_ENABLED] --ssh-user value ssh用户名称 [$SSH_USER] --ssh-password value ssh用户密码[$SSH_PASSWORD] --ssh-host value ssh远程主机[$SSH_HOST] --ssh-port value ssh远程主机端口 [$SSH_PORT] --ssh-identity-file value ssh认证文件 [$SSH_IDENTITY_FILE] --docker-host value Docker主机地址 [$DOCKER_HOST] --docker-cert-path value Docker证书路径 [$DOCKER_CERT_PATH] --docker-tlsverify Docker使用TLS并验证远程 [$DOCKER_TLS_VERIFY] --docker-hostname value 自定义容器主机名称 [$DOCKER_HOSTNAME] --docker-image value 定义Docker镜像[$DOCKER_IMAGE] --docker-runtime value Docker runtime to be used [$DOCKER_RUNTIME] --docker-memory value 内存限制 Unit [b, k, m, or g] 4M [$DOCKER_MEMORY] --docker-memory-swap value 内存限制memory + swap，Unit[b, k, m, or g][$DOCKER_MEMORY_SWAP] --docker-memory-reservation value 内存软限制[$DOCKER_MEMORY_RESERVATION] --docker-cpuset-cpus value cpu限制[$DOCKER_CPUSET_CPUS] --docker-cpus value cpu数量 [$DOCKER_CPUS] --docker-cpu-shares value CPU shares (default: \u0026#34;0\u0026#34;) [$DOCKER_CPU_SHARES] --docker-dns value A list of DNS servers for the container to use [$DOCKER_DNS] --docker-dns-search value A list of DNS search domains [$DOCKER_DNS_SEARCH] --docker-privileged Give extended privileges to container [$DOCKER_PRIVILEGED] --docker-disable-entrypoint-overwrite Disable the possibility for a container to overwrite the default image entrypoint [$DOCKER_DISABLE_ENTRYPOINT_OVERWRITE] --docker-userns value User namespace to use [$DOCKER_USERNS_MODE] --docker-cap-add value Add Linux capabilities [$DOCKER_CAP_ADD] --docker-cap-drop value Drop Linux capabilities [$DOCKER_CAP_DROP] --docker-oom-kill-disable Do not kill processes in a container if an out-of-memory (OOM) error occurs [$DOCKER_OOM_KILL_DISABLE] --docker-oom-score-adjust value Adjust OOM score (default: \u0026#34;0\u0026#34;) [$DOCKER_OOM_SCORE_ADJUST] --docker-security-opt value Security Options [$DOCKER_SECURITY_OPT] --docker-devices value Add a host device to the container [$DOCKER_DEVICES] --docker-disable-cache Disable all container caching [$DOCKER_DISABLE_CACHE] --docker-volumes value Bind-mount a volume and create it if it doesn\u0026#39;t exist prior to mounting. Can be specified multiple times once per mountpoint, e.g. --docker-volumes \u0026#39;test0:/test0\u0026#39; --docker-volumes \u0026#39;test1:/test1\u0026#39; [$DOCKER_VOLUMES] --docker-volume-driver value Volume driver to be used [$DOCKER_VOLUME_DRIVER] --docker-cache-dir value Directory where to store caches [$DOCKER_CACHE_DIR] --docker-extra-hosts value Add a custom host-to-IP mapping [$DOCKER_EXTRA_HOSTS] --docker-volumes-from value A list of volumes to inherit from another container [$DOCKER_VOLUMES_FROM] --docker-network-mode value Add container to a custom network [$DOCKER_NETWORK_MODE] --docker-links value Add link to another container [$DOCKER_LINKS] --docker-services value Add service that is started with container [$DOCKER_SERVICES] --docker-wait-for-services-timeout value How long to wait for service startup (default: \u0026#34;0\u0026#34;) [$DOCKER_WAIT_FOR_SERVICES_TIMEOUT] --docker-allowed-images value Whitelist allowed images [$DOCKER_ALLOWED_IMAGES] --docker-allowed-services value Whitelist allowed services [$DOCKER_ALLOWED_SERVICES] --docker-pull-policy value Image pull policy: never, if-not-present, always [$DOCKER_PULL_POLICY] --docker-shm-size value Shared memory size for docker images (in bytes) (default: \u0026#34;0\u0026#34;) [$DOCKER_SHM_SIZE] --docker-tmpfs value A toml table/json object with the format key=values. When set this will mount the specified path in the key as a tmpfs volume in the main container, using the options specified as key. For the supported options, see the documentation for the unix \u0026#39;mount\u0026#39; command (default: \u0026#34;{}\u0026#34;) [$DOCKER_TMPFS] --docker-services-tmpfs value A toml table/json object with the format key=values. When set this will mount the specified path in the key as a tmpfs volume in all the service containers, using the options specified as key. For the supported options, see the documentation for the unix \u0026#39;mount\u0026#39; command (default: \u0026#34;{}\u0026#34;) [$DOCKER_SERVICES_TMPFS] --docker-sysctls value Sysctl options, a toml table/json object of key=value. Value is expected to be a string. (default: \u0026#34;{}\u0026#34;) [$DOCKER_SYSCTLS] --docker-helper-image value [ADVANCED] Override the default helper image used to clone repos and upload artifacts [$DOCKER_HELPER_IMAGE] --parallels-base-name value VM name to be used [$PARALLELS_BASE_NAME] --parallels-template-name value VM template to be created [$PARALLELS_TEMPLATE_NAME] --parallels-disable-snapshots Disable snapshoting to speedup VM creation [$PARALLELS_DISABLE_SNAPSHOTS] --parallels-time-server value Timeserver to sync the guests time from. Defaults to time.apple.com [$PARALLELS_TIME_SERVER] --virtualbox-base-name value VM name to be used [$VIRTUALBOX_BASE_NAME] --virtualbox-base-snapshot value Name or UUID of a specific VM snapshot to clone [$VIRTUALBOX_BASE_SNAPSHOT] --virtualbox-disable-snapshots Disable snapshoting to speedup VM creation [$VIRTUALBOX_DISABLE_SNAPSHOTS] --cache-type value Select caching method [$CACHE_TYPE] --cache-path value Name of the path to prepend to the cache URL [$CACHE_PATH] --cache-shared Enable cache sharing between runners. [$CACHE_SHARED] --cache-s3-server-address value A host:port to the used S3-compatible server [$CACHE_S3_SERVER_ADDRESS] --cache-s3-access-key value S3 Access Key [$CACHE_S3_ACCESS_KEY] --cache-s3-secret-key value S3 Secret Key [$CACHE_S3_SECRET_KEY] --cache-s3-bucket-name value Name of the bucket where cache will be stored [$CACHE_S3_BUCKET_NAME] --cache-s3-bucket-location value Name of S3 region [$CACHE_S3_BUCKET_LOCATION] --cache-s3-insecure Use insecure mode (without https) [$CACHE_S3_INSECURE] --cache-gcs-access-id value ID of GCP Service Account used to access the storage [$CACHE_GCS_ACCESS_ID] --cache-gcs-private-key value Private key used to sign GCS requests [$CACHE_GCS_PRIVATE_KEY] --cache-gcs-credentials-file value File with GCP credentials, containing AccessID and PrivateKey [$GOOGLE_APPLICATION_CREDENTIALS] --cache-gcs-bucket-name value Name of the bucket where cache will be stored [$CACHE_GCS_BUCKET_NAME] --machine-idle-nodes value Maximum idle machines (default: \u0026#34;0\u0026#34;) [$MACHINE_IDLE_COUNT] --machine-idle-time value Minimum time after node can be destroyed (default: \u0026#34;0\u0026#34;) [$MACHINE_IDLE_TIME] --machine-max-builds value Maximum number of builds processed by machine (default: \u0026#34;0\u0026#34;) [$MACHINE_MAX_BUILDS] --machine-machine-driver value The driver to use when creating machine [$MACHINE_DRIVER] --machine-machine-name value The template for machine name (needs to include %s) [$MACHINE_NAME] --machine-machine-options value Additional machine creation options [$MACHINE_OPTIONS] --machine-off-peak-periods value Time periods when the scheduler is in the OffPeak mode [$MACHINE_OFF_PEAK_PERIODS] --machine-off-peak-timezone value Timezone for the OffPeak periods (defaults to Local) [$MACHINE_OFF_PEAK_TIMEZONE] --machine-off-peak-idle-count value Maximum idle machines when the scheduler is in the OffPeak mode (default: \u0026#34;0\u0026#34;) [$MACHINE_OFF_PEAK_IDLE_COUNT] --machine-off-peak-idle-time value Minimum time after machine can be destroyed when the scheduler is in the OffPeak mode (default: \u0026#34;0\u0026#34;) [$MACHINE_OFF_PEAK_IDLE_TIME] --kubernetes-host value Optional Kubernetes master host URL (auto-discovery attempted if not specified) [$KUBERNETES_HOST] --kubernetes-cert-file value Optional Kubernetes master auth certificate [$KUBERNETES_CERT_FILE] --kubernetes-key-file value Optional Kubernetes master auth private key [$KUBERNETES_KEY_FILE] --kubernetes-ca-file value Optional Kubernetes master auth ca certificate [$KUBERNETES_CA_FILE] --kubernetes-bearer_token_overwrite_allowed Bool to authorize builds to specify their own bearer token for creation. [$KUBERNETES_BEARER_TOKEN_OVERWRITE_ALLOWED] --kubernetes-bearer_token value Optional Kubernetes service account token used to start build pods. [$KUBERNETES_BEARER_TOKEN] --kubernetes-image value Default docker image to use for builds when none is specified [$KUBERNETES_IMAGE] --kubernetes-namespace value Namespace to run Kubernetes jobs in [$KUBERNETES_NAMESPACE] --kubernetes-namespace_overwrite_allowed value Regex to validate \u0026#39;KUBERNETES_NAMESPACE_OVERWRITE\u0026#39; value [$KUBERNETES_NAMESPACE_OVERWRITE_ALLOWED] --kubernetes-privileged Run all containers with the privileged flag enabled [$KUBERNETES_PRIVILEGED] --kubernetes-cpu-limit value The CPU allocation given to build containers [$KUBERNETES_CPU_LIMIT] --kubernetes-memory-limit value The amount of memory allocated to build containers [$KUBERNETES_MEMORY_LIMIT] --kubernetes-service-cpu-limit value The CPU allocation given to build service containers [$KUBERNETES_SERVICE_CPU_LIMIT] --kubernetes-service-memory-limit value The amount of memory allocated to build service containers [$KUBERNETES_SERVICE_MEMORY_LIMIT] --kubernetes-helper-cpu-limit value The CPU allocation given to build helper containers [$KUBERNETES_HELPER_CPU_LIMIT] --kubernetes-helper-memory-limit value The amount of memory allocated to build helper containers [$KUBERNETES_HELPER_MEMORY_LIMIT] --kubernetes-cpu-request value The CPU allocation requested for build containers [$KUBERNETES_CPU_REQUEST] --kubernetes-memory-request value The amount of memory requested from build containers [$KUBERNETES_MEMORY_REQUEST] --kubernetes-service-cpu-request value The CPU allocation requested for build service containers [$KUBERNETES_SERVICE_CPU_REQUEST] --kubernetes-service-memory-request value The amount of memory requested for build service containers [$KUBERNETES_SERVICE_MEMORY_REQUEST] --kubernetes-helper-cpu-request value The CPU allocation requested for build helper containers [$KUBERNETES_HELPER_CPU_REQUEST] --kubernetes-helper-memory-request value The amount of memory requested for build helper containers [$KUBERNETES_HELPER_MEMORY_REQUEST] --kubernetes-pull-policy value Policy for if/when to pull a container image (never, if-not-present, always). The cluster default will be used if not set [$KUBERNETES_PULL_POLICY] --kubernetes-node-selector value A toml table/json object of key=value. Value is expected to be a string. When set this will create pods on k8s nodes that match all the key=value pairs. (default: \u0026#34;{}\u0026#34;) [$KUBERNETES_NODE_SELECTOR] --kubernetes-node-tolerations value A toml table/json object of key=value:effect. Value and effect are expected to be strings. When set, pods will tolerate the given taints. Only one toleration is supported through environment variable configuration. (default: \u0026#34;{}\u0026#34;) [$KUBERNETES_NODE_TOLERATIONS] --kubernetes-image-pull-secrets value A list of image pull secrets that are used for pulling docker image [$KUBERNETES_IMAGE_PULL_SECRETS] --kubernetes-helper-image value [ADVANCED] Override the default helper image used to clone repos and upload artifacts [$KUBERNETES_HELPER_IMAGE] --kubernetes-terminationGracePeriodSeconds value Duration after the processes running in the pod are sent a termination signal and the time when the processes are forcibly halted with a kill signal. (default: \u0026#34;0\u0026#34;) [$KUBERNETES_TERMINATIONGRACEPERIODSECONDS] --kubernetes-poll-interval value How frequently, in seconds, the runner will poll the Kubernetes pod it has just created to check its status (default: \u0026#34;0\u0026#34;) [$KUBERNETES_POLL_INTERVAL] --kubernetes-poll-timeout value The total amount of time, in seconds, that needs to pass before the runner will timeout attempting to connect to the pod it has just created (useful for queueing more builds that the cluster can handle at a time) (default: \u0026#34;0\u0026#34;) [$KUBERNETES_POLL_TIMEOUT] --kubernetes-pod-labels value A toml table/json object of key-value. Value is expected to be a string. When set, this will create pods with the given pod labels. Environment variables will be substituted for values here. (default: \u0026#34;{}\u0026#34;) --kubernetes-service-account value Executor pods will use this Service Account to talk to kubernetes API [$KUBERNETES_SERVICE_ACCOUNT] --kubernetes-service_account_overwrite_allowed value Regex to validate \u0026#39;KUBERNETES_SERVICE_ACCOUNT\u0026#39; value [$KUBERNETES_SERVICE_ACCOUNT_OVERWRITE_ALLOWED] --kubernetes-pod-annotations value A toml table/json object of key-value. Value is expected to be a string. When set, this will create pods with the given annotations. Can be overwritten in build with KUBERNETES_POD_ANNOTATION_* variables (default: \u0026#34;{}\u0026#34;) --kubernetes-pod_annotations_overwrite_allowed value Regex to validate \u0026#39;KUBERNETES_POD_ANNOTATIONS_*\u0026#39; values [$KUBERNETES_POD_ANNOTATIONS_OVERWRITE_ALLOWED] --kubernetes-pod-security-context-fs-group value A special supplemental group that applies to all containers in a pod [$KUBERNETES_POD_SECURITY_CONTEXT_FS_GROUP] --kubernetes-pod-security-context-run-as-group value The GID to run the entrypoint of the container process [$KUBERNETES_POD_SECURITY_CONTEXT_RUN_AS_GROUP] --kubernetes-pod-security-context-run-as-non-root value Indicates that the container must run as a non-root user [$KUBERNETES_POD_SECURITY_CONTEXT_RUN_AS_NON_ROOT] --kubernetes-pod-security-context-run-as-user value The UID to run the entrypoint of the container process [$KUBERNETES_POD_SECURITY_CONTEXT_RUN_AS_USER] --kubernetes-pod-security-context-supplemental-groups value A list of groups applied to the first process run in each container, in addition to the container\u0026#39;s primary GID --kubernetes-services value Add service that is started with container --custom-config-exec value Executable that allows to inject configuration values to the executor [$CUSTOM_CONFIG_EXEC] --custom-config-args value Arguments for the config executable --custom-config-exec-timeout value Timeout for the config executable (in seconds) [$CUSTOM_CONFIG_EXEC_TIMEOUT] --custom-prepare-exec value Executable that prepares executor [$CUSTOM_PREPARE_EXEC] --custom-prepare-args value Arguments for the prepare executable --custom-prepare-exec-timeout value Timeout for the prepare executable (in seconds) [$CUSTOM_PREPARE_EXEC_TIMEOUT] --custom-run-exec value Executable that runs the job script in executor [$CUSTOM_RUN_EXEC] --custom-run-args value Arguments for the run executable --custom-cleanup-exec value Executable that cleanups after executor run [$CUSTOM_CLEANUP_EXEC] --custom-cleanup-args value Arguments for the cleanup executable --custom-cleanup-exec-timeout value Timeout for the cleanup executable (in seconds) [$CUSTOM_CLEANUP_EXEC_TIMEOUT] --custom-graceful-kill-timeout value Graceful timeout for scripts execution after SIGTERM is sent to the process (in seconds). This limits the time given for scripts to perform the cleanup before exiting [$CUSTOM_GRACEFUL_KILL_TIMEOUT] --custom-force-kill-timeout value Force timeout for scripts execution (in seconds). Counted from the force kill call; if process will be not terminated, Runner will abandon process termination and log an error [$CUSTOM_FORCE_KILL_TIMEOUT] Docker runner执行器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ZeyangdeMacBook-Pro:~ zeyang$ gitlab-runner register Runtime platform arch=amd64 os=darwin pid=92353 revision=ac8e767a version=12.6.0 WARNING: Running in user-mode. WARNING: Use sudo for system-mode: WARNING: $ sudo gitlab-runner... Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): http://gitlab.devops.com/ Please enter the gitlab-ci token for this runner: RjAoLah8Vp7JCGyNzZwf Please enter the gitlab-ci description for this runner: [ZeyangdeMacBook-Pro.local]: test Please enter the gitlab-ci tags for this runner (comma separated): docker Registering runner... succeeded runner=RjAoLah8 Please enter the executor: virtualbox, docker-ssh, parallels, shell, ssh, kubernetes, custom, docker, docker+machine, docker-ssh+machine: docker Please enter the default Docker image (e.g. ruby:2.6): maven:3.6.3-jdk-8 Runner registered successfully. Feel free to start it, but if it\u0026#39;s running already the config should be automatically reloaded! 4、GitLab Runner命令 GitLab Runner包含一组命令，可用于注册，管理和运行构建。\n启动命令 1 2 3 4 gitlab-runner --debug \u0026lt;command\u0026gt; #调试模式排查错误特别有用。 gitlab-runner \u0026lt;command\u0026gt; --help #获取帮助信息 gitlab-runner run #普通用户模式 配置文件位置 ~/.gitlab-runner/config.toml sudo gitlab-runner run # 超级用户模式 配置文件位置/etc/gitlab-runner/config.toml 注册命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 gitlab-runner register #默认交互模式下使用，非交互模式添加 --non-interactive gitlab-runner list #此命令列出了保存在配置文件中的所有运行程序 gitlab-runner verify #此命令检查注册的runner是否可以连接，但不验证GitLab服务是否正在使用runner。 --delete 删除 gitlab-runner unregister #该命令使用GitLab取消已注册的runner。 #使用令牌注销 gitlab-runner unregister --url http://gitlab.example.com/ --token t0k3n #使用名称注销（同名删除第一个） gitlab-runner unregister --name test-runner #注销所有 gitlab-runner unregister --all-runners 服务管理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner # --user指定将用于执行构建的用户 #`--working-directory 指定将使用**Shell** executor 运行构建时所有数据将存储在其中的根目录 gitlab-runner uninstall #该命令停止运行并从服务中卸载GitLab Runner。 gitlab-runner start #该命令启动GitLab Runner服务。 gitlab-runner stop #该命令停止GitLab Runner服务。 gitlab-runner restart #该命令将停止，然后启动GitLab Runner服务。 gitlab-runner status #此命令显示GitLab Runner服务的状态。当服务正在运行时，退出代码为零；而当服务未运行时，退出代码为非零。 5、运行流水线任务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 stages: - build - deploy build: stage: build tags: - build only: - master script: - echo \u0026#34;mvn clean \u0026#34; - echo \u0026#34;mvn install\u0026#34; deploy: stage: deploy tags: - deploy only: - master script: - echo \u0026#34;hello deploy\u0026#34; ","date":"2022-10-31T10:18:30Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/202210311018/","title":"GitLab集成gitlab-runner"},{"content":"Prometheus监控Linux主机 Prometheus node-exporter 监控Linux服务器\nnode-export 主要用来做Linux服务器监控，比如服务器的进程数、消耗了多少 CPU、内存，磁盘空间，iops，tcp连接数等资源。\nNode Exporter 是用于暴露 *NIX 主机指标的 Exporter，比如采集 CPU、内存、磁盘等信息。采用 Go 编写，不存在任何第三方依赖，所以只需要下载解压即可运行。\nExporter是Prometheus的一类数据采集组件的总称。它负责从目标处搜集数据，并将其转化为Prometheus支持的格式。与传统的数据采集组件不同的是，它并不向中央服务器发送数据，而是等待中央服务器主动前来抓取。\nnode-exporter用于采集服务器层面的运行指标，包括机器的loadavg、filesystem、meminfo等基础监控，类似于传统主机监控维度的zabbix-agent\nnode_exporter：用于监控Linux系统的指标采集器。\n常用指标：\n•CPU\n• 内存\n• 硬盘\n• 网络流量\n• 文件描述符\n• 系统负载\n• 系统服务\n数据接口：http://IP:9100/metrics\n使用文档：https://prometheus.io/docs/guides/node-exporter/\nGitHub：GitHub - prometheus/node_exporter: Exporter for machine metrics\n一、node-exporter 使用 ‍\n1.1 下载 node-exporter node-exporter 下载地址：https://prometheus.io/download/\nnode-expoeter\nnode-exporter 可以使用命令行参数也可以指定参数命令启动\n1.2 配置node-exporter 该 metrics 接口数据就是一个标准的 Prometheus 监控指标格式，我们只需要将该端点配置到 Prometheus 中即可抓取该指标数据。为了了解 node_exporter 可配置的参数，我们可以使用 ./node_exporter \\-h 来查看帮助信息：\n1 2 3 4 5 6 7 8 9 10 11 12 ☸ ➜ ./node_exporter -h --web.listen-address=\u0026#34;:9100\u0026#34; # 监听的端口，默认是9100 --web.telemetry-path=\u0026#34;/metrics\u0026#34; # metrics的路径，默认为/metrics --web.disable-exporter-metrics # 是否禁用go、prome默认的metrics --web.max-requests=40 # 最大并行请求数，默认40，设置为0时不限制 --log.level=\u0026#34;info\u0026#34; # 日志等级: [debug, info, warn, error, fatal] --log.format=logfmt # 置日志打印target和格式: [logfmt, json] --version # 版本号 --collector.{metric-name} # 各个metric对应的参数 以使用 --collectors.enabled参数指定node_exporter收集的功能模块,或者用--no-collector指定不需要的模块，如果不指定，将使用默认配置。 最重要的参数就是 --collector.，通过该参数可以启用我们收集的功能模块，node_exporter 会默认采集一些模块，要禁用这些默认启用的收集器可以通过 --no-collector. 标志来禁用，如果只启用某些特定的收集器，基于先使用 --collector.disable-defaults 标志禁用所有默认的，然后在通过指定具体的收集器 --collector. 来进行启用。下图列出了默认启用的收集器：\n相关文档地址\n‍\n1.3 编写启动脚本 创建prometheus组和用户\n1 2 sudo groupadd -r prometheus sudo useradd -r -g prometheus -s /sbin/nologin -M -c \u0026#34;prometheus Daemons\u0026#34; prometheus 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 vim /usr/lib/systemd/system/node_exporter.service [Unit] Description=node_exporter After=network.target #可以创建相应的用户和组 启动 User=prometheus Group=prometheus [Service] ExecStart=/usr/local/node_exporter/node_exporter\\ --web.listen-address=:9100\\ --collector.systemd\\ --collector.systemd.unit-whitelist=(sshd|nginx).service\\ --collector.processes\\ --collector.tcpstat [Install] WantedBy=multi-user.target 1.4 启动 node_exporter 1 2 3 systemctl daemon-reload systemctl start process_exporter systemctl enable process_exporter 验证监控数据\n1 curl http://localhost:9100/metrics ​\n二、prometheus 配置 添加或修改配置\n1 2 3 4 5 6 7 8 9 10 - job_name: \u0026#39;dev_prometheus\u0026#39; scrape_interval: 10s honor_labels: true metrics_path: \u0026#39;/metrics\u0026#39; static_configs: - targets: [\u0026#39;127.0.0.1:9090\u0026#39;,\u0026#39;127.0.0.1:9100\u0026#39;] labels: {cluster: \u0026#39;dev\u0026#39;,type: \u0026#39;basic\u0026#39;,env: \u0026#39;dev\u0026#39;,job: \u0026#39;prometheus\u0026#39;,export: \u0026#39;node-exporter\u0026#39;} - targets: [\u0026#39;127.0.0.1:9256\u0026#39;] labels: {cluster: \u0026#39;dev\u0026#39;,type: \u0026#39;process\u0026#39;,env: \u0026#39;dev\u0026#39;,job: \u0026#39;prometheus\u0026#39;,export: \u0026#39;process_exporter\u0026#39;} 重启 prometheus 服务\n1 curl -X POST http://127.0.0.1:9090/-/reload 三、grafana 出图 process-exporter 对应的 dashboard 为：https://grafana.com/grafana/dashboards/16098-1-node-exporter-for-prometheus-dashboard-cn-0417-job/\n效果如下\n四、常用监控规则 Prometheus 告警规则\n参考网站：https://awesome-prometheus-alerts.grep.to/rules\n这个网站上有好多常用软件的告警规则，但是有些并不一定实用，有些使用起来会有错误，这里就把这些都给排除掉，只保留能使用的\n结合文章：https://www.cnblogs.com/sanduzxcvbnm/p/13589792.html 来使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 groups: - name: 主机状态-监控告警 rules: - alert: 主机状态 expr: up == 0 for: 1m labels: status: 非常严重 annotations: summary: \u0026#34;{{$labels.instance}}:服务器宕机\u0026#34; description: \u0026#34;{{$labels.instance}}:服务器延时超过5分钟\u0026#34; - alert: CPU使用情况 expr: 100-(avg(irate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) by(instance)* 100) \u0026gt; 60 for: 1m labels: status: 一般告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} CPU使用率过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }} CPU使用大于60%(目前使用:{{$value}}%)\u0026#34; - alert: 内存使用 expr: 100 -(node_memory_MemTotal_bytes -node_memory_MemFree_bytes+node_memory_Buffers_bytes+node_memory_Cached_bytes ) / node_memory_MemTotal_bytes * 100\u0026gt; 80 for: 1m labels: status: 严重告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} 内存使用率过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }} 内存使用大于80%(目前使用:{{$value}}%)\u0026#34; - alert: IO性能 expr: 100-(avg(irate(node_disk_io_time_seconds_total[1m])) by(instance)* 100) \u0026lt; 60 for: 1m labels: status: 严重告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} 流入磁盘IO使用率过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }} 流入磁盘IO大于60%(目前使用:{{$value}})\u0026#34; - alert: 网络 expr: ((sum(rate (node_network_receive_bytes_total{device!~\u0026#39;tap.*|veth.*|br.*|docker.*|virbr*|lo*\u0026#39;}[5m])) by (instance)) / 100) \u0026gt; 102400 for: 1m labels: status: 严重告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} 流入网络带宽过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }}流入网络带宽持续2分钟高于100M. RX带宽使用率{{$value}}\u0026#34; - alert: 网络 expr: ((sum(rate (node_network_transmit_bytes_total{device!~\u0026#39;tap.*|veth.*|br.*|docker.*|virbr*|lo*\u0026#39;}[5m])) by (instance)) / 100) \u0026gt; 102400 for: 1m labels: status: 严重告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} 流出网络带宽过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }}流出网络带宽持续2分钟高于100M. RX带宽使用率{{$value}}\u0026#34; - alert: TCP会话 expr: node_netstat_Tcp_CurrEstab \u0026gt; 1000 for: 1m labels: status: 严重告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} TCP_ESTABLISHED过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }} TCP_ESTABLISHED大于1000%(目前使用:{{$value}}%)\u0026#34; - alert: 磁盘容量 expr: 100-(node_filesystem_free_bytes{fstype=~\u0026#34;ext4|xfs\u0026#34;}/node_filesystem_size_bytes {fstype=~\u0026#34;ext4|xfs\u0026#34;}*100) \u0026gt; 80 for: 1m labels: status: 严重告警 annotations: summary: \u0026#34;{{$labels.mountpoint}} 磁盘分区使用率过高！\u0026#34; description: \u0026#34;{{$labels.mountpoint }} 磁盘分区使用大于80%(目前使用:{{$value}}%)\u0026#34; 五、Ansible 批量添加部署 这里采用 Consul 注册发现方式，相关类容可以查询网上\n5.1Consul 注册脚本 1 2 3 4 5 6 7 #!/bin/bash service_name=$1 instance_id=$2 ip=$3 port=$4 curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$instance_id\u0026#34;\u0026#39;\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$service_name\u0026#34;\u0026#39;\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$ip\u0026#34;\u0026#39;\u0026#34;,\u0026#34;port\u0026#34;: \u0026#39;\u0026#34;$port\u0026#34;\u0026#39;,\u0026#34;tags\u0026#34;: [\u0026#34;\u0026#39;\u0026#34;$service_name\u0026#34;\u0026#39;\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://\u0026#39;\u0026#34;$ip\u0026#34;\u0026#39;:\u0026#39;\u0026#34;$port\u0026#34;\u0026#39;\u0026#34;,\u0026#34;interval\u0026#34;: \u0026#34;5s\u0026#34;}]}\u0026#39; http://10.1.8.202:8500/v1/agent/service/register 5.2 Ansible 剧本脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@openvpn node]# cat playbook.yml - hosts: Sm remote_user: root gather_facts: no tasks: - name: 推送采集器安装包 unarchive: src=node_exporter-1.2.2.linux-amd64.tar.gz dest=/usr/local/ - name: 重命名 shell: | cd /usr/local/ if [ ! -d node_exporter ];then mv node_exporter-1.2.2.linux-amd64 node_exporter fi - name: 查询主机名称 shell: echo \u0026#34;`hostname`\u0026#34; register: name_host - name: 推送system文件 copy: src=node_exporter.service dest=/usr/lib/systemd/system - name: 启动服务 systemd: name=node_exporter state=started enabled=yes - name: 推送注册脚本 copy: src=consul-register.sh dest=/usr/local/node_exporter - name: 注册当前节点 shell: /bin/sh /usr/local/node_exporter/consul-register.sh {{ group_names[0] }} {{ name_host.stdout }} {{ inventory_hostname }} 9100 ‍\n六、指标自定义监控 6.1 自定义监控 node_exporter的–collector.textfile是一个收集器，这个收集器可以允许我们暴露自定义指标，比如某些pushgateway功能中自定义的指标，就可以使用–collector.textfile功能来实现，而且，node_exporter实现起来更加优雅。用node_expoerter ，直接在现在基础上做textfile collector即可。如果有pushgateway的话，可是使用pushgateway的，也可以使用textfilecollector。\n‍\nnode_exporter的collector.textfile 功能自定义监控，collector.text收集器通过扫描指定目录中的文件，提取所有格式为Prometheus指标的字符串，然后暴露它们以便抓取\n参数：–collector.textfile.directory\n6.2 Textfile Collector使用 因为node_exporter之前已经安装过，如果node_exporter启动时没有指定–collector.textfile.directory参数，需要在启动文件里面，添加上参数，并确认文件指的目录存在。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@openvpn disk]# cat node_exporter.service [Unit] Description=node_exporter After=network.target [Service] ExecStart=/usr/local/node_exporter/node_exporter\\ --web.listen-address=:9100\\ --collector.systemd\\ --collector.systemd.unit-whitelist=(sshd|nginx).service\\ --collector.processes\\ --collector.tcpstat\\ --collector.supervisord\\ --collector.textfile.directory=\u0026#34;/usr/local/node_exporter\u0026#34; #指定以的监控目录 [Install] WantedBy=multi-user.target 6.3 启动 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #启动 # systemd 方式启动 systemctl daemon-reload systemctl enable node_exporter systemctl start node_exporter systemctl status node_exporter # supervisor方式启动 supervisorctl update supervisorctl status supervisorctl start node_exporter supervisorctl restart node_exporter ​ ​ #检查是否启动成功 ss -untlp |grep 9100 ps -ef |grep node_exporter 6.4 写入自定义指标 写入自定义指标，这个需要自己开发脚本，将数据写入对应的文件即可。也可以在github（点我点我…）中看实例文件，熟悉python或者shell都可以写。\nGitHub 中有提供模板的，比如 directory-size.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # cat directory-size.sh #!/bin/sh # # Expose directory usage metrics, passed as an argument. # # Usage: add this to crontab: # # */5 * * * * prometheus directory-size.sh /var/lib/prometheus | sponge /var/lib/node_exporter/directory_size.prom # # sed pattern taken from https://www.robustperception.io/monitoring-directory-sizes-with-the-textfile-collector/ # # Author: Antoine Beaupré \u0026lt;anarcat@debian.org\u0026gt; echo \u0026#34;# HELP node_directory_size_bytes Disk space used by some directories\u0026#34; echo \u0026#34;# TYPE node_directory_size_bytes gauge\u0026#34; du --block-size=1 --summarize \u0026#34;$@\u0026#34; \\ | sed -ne \u0026#39;s/\\\\/\\\\\\\\/;s/\u0026#34;/\\\\\u0026#34;/g;s/^\\([0-9]\\+\\)\\t\\(.*\\)$/node_directory_size_bytes{directory=\u0026#34;\\2\u0026#34;} \\1/p\u0026#39; 此处是我给的简单shell脚本获取数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@openvpn disk]# cat disk_cheak.sh #!/bin/sh system=$(lsblk -fs |grep -w / |awk \u0026#39;{print $1}\u0026#39;) disk=$(lsblk -rfs|grep -E \u0026#39;xfs|f2fs|ntfs\u0026#39;| grep -v $system|awk \u0026#39;{print $5}\u0026#39; |awk -F \u0026#39;%\u0026#39; \u0026#39;{print $1}\u0026#39; |wc -l) theNum=$( df -h |grep /data/plot |wc -l) name=\u0026#34;check_disk_nums\u0026#34; pool_nums=$(tail -n 1 /root/hpool/log/miner.log.log | awk \u0026#39;{print $6}\u0026#39;|cut -d \u0026#39;=\u0026#39; -f 2|awk -F \u0026#39;[ \u0026#34;]\u0026#39; \u0026#39;{print $4,$NF}\u0026#39;|awk \u0026#39;{sub(/^[\\t ]*/,\u0026#34;\u0026#34;);print}\u0026#39;) CPU_sensors=$(sensors | grep \u0026#34;^Package id 0:\u0026#34; | sed \u0026#39;s/Package id 0://\u0026#39; | sed \u0026#39;s/^\\s*.+//\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | sed \u0026#39;s/°C//\u0026#39;) echo \u0026#34;check_cpu_sensors $CPU_sensors\u0026#34; \u0026gt; /usr/local/node_exporter/check_cpu_sensors.prom echo \u0026#34;check_pool_nums $pool_nums\u0026#34; \u0026gt; /usr/local/node_exporter/check_pool_nums.prom if [ $disk != $theNum ]; then echo \u0026#34;$name 0\u0026#34; \u0026gt; /usr/local/node_exporter/check_disk_nums.prom else echo \u0026#34;$name $disk\u0026#34; \u0026gt; /usr/local/node_exporter/check_disk_nums.prom fi 6.5 配置定时任务启动 1 2 3 4 5 chmod a+x disk_cheak.sh crontab -l #Ansible: check_disk */5 * * * * /bin/bash /usr/local/node_exporter/disk_cheak.sh 6.6 ansible批量添加执行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@openvpn disk]# cat playbook.yml - hosts: all remote_user: root gather_facts: no tasks: - name: 推送system文件 copy: src=node_exporter.service dest=/usr/lib/systemd/system - name: 开机启动服务 systemd: name=node_exporter state=restarted enabled=yes - name: 推送磁盘脚本 copy: src=disk_cheak.sh dest=/usr/local/node_exporter mode=u+x - name: 设置定时任务 cron: name=\u0026#34;check_disk\u0026#34; minute=\u0026#34;*/5\u0026#34; job=\u0026#34;/bin/bash /usr/local/node_exporter/disk_cheak.sh\u0026#34; state=\u0026#34;present\u0026#34; - name: 执行脚本 shell: /bin/bash /usr/local/node_exporter/disk_cheak.sh 执行命令 ansible-playbook playbook.yml -i ./hosts1 -f 10 6.7 验证数据 ‍\n","date":"2022-09-22T12:06:49Z","image":"https://www.ownit.top/title_pic/50.jpg","permalink":"https://www.ownit.top/p/202209221206/","title":"Prometheus监控Linux主机（node-exporter）"},{"content":"Prometheus监控进程 process-export主要用来做进程监控，比如某个服务的进程数、消耗了多少CPU、内存等资源。\n一、process-exporter使用 ‍\n1.1 下载 process-exporter process-exporter GibHUB地址\nprocess-exporter 下载地址\nprocess-exporter可以使用命令行参数也可以指定配置文件启动\n1.2 配置 process-exporter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim /usr/local/process-exporter/process_name.yaml #存放脚本的地方 process_names: # - name: \u0026#34;{{.Comm}}\u0026#34; # cmdline: # - \u0026#39;.+\u0026#39; - name: \u0026#34;{{.Matches}}\u0026#34; cmdline: - \u0026#39;nginx\u0026#39; #唯一标识 - name: \u0026#34;{{.Matches}}\u0026#34; cmdline: - \u0026#39;/opt/atlassian/confluence/bin/tomcat-juli.jar\u0026#39; - name: \u0026#34;{{.Matches}}\u0026#34; cmdline: - \u0026#39;vsftpd\u0026#39; - name: \u0026#34;{{.Matches}}\u0026#34; cmdline: - \u0026#39;redis-server\u0026#39; 示例：\ncmdline: 所选进程的唯一标识，ps -ef 可以查询到。如果改进程不存在，则不会有该进程的数据采集到。\n例如：\u0026gt; ps -ef | grep redis\nredis 4287 4127 0 Oct31 ? 00:58:12 redis-server *:6379\n{{.Comm}} groupname=”redis-server” exe或者sh文件名称 {{.ExeBase}} groupname=”redis-server *:6379” / {{.ExeFull}} groupname=”/usr/bin/redis-server *:6379” ps中的进程完成信息 {{.Username}} groupname=”redis” 使用进程所属的用户进行分组 {{.Matches}} groupname=”map[:redis]” 表示配置到关键字“redis” 1.3 编写启动脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 vim /usr/lib/systemd/system/process_exporter.service [Unit] Description=Prometheus exporter for processors metrics, written in Go with pluggable metric collectors. Documentation=https://github.com/ncabatoff/process-exporter After=network.target [Service] Type=simple User=root WorkingDirectory=/usr/local/process-exporter ExecStart=/usr/local/process-exporter/process-exporter -config.path=/usr/local/process-exporter/process-exporter.yaml Restart=on-failure [Install] WantedBy=multi-user.target 1.4 启动 procexx-export 1 2 3 systemctl daemon-reload systemctl start process_exporter systemctl enable process_exporter 验证监控数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 curl http://localhost:9256/metrics #相关测试的数据 # HELP http_response_size_bytes The HTTP response sizes in bytes. # TYPE http_response_size_bytes summary http_response_size_bytes{handler=\u0026#34;prometheus\u0026#34;,quantile=\u0026#34;0.5\u0026#34;} 2988 http_response_size_bytes{handler=\u0026#34;prometheus\u0026#34;,quantile=\u0026#34;0.9\u0026#34;} 2996 http_response_size_bytes{handler=\u0026#34;prometheus\u0026#34;,quantile=\u0026#34;0.99\u0026#34;} 3006 http_response_size_bytes_sum{handler=\u0026#34;prometheus\u0026#34;} 1.34205181e+08 http_response_size_bytes_count{handler=\u0026#34;prometheus\u0026#34;} 45188 # HELP namedprocess_namegroup_context_switches_total Context switches # TYPE namedprocess_namegroup_context_switches_total counter namedprocess_namegroup_context_switches_total{ctxswitchtype=\u0026#34;nonvoluntary\u0026#34;,groupname=\u0026#34;map[:bladebit]\u0026#34;} 7.7977455e+07 namedprocess_namegroup_context_switches_total{ctxswitchtype=\u0026#34;nonvoluntary\u0026#34;,groupname=\u0026#34;map[:pw_python.py]\u0026#34;} 2.02666e+06 namedprocess_namegroup_context_switches_total{ctxswitchtype=\u0026#34;voluntary\u0026#34;,groupname=\u0026#34;map[:bladebit]\u0026#34;} 3.335109e+06 namedprocess_namegroup_context_switches_total{ctxswitchtype=\u0026#34;voluntary\u0026#34;,groupname=\u0026#34;map[:pw_python.py]\u0026#34;} 8.22652233e+08 # HELP namedprocess_namegroup_cpu_system_seconds_total Cpu system usage in seconds # TYPE namedprocess_namegroup_cpu_system_seconds_total counter namedprocess_namegroup_cpu_system_seconds_total{groupname=\u0026#34;map[:bladebit]\u0026#34;} 94275.01000000017 namedprocess_namegroup_cpu_system_seconds_total{groupname=\u0026#34;map[:pw_python.py]\u0026#34;} 64818.93000000004 # HELP namedprocess_namegroup_cpu_user_seconds_total Cpu user usage in seconds # TYPE namedprocess_namegroup_cpu_user_seconds_total counter namedprocess_namegroup_cpu_user_seconds_total{groupname=\u0026#34;map[:bladebit]\u0026#34;} 2.42621264299998e+07 namedprocess_namegroup_cpu_user_seconds_total{groupname=\u0026#34;map[:pw_python.py]\u0026#34;} 85.29000000000613 # HELP namedprocess_namegroup_major_page_faults_total Major page faults # TYPE namedprocess_namegroup_major_page_faults_total counter namedprocess_namegroup_major_page_faults_total{groupname=\u0026#34;map[:bladebit]\u0026#34;} 18261 namedprocess_namegroup_major_page_faults_total{groupname=\u0026#34;map[:pw_python.py]\u0026#34;} 1236 # HELP namedprocess_namegroup_memory_bytes number of bytes of memory in use # TYPE namedprocess_namegroup_memory_bytes gauge namedprocess_namegroup_memory_bytes{groupname=\u0026#34;map[:bladebit]\u0026#34;,memtype=\u0026#34;resident\u0026#34;} 4.46810939392e+11 namedprocess_namegroup_memory_bytes{groupname=\u0026#34;map[:bladebit]\u0026#34;,memtype=\u0026#34;swapped\u0026#34;} 0 namedprocess_namegroup_memory_bytes{groupname=\u0026#34;map[:bladebit]\u0026#34;,memtype=\u0026#34;virtual\u0026#34;} 4.47847292928e+11 namedprocess_namegroup_memory_bytes{groupname=\u0026#34;map[:pw_python.py]\u0026#34;,memtype=\u0026#34;resident\u0026#34;} 1.2959744e+07 namedprocess_namegroup_memory_bytes{groupname=\u0026#34;map[:pw_python.py]\u0026#34;,memtype=\u0026#34;swapped\u0026#34;} 0 namedprocess_namegroup_memory_bytes{groupname=\u0026#34;map[:pw_python.py]\u0026#34;,memtype=\u0026#34;virtual\u0026#34;} 2.4733696e+08 ‍\n二、prometheus 配置 添加或修改配置\n1 2 3 4 5 6 7 8 9 10 - job_name: \u0026#39;dev_prometheus\u0026#39; scrape_interval: 10s honor_labels: true metrics_path: \u0026#39;/metrics\u0026#39; static_configs: - targets: [\u0026#39;127.0.0.1:9090\u0026#39;,\u0026#39;127.0.0.1:9100\u0026#39;] labels: {cluster: \u0026#39;dev\u0026#39;,type: \u0026#39;basic\u0026#39;,env: \u0026#39;dev\u0026#39;,job: \u0026#39;prometheus\u0026#39;,export: \u0026#39;prometheus\u0026#39;} - targets: [\u0026#39;127.0.0.1:9256\u0026#39;] labels: {cluster: \u0026#39;dev\u0026#39;,type: \u0026#39;process\u0026#39;,env: \u0026#39;dev\u0026#39;,job: \u0026#39;prometheus\u0026#39;,export: \u0026#39;process_exporter\u0026#39;} 重启prometheus服务\n1 curl -X POST http://127.0.0.1:9090/-/reload 三、grafana出图 process-exporter对应的dashboard为：https://grafana.com/grafana/dashboards/249\n效果如下\n四、常用监控规则 进程数 1 2 3 4 5 6 7 alert: 进程告警 expr: sum(namedprocess_namegroup_states) by (cluster,job,instance) \u0026gt; 500 for: 20s labels: severity: warning annotations: value: 服务器当前已产生 {{ $value }} 个进程，大于告警阈值 僵尸进程数 1 2 3 4 5 6 7 alert: 进程告警 expr: sum by(cluster, job, instance, groupname) (namedprocess_namegroup_states{state=\u0026#34;Zombie\u0026#34;}) \u0026gt; 0 for: 1m labels: severity: warning annotations: value: 当前产生 {{ $value }} 个僵尸进程 进程重启 1 2 3 4 5 6 7 8 alert: 进程重启告警 expr: ceil(time() - max by(cluster, job, instance, groupname) (namedprocess_namegroup_oldest_start_time_seconds)) \u0026lt; 60 for: 25s labels: label: alert_once severity: warning annotations: value: 进程 {{ $labels.groupname }} 在 {{ $value }} 秒前发生重启 进程退出 1 2 3 4 5 6 7 alert: 进程退出告警 expr: up{export=\u0026#34;process_exporter\u0026#34;} == 0 or max by(cluster, job, instance, groupname) (delta(namedprocess_namegroup_oldest_start_time_seconds{groupname=~\u0026#34;^map.*\u0026#34;}[10d])) \u0026lt; 0 for: 55s labels: severity: warning annotations: value: 进程 {{ $labels.export}} 已退出 五、Ansible批量添加 这里采用Consul注册发现方式，相关类容可以查询网上\n5.1Consul注册脚本 1 2 3 4 5 6 7 #!/bin/bash service_name=$1 instance_id=$2 ip=$3 port=$4 curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$instance_id\u0026#34;\u0026#39;\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$service_name\u0026#34;\u0026#39;\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$ip\u0026#34;\u0026#39;\u0026#34;,\u0026#34;port\u0026#34;: \u0026#39;\u0026#34;$port\u0026#34;\u0026#39;,\u0026#34;tags\u0026#34;: [\u0026#34;\u0026#39;\u0026#34;$service_name\u0026#34;\u0026#39;\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://\u0026#39;\u0026#34;$ip\u0026#34;\u0026#39;:\u0026#39;\u0026#34;$port\u0026#34;\u0026#39;\u0026#34;,\u0026#34;interval\u0026#34;: \u0026#34;5s\u0026#34;}]}\u0026#39; http://10.1.8.202:8500/v1/agent/service/register Ansible剧本脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@openvpn process]# cat playbook.yml - hosts: Harvester remote_user: root gather_facts: no tasks: - name: 推送采集器安装包 unarchive: src=process-exporter.tar.gz dest=/usr/local/ - name: 重命名 shell: | cd /usr/local/ if [ ! -d process-exporter ];then mv process-exporter-0.4.0.linux-amd64 process-exporter fi - name: 查询主机名称 shell: echo \u0026#34;h-`hostname`\u0026#34; register: name_host - name: 推送system文件 copy: src=process_exporter.service dest=/usr/lib/systemd/system - name: 启动服务 systemd: name=process_exporter state=started enabled=yes - name: 推送注册脚本 copy: src=consul-register.sh dest=/usr/local/process-exporter - name: 注册当前节点 shell: /bin/sh /usr/local/process-exporter/consul-register.sh {{ group_names[0] }} {{ name_host.stdout }} {{ inventory_hostname }} 9256 ‍\n","date":"2022-09-21T14:37:01Z","image":"https://www.ownit.top/title_pic/33.jpg","permalink":"https://www.ownit.top/p/202209211437/","title":"Prometheus监控进程"},{"content":"监控环境：Prometheus\n数据库：MongoDB 3.4.6 集群，3个节点\n监控工具：mongodb_exporter\n1、创建Mongodb监控可读账号 1 2 3 4 5 6 7 8 9 10 11 12 mongodb admin 库中执行 use admin db.createUser({ user: \u0026#34;prometheus\u0026#34;, pwd: \u0026#34;prometheus\u0026#34;, roles: [ { role: \u0026#34;read\u0026#34;, db: \u0026#34;admin\u0026#34; }, { role: \u0026#34;readAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34; }, { role: \u0026#34;clusterMonitor\u0026#34;, db: \u0026#34;admin\u0026#34; } ] }); 2、下载MongoDB监控软件 1 地址： https://github.com/percona/mongodb_exporter 我使用的：mongodb_exporter-0.11.2.linux-amd64.tar.gz 版本\n3、配置文件 1 nohup ./mongodb_exporter --mongodb.uri mongodb://root:heian@192.168.82.105:27017/admin --collect.database --collect.collection --collect.topmetrics --collect.indexusage --collect.connpoolstats --suppress.collectshardingstatus \u0026amp; mongodb_exporter暴露的endpoint端口默认为9216\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [root@prometheus-server exporter]# ./mongodb_exporter --help usage: mongodb_exporter [\u0026lt;flags\u0026gt;] exports various MongoDB metrics in Prometheus format. Flags: -h, --help Show context-sensitive help (also try --help-long and --help-man). --web.auth-file=WEB.AUTH-FILE Path to YAML file with server_user, server_password keys for HTTP Basic authentication (overrides HTTP_AUTH environment variable). --web.ssl-cert-file=WEB.SSL-CERT-FILE Path to SSL certificate file. --web.ssl-key-file=WEB.SSL-KEY-FILE Path to SSL key file. --web.listen-address=\u0026#34;:9216\u0026#34; Address to listen on for web interface and telemetry. --web.telemetry-path=\u0026#34;/metrics\u0026#34; Path under which to expose metrics. --collect.database Enable collection of Database metrics --collect.collection Enable collection of Collection metrics --collect.topmetrics Enable collection of table top metrics --collect.indexusage Enable collection of per index usage stats --collect.connpoolstats Collect MongoDB connpoolstats --suppress.collectshardingstatus Suppress the collection of Sharding Status --mongodb.uri=[mongodb://][user:pass@]host1[:port1][,host2[:port2],...][/database][?options] MongoDB URI, format --test Check MongoDB connection, print buildInfo() information and exit. --version Show application version. --log.level=\u0026#34;info\u0026#34; Only log messages with the given severity or above. Valid levels: [debug, info, warn, error, fatal] --log.format=\u0026#34;logger:stderr\u0026#34; Set the log target and format. Example: \u0026#34;logger:syslog?appname=bob\u0026amp;local=7\u0026#34; or \u0026#34;logger:stdout?json=true\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 配置启动服务 vim /usr/lib/systemd/system/mongodb_exporter.service [Unit] Description=mongodb_exporter Documentation=https://github.com/percona/mongodb_exporter After=network.target [Service] Type=simple User=prometheus Environment=\u0026#34;MONGODB_URI=mongodb://mongodb_exporter:123456@localhost:27017\u0026#34; ExecStart=/usr/local/bin/mongodb_exporter --log.level=error \\ --collect.database \\ --collect.collection \\ --collect.topmetrics \\ --collect.indexusage \\ --collect.connpoolstats Restart=on-failure [Install] WantedBy=multi-user.target 4、prometheus配置基于文件的自动发现 1 2 3 4 5 6 7 8 9 10 11 12 13 - job_name: \u0026#39;mongo_cluster\u0026#39; file_sd_configs: - files: [\u0026#39;/usr/local/prometheus/sd_config/mongo_cluster.yaml\u0026#39;] refresh_interval: 5s root:/usr/local/prometheus# cat /usr/local/prometheus/sd_config/mongo_cluster.yaml - targets: - \u0026#34;192.168.88.140:9216\u0026#34; - \u0026#34;192.168.88.141:9216\u0026#34; - \u0026#34;192.168.88.142:9216\u0026#34; labels: project: mongo unitname: \u0026#34;Mongodb_exporter\u0026#34; service: mongo 5、grafana配置mongo展示图 导入图：16974\nMongoDB信息 | Grafana Labs （这个模板是自己绘制的，有基础的可以二次开发）\n","date":"2022-09-16T10:07:26Z","image":"https://www.ownit.top/title_pic/69.jpg","permalink":"https://www.ownit.top/p/202209161007/","title":"Prometheus监控MongoDB数据库"},{"content":"因为业务需求，需要连接数据库查询数据\n数据库类型：mysql，mongodb\n需求：有中连机制，读取配置文件，可实例化，有日志记录\n配置文件 dbconfig.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Mongodbtest] host=192.168.99.42 port=27018 user= password= database=ace_sms [mysql] host=192.168.99.42 port=27018 user= password= database=ace_sms 日志类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # -*- coding: utf-8 -*- # @Time : 2022/7/12 18:13 # @File : Loggers.py # @Software: PyCharm # 加入日志 # 获取logger实例 import logging import os import sys logger = logging.getLogger(\u0026#34;baseSpider\u0026#34;) # 指定输出格式 formatter = logging.Formatter(\u0026#39;%(asctime)s\\ %(filename)s-%(lineno)d\\ %(levelname)s\\ %(message)s\u0026#39;) # 文件日志 # 获取当前文件路径 current_path = os.path.abspath(os.path.dirname(__file__)) father_path = os.path.abspath(os.path.dirname(current_path) + os.path.sep + \u0026#34;.\u0026#34;) log_file_path = os.path.join(father_path + \u0026#34;\\logs\\dingding_message.log\u0026#34;) file_handler = logging.FileHandler(log_file_path) file_handler.setFormatter(formatter) # 控制台日志 console_handler = logging.StreamHandler(sys.stdout) console_handler.setFormatter(formatter) # 为logge添加具体的日志处理器 logger.addHandler(file_handler) logger.addHandler(console_handler) logger.setLevel(logging.INFO) mysql连接类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 # -*- coding: utf-8 -*- # @Time : 2022-07-08 21:32 # @File : MyDB.py # @Software: PyCharm import time import pymysql import configparser import logging import sys from tool.Loggers import logger class mysql: def __init__(self, config_file, db): \u0026#34;\u0026#34;\u0026#34; :param config_file: :param db: \u0026#34;\u0026#34;\u0026#34; # 实例化configparser config = configparser.ConfigParser() # 从配置文件中读取数据库的相关信息 config.read(config_file, encoding=\u0026#39;utf-8\u0026#39;) self.host = config[db][\u0026#39;host\u0026#39;] self.port = int(config[db][\u0026#39;port\u0026#39;]) self.user = config[db][\u0026#39;user\u0026#39;] self.password = config[db][\u0026#39;password\u0026#39;] self.database = config[db][\u0026#39;database\u0026#39;] self.db = db self.conn = None self._conn() def _conn(self): try: logger.info(f\u0026#34;读取环境：{self.db}，连接信息：主机ip：{self.host},端口：{self.port},用户：{self.user},连接数据库：{self.database}\u0026#34;) self.conn = pymysql.Connection(host=self.host, user=self.user, password=self.password, database=self.database, port=self.port) logger.info(f\u0026#34;数据库: {self.database}初始化连接成功\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;数据库: {self.database}初始化连接失败，错误：{e}\u0026#34;) return False def close(self): self.conn.close() logger.info(f\u0026#34;数据库关闭成功\u0026#34;) def _reConn(self, num=28800, stime=3): # 重试连接总次数为1天,这里根据实际情况自己设置,如果服务器宕机1天都没发现就...... _number = 0 _status = True logger.info(f\u0026#34;检查数据库{self.database}连通性,连接IP：{self.host}\u0026#34;) while _status and _number \u0026lt;= num: try: self.conn.ping() # cping 校验连接是否异常 _status = False logger.info(f\u0026#34;数据库{self.database}连接============正常,连接IP：{self.host} \u0026#34;) except: if self._conn() == True: # 重新连接,成功退出 _status = False break _number += 1 logger.info(f\u0026#34;数据库{self.database}连接============失败,连接IP：{self.host} \u0026#34;) time.sleep(stime) # 连接不成功,休眠3秒钟,继续循环，知道成功或重试次数结束 def select(self, sql=\u0026#39;\u0026#39;): try: self._reConn() logger.info(\u0026#39;查询的语句:%s\u0026#39; % sql) # 建立游标 db_cursor = self.conn.cursor() db_cursor.execute(sql) result = db_cursor.fetchall() # 返回值和数据表字段组成json格式 lists = [] t = 0 for x in result: i = 0 onelist = {} for field in db_cursor.description: onelist[field[0]] = x[i] i = i + 1 lists.append(onelist) logger.info(\u0026#39;组合数据:%s\u0026#39; % lists) return lists # return result except pymysql.Error as e: logger.error(\u0026#39;数据库查询数据失败：%s\u0026#39; % e) return False def select_limit(self, sql=\u0026#39;\u0026#39;, offset=0, length=20): sql = \u0026#39;%s limit %d , %d ;\u0026#39; % (sql, offset, length) return self.select(sql) # # 插入 # def execute_insert(self, query): # print(\u0026#39;query:%s\u0026#39; % query) # try: # # 建立游标 # db_cursor = self.dbconn.cursor() # db_cursor.execute(query) # db_cursor.execute(\u0026#39;commit\u0026#39;) # return True # except Exception as e: # print(\u0026#39;数据库插入数据失败：%s\u0026#39; % e) # # 事务回滚 # db_cursor.execute(\u0026#39;rollback\u0026#39;) # db_cursor.close() # exit() MongoDB连接类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 # -*- coding: utf-8 -*- # @Time : 2022/7/12 10:44 # @File : MongoDB.py # @Software: PyCharm import configparser import time import pymongo from alarm.tool.Loggers import logger class mongo(object): def __init__(self, config_file, db): \u0026#34;\u0026#34;\u0026#34; :param config_file: 配置文件路径 :param db: 获取配置的环境 \u0026#34;\u0026#34;\u0026#34; # 实例化configparser config = configparser.ConfigParser() # 从配置文件中读取数据库的相关信息 config.read(config_file, encoding=\u0026#39;utf-8\u0026#39;) self.host = config[db][\u0026#39;host\u0026#39;] self.port = int(config[db][\u0026#39;port\u0026#39;]) self.user = config[db][\u0026#39;user\u0026#39;] self.password = config[db][\u0026#39;password\u0026#39;] self.database = config[db][\u0026#39;database\u0026#39;] self.db = db self.conn = None self._conn() def _conn(self): try: logger.info(f\u0026#34;读取环境：{self.db}，连接信息：主机ip：{self.host},端口：{self.port},用户：{self.user},连接数据库：{self.database}\u0026#34;) # self.conn = pymongo.MongoClient(host=self.host, # port=self.port) self.conn = pymongo.MongoClient(host=self.host, port=self.port) # username=self.user, password=self.password # self.db_conn = self.conn[self.database] # self.db_conn=self.db_conn.authenticate(self.user,self.password) self.conn[self.database].authenticate(self.user, self.password, self.database) self.db_conn = self.conn[self.database] if self.conn.server_info(): logger.info(f\u0026#34;数据库: {self.database}初始化连接成功\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;数据库: {self.database}初始化连接失败，错误：{e}\u0026#34;) return False # MongoDB数据库关闭 def close(self): self.conn.close() logger.info(f\u0026#34;数据库关闭成功\u0026#34;) # 查询调用状态 def get_state(self): return self.conn is not None # and self.db_conn is not None def _reConn(self, num=28800, stime=3): # 重试连接总次数为1天,这里根据实际情况自己设置,如果服务器宕机1天都没发现就...... _number = 0 _status = True logger.info(f\u0026#34;检查数据库{self.database}连通性,连接IP：{self.host}\u0026#34;) while _status and _number \u0026lt;= num: try: self.conn.server_info() # 检查数据库是否正常连通 _status = False logger.info(f\u0026#34;数据库{self.database}连接============正常,连接IP：{self.host} \u0026#34;) except: if self._conn() == True: # 重新连接,成功退出 _status = False break _number += 1 logger.info(f\u0026#34;数据库{self.database}连接============失败,连接IP：{self.host} \u0026#34;) time.sleep(stime) # 连接不成功,休眠3秒钟,继续循环，知道成功或重试次数结束 def insert_one(self, collection, data): self._reConn() if self.get_state(): ret = self.db_conn[collection].insert_one(data) return ret.inserted_id else: return \u0026#34;\u0026#34; def insert_many(self, collection, data): if self.get_state(): ret = self.db_conn[collection].insert_many(data) return ret.inserted_id else: return \u0026#34;\u0026#34; def update(self, collection, data): # data format: # {key:[old_data,new_data]} data_filter = {} data_revised = {} for key in data.keys(): data_filter[key] = data[key][0] data_revised[key] = data[key][1] if self.get_state(): return self.db_conn[collection].update_many(data_filter, {\u0026#34;$set\u0026#34;: data_revised}).modified_count return 0 def find(self, col, condition, column=None): \u0026#34;\u0026#34;\u0026#34; 查询数据代码 :param col: 数据库中的集合 :param condition: 查询条件,查询条件必须是个字典 :param column: find 的第二个参数是可选的，可以指定需要返回的键。这个特别的 \u0026#34;$slice\u0026#34; 运算符可以返回一个数组键中元素的子集。 :return: list 返回查询到记录的列表 \u0026#34;\u0026#34;\u0026#34; # print(col, condition) # data= self.db_conn[\u0026#34;sms_log\u0026#34;] # data=self.db_conn[\u0026#34;sms_log\u0026#34;].find({\u0026#34;status\u0026#34;:\u0026#34;2\u0026#34;,\u0026#34;createTime\u0026#34;:{\u0026#34;$gte\u0026#34;: \u0026#34;2022/07/12 22:18:26\u0026#34;}},{\u0026#34;status\u0026#34;:1,\u0026#34;channelCode\u0026#34;:1,\u0026#34;_id\u0026#34;:0}) # data = self.db_conn[\u0026#34;authCode\u0026#34;].find({\u0026#34;use\u0026#34;: False,\u0026#34;createdTime\u0026#34;: {\u0026#34;$gte\u0026#34;: 1657865035}}) # print(list(data)) self._reConn() if self.get_state(): if column is None: return list(self.db_conn[col].find(condition)) else: return list(self.db_conn[col].find(condition, column)) else: return None def get_last_data(self, col, number=1): if self.get_state(): # last_data = list(self.db_conn[\u0026#34;authCode\u0026#34;].find().sort(\u0026#34;_id\u0026#34;, -1 ).limit(50)) last_data = list(self.db_conn[col].find().sort(\u0026#34;_id\u0026#34;, -1).limit(number)) return last_data def delete(self, col, condition): if self.get_state(): return self.db_conn[col].delete_many(filter=condition).deleted_count return 0 def aggregate(self, col, condition): if self.get_state(): return list(self.db_conn[col].aggregate(condition)) # 时间戳转换时间 def timestamp_to_time(timestamp): timeArray = time.localtime(timestamp) # 转换为可用的时间，就是下面的%Y %m %d # day_time = time.strftime(\u0026#34;%Y-%m-%d\u0026#34;, timeArray) # 取上面的timeArray中的对应值0 second_time = time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;, timeArray) # 这个一样的 return second_time # 返回相应的值 # 时间转换时间戳 def time_to_timestamp(time_str): # 转换成时间数组 timeArray = time.strptime(time_str, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) # 转换成时间戳 timestamp = time.mktime(timeArray) # print(timestamp) return timestamp if __name__ == \u0026#39;__main__\u0026#39;: logger.info(\u0026#34;开始实例化数据库对象 \u0026#34;) config_file = \u0026#39;../config/dbconfig.conf\u0026#39; db = \u0026#39;Devfubaodai\u0026#39; db = mongo(config_file, db) # 获取时间 data_time = \u0026#39;2022-07-18 00:00:00\u0026#39; timestamp = time_to_timestamp(data_time) # print(timestamp) data = db.get_last_data(\u0026#34;authCode\u0026#34;, 1) last_time_data = data[0][\u0026#34;createdTime\u0026#34;] satrt_time_data = data[0][\u0026#34;createdTime\u0026#34;]-600 print(timestamp_to_time(last_time_data)) print(timestamp_to_time(satrt_time_data)) data = db.find(\u0026#34;authCode\u0026#34;, {\u0026#34;createdTime\u0026#34;: {\u0026#34;$gte\u0026#34;: satrt_time_data, \u0026#34;$lte\u0026#34;: last_time_data}}, {\u0026#34;name\u0026#34;: 1, \u0026#34;use\u0026#34;: 1, \u0026#34;_id\u0026#34;: 0}) num, fail = 0, 0 for i in data: num += 1 print(i) print(num) # logger.info(f\u0026#34;短信---最新50条数据，成功使用数量：{num}，未使用的数量：{fail}\u0026#34;) db.close() logger.info(\u0026#34;--------------------------------------------------------------------------------------------------\u0026#34;) mian类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def main(): #mysqld_test() schedule.every(1).minutes.do(mysqld_test) while True: schedule.run_pending() time.sleep(1) if __name__ == \u0026#39;__main__\u0026#39;: res = os.path.abspath(__file__) # 获取当前文件的绝对路径 print(res) base_path = os.path.dirname(os.path.dirname(res)) # 获取当前文件的上两级目录 print(base_path) base_path2 = os.path.dirname(res) print(base_path2) sys.path.append(base_path2) sys.path.append(base_path) sys.path.insert(0, base_path) # 加入环境变量 # 以上5行代码必须要加入到文件的最上方 print(sys.path) main() ","date":"2022-07-25T17:44:50Z","image":"https://www.ownit.top/title_pic/19.jpg","permalink":"https://www.ownit.top/p/202207251744/","title":"Python开发mysql和mongo 连接类"},{"content":"\nAlertmanager告警 系统整合 流程 （1）监控端，可以使用Python 或者 shell 进行监控，把 相关的json数据推送到Alertmanager\n（2）Alertmanager端 进行 汇总，发送，后续可以静默，抑制等功能\n（3）把告警的数据发送到prometheusalert，进行钉钉的发送\n环境 软件\nprometheusalert 告警系统 ：主要是数据的各个渠道发送\nAlertmanager 告警处理：主要汇总数据，抑制，静默\n思路 编写告警测试数据时，我们可以不用生成时间，仅生成相关的告警指标，时间程序会帮忙生成（待测试）\n步骤 获取Alertmanager数据格式 方法，通过flask 编写接口，Alertmanager配置，告警时回把数据发送到此接口 ，进行展示查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from flask import Flask, request import json app = Flask(__name__) @app.route(\u0026#34;/send/\u0026#34;, methods=[\u0026#34;POST\u0026#34;]) def send(): try: data = json.loads(request.data) print(data) alerts = data[\u0026#39;alerts\u0026#39;] for i in alerts: print(\u0026#39;SEND SMS: \u0026#39; + str(i)) except Exception as e: print(e) return \u0026#39;ok\u0026#39; if __name__ == \u0026#39;__main__\u0026#39;: app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=8082) 在Alertmanager配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@prometheus-server alertmanager-0.24.0.linux-amd64]# cat alertmanager.yml global: resolve_timeout: 5m route: group_by: [\u0026#39;instance\u0026#39;] group_wait: 30s group_interval: 10s repeat_interval: 10m receiver: \u0026#39;web.hook.prometheusalert\u0026#39; receivers: - name: \u0026#39;web.hook.prometheusalert\u0026#39; webhook_configs: - url: http://192.168.96.19:8082/send/ 产生prometheus的测试数据\n关闭客户端，产生告警 告警数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 { \u0026#39;receiver\u0026#39;: \u0026#39;web\\\\.hook\\\\.prometheusalert\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;firing\u0026#39;, \u0026#39;alerts\u0026#39;: [{ \u0026#39;status\u0026#39;: \u0026#39;firing\u0026#39;, \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;startsAt\u0026#39;: \u0026#39;2022-07-05T05:54:37.452Z\u0026#39;, \u0026#39;endsAt\u0026#39;: \u0026#39;0001-01-01T00:00:00Z\u0026#39;, \u0026#39;generatorURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9090/graph?g0.expr=up+%3D%3D+0\u0026amp;g0.tab=1\u0026#39;, \u0026#39;fingerprint\u0026#39;: \u0026#39;4a890f7c225c3bef\u0026#39; }], \u0026#39;groupLabels\u0026#39;: { \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39; }, \u0026#39;commonLabels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;commonAnnotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;externalURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9093\u0026#39;, \u0026#39;version\u0026#39;: \u0026#39;4\u0026#39;, \u0026#39;groupKey\u0026#39;: \u0026#39;{}:{instance=\u0026#34;192.168.82.105:9100\u0026#34;}\u0026#39;, \u0026#39;truncatedAlerts\u0026#39;: 0 } 有用的告警数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#39;status\u0026#39;: \u0026#39;firing\u0026#39;, \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;startsAt\u0026#39;: \u0026#39;2022-07-05T05:54:37.452Z\u0026#39;, \u0026#39;endsAt\u0026#39;: \u0026#39;0001-01-01T00:00:00Z\u0026#39;, \u0026#39;generatorURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9090/graph?g0.expr=up+%3D%3D+0\u0026amp;g0.tab=1\u0026#39;, \u0026#39;fingerprint\u0026#39;: \u0026#39;4a890f7c225c3bef\u0026#39; } 恢复数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 { \u0026#39;receiver\u0026#39;: \u0026#39;web\\\\.hook\\\\.prometheusalert\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;resolved\u0026#39;, \u0026#39;alerts\u0026#39;: [{ \u0026#39;status\u0026#39;: \u0026#39;resolved\u0026#39;, \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;startsAt\u0026#39;: \u0026#39;2022-07-05T05:54:37.452Z\u0026#39;, \u0026#39;endsAt\u0026#39;: \u0026#39;2022-07-05T05:56:52.452Z\u0026#39;, \u0026#39;generatorURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9090/graph?g0.expr=up+%3D%3D+0\u0026amp;g0.tab=1\u0026#39;, \u0026#39;fingerprint\u0026#39;: \u0026#39;4a890f7c225c3bef\u0026#39; }], \u0026#39;groupLabels\u0026#39;: { \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39; }, \u0026#39;commonLabels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;commonAnnotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;externalURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9093\u0026#39;, \u0026#39;version\u0026#39;: \u0026#39;4\u0026#39;, \u0026#39;groupKey\u0026#39;: \u0026#39;{}:{instance=\u0026#34;192.168.82.105:9100\u0026#34;}\u0026#39;, \u0026#39;truncatedAlerts\u0026#39;: 0 } 有用的恢复数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#39;status\u0026#39;: \u0026#39;resolved\u0026#39;, \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;startsAt\u0026#39;: \u0026#39;2022-07-05T05:54:37.452Z\u0026#39;, \u0026#39;endsAt\u0026#39;: \u0026#39;2022-07-05T05:56:52.452Z\u0026#39;, \u0026#39;generatorURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9090/graph?g0.expr=up+%3D%3D+0\u0026amp;g0.tab=1\u0026#39;, \u0026#39;fingerprint\u0026#39;: \u0026#39;4a890f7c225c3bef\u0026#39; } Python发送告警数据 发送告警信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import json import requests new_packinf = [{ \u0026#39;status\u0026#39;: \u0026#39;firing\u0026#39;, \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风小贷-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;startsAt\u0026#39;: \u0026#39;2022-07-05T05:54:37.452Z\u0026#39;, \u0026#39;endsAt\u0026#39;: \u0026#39;0001-01-01T00:00:00Z\u0026#39;, \u0026#39;generatorURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9090/graph?g0.expr=up+%3D%3D+0\u0026amp;g0.tab=1\u0026#39;, \u0026#39;fingerprint\u0026#39;: \u0026#39;4a890f7c225c3bef\u0026#39; }] jsons = json.dumps(new_packinf) url = \u0026#34;http://192.168.82.105:9093/api/v2/alerts\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} responses = requests.post(url=url, headers=headers, data=jsons) print(responses.status_code, responses.url) # print(responses.json()) 发送恢复告警信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import json import requests new_packinf = [{ \u0026#39;status\u0026#39;: \u0026#39;resolved\u0026#39;, \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;乘风-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;startsAt\u0026#39;: \u0026#39;2022-07-05T05:54:37.452Z\u0026#39;, \u0026#39;endsAt\u0026#39;: \u0026#39;2022-07-05T05:56:52.452Z\u0026#39;, \u0026#39;generatorURL\u0026#39;: \u0026#39;http://prometheus-server.0101101300.fjf:9090/graph?g0.expr=up+%3D%3D+0\u0026amp;g0.tab=1\u0026#39;, \u0026#39;fingerprint\u0026#39;: \u0026#39;4a890f7c225c3bef\u0026#39; }] jsons = json.dumps(new_packinf) url = \u0026#34;http://192.168.82.105:9093/api/v2/alerts\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} responses = requests.post(url=url, headers=headers, data=jsons) print(responses.status_code, responses.url) # print(responses.json()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import json import requests import time year_time = time.strftime(\u0026#39;%Y-%m-%d\u0026#39;, time.localtime()) now_time = time.strftime(\u0026#39;%H:%M:%S\u0026#39;, time.localtime()) start_time = year_time + \u0026#34;T\u0026#34; + now_time + \u0026#34;.000+08:00\u0026#34; is_end_time = False if is_end_time: print(\u0026#34;告警未解决\u0026#34;) end_time = \u0026#39;0001-01-01T00:00:00Z\u0026#39; else: print(\u0026#34;告警已经解决\u0026#34;) end_time = start_time new_packinf = [{ \u0026#39;labels\u0026#39;: { \u0026#39;alertname\u0026#39;: \u0026#39;主机存活状态警告！\u0026#39;, \u0026#39;cloud\u0026#39;: \u0026#39;小贷-Dev环境\u0026#39;, \u0026#39;instance\u0026#39;: \u0026#39;192.168.82.105:9100\u0026#39;, \u0026#39;job\u0026#39;: \u0026#39;node_exporter\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;非常严重\u0026#39;, \u0026#39;team\u0026#39;: \u0026#39;ops\u0026#39; }, \u0026#39;annotations\u0026#39;: { \u0026#39;description\u0026#39;: \u0026#39;192.168.82.105:9100:服务器延时超过5分钟\u0026#39;, \u0026#39;summary\u0026#39;: \u0026#39;192.168.82.105:9100:服务器宕机\u0026#39; }, \u0026#39;endsAt\u0026#39;: end_time, }] jsons = json.dumps(new_packinf) url = \u0026#34;http://192.168.82.105:9093/api/v2/alerts\u0026#34; headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} responses = requests.post(url=url, headers=headers, data=jsons) print(responses.status_code, responses.url) # print(responses.json()) 钉钉告警模板 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 {{ $var := .externalURL}}{{ range $k,$v:=.alerts }} {{if eq $v.status \u0026#34;resolved\u0026#34;}} #### [Prometheus恢复信息]({{$v.generatorURL}}) ##### \u0026lt;font color=\u0026#34;#02b340\u0026#34;\u0026gt;告警名称\u0026lt;/font\u0026gt;：[{{$v.labels.alertname}}]({{$var}}) ##### \u0026lt;font color=\u0026#34;#02b340\u0026#34;\u0026gt;告警级别\u0026lt;/font\u0026gt;：{{$v.labels.severity}} ##### \u0026lt;font color=\u0026#34;#02b340\u0026#34;\u0026gt;开始时间\u0026lt;/font\u0026gt;：{{TimeFormat $v.startsAt \u0026#34;2006-01-02 15:04:05\u0026#34;}} ##### \u0026lt;font color=\u0026#34;#02b340\u0026#34;\u0026gt;结束时间\u0026lt;/font\u0026gt;：{{TimeFormat $v.endsAt \u0026#34;2006-01-02 15:04:05\u0026#34;}} ##### \u0026lt;font color=\u0026#34;#02b340\u0026#34;\u0026gt;实例地址\u0026lt;/font\u0026gt;：{{$v.labels.instance}} ##### \u0026lt;font color=\u0026#34;#02b340\u0026#34;\u0026gt;主机名称\u0026lt;/font\u0026gt;：{{$v.labels.hostname}} **{{$v.annotations.description}}** {{else}} #### [Prometheus告警信息]({{$v.generatorURL}}) ##### \u0026lt;font color=\u0026#34;#FF0000\u0026#34;\u0026gt;告警名称\u0026lt;/font\u0026gt;：[{{$v.labels.alertname}}]({{$var}}) ##### \u0026lt;font color=\u0026#34;#FF0000\u0026#34;\u0026gt;告警级别\u0026lt;/font\u0026gt;：{{$v.labels.severity}} ##### \u0026lt;font color=\u0026#34;#FF0000\u0026#34;\u0026gt;开始时间\u0026lt;/font\u0026gt;：{{ TimeFormat $v.startsAt \u0026#34;2006-01-02 15:04:05\u0026#34;}} ##### \u0026lt;font color=\u0026#34;#FF0000\u0026#34;\u0026gt;结束时间\u0026lt;/font\u0026gt;：{{TimeFormat $v.endsAt \u0026#34;2006-01-02 15:04:05\u0026#34;}} ##### \u0026lt;font color=\u0026#34;#FF0000\u0026#34;\u0026gt;实例地址\u0026lt;/font\u0026gt;：{{$v.labels.instance}} ##### \u0026lt;font color=\u0026#34;#FF0000\u0026#34;\u0026gt;主机名称\u0026lt;/font\u0026gt;：{{$v.labels.hostname}} **{{$v.annotations.description}}** {{end}} {{ end }} {{ $urimsg:=\u0026#34;\u0026#34;}}{{ range $key,$value:=.commonLabels }}{{$urimsg = print $urimsg $key \u0026#34;%3D%22\u0026#34; $value \u0026#34;%22%2C\u0026#34; }}{{end}}[*** 点我屏蔽该告警]({{$var}}/#/silences/new?filter=%7B{{SplitString $urimsg 0 -3}}%7D) ","date":"2022-07-05T14:28:36Z","image":"https://www.ownit.top/title_pic/75.jpg","permalink":"https://www.ownit.top/p/202207051428/","title":"Python编写告警信息，整合Alertmanager告警"},{"content":"Confluence目前版本 Confluence的授权信息\nConfluence 6.9.3\n版权声明； 2003 - 2018 Atlassian 股份有限公司\nConfluence最新版本 Confluence 7.13.7\n地址：https://www.atlassian.com/zh/software/confluence/download-archives\n升级版本选择 原 Confluence 6.9.3 地址下载：https://product-downloads.atlassian.com/software/confluence/do\nwnloads/atlassian-confluence-6.9.3-x64.bin\n新 Confluence 7.13.7 地址下载：https://product-downloads.atlassian.com/software/confluence/d\nownloads/atlassian-confluence-7.13.7-x64.bin\n数据备份 生产环境：127.0.0.1\n程序目录：/opt/atlassian\n数据目录：/var/atlassian\n数据库：mysql 5.6\nmysql数据备份 1 2 3 cd /opt/mysql_backup mysqldump -uroot -hlocalhost -p confluence \u0026gt; confluence-20220616.sql scp confluence-20220616.sql 目标机器 数据还原 注意：数据库必须使用root 才行，不然会报 数据库触发器失败的错误（错误如下图）\n1 2 3 4 5 6 [root@localhost ~]# mysql -u root -p mysql\u0026gt; CREATE DATABASE IF NOT EXISTS confluence DEFAULT CHARSET utf8 COLLATE utf8_general_ci; mysql\u0026gt; use confluence; mysql\u0026gt; source /root/confluence-20220616.sql; #注意，这里需要写入confluence.sql的 绝对路径 修改Confluence配置文件中数据库连接为新数据库\n1 vi /var/atlassian/application-data/confluence/confluence.cfg.xml 启动Conflucence 1 2 3 4 5 6 授权 chown -R confluence:confluence /opt/atlassian chown -R confluence:confluence /var/atlassian 切换用户启动 su confluence /opt/atlassian/confluence/bin/startup.sh 版本更新（ 7.13.7） 1 2 3 4 开始升级 [root@prometheus-server confluence]# ls atlassian-confluence-6.9.3-x64.bin atlassian-confluence-7.13.7-x64.bin [root@prometheus-server confluence]# ./atlassian-confluence-7.13.7-x64.bin 访问页面（key失效） 进行破解恢复 注意：数据库必须使用root 才行，不然会报 数据库触发器失败的错误\n1 2 3 vim confluence.cfg.xml 查询新的server.id \u0026lt;property name=\u0026#34;confluence.setup.server.id\u0026#34;\u0026gt;BJ5V-W7UB-4PO5-2J39\u0026lt;/property\u0026gt; 备份jar包破解 1 2 sz /opt/atlassian/confluence/confluence/WEB-INF/lib/atlassian-extras-decoder-v2- 3.4.1.jar 破解完成的包 上传到其相应目录 进行重启 1 2 3 [root@prometheus-server ~]# mv atlassian-extras-decoder-v2-3.4.1.jar /opt/atlassian/confluence/confluence/WEB-INF/lib/ mv: overwrite ‘/opt/atlassian/confluence/confluence/WEB-INF/lib/atlassian-extras-decoder-v2-3.4.1.jar’? y 成功测试 新版测试地址（http://127.0.0.1:8090/） 账号和密码 原有的即可登陆\n","date":"2022-07-04T10:39:47Z","image":"https://www.ownit.top/title_pic/50.jpg","permalink":"https://www.ownit.top/p/202207041039/","title":"Confluence升级方案"},{"content":"\n前文 Jenkins安装部署使用_南宫乘风的博客-CSDN博客\nJenkins入门配置_南宫乘风的博客-CSDN博客\nJenkins集成Sonar Qube_南宫乘风的博客-CSDN博客\nJenkins的流水线（Pipeline）\nJenkins流水线整合钉钉_南宫乘风的博客-CSDN博客Kubernetes安装Jenkins_南宫乘风的博客-CSDN博客\n目录\n1、Kubernetes 环境安装 Jenkins\n2、Jenkins 安装插件\n3、云配置\n4、Template 模板配置\n5、Jenkins-slave 启动测试\n6、实战环境\n7、运行结果\n1、Kubernetes 环境安装 Jenkins https://blog.csdn.net/heian_99/article/details/124985786\n2、Jenkins 安装插件 1 2 3 4 5 6 7 8 9 10 插件: kubernets pipeline docker pipeline docker Kubernetes Cli Config File Provider Pipeline Utility Steps Jenkins源：https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 3、云配置 Dashboard \u0026gt; 系统管理 \u0026gt; 节点管理 \u0026gt; configureClouds\n参考：https://github.com/jenkinsci/kubernetes-plugin\n这里是配置连接Kubernetes集群，启动 Jenkins Slave 代理的相关配置。\n名称： kubernetes Kubernetes 地址： kubernetes.default.svc.cluster.local (默认集群内调用 k8s api 地址) 禁用 HTTPS 证书检查： 勾选 (不验证https) 凭据： 新增凭据—\u0026gt;Secret text—\u0026gt;Secret 设置 kubernetes 的 Token (进入 k8s dashboard 的 token 等都行) Jenkins地址： jenkins.mydlqcloud:8080/jenkins (用于代理与 Jenkins 连接的地址，用的是 k8s 集群中 jenkins 服务的地址为“http://jenkins服务名.jenkins所在namespace:jenkins端口号/jenkins后缀”) 其他： 默认即可 4、Template 模板配置 这里配置 Jenkins Slave 在 kubernetes 集群中启动的 Pod 的配置，这里将设置四个镜像，分别是：\nJenkins Slave： 用于执行 Jenkins Job 命令。 Helm-Kubectl： 用于执行 Helm 命令。 Docker 用于编译、推送 Docker 镜像 Maven： 用于Maven编译、打包。 这里将这四个镜像融入到一个 Pod 之中，方便执行各种命令来完成持续部署交互过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@master01 Jenkins]# cat maven.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: maven-pvc namespace: jenkins spec: storageClassName: \u0026#34;nfsdata\u0026#34; accessModes: - ReadWriteMany resources: requests: storage: 10Gi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Kubernetes地址：https://kubernetes.default.svc.cluster.local/ Jenkins-master：http://jenkins.jenkins.svc.cluster.local:8080 Jenkins-slave：jenkins.jenkins.svc.cluster.local:50000 jnlp-slave cnych/jenkins:jnlp6 maven maven:3.8.5-openjdk-8-slim docker registry.cn-shanghai.aliyuncs.com/mydlq/docker:18.06.2-dind helm-kubectl registry.cn-shanghai.aliyuncs.com/mydlq/helm-kubectl:2.13.1 /var/run/docker.sock /usr/bin/docker /etc/docker 注意：docker需要填写sleep\n5、Jenkins-slave 启动测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def label = \u0026#34;jnlp-slave\u0026#34; podTemplate(label: label,cloud: \u0026#39;kubernetes\u0026#39; ){ node (label) { stage(\u0026#39;Git阶段\u0026#39;){ echo \u0026#34;1、开始拉取代码\u0026#34; sh \u0026#34;git version\u0026#34; } stage(\u0026#39;Maven阶段\u0026#39;){ container(\u0026#39;maven\u0026#39;) { echo \u0026#34;2、开始Maven编译、推送到本地库\u0026#34; sh \u0026#34;mvn -version\u0026#34; } } stage(\u0026#39;Docker阶段\u0026#39;){ container(\u0026#39;docker\u0026#39;) { echo \u0026#34;3、开始读取Maven pom变量，并执行Docker编译、推送、删除\u0026#34; sh \u0026#34;docker version\u0026#34; } } stage(\u0026#39;Helm阶段\u0026#39;){ container(\u0026#39;helm-kubectl\u0026#39;) { echo \u0026#34;4、开始检测Kubectl环境，测试执行Helm部署，与执行部署\u0026#34; sh \u0026#34;helm version\u0026#34; } } } } 6、实战环境 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pipeline { agent { label \u0026#39;jnlp-slave\u0026#39; } // 存放所有任务的合集 stages { stage(\u0026#39;拉取Git代码\u0026#39;) { steps { echo \u0026#39;拉取Git代码\u0026#39; sh \u0026#39;git clone https://gitee.com/chengfeng99/java-demo.git\u0026#39; } } stage(\u0026#39;检测代码质量\u0026#39;) { steps { echo \u0026#39;检测代码质量\u0026#39; withSonarQubeEnv(\u0026#39;sonarqube\u0026#39;) { // Will pick the global server connection you have configured // 这里使用名字叫做maven的容器运行 container(\u0026#34;maven\u0026#34;) { sh \u0026#39;\u0026#39;\u0026#39; cd /home/jenkins/agent/workspace/k8s-demo/java-demo mvn sonar:sonar -Dsonar.projectname=${JOB_NAME} -Dsonar.projectKey=${JOB_NAME} -Dsonar.java.binaries=target/ -Dsonar.login=19d0d6b885e18455d257d61da08776bd4e180c04 \u0026#39;\u0026#39;\u0026#39; } } } } stage(\u0026#39;构建代码\u0026#39;) { steps { echo \u0026#39;构建代码\u0026#39; container(\u0026#39;maven\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; cd /home/jenkins/agent/workspace/k8s-demo/java-demo mvn clean package -Dmaven.test.skip=true \u0026#39;\u0026#39;\u0026#39; //打包跳过测试 } } } stage(\u0026#39;制作自定义镜像并发布Harbor\u0026#39;) { steps { echo \u0026#39;制作自定义镜像并发布Harbor\u0026#39; } } stage(\u0026#39;基于Harbor部署工程\u0026#39;) { steps { echo \u0026#39;基于Harbor部署工程\u0026#39; } } } } 7、运行结果 ","date":"2022-05-26T18:26:46Z","image":"https://www.ownit.top/title_pic/70.jpg","permalink":"https://www.ownit.top/p/202205261826/","title":"Jenkins集成Kubernetes集群"},{"content":"前文 Jenkins安装部署使用_南宫乘风的博客-CSDN博客\nJenkins入门配置_南宫乘风的博客-CSDN博客\nJenkins集成Sonar Qube_南宫乘风的博客-CSDN博客\nJenkins的流水线（Pipeline）\nJenkins流水线整合钉钉_南宫乘风的博客-CSDN博客 目录\n环境\n思路\n1、NFS（动态存储）\n2、helm安装nfs-client\n3、创建namespace 4、持久化Jenkins数据\n5、创建service account\n6、安装Jenkins\n7、授权对Jenkins服务的访问权限\n8、打开浏览器IP:31400/\n环境 生产实践-k8s安装Jenkins和Jenkins Kubernetes插件\n环境要求：你需要一个正常可以使用的Kubernetes集群，集群中可以使用的内存大于等于4G。\nKubernetes版本1.18\n思路 Jenkins插件可以在Kubernetes集群中运行动态jenkins-slave代理。\n基于Kubernetes的docker，自动化在Kubernetes中运行的Jenkins-slave代理的缩放。\n该插件为每个jenkins-slave代理创建Kubernetes Pod，并在每个构建后停止它。\n在Kubernetes中jenkins-slave代理启动，会自动连接到Jenkins主控制器。 对于某些环境变量，会自动注入：\nJenkins_URL：Jenkins Web界面URL\njenkins_secret：身份验证的秘密密钥\njenkins_agent_name：jenkins代理的名称\njenkins_name：jenkins代理的名称（已弃用。仅用于向后兼容性）\n不需要在Kubernetes内运行Jenkins Controller。\n1、NFS（动态存储） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #安装 yum install -y nfs-utils rpcbind mkdir -p /data/nfsdata # 修改配置 $ vim /etc/exports /data/nfsdata 192.168.31.* (rw,async,no_root_squash) # 使配置生效 $ exportfs -r # 服务端查看下是否生效 $ showmount -e localhost Export list for localhost: /data/nfsdata (everyone) 2、helm安装nfs-client 1 2 stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm添加这个源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 下载helm包 helm pull aliyuncs/nfs-client-provisioner 解压 tar -zxvf nfs-client-provisioner-1.2.8.tgz 修复values.yaml 三处 image: repository: quay.io/external_storage/nfs-client-provisioner tag: v3.1.0-k8s1.11 pullPolicy: IfNotPresent nfs: server: 192.168.31.73 path: /data/nfsdata reclaimPolicy: Retain 3、创建namespace 1 2 kubectl create namespace jenkins kubectl get namespaces 4、持久化Jenkins数据 pvc.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-pvc namespace: jenkins spec: storageClassName: \u0026#34;nfsdata\u0026#34; accessModes: - ReadWriteMany resources: requests: storage: 10Gi 通过kubectl部署volume\n1 kubectl apply -f pvc.yaml 5、创建service account 创建pod时，如果不指定服务账户，则会自动为其分配一个名为default的同一namespace中的服务账户。但是通常应用程序时存在权限不足的情况，所以需要我们自己创建一个服务账户。\n①下载jenkins-sa.yaml\n1 wget https://raw.githubusercontent.com/jenkins-infra/jenkins.io/master/content/doc/tutorials/kubernetes/installing-jenkins-on-kubernetes/jenkins-sa.yaml ②通过kubectl部署jenkins-sa.yaml\n1 kubectl apply -f jenkins-sa.yaml 或者使用下面的文件\njenkins-sa.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 --- apiVersion: v1 kind: ServiceAccount metadata: name: jenkins namespace: jenkins --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: jenkins rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - statefulsets - services - replicationcontrollers - replicasets - podtemplates - podsecuritypolicies - pods - pods/log - pods/exec - podpreset - poddisruptionbudget - persistentvolumes - persistentvolumeclaims - jobs - endpoints - deployments - deployments/scale - daemonsets - cronjobs - configmaps - namespaces - events - secrets verbs: - create - get - watch - delete - list - patch - update - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - list - watch - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: jenkins roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: jenkins subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:serviceaccounts:jenkins 6、安装Jenkins jenkins-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apps/v1 kind: Deployment metadata: name: jenkins namespace: jenkins spec: replicas: 1 selector: matchLabels: app: jenkins template: metadata: labels: app: jenkins spec: serviceAccountName: jenkins #指定我们前面创建的服务账号 containers: - name: jenkins image: registry.cn-hangzhou.aliyuncs.com/s-ops/jenkins:2.346 ports: - containerPort: 8080 - containerPort: 50000 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home volumes: - name: jenkins-home persistentVolumeClaim: claimName: jenkins-pvc #指定前面创建的PVC 通过kubectl部署jenkins-deployment.yaml\n1 kubectl create -f jenkins-deployment.yaml -n jenkins 7、授权对Jenkins服务的访问权限 主要目的暴露外部访问Jenkins的8080端口，我将31400定义为8080的映射端口。\njenkins-service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Service metadata: name: jenkins namespace: jenkins spec: type: NodePort ports: - name: http port: 8080 targetPort: 8080 nodePort: 31400 - name: agent port: 50000 targetPort: 50000 nodePort: 31401 selector: app: jenkins 通过kubectl部署服务\n1 kubectl create -f jenkins-service.yaml -n jenkins 8、打开浏览器IP:31400/ 查看密码\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl get pod -n jenkins //查询podname kubectl logs podname -n jenkins ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: cf8d9da9de0346fd90461be366915d76 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* 选择推荐插件安装，创建管理员~完成！\n","date":"2022-05-26T15:48:35Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202205261548/","title":"Kubernetes安装Jenkins"},{"content":"前文 Jenkins安装部署使用_南宫乘风的博客-CSDN博客\nJenkins入门配置_南宫乘风的博客-CSDN博客\nJenkins集成Sonar Qube_南宫乘风的博客-CSDN博客\nJenkins的流水线（Pipeline）\n在程序部署成功后，可以通过钉钉的机器人及时向群众发送部署的最终结果通知\n安装插件 钉钉内部创建群组并构建机器人\n最终或获取到Webhook信息\n1 https://oapi.dingtalk.com/robot/send?access_token=kej4ehkj34gjhg34jh5bh5jb34hj53b4 系统配置添加钉钉通知\n任务中追加流水线配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 pipeline { agent any environment{ harborRepo = \u0026#39;heianapp\u0026#39; harborUser = \u0026#39;heian99\u0026#39; harborPasswd = \u0026#39;NG+.mK4M-(s4CYX\u0026#39; } // 存放所有任务的合集 stages { stage(\u0026#39;拉取Git代码\u0026#39;) { steps { echo \u0026#39;拉取Git代码\u0026#39; checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;${tag}\u0026#39;]], extensions: [], userRemoteConfigs: [[url: \u0026#39;https://gitee.com/chengfeng99/java-demo.git\u0026#39;]]]) } } stage(\u0026#39;检测代码质量\u0026#39;) { steps { echo \u0026#39;检测代码质量\u0026#39; sh \u0026#39;/var/jenkins_home/sonar-scanner/bin/sonar-scanner -Dsonar.sources=./ -Dsonar.projectname=${JOB_NAME} -Dsonar.projectKey=${JOB_NAME} -Dsonar.java.binaries=target/ -Dsonar.login=19d0d6b885e18455d257d61da08776bd4e180c04\u0026#39; } } stage(\u0026#39;构建代码\u0026#39;) { steps { echo \u0026#39;构建代码\u0026#39; sh \u0026#39;/var/jenkins_home/maven/bin/mvn clean package -DskipTests\u0026#39; } } stage(\u0026#39;制作自定义镜像并发布Harbor\u0026#39;) { steps { echo \u0026#39;制作自定义镜像并发布Harbor\u0026#39; sh \u0026#39;\u0026#39;\u0026#39; cp ./target/*.jar ./docker/demo.jar cd ./docker docker build -t ${JOB_NAME}:${BUILD_NUMBER} . \u0026#39;\u0026#39;\u0026#39; sh \u0026#39;\u0026#39;\u0026#39;docker login -u ${harborUser} -p ${harborPasswd} docker tag ${JOB_NAME}:${BUILD_NUMBER} ${harborUser}/${harborRepo}:${JOB_NAME}_${BUILD_NUMBER} docker push ${harborUser}/${harborRepo}:${JOB_NAME}_${BUILD_NUMBER}\u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#39;基于Harbor部署工程\u0026#39;) { steps { echo \u0026#39;基于Harbor部署工程\u0026#39; sshPublisher(publishers: [sshPublisherDesc(configName: \u0026#39;node-Linux32\u0026#39;, transfers: [sshTransfer(cleanRemote: false, excludes: \u0026#39;\u0026#39;, execCommand: \u0026#39;\u0026#39;\u0026#39;cd /opt/java/ echo \u0026#34;测试成功\u0026#34; \u0026gt;\u0026gt; log.txt date \u0026gt;\u0026gt; log.txt\u0026#39;\u0026#39;\u0026#39;, execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: \u0026#39;[, ]+\u0026#39;, remoteDirectory: \u0026#39;\u0026#39;, remoteDirectorySDF: false, removePrefix: \u0026#39;\u0026#39;, sourceFiles: \u0026#39;target/*.jar,docker/*\u0026#39;)], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) } } } post { success { dingtalk ( robot: \u0026#39;Jenkins-DingDing\u0026#39;, type:\u0026#39;MARKDOWN\u0026#39;, title: \u0026#34;success: ${JOB_NAME}\u0026#34;, text: [\u0026#34;- 成功构建:${JOB_NAME}项目!\\n- 版本:${tag}\\n- 持续时间:${currentBuild.durationString}\\n- 任务:#${JOB_NAME}\u0026#34;] ) } failure { dingtalk ( robot: \u0026#39;Jenkins-DingDing\u0026#39;, type:\u0026#39;MARKDOWN\u0026#39;, title: \u0026#34;fail: ${JOB_NAME}\u0026#34;, text: [\u0026#34;- 失败构建:${JOB_NAME}项目!\\n- 版本:${tag}\\n- 持续时间:${currentBuild.durationString}\\n- 任务:#${JOB_NAME}\u0026#34;] ) } } } 查看效果\n","date":"2022-05-17T10:58:10Z","image":"https://www.ownit.top/title_pic/52.jpg","permalink":"https://www.ownit.top/p/202205171058/","title":"Jenkins流水线整合钉钉"},{"content":"目录 Jenkins安装部署使用_南宫乘风的博客-CSDN博客\nJenkins入门配置_南宫乘风的博客-CSDN博客\nJenkins集成Sonar Qube_南宫乘风的博客-CSDN博客\nJenkins流水线 Jenkins流水线任务介绍 之前采用Jenkins的自由风格构建的项目，每个步骤流程都要通过不同的方式设置，并且构建过程中整体流程是不可见的，无法确认每个流程花费的时间，并且问题不方便定位问题。\nJenkins的Pipeline可以让项目的发布整体流程可视化，明确执行的阶段，可以快速的定位问题。并且整个项目的生命周期可以通过一个Jenkinsfile文件管理，而且Jenkinsfile文件是可以放在项目中维护。\n所以Pipeline相对自由风格或者其他的项目风格更容易操作。\nJenkins流水线任务 构建Jenkins流水线任务 构建任务\n生成Groovy脚本\nHello World脚本生成\n构建后查看视图\nGroovy脚本 Groovy脚本基础语法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // 所有脚本命令包含在pipeline{}中 pipeline { // 指定任务在哪个节点执行（Jenkins支持分布式） agent any // 配置全局环境，指定变量名=变量值信息 environment{ host = \u0026#39;172.17.1.22\u0026#39; } // 存放所有任务的合集 stages { // 单个任务 stage(\u0026#39;任务1\u0026#39;) { // 实现任务的具体流程 steps { echo \u0026#39;do something\u0026#39; } } // 单个任务 stage(\u0026#39;任务2\u0026#39;) { // 实现任务的具体流程 steps { echo \u0026#39;do something\u0026#39; } } // …… } } 编写例子测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 pipeline { agent any // 存放所有任务的合集 stages { stage(\u0026#39;拉取Git代码\u0026#39;) { steps { echo \u0026#39;拉取Git代码\u0026#39; } } stage(\u0026#39;检测代码质量\u0026#39;) { steps { echo \u0026#39;检测代码质量\u0026#39; } } stage(\u0026#39;构建代码\u0026#39;) { steps { echo \u0026#39;构建代码\u0026#39; } } stage(\u0026#39;制作自定义镜像并发布Harbor\u0026#39;) { steps { echo \u0026#39;制作自定义镜像并发布Harbor\u0026#39; } } stage(\u0026#39;基于Harbor部署工程\u0026#39;) { steps { echo \u0026#39;基于Harbor部署工程\u0026#39; } } } } 配置Grovvy脚本\n查看效果\nPs：涉及到特定脚本，Jenkins给予了充足的提示，可以自动生成命令\n生成命令位置\nJenkinsfile实现 Jenkinsfile方式需要将脚本内容编写到项目中的Jenkinsfile文件中，每次构建会自动拉取项目并且获取项目中Jenkinsfile文件对项目进行构建\n配置pipeline\n准备Jenkinsfile\nJenkinsfile · 南宫乘风/java-demo - Gitee.com\n测试效果\nJenkins流水线任务实现 参数化构建\n添加参数化构建，方便选择不的项目版本\n拉取Git代码\n通过流水线语法生成Checkout代码的脚本\n将*/master更改为标签${tag}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 pipeline { agent any environment{ harborRepo = \u0026#39;heianapp\u0026#39; harborUser = \u0026#39;heian99\u0026#39; harborPasswd = \u0026#39;NG+.mK4M-(s4CYX\u0026#39; } // 存放所有任务的合集 stages { stage(\u0026#39;拉取Git代码\u0026#39;) { steps { echo \u0026#39;拉取Git代码\u0026#39; checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;${tag}\u0026#39;]], extensions: [], userRemoteConfigs: [[url: \u0026#39;https://gitee.com/chengfeng99/java-demo.git\u0026#39;]]]) } } stage(\u0026#39;检测代码质量\u0026#39;) { steps { echo \u0026#39;检测代码质量\u0026#39; sh \u0026#39;/var/jenkins_home/sonar-scanner/bin/sonar-scanner -Dsonar.sources=./ -Dsonar.projectname=${JOB_NAME} -Dsonar.projectKey=${JOB_NAME} -Dsonar.java.binaries=target/ -Dsonar.login=19d0d6b885e18455d257d61da08776bd4e180c04\u0026#39; } } stage(\u0026#39;构建代码\u0026#39;) { steps { echo \u0026#39;构建代码\u0026#39; sh \u0026#39;/var/jenkins_home/maven/bin/mvn clean package -DskipTests\u0026#39; } } stage(\u0026#39;制作自定义镜像并发布Harbor\u0026#39;) { steps { echo \u0026#39;制作自定义镜像并发布Harbor\u0026#39; sh \u0026#39;\u0026#39;\u0026#39; cp ./target/*.jar ./docker/demo.jar cd ./docker docker build -t ${JOB_NAME}:${BUILD_NUMBER} . \u0026#39;\u0026#39;\u0026#39; sh \u0026#39;\u0026#39;\u0026#39;docker login -u ${harborUser} -p ${harborPasswd} docker tag ${JOB_NAME}:${BUILD_NUMBER} ${harborUser}/${harborRepo}:${JOB_NAME}_${BUILD_NUMBER} docker push ${harborUser}/${harborRepo}:${JOB_NAME}_${BUILD_NUMBER}\u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#39;基于Harbor部署工程\u0026#39;) { steps { echo \u0026#39;基于Harbor部署工程\u0026#39; sshPublisher(publishers: [sshPublisherDesc(configName: \u0026#39;node-Linux32\u0026#39;, transfers: [sshTransfer(cleanRemote: false, excludes: \u0026#39;\u0026#39;, execCommand: \u0026#39;\u0026#39;\u0026#39;cd /opt/java/ echo \u0026#34;测试成功\u0026#34; \u0026gt;\u0026gt; log.txt date \u0026gt;\u0026gt; log.txt\u0026#39;\u0026#39;\u0026#39;, execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: \u0026#39;[, ]+\u0026#39;, remoteDirectory: \u0026#39;\u0026#39;, remoteDirectorySDF: false, removePrefix: \u0026#39;\u0026#39;, sourceFiles: \u0026#39;target/*.jar,docker/*\u0026#39;)], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) } } } } Ps：由于采用变量，记得使用双引号\n进行构建\n已经成功\n","date":"2022-05-17T10:32:21Z","image":"https://www.ownit.top/title_pic/26.jpg","permalink":"https://www.ownit.top/p/202205171032/","title":"Jenkins的流水线（Pipeline）"},{"content":"前文目录 Jenkins安装部署使用_南宫乘风的博客-CSDN博客\nJenkins入门配置_南宫乘风的博客-CSDN博客\nSonar Qube介绍 Sonar Qube是一个开源的代码分析平台，支持Java、Python、PHP、JavaScript、CSS等25种以上的语言，可以检测出重复代码、代码漏洞、代码规范和安全性漏洞的问题。\nSonar Qube可以与多种软件整合进行代码扫描，比如Maven，Gradle，Git，Jenkins等，并且会将代码检测结果推送回Sonar Qube并且在系统提供的UI界面上显示出来\nSonar Qube环境搭建 Sonar Qube安装\nSonar Qube在7.9版本中已经放弃了对MySQL的支持，并且建议在商业环境中采用PostgreSQL，那么安装Sonar Qube时需要依赖PostgreSQL。\n并且这里会安装Sonar Qube的长期支持版本8.9\n拉取镜像 1 2 docker pull postgres docker pull sonarqube:8.9.3-community 编写docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 version: \u0026#34;3.1\u0026#34; services: db: image: postgres container_name: db ports: - 5432:5432 networks: - sonarnet environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar sonarqube: image: sonarqube:8.9.3-community container_name: sonarqube depends_on: - db ports: - \u0026#34;9000:9000\u0026#34; networks: - sonarnet environment: SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar SONAR_JDBC_USERNAME: sonar SONAR_JDBC_PASSWORD: sonar networks: sonarnet: driver: bridge 启动容器\n1 docker-compose up -d 需要设置sysctl.conf文件信息\n设置vm.max_map_count\n并执行命令刷新\n1 sysctl -p 重新启动需要一定时间启动，可以可以查看容器日志，看到如下内容代表启动成功\n容器日志\n访问Sonar Qube首页\n还需要重新设置一次密码 Sonar Qube首页\n安装中文插件\n安装成功后需要重启，安装失败重新点击install重装即可。\n安装成功后，会查看到重启按钮，点击即可\n重启后查看效果\nSonar Qube基本使用 Sonar Qube的使用方式很多，Maven可以整合，也可以采用sonar-scanner的方式，再查看Sonar Qube的检测效果\nMaven实现代码检测 修改Maven的settings.xml文件配置Sonar Qube信息 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;sonar\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;sonar.login\u0026gt;admin\u0026lt;/sonar.login\u0026gt; \u0026lt;sonar.password\u0026gt;admin123456\u0026lt;/sonar.password\u0026gt; \u0026lt;sonar.host.url\u0026gt;http://172.17.1.22:9000\u0026lt;/sonar.host.url\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; 在代码位置执行命令：mvn sonar:sonar\n查看Sonar Qube界面检测结果\nSonar-scanner实现代码检测 下载Sonar-scanner：https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/\n下载4.6.x版本即可，要求Linux版本\n解压并配置sonar服务端信息\n由于是zip压缩包，需要安装unzip解压插件 1 yum -y install unzip 解压压缩包\n1 unzip sonar-scanner-cli/sonar-scanner-cli-4.6.0.2311-linux.zip 配置sonarQube服务端地址，修改conf下的sonar-scanner.properties\n执行命令检测代码\n1 2 # 在项目所在目录执行以下命令 ~/sonar-scanner/bin/sonar-scanner -Dsonar.sources=./ -Dsonar.projectname=demo -Dsonar.projectKey=java -Dsonar.java.binaries=target/ 查看日志信息\n查看SonarQube界面检测结果\nJenkins集成Sonar Qube Jenkins继承Sonar Qube实现代码扫描需要先下载整合插件\nJenkins安装插件\nJenkins配置Sonar Qube\n开启Sonar Qube权限验证\n获取Sonar Qube的令牌\n配置Jenkins的Sonar Qube信息\n配置Sonar-scanner\n将Sonar-scaner添加到Jenkins数据卷中并配置全局配置 配置任务的Sonar-scanner 1 2 3 4 5 6 7 8 ~/sonar-scanner/bin/sonar-scanner -Dsonar.sources=./ -Dsonar.projectname=demo -Dsonar.projectKey=java -Dsonar.java.binaries=target/ #主要下面这个 sonar.projectname=${JOB_NAME} sonar.projectKey=${JOB_NAME} sources=./ sonar.java.binaries=target/ 构建任务\n已经上传镜像包\n注意：我这里代码编译这一块，缺少那个 切换分支编译，如果有需要，需要自己配置 ","date":"2022-05-17T10:06:54Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/202205171006/","title":"Jenkins集成Sonar Qube"},{"content":"Jenkins安装部署使用_南宫乘风的博客-CSDN博客\n安装可以参考这篇文章，后续在这基础进行构建\n由于Jenkins需要从Git拉取代码、需要本地构建、甚至需要直接发布自定义镜像到Docker仓库，所以Jenkins需要配置大量内容。\n构建任务 准备好GitLab仓库中的项目，并且通过Jenkins配置项目的实现当前项目的DevOps基本流程。\n构建Maven工程发布到GitLab（Gitee、Github均可） java-demo: api-gateway-demo\nJenkins点击左侧导航新建任务\n选择自由风格构建任务\n配置源码拉取地址 Jenkins需要将Git上存放的源码存储到Jenkins服务所在磁盘的本地\n配置任务源码拉取的地址 Jenkins立即构建\n点击任务demo中的立即构建\n查看构建工程的日志，点击上述的任务条即可\n可以看到源码已经拉取带Jenkins本地，可以根据第三行日志信息，查看Jenkins本地拉取到的源码。\n查看Jenkins容器中/var/jenkins_home/workspace/demo的源码\n源码存放位置\n配置Maven构建代码 代码拉取到Jenkins本地后，需要在Jenkins中对代码进行构建，这里需要Maven的环境，而Maven需要Java的环境，接下来需要在Jenkins中安装JDK和Maven，并且配置到Jenkins服务。\n准备JDK、Maven压缩包通过数据卷映射到Jenkins容器内部 解压压缩包，并配置Maven的settings.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;!-- 阿里云镜像地址 --\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;!-- JDK1.8编译插件 --\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;jdk-1.8\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;jdk\u0026gt;1.8\u0026lt;/jdk\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;maven.compiler.compilerVersion\u0026gt;1.8\u0026lt;/maven.compiler.compilerVersion\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; Jenkins配置JDK\u0026amp;Maven并保存\n配置Jenkins任务构建代码\n立即构建测试，查看target下的jar包\njava-demo: api-gateway-demo\n配置Publish发布\u0026amp;远程操作 jar包构建好之后，就可以根据情况发布到测试或生产环境，这里需要用到之前下载好的插件Publish Over SSH。\n配置Publish Over SSH连接测试、生产环境 Publish Over SSH配置\n配置任务的构建后操作，发布jar包到目标服务\n已经完成一次简单的构建交付\n持续交付、部署 程序代码在经过多次集成操作到达最终可以交付，持续交付整体流程和持续集成类似，不过需要选取指定的发行版本\n下载Git Parameter插件 设置项目参数化构建\n基于Git标签构建\n给项目添加tag版本\n任务构建时，采用Shell方式构建，拉取指定tag版本代码\n基于Parameter构建任务，任务发布到目标服务器\n","date":"2022-05-17T09:41:41Z","image":"https://www.ownit.top/title_pic/21.jpg","permalink":"https://www.ownit.top/p/202205170941/","title":"Jenkins入门配置"},{"content":"介绍 Jenkins是一个独立的开源软件项目，是基于Java开发的一种持续集成工具，用于监控持续重复的工作，旨在提供一个开放易用的软件平台，使软件的持续集成变成可能。前身是Hudson是一个可扩展的持续集成引擎。可用于自动化各种任务，如构建，测试和部署软件。Jenkins可以通过本机系统包Docker安装，甚至可以通过安装Java Runtime Environment的任何机器独立运行。\nJenkins特点 开源免费; 跨平台，支持所有的平台; master/slave支持分布式的build; web形式的可视化的管理页面; 安装配置超级简单; tips及时快速的帮助； 已有的200多个插件 安装教程 这里我们使用的是离线包方式安装。\n官网镜像地址: Index of /\n下载地址： Jenkins download and deployment\n华为镜像地址: 华为开源镜像站_软件开发服务_华为云\n直接下载war包，并安装好jdk之后，输入:nohup java -jar jenkins.war --httpPort=8888 \u0026amp;\n进行启动，然后网页浏览器输入 ip:8888打开设置好账号密码之后登录即可，插件安装推荐使用官方推荐。\nDocker安装 拉取Jenkins镜像\n1 docker pull jenkins/jenkins 编写docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 version: \u0026#34;3.1\u0026#34; services: jenkins: image: jenkins/jenkins container_name: jenkins ports: - 8080:8080 - 50000:50000 volumes: - ./data/:/var/jenkins_home/ - /usr/bin/docker:/usr/bin/docker - /var/run/docker.sock:/var/run/docker.sock - /etc/docker/daemon.json:/etc/docker/daemon.json 首次启动会因为数据卷data目录没有权限导致启动失败，设置data目录写权限\n1 chmod -R a+w data/ 重新启动Jenkins容器后，由于Jenkins需要下载大量内容，但是由于默认下载地址下载速度较慢，需要重新设置下载地址为国内镜像站\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 修改数据卷中的hudson.model.UpdateCenter.xml文件 \u0026lt;?xml version=\u0026#39;1.1\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;sites\u0026gt; \u0026lt;site\u0026gt; \u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://updates.jenkins.io/update-center.json\u0026lt;/url\u0026gt; \u0026lt;/site\u0026gt; \u0026lt;/sites\u0026gt; # 将下载地址替换为http://mirror.esuni.jp/jenkins/updates/update-center.json \u0026lt;?xml version=\u0026#39;1.1\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;sites\u0026gt; \u0026lt;site\u0026gt; \u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://mirror.esuni.jp/jenkins/updates/update-center.json\u0026lt;/url\u0026gt; \u0026lt;/site\u0026gt; \u0026lt;/sites\u0026gt; # 清华大学的插件源也可以https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 再次重启Jenkins容器，访问Jenkins（需要稍微等会）\n1 docker-compose restart 查看密码登录Jenkins，并登录下载插件\n1 docker exec -it jenkins cat /var/jenkins_home/secrets/initialAdminPassword ","date":"2022-05-16T21:47:34Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/202205162147/","title":"Jenkins安装部署使用"},{"content":"目录\n相关 思路\n主库\n1、检查主库是否开启log-bin\n2、开启log-bin\n3、重启MySQL\n4、检查生成的log-bin目录\n5、进入MySQL，锁表，记录POS号\n6、备份mysql文件，解表\n7、主库传输mysql文件到从库\n8、添加iptables防火墙规则（ip为从库IP）\n从库\n1、停止从库的业务程序\n2、停止票务开机自启程序\n3、检查主库是否开启log-bin\n4、开启log-bin，添加配置，修改id\n4、停止mysql ，备份目录\n5、覆盖mysql目录，删除auto.cnf\n6、启动mysql\n7、启动主从\n相关 相关原理可以查看此博客 https://blog.csdn.net/heian_99/article/details/106647572http://xn\u0026ndash;mysql-ni1h1kkd34cien8anzj00an2yqrx8k1a1u9c2j7e9svvilaaa\n以下作为 实际操作\n环境：\n主库： mysql\n从库： mysql\n思路 （1）主库A数据全，数据库B空的\n（2）锁表数据库A，记录pos号\n（3）复制数据库A目录数据（进行cp备份）然后 解锁表\n（4）把数据库A备份的数据进行scp 数据库B\n（5）把数据库A的目录，进行覆盖数据库B的目录。（数据覆盖，相当于备份恢复）\n（6）进行主共同步\n**注意：日志格式 **statement row mixed 建议使用 mixed\n主库 1、检查主库是否开启log-bin 1 2 [root@92-69-xy-bak ~]# cat /etc/my.cnf | grep log-bin log-bin=/databak/mysql-bin/mysql-bin 2、开启log-bin 如果上面没有显示，需要开启log-bin\n1 2 3 4 5 6 开启 vim /etc/my.cnf log-bin=/databak/mysql-bin/mysql-bin 创建目录，授权 mkdir /databak/mysql-bin chown mysql.mysql /databak/mysql-bin -R 3、重启MySQL 重启mysql是因为刚开启log-bin日志，需要生成数据\n1 service mysql restart 4、检查生成的log-bin****目录 1 目的：是否开启log-bin成功，是否生成日志数据 1 2 3 4 ls /databak/mysql-bin/ 生成的数据 mysql-bin.000001 mysql-bin.000002 mysql-bin.000003 mysql-bin.index 5、进入MySQL，锁表，记录POS****号 1 2 3 4 mysql -ucpms -pudqjHDMkxQfGP4iy FLUSH TABLES WITH READ LOCK; show master status; echo \u0026#34;日志名字 偏移量\u0026#34; \u0026gt; /var/lib/mysql/记录文件 6、备份mysql文件，解表 1 2 3 cp -r /var/lib/mysql /opt/mysql_bak UNLOCK TABLES; 7、主库传输mysql****文件到从库 1 2 cd /opt scp -r mysql_bak soft CMS3.10.21 192.168.0.3:/opt 8、添加iptables防火墙规则（ip为从库IP**）** 1 2 3 vim /etc/sysconfig/iptables -A INPUT -s 10.92.69.3 -p tcp --dport 3306 -j ACCEPT /etc/init.d/iptables restart 以上操作主库完成\n从库 1、停止从库的业务程序 1 pkill java 2、停止票务开机自启程序 1 chkconfig --list | grep cms |awk \u0026#39;{print \u0026#34;chkconfig\u0026#34;,$1,\u0026#34;off\u0026#34;}\u0026#39;|bash 3、检查主库是否开启log-bin 1 2 [root@92-69-xy-bak ~]# cat /etc/my.cnf | grep log-bin log-bin=/databak/mysql-bin/mysql-bin 4、开启log-bin，添加配置，修改id 如果上面没有显示，需要开启log-bin\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 开启 vim /etc/my.cnf log-bin=/databak/mysql-bin/mysql-bin server-id=2 replicate_ignore_table=CMS.QC_TEMP replicate_ignore_table=CMS.QM_TEMP replicate_ignore_table=CMS.STORE_IN_TEMP replicate_ignore_table=CMS.TEMP_POS_SALE_GOODS_ITEM_COST replicate_ignore_table=CMS.TEMP_POS_REJECT_GOODS_ITEM_COST replicate_ignore_table=CMS.MER_INTERFACE_DAY_STORE_CHECK slave-skip-errors = 1062 创建目录，授权 mkdir /databak/mysql-bin chown mysql.mysql /databak/mysql-bin -R 4、停止mysql ，备份目录 1 2 3 service mysql stop mv /var/lib/mysql /var/lib/bak_mysql 5、覆盖mysql目录，删除auto.cnf 1 2 3 cp -r /opt/mysql_bak /var/lib/mysql \\rm /var/lib/mysql/auto.cnf 6、启动mysql 1 service mysql start 7、启动主从 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 进入mysql mysql -uroot -p^passswd 停止slave stop slave; 写入 配置信息（注意pos号，账号，主库ip 端口） change master to master_host=\u0026#39;10.92.69.2\u0026#39;, master_port=3306, master_user=\u0026#39;admin\u0026#39;, master_password=\u0026#39;udq5545adadefiy\u0026#39;, master_log_file=\u0026#39;mysql-bin.000001\u0026#39;, master_log_pos=17503; 开启从库 start slave; 查看状态 show slave status\\G 两个yes为正常，检查pos号是否已经和主库同步 以上操作需谨慎，尤其主库上，输入命令需三思（文档仅供参考）\n","date":"2022-05-13T11:40:32Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/202205131140/","title":"Mysql主从同步复制（快速构建，基于CP数据备份 恢复）"},{"content":"首先我们日常运维中，服务器会跑大量的任务。\n（1）我们可以通prometheus和grafana 展示整个服务器的cpu 内存 和磁盘IO的趋势\n（2）可以每台部署相应的脚本，可以定位到每个时间段执行的业务，所消耗的各项资源\n统计前十的CUP消耗应用（降序 ） 1 ps aux|head -1 \u0026amp;\u0026amp; ps aux|grep -v PID|sort -rn -k +3|head 统计前十的内存消耗应用（降序 ） 1 ps aux|head -1 \u0026amp;\u0026amp; ps aux|grep -v PID|sort -rn -k +4|head 统计前十的磁盘IO消耗应用（降序 ） 1 /usr/sbin/iotop -btoq --iter=1 |awk \u0026#39;NR\u0026gt;3{print}\u0026#39;|sort -rn -k 11 切记需要安装iotop\n统计网络状态标识（状态） 1 netstat -nt | grep -e 127.0.0.1 -e 0.0.0.0 -e ::: -v | awk \u0026#39;/^tcp/ {++state[$NF]} END {for(i in state) print i,\u0026#34;\\t\u0026#34;,state[i]}\u0026#39; 统计网络状态标识（IP） 1 netstat -n | awk \u0026#39;/^tcp/ {print $0}\u0026#39; | awk \u0026#39;{print $(NF-1),$NF}\u0026#39; | awk -F \u0026#39;:| \u0026#39; \u0026#39;{print $1,$NF}\u0026#39; |awk \u0026#39;BEGIN{print \u0026#34;IP\\t\\t状态\\t次数统计\u0026#34;} {a[$1\u0026#34; \u0026#34;$2]++}END{for(i in a) print(i,a[i])}\u0026#39; |sort -nk 3 连接状态_学习笔记-TCP连接状态解析_六十度灰的博客-CSDN博客前面两篇详细的给大家介绍过了TCP的三次握手和四次挥手流程(详见学习笔记-TCP三次握手， 学习笔记-TCP四次挥手 )，本文主要是介绍在TCP连接过程中的各种状态变化。状态介绍CLOSED:表示初始状态。LISTEN:表示服务器端的某个SOCKET处于，可以接受连接了。SYN_RCVD:这个状态表示接受到了SYN，在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的会话过程中的一个\u0026hellip;https://blog.csdn.net/weixin_31304599/article/details/112671431?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.pc_relevant_default\u0026amp;spm=1001.2101.3001.4242.1\u0026amp;utm_relevant_index=3\n相关脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash #时间 time=$(date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34;) day=$(date \u0026#34;+%Y-%m-%d\u0026#34;) #首生成日志目录 mkdir -p /system/log/{cpu,mem,io,net} cpudir=\u0026#39;/system/log/cpu\u0026#39; memdir=\u0026#39;/system/log/mem\u0026#39; iodir=\u0026#39;/system/log/io\u0026#39; netdir=\u0026#39;/system/log/net\u0026#39; #统计前10的cpu echo \u0026#34;=========================================${time}======================================================\u0026#34; \u0026gt;\u0026gt; ${cpudir}/${day}.log ps aux|head -1 \u0026gt;\u0026gt; ${cpudir}/${day}.log ps aux|grep -v PID|sort -rn -k +3|head \u0026gt;\u0026gt; ${cpudir}/${day}.log #统计前10的内存 echo \u0026#34;=========================================${time}======================================================\u0026#34; \u0026gt;\u0026gt; ${memdir}/${day}.log ps aux|head -1 \u0026gt;\u0026gt; ${memdir}/${day}.log ps aux|grep -v PID|sort -rn -k +4|head \u0026gt;\u0026gt; ${memdir}/${day}.log #统计前10的磁盘io echo \u0026#34;=========================================${time}======================================================\u0026#34; \u0026gt;\u0026gt; ${iodir}/${day}.log /usr/sbin/iotop -btoq --iter=1 |head -n 3 \u0026gt;\u0026gt; ${iodir}/${day}.log /usr/sbin/iotop -btoq --iter=1 |awk \u0026#39;NR\u0026gt;3{print}\u0026#39;|sort -rn -k 11 \u0026gt;\u0026gt; ${iodir}/${day}.log #统计机器的网络状态(ip,状态) echo \u0026#34;=========================================${time}======================================================\u0026#34; \u0026gt;\u0026gt; ${netdir}/${day}.log netstat -nt | grep -e 127.0.0.1 -e 0.0.0.0 -e ::: -v | awk \u0026#39;/^tcp/ {++state[$NF]} END {for(i in state) print i,\u0026#34;\\t\u0026#34;,state[i]}\u0026#39; \u0026gt;\u0026gt; ${netdir}/${day}.log echo -n \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${netdir}/${day}.log netstat -n | awk \u0026#39;/^tcp/ {print $0}\u0026#39; | awk \u0026#39;{print $(NF-1),$NF}\u0026#39; | awk -F \u0026#39;:| \u0026#39; \u0026#39;{print $1,$NF}\u0026#39; |awk \u0026#39;BEGIN{print \u0026#34;IP\\t\\t状态\\t次数统计\u0026#34;} {a[$1\u0026#34; \u0026#34;$2]++}END{for(i in a) print(i,a[i])}\u0026#39; |sort -nk 3 \u0026gt;\u0026gt; ${netdir}/${day}.log #删除老化的文件（保留7天） find /system/log/ -type f -mtime +7 -exec rm -f {} \\; 设置定时任务 1 2 3 4 5 6 7 8 #拷贝文件 ansible hadoop -i ./hadoopip -m copy -a \u0026#34;src=/system/check_system.sh dest=/system/check_system.sh \u0026#34; -f 20 #给权限 ansible hadoop -i ./hadoopip -m shell -a \u0026#34;chmod a+x /system/check_system.sh \u0026#34; -f 20 #安装依赖 ansible hadoop -i ./hadoopip -m shell -a \u0026#34;yum install -y iotop \u0026#34; -f 20 #设置定时任务 ansible hadoop -i ./hadoopip -m cron -a \u0026#34;minute=*/5 job=\u0026#39;sh /system/check_system.sh\u0026#39; name=check_system disabled=no\u0026#34; -f 20 运行结果 日志格式 ","date":"2022-05-06T17:06:55Z","image":"https://www.ownit.top/title_pic/43.jpg","permalink":"https://www.ownit.top/p/202205061706/","title":"Linux任务分析脚本"},{"content":"告警日志统计\n告警去重统计 告警人分组邮件 原理图\n此项目主要使用Django开发告警接口，对接Altermanager告警，实现告警人分组，邮件等\n告警信息日志，告警信息统计等等\n开发过程\nmodel\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from django.db import models # Create your models here. class alerts(models.Model): startsAt = models.DateTimeField(verbose_name=\u0026#39;告警产生时间\u0026#39;) endsAt = models.DateTimeField(verbose_name=\u0026#39;告警恢复时间\u0026#39;) instance = models.CharField(max_length=50, verbose_name=\u0026#39;实例\u0026#39;, blank=True) alertname = models.CharField(max_length=100, verbose_name=\u0026#39;告警名称\u0026#39;) status = models.CharField(max_length=20, verbose_name=\u0026#39;状态\u0026#39;, blank=True) severity = models.CharField(max_length=20, verbose_name=\u0026#39;告警级别\u0026#39;, blank=True) message = models.CharField(max_length=1000, verbose_name=\u0026#39;告警信息\u0026#39;, blank=True) known = models.BooleanField(default=False, verbose_name=\u0026#39;知悉\u0026#39;) memo = models.CharField(max_length=50, verbose_name=\u0026#39;知悉备注\u0026#39;, blank=True) def __str__(self): return self.message class Meta: db_table = \u0026#39;alerts\u0026#39; verbose_name = \u0026#39;告警日志\u0026#39; verbose_name_plural = verbose_name # ordering = [\u0026#39;-startsAt\u0026#39;] # 按故障时间倒排 class production(models.Model): startsAt = models.DateTimeField(verbose_name=\u0026#39;告警产生时间\u0026#39;) endsAt = models.DateTimeField(verbose_name=\u0026#39;告警恢复时间\u0026#39;) instance = models.CharField(max_length=50, verbose_name=\u0026#39;实例\u0026#39;, blank=True) alertname = models.CharField(max_length=100, verbose_name=\u0026#39;告警名称\u0026#39;) status = models.CharField(max_length=20, verbose_name=\u0026#39;状态\u0026#39;, blank=True) severity = models.CharField(max_length=20, verbose_name=\u0026#39;告警级别\u0026#39;, blank=True) message = models.CharField(max_length=1000, verbose_name=\u0026#39;告警信息\u0026#39;, blank=True) known = models.BooleanField(default=False, verbose_name=\u0026#39;知悉\u0026#39;) # memo = models.CharField(max_length=50, verbose_name=\u0026#39;知悉备注\u0026#39;, blank=True) def __str__(self): return self.message class Meta: db_table = \u0026#39;production\u0026#39; verbose_name = \u0026#39;告警统计\u0026#39; verbose_name_plural = verbose_name class alarmuser(models.Model): username = models.CharField(max_length=20, verbose_name=\u0026#39;告警人\u0026#39;, blank=True) useremail = models.EmailField(max_length=20, verbose_name=\u0026#39;告警邮件\u0026#39;, blank=True) group = models.CharField(max_length=20, verbose_name=\u0026#39;分组\u0026#39;, blank=True) def __str__(self): return self.username class Meta: db_table = \u0026#39;alarmuser\u0026#39; verbose_name = \u0026#39;告警人配置\u0026#39; verbose_name_plural = verbose_name 注册xadmin后台\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import xadmin from django.contrib import admin from xadmin import views from webhook.models import alerts, production, alarmuser class GlobalSetting: site_title = \u0026#34;长风破浪\u0026#34; site_footer = \u0026#34;长风破浪\u0026#34; menu_style = \u0026#34;accordion\u0026#34; # 这个是设置菜单主题 enable_themes = True use_bootswatch = True refresh_times = [5, 10, 30, 60] xadmin.site.register(views.CommAdminView, GlobalSetting) class AlertsAdmin(object): \u0026#34;\u0026#34;\u0026#34;xadmin的全局配置\u0026#34;\u0026#34;\u0026#34; site_title = \u0026#34;长风破浪\u0026#34; # 设置站点标题 site_footer = \u0026#34;长风破浪\u0026#34; # 设置站点的页脚 menu_style = \u0026#34;accordion\u0026#34; # 设置菜单折叠 \u0026#39;\u0026#39;\u0026#39;设置列表可显示的字段\u0026#39;\u0026#39;\u0026#39; list_display = (\u0026#39;startsAt\u0026#39;, \u0026#39;endsAt\u0026#39;, \u0026#39;instance\u0026#39;, \u0026#39;alertname\u0026#39;, \u0026#39;status\u0026#39;, \u0026#39;severity\u0026#39;, \u0026#39;message\u0026#39;, \u0026#39;known\u0026#39;, \u0026#39;memo\u0026#39;,) list_filter = [\u0026#39;status\u0026#39;, \u0026#39;severity\u0026#39;, \u0026#39;startsAt\u0026#39;, \u0026#39;endsAt\u0026#39;] search_fields = [\u0026#39;instance\u0026#39;, \u0026#39;alertname\u0026#39;] # list_per_page设置每页显示多少条记录，默认是100条 list_per_page = 50 class ProductionAdmin(object): menu_style = \u0026#34;accordion\u0026#34; # 设置菜单折叠 list_display = (\u0026#39;startsAt\u0026#39;, \u0026#39;endsAt\u0026#39;, \u0026#39;instance\u0026#39;, \u0026#39;alertname\u0026#39;, \u0026#39;status\u0026#39;, \u0026#39;severity\u0026#39;, \u0026#39;message\u0026#39;, \u0026#39;known\u0026#39;,) list_filter = [\u0026#39;status\u0026#39;, \u0026#39;severity\u0026#39;, \u0026#39;startsAt\u0026#39;, \u0026#39;endsAt\u0026#39;] search_fields = [\u0026#39;instance\u0026#39;, \u0026#39;alertname\u0026#39;] # list_per_page设置每页显示多少条记录，默认是100条 list_per_page = 50 class AlarmuserAdmin(object): list_display = (\u0026#39;username\u0026#39;, \u0026#39;useremail\u0026#39;, \u0026#39;group\u0026#39;) xadmin.site.register(alerts, AlertsAdmin) xadmin.site.register(production, ProductionAdmin) xadmin.site.register(alarmuser, AlarmuserAdmin) view视图\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 import json import smtplib from email.mime.text import MIMEText import yaml from dateutil import parser from django.http import HttpResponse from webhook.models import alerts as alerts_t, alarmuser from webhook.models import production import datetime from jinja2 import Environment, FileSystemLoader # 获取html目录 class ParseingTemplate: def __init__(self, templatefile): self.templatefile = templatefile def template(self, **kwargs): try: env = Environment(loader=FileSystemLoader(\u0026#39;templates\u0026#39;)) template = env.get_template(self.templatefile) template_content = template.render(kwargs) return template_content except Exception as error: raise error # 时区转换（增加八个小时） def time_zone_conversion(utctime): format_time = parser.parse(utctime).strftime(\u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) time_format = datetime.datetime.strptime(format_time, \u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) return str(time_format + datetime.timedelta(hours=8)) # 获取告警人的分组和邮件 def get_email(email_name=None, action=0): \u0026#34;\u0026#34;\u0026#34; :param email_name: 发送的邮件列表名 :param action: 操作类型，0: 查询收件人的邮件地址列表, 1: 查询收件人的列表名称, 2: 获取邮件账号信息 :return: 根据action的值，返回不通的数据结构 \u0026#34;\u0026#34;\u0026#34; if action == 0: email = alarmuser.objects.filter(group=email_name) email_lsit = [] for i in email: email_lsit.append(i.useremail) print(\u0026#39;显示邮件\u0026#39;, email_lsit) return email_lsit elif action == 1: group_list = [] group = alarmuser.objects.values(\u0026#34;group\u0026#34;).distinct() for i in group: group_list.append(i[\u0026#39;group\u0026#39;]) print(\u0026#39;显示组:\u0026#39;, group_list) # 获取邮件的地址的配置 def get_email_conf(file, email_name=None, action=0): \u0026#34;\u0026#34;\u0026#34; :param file: yaml格式的文件类型 :param email_name: 发送的邮件列表名 :param action: 操作类型，0: 查询收件人的邮件地址列表, 1: 查询收件人的列表名称, 2: 获取邮件账号信息 :return: 根据action的值，返回不通的数据结构 \u0026#34;\u0026#34;\u0026#34; try: with open(file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fr: read_conf = yaml.safe_load(fr) if action == 0: for email in read_conf[\u0026#39;email\u0026#39;]: if email[\u0026#39;name\u0026#39;] == email_name: return email[\u0026#39;receive_addr\u0026#39;] else: print(\u0026#34;%s does not match for %s\u0026#34; % (email_name, file)) else: print(\u0026#34;No recipient address configured\u0026#34;) elif action == 1: return [items[\u0026#39;name\u0026#39;] for items in read_conf[\u0026#39;email\u0026#39;]] elif action == 2: return read_conf[\u0026#39;send\u0026#39;] except KeyError: print(\u0026#34;%s not exist\u0026#34; % email_name) exit(-1) except FileNotFoundError: print(\u0026#34;%s file not found\u0026#34; % file) exit(-2) except Exception as e: raise e # 发送邮件地址 def sendEmail(title, content, receivers=None): if receivers is None: receivers = [\u0026#39;chenf-o@glodon.com\u0026#39;] send_dict = get_email_conf(\u0026#39;email.yaml\u0026#39;, action=2) mail_host = send_dict[\u0026#39;smtp_host\u0026#39;] mail_user = send_dict[\u0026#39;send_user\u0026#39;] mail_pass = send_dict[\u0026#39;send_pass\u0026#39;] sender = send_dict[\u0026#39;send_addr\u0026#39;] print(mail_host, mail_user, mail_pass, sender) msg = MIMEText(content, \u0026#39;html\u0026#39;, \u0026#39;utf-8\u0026#39;) msg[\u0026#39;From\u0026#39;] = \u0026#34;{}\u0026#34;.format(sender) msg[\u0026#39;To\u0026#39;] = \u0026#34;,\u0026#34;.join(receivers) print(receivers) print(msg[\u0026#39;To\u0026#39;]) msg[\u0026#39;Subject\u0026#39;] = title try: smtpObj = smtplib.SMTP_SSL(mail_host, 465) smtpObj.login(mail_user, mail_pass) smtpObj.sendmail(sender, receivers, msg.as_string()) print(\u0026#39;mail send successful.\u0026#39;) except smtplib.SMTPException as e: print(e) # 先保存，后发邮件 def webhook(request): if request.method == \u0026#34;GET\u0026#34;: # email = get_email(action=1) return HttpResponse(\u0026#39;禁止get\u0026#39;) if request.method == \u0026#39;POST\u0026#39;: try: request_data = request.body print(request_data.decode()) request_dict = json.loads(request_data.decode(\u0026#39;utf-8\u0026#39;)) alerts = request_dict[\u0026#39;alerts\u0026#39;] prometheus_data = json.loads(request.body) for i in alerts: msg = i[\u0026#39;annotations\u0026#39;][\u0026#39;message\u0026#39;] if \u0026#39;message\u0026#39; in i[\u0026#39;annotations\u0026#39;] else \u0026#39;null\u0026#39; if msg == \u0026#39;null\u0026#39; and \u0026#39;summary\u0026#39; in i[\u0026#39;annotations\u0026#39;]: msg = i[\u0026#39;annotations\u0026#39;][\u0026#39;summary\u0026#39;] print(i[\u0026#39;startsAt\u0026#39;][0:19] + i[\u0026#39;status\u0026#39;] + \u0026#34; :\u0026#34; + msg) if msg == \u0026#39;null\u0026#39;: print(i) ints = i[\u0026#39;labels\u0026#39;][\u0026#39;instance\u0026#39;] if \u0026#39;instance\u0026#39; in i[\u0026#39;labels\u0026#39;] else \u0026#39;unknown\u0026#39; print(ints + \u0026#34; --- \u0026#34; + i[\u0026#39;labels\u0026#39;][\u0026#39;alertname\u0026#39;] + \u0026#34; ---- \u0026#34; + i[\u0026#39;labels\u0026#39;][\u0026#39;severity\u0026#39;]) print(i[\u0026#39;endsAt\u0026#39;]) a = alerts_t() a.startsAt = time_zone_conversion(i[\u0026#39;startsAt\u0026#39;]) str = \u0026#39;0001-01-01\u0026#39; print(str in i[\u0026#39;endsAt\u0026#39;]) if (str in i[\u0026#39;endsAt\u0026#39;]): a.endsAt = \u0026#39;0001-01-01 00:00:00\u0026#39; else: a.endsAt = time_zone_conversion(i[\u0026#39;endsAt\u0026#39;]) print(a.endsAt) a.instance = ints a.alertname = i[\u0026#39;labels\u0026#39;][\u0026#39;alertname\u0026#39;] if i[\u0026#39;status\u0026#39;] == \u0026#39;firing\u0026#39;: a.status = \u0026#39;告警中\u0026#39; if i[\u0026#39;status\u0026#39;] == \u0026#39;resolved\u0026#39;: a.status = \u0026#39;已恢复\u0026#39; a.known = True a.severity = i[\u0026#39;labels\u0026#39;][\u0026#39;severity\u0026#39;] a.message = msg a.save() startime = time_zone_conversion(i[\u0026#39;startsAt\u0026#39;]) endtime = a.endsAt instances = ints AlarmObject = production.objects.filter(startsAt=startime, endsAt=endtime, instance=instances) print(AlarmObject) if AlarmObject.exists(): pass else: b = production() b.startsAt = time_zone_conversion(i[\u0026#39;startsAt\u0026#39;]) # print(str in i[\u0026#39;endsAt\u0026#39;]) if (str in i[\u0026#39;endsAt\u0026#39;]): b.endsAt = \u0026#39;0001-01-01 00:00:00\u0026#39; else: b.endsAt = time_zone_conversion(i[\u0026#39;endsAt\u0026#39;]) # print(b.endsAt) b.instance = ints b.alertname = i[\u0026#39;labels\u0026#39;][\u0026#39;alertname\u0026#39;] if i[\u0026#39;status\u0026#39;] == \u0026#39;firing\u0026#39;: b.status = \u0026#39;告警中\u0026#39; if i[\u0026#39;status\u0026#39;] == \u0026#39;resolved\u0026#39;: b.status = \u0026#39;已恢复\u0026#39; b.known = True b.severity = i[\u0026#39;labels\u0026#39;][\u0026#39;severity\u0026#39;] b.message = msg b.save() print(\u0026#39;显示a:\u0026#39;, a) # 时间转换，转换成东八区时间 for k, v in prometheus_data.items(): if k == \u0026#39;alerts\u0026#39;: for items in v: if items[\u0026#39;status\u0026#39;] == \u0026#39;firing\u0026#39;: items[\u0026#39;startsAt\u0026#39;] = time_zone_conversion(items[\u0026#39;startsAt\u0026#39;]) else: items[\u0026#39;startsAt\u0026#39;] = time_zone_conversion(items[\u0026#39;startsAt\u0026#39;]) items[\u0026#39;endsAt\u0026#39;] = time_zone_conversion(items[\u0026#39;endsAt\u0026#39;]) print(prometheus_data) team_name = prometheus_data[\u0026#34;commonLabels\u0026#34;][\u0026#34;team\u0026#34;] print(team_name) generate_html_template_subj = ParseingTemplate(\u0026#39;email_template_firing.html\u0026#39;) html_template_content = generate_html_template_subj.template( prometheus_monitor_info=prometheus_data ) # 获取收件人邮件列表 email_list = get_email(email_name=team_name, action=0) print(email_list) print(prometheus_data[\u0026#39;commonLabels\u0026#39;][\u0026#39;alertname\u0026#39;]) sendEmail( prometheus_data[\u0026#39;commonLabels\u0026#39;][\u0026#39;alertname\u0026#39;], html_template_content, receivers=email_list ) # return \u0026#34;prometheus monitor\u0026#34; return HttpResponse(1) except Exception as e: print(e) raise e # finally: # return HttpResponse(1) 需要完整代码，请留言\n稍后整理完毕传到github上\nGitHub - nangongchengfeng/Alerts\n屎一样的代码，请大佬忽略。后期会使用falsk 更新\n","date":"2022-04-03T15:44:28Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202204031544/","title":"Django开发告警接口（webhook）对接Altermanager告警"},{"content":"1.Admin组件使用\nDjango内集成了web管理工具，Django在启动过程中会执行setting.py文件，初始化Django内置组件、注册APP、添加环境变量等\n极简美化 效果图\n登录页面\n主页面\n使用django-simpleui模块；\n直接pip安装即可：\n1 pip install django-simpleui 然后在setting.py中注册即可：\n1 2 3 4 5 6 7 8 9 10 11 INSTALLED_APPS = [ \u0026#39;simpleui\u0026#39;, #主要是这个美化页面的 \u0026#39;django.contrib.admin\u0026#39;, \u0026#39;django.contrib.auth\u0026#39;, \u0026#39;django.contrib.contenttypes\u0026#39;, \u0026#39;django.contrib.sessions\u0026#39;, \u0026#39;django.contrib.messages\u0026#39;, \u0026#39;django.contrib.staticfiles\u0026#39;, \u0026#39;blog.apps.BlogConfig\u0026#39;, # 注册app应用 \u0026#39;DjangoUeditor\u0026#39;, # 注册APP应用 ] 然后就完事了，打开admin即可，如下：\n","date":"2022-03-18T21:44:27Z","image":"https://www.ownit.top/title_pic/56.jpg","permalink":"https://www.ownit.top/p/202203182144/","title":"Django admin极简美化"},{"content":"场景： 公司业务变更，部分机器需要变更，下线重新调整相关的业务。\n我们机器上有zabbix和prometheus监控，下线需要清空这些监控。\nprometheus比较监控，可以直接使用consul自动注册。\nzabbix 删除下线就很麻烦。一台就可以手动删除，但是上百台那，一个个手动吗，很麻烦的。\n所以直接考虑zabbix的APi来批量删除主机\n介绍 代码放在下方\n代码使用的是python2.0的\n因为我是纯内网环境需要先在外网部署好环境 ，然后打包使用的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 Python3虚拟环境创建 /usr/local/python3/bin/python3.7 -m venv py3 加载虚拟环境 source py3/bin/activate 安装依赖 pip install -r requirements.txt 生成依赖 pip freeze \u0026gt; requirements.txt python2 虚拟环境 1.安装virtualenv pip install virtualenv 2.创建虚拟环境 virtualenv 虚拟环境名称 3.激活虚拟环境 source 虚拟环境目录/bin/activate （./虚拟环境目录/bin/activate） 4.退出虚拟环境 deactivate 我们采用python2.0创建虚拟环境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@zabbix zabbix]# virtualenv py2 [root@zabbix zabbix]# ls py2 zbx_tool.py [root@zabbix zabbix]# source py2/ bin/ .gitignore include/ lib/ lib64/ pyvenv.cfg python wheel2 [root@zabbix zabbix]# source py2/bin/activate (py2) [root@zabbix zabbix]# ls py2 zbx_tool.py (py2) [root@zabbix zabbix]# pip install argparse ......... (py2) [root@zabbix zabbix]# ls py2 zbx_tool.py (py2) [root@zabbix zabbix]# python 打包 上传虚拟环境 tar -zcvf zabbix.tar.gz zabbix 上传内网环境进行解压，然后加载就可以使用虚拟环境\n1 source py2/bin/activate 生产环境实战 删除单台主机 批量删除 for循环删除 首先新建有一个文本，然后通过for 循环批量删除\n1 for ip in `cat zabbixip`;do python zbx_tool.py --delete $ip ;done ** 开始**\n批量操作\n代码 集合脚本，包含查询主机，主机组，模板，添加开启禁用删除主机等功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 #!/usr/bin/env python #-*- coding: utf-8 -*- import json import sys import urllib2 import argparse from urllib2 import URLError reload(sys) sys.setdefaultencoding(\u0026#39;utf-8\u0026#39;) class zabbix_api: def __init__(self): self.url = \u0026#39;http://zabbix.xxx.com/api_jsonrpc.php\u0026#39; self.header = {\u0026#34;Content-Type\u0026#34;:\u0026#34;application/json\u0026#34;} def user_login(self): data = json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;user.login\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;zabbix\u0026#34; }, \u0026#34;id\u0026#34;: 0 }) request = urllib2.Request(self.url, data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: print \u0026#34;\\033[041m 认证失败，请检查URL !\\033[0m\u0026#34;,e.code except KeyError as e: print \u0026#34;\\033[041m 认证失败，请检查用户名密码 !\\033[0m\u0026#34;,e else: response = json.loads(result.read()) result.close() #print response[\u0026#39;result\u0026#39;] self.authID = response[\u0026#39;result\u0026#39;] return self.authID def hostid_get_hostname(self, hostId=\u0026#39;\u0026#39;): data = json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;host.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: \u0026#34;extend\u0026#34;, \u0026#34;filter\u0026#34;: {\u0026#34;hostid\u0026#34;: hostId} }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url, data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: if hasattr(e, \u0026#39;reason\u0026#39;): print \u0026#39;We failed to reach a server.\u0026#39; print \u0026#39;Reason: \u0026#39;, e.reason elif hasattr(e, \u0026#39;code\u0026#39;): print \u0026#39;The server could not fulfill the request.\u0026#39; print \u0026#39;Error code: \u0026#39;, e.code else: response = json.loads(result.read()) #print response result.close() if not len(response[\u0026#39;result\u0026#39;]): print \u0026#34;hostId is not exist\u0026#34; return False #print \u0026#34;主机数量: \\33[31m%s\\33[0m\u0026#34; % (len(response[\u0026#39;result\u0026#39;])) host_dict=dict() for host in response[\u0026#39;result\u0026#39;]: status = {\u0026#34;0\u0026#34;: \u0026#34;OK\u0026#34;, \u0026#34;1\u0026#34;: \u0026#34;Disabled\u0026#34;} available = {\u0026#34;0\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;1\u0026#34;: \u0026#34;available\u0026#34;, \u0026#34;2\u0026#34;: \u0026#34;Unavailable\u0026#34;} #if len(hostId) == 0: # print \u0026#34;HostID : %s\\t HostName : %s\\t Status :\\33[32m%s\\33[0m \\t Available :\\33[31m%s\\33[0m\u0026#34; % ( # host[\u0026#39;hostid\u0026#39;], host[\u0026#39;name\u0026#39;], status[host[\u0026#39;status\u0026#39;]], available[host[\u0026#39;available\u0026#39;]]) #else: # print \u0026#34;HostID : %s\\t HostName : %s\\t Status :\\33[32m%s\\33[0m \\t Available :\\33[31m%s\\33[0m\u0026#34; % ( # host[\u0026#39;hostid\u0026#39;], host[\u0026#39;name\u0026#39;], status[host[\u0026#39;status\u0026#39;]], available[host[\u0026#39;available\u0026#39;]]) host_dict[\u0026#39;name\u0026#39;]=host[\u0026#39;name\u0026#39;] host_dict[\u0026#39;status\u0026#39;]=status[host[\u0026#39;status\u0026#39;]] host_dict[\u0026#39;available\u0026#39;]=available[host[\u0026#39;available\u0026#39;]] return host_dict def hostid_get_hostip(self, hostId=\u0026#39;\u0026#39;): data = json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;hostinterface.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: \u0026#34;extend\u0026#34;, \u0026#34;filter\u0026#34;: {\u0026#34;hostid\u0026#34;: hostId} }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url, data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: if hasattr(e, \u0026#39;reason\u0026#39;): print \u0026#39;We failed to reach a server.\u0026#39; print \u0026#39;Reason: \u0026#39;, e.reason elif hasattr(e, \u0026#39;code\u0026#39;): print \u0026#39;The server could not fulfill the request.\u0026#39; print \u0026#39;Error code: \u0026#39;, e.code else: response = json.loads(result.read()) # print response result.close() if not len(response[\u0026#39;result\u0026#39;]): print \u0026#34;\\033[041m hostid \\033[0m is not exist\u0026#34; return False #print \u0026#34;主机数量: \\33[31m%s\\33[0m\u0026#34; % (len(response[\u0026#39;result\u0026#39;])) for hostip in response[\u0026#39;result\u0026#39;]: #print hostip #if len(hostip) == 0: # print \u0026#34;HostID : %s\\t HostIp : %s \\t Port : %s \u0026#34; % (hostip[\u0026#39;hostid\u0026#39;], hostip[\u0026#39;ip\u0026#39;], hostip[\u0026#39;port\u0026#39;]) #else: # print \u0026#34;HostID : %s\\t HostIp :\\33[32m%s\\33[0m \\t Port :\\33[31m%s\\33[0m\u0026#34; % ( # hostip[\u0026#39;hostid\u0026#39;], hostip[\u0026#39;ip\u0026#39;], hostip[\u0026#39;port\u0026#39;]) return hostip[\u0026#39;ip\u0026#39;] def host_get(self,hostName=\u0026#39;\u0026#39;): data=json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;host.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: \u0026#34;extend\u0026#34;, #\u0026#34;filter\u0026#34;:{\u0026#34;host\u0026#34;:\u0026#34;\u0026#34;} \u0026#34;filter\u0026#34;:{\u0026#34;host\u0026#34;:hostName} }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url,data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: if hasattr(e, \u0026#39;reason\u0026#39;): print \u0026#39;We failed to reach a server.\u0026#39; print \u0026#39;Reason: \u0026#39;, e.reason elif hasattr(e, \u0026#39;code\u0026#39;): print \u0026#39;The server could not fulfill the request.\u0026#39; print \u0026#39;Error code: \u0026#39;, e.code else: response = json.loads(result.read()) #print reqponse result.close() if not len(response[\u0026#39;result\u0026#39;]): print \u0026#34;\\033[041m %s \\033[0m is not exist\u0026#34; % hostName return False print \u0026#34;主机数量: \\033[31m%s\\033[0m\u0026#34;%(len(response[\u0026#39;result\u0026#39;])) for host in response[\u0026#39;result\u0026#39;]: status={\u0026#34;0\u0026#34;:\u0026#34;OK\u0026#34;,\u0026#34;1\u0026#34;:\u0026#34;Disabled\u0026#34;} available={\u0026#34;0\u0026#34;:\u0026#34;Unknown\u0026#34;,\u0026#34;1\u0026#34;:\u0026#34;available\u0026#34;,\u0026#34;2\u0026#34;:\u0026#34;Unavailable\u0026#34;} #print host if len(hostName)==0: print \u0026#34;HostID : %s\\t HostName : %s\\t HostIp : %s\\t Status :%s \\t Available :%s\u0026#34;%(host[\u0026#39;hostid\u0026#39;],host[\u0026#39;name\u0026#39;],self.hostid_get_hostip(hostId=host[\u0026#39;hostid\u0026#39;]),status[host[\u0026#39;status\u0026#39;]],available[host[\u0026#39;available\u0026#39;]]) else: print \u0026#34;HostID : %s\\t HostName : %s\\t HostIp : %s\\t Status :\\033[32m%s\\033[0m \\t Available :\\033[31m%s\\033[0m\u0026#34;%(host[\u0026#39;hostid\u0026#39;],host[\u0026#39;name\u0026#39;],self.hostid_get_hostip(hostId=host[\u0026#39;hostid\u0026#39;]),status[host[\u0026#39;status\u0026#39;]],available[host[\u0026#39;available\u0026#39;]]) return host[\u0026#39;hostid\u0026#39;] def hostip_get(self, hostIp=\u0026#39;\u0026#39;): data = json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;hostinterface.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: \u0026#34;extend\u0026#34;, \u0026#34;filter\u0026#34;: {\u0026#34;ip\u0026#34;: hostIp} }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url, data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: if hasattr(e, \u0026#39;reason\u0026#39;): print \u0026#39;We failed to reach a server.\u0026#39; print \u0026#39;Reason: \u0026#39;, e.reason elif hasattr(e, \u0026#39;code\u0026#39;): print \u0026#39;The server could not fulfill the request.\u0026#39; print \u0026#39;Error code: \u0026#39;, e.code else: response = json.loads(result.read()) # print response result.close() if not len(response[\u0026#39;result\u0026#39;]): print \u0026#34;\\033[041m hostip \\033[0m is not exist\u0026#34; return False print \u0026#34;主机数量: \\33[31m%s\\33[0m\u0026#34; % (len(response[\u0026#39;result\u0026#39;])) for hostip in response[\u0026#39;result\u0026#39;]: host = self.hostid_get_hostname(hostip[\u0026#39;hostid\u0026#39;]) if len(hostip) == 0: print \u0026#34;HostID : %s\\t HostName : %s\\t HostIp : %s\\t Status :\\33[32m%s\\33[0m \\t Available :\\33[31m%s\\33[0m\u0026#34;%(hostip[\u0026#39;hostid\u0026#39;],host[\u0026#39;name\u0026#39;],hostip[\u0026#39;ip\u0026#39;],host[\u0026#39;status\u0026#39;],host[\u0026#39;available\u0026#39;]) else: print \u0026#34;HostID : %s\\t HostName : %s\\t HostIp : %s\\t Status :\\33[32m%s\\33[0m \\t Available :\\33[31m%s\\33[0m\u0026#34;%(hostip[\u0026#39;hostid\u0026#39;],host[\u0026#39;name\u0026#39;],hostip[\u0026#39;ip\u0026#39;],host[\u0026#39;status\u0026#39;],host[\u0026#39;available\u0026#39;]) return hostip[\u0026#39;hostid\u0026#39;] def hostgroup_get(self, hostgroupName=\u0026#39;\u0026#39;): data = json.dumps({ \u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;:\u0026#34;hostgroup.get\u0026#34;, \u0026#34;params\u0026#34;:{ \u0026#34;output\u0026#34;: \u0026#34;extend\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;name\u0026#34;: hostgroupName } }, \u0026#34;auth\u0026#34;:self.user_login(), \u0026#34;id\u0026#34;:1, }) request = urllib2.Request(self.url,data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: print \u0026#34;Error as \u0026#34;, e else: # result.read() response = json.loads(result.read()) result.close() #print response() if not len(response[\u0026#39;result\u0026#39;]): print \u0026#34;\\033[041m %s \\033[0m is not exist\u0026#34; % hostgroupName return False for group in response[\u0026#39;result\u0026#39;]: if len(hostgroupName)==0: print \u0026#34;hostgroup: \\033[31m%s\\033[0m \\tgroupid : %s\u0026#34; %(group[\u0026#39;name\u0026#39;],group[\u0026#39;groupid\u0026#39;]) else: print \u0026#34;hostgroup: \\033[31m%s\\033[0m\\tgroupid : %s\u0026#34; %(group[\u0026#39;name\u0026#39;],group[\u0026#39;groupid\u0026#39;]) self.hostgroupID = group[\u0026#39;groupid\u0026#39;] return group[\u0026#39;groupid\u0026#39;] def template_get(self,templateName=\u0026#39;\u0026#39;): data = json.dumps({ \u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;template.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: \u0026#34;extend\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;name\u0026#34;:templateName } }, \u0026#34;auth\u0026#34;:self.user_login(), \u0026#34;id\u0026#34;:1, }) request = urllib2.Request(self.url, data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: print \u0026#34;Error as \u0026#34;, e else: response = json.loads(result.read()) result.close() #print response if not len(response[\u0026#39;result\u0026#39;]): print \u0026#34;\\033[041m %s \\033[0m is not exist\u0026#34; % templateName return False for template in response[\u0026#39;result\u0026#39;]: if len(templateName)==0: print \u0026#34;template : %s \\t id : %s\u0026#34; % (template[\u0026#39;name\u0026#39;], template[\u0026#39;templateid\u0026#39;]) else: self.templateID = response[\u0026#39;result\u0026#39;][0][\u0026#39;templateid\u0026#39;] print \u0026#34;Template Name :%s\u0026#34;%templateName return response[\u0026#39;result\u0026#39;][0][\u0026#39;templateid\u0026#39;] def hostgroup_create(self,hostgroupName): if self.hostgroup_get(hostgroupName): print \u0026#34;hostgroup \\033[42m%s\\033[0m is exist !\u0026#34; % hostgroupName sys.exit(1) data = json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;hostgroup.create\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;name\u0026#34;: hostgroupName }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request=urllib2.Request(self.url,data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) except URLError as e: print \u0026#34;Error as \u0026#34;, e else: response = json.loads(result.read()) result.close() print \u0026#34;添加主机组:%s hostgroupID : %s\u0026#34;%(hostgroupName,self.hostgroup_get(hostgroupName)) def host_create(self, hostIp, hostgroupName, templateName, hostName): if self.host_get(hostName) or self.hostip_get(hostIp): print \u0026#34;该主机已经添加!\u0026#34; sys.exit(1) group_list=[] template_list=[] for i in hostgroupName.split(\u0026#39;,\u0026#39;): var = {} var[\u0026#39;groupid\u0026#39;] = self.hostgroup_get(i) group_list.append(var) for i in templateName.split(\u0026#39;,\u0026#39;): var={} var[\u0026#39;templateid\u0026#39;]=self.template_get(i) template_list.append(var) data = json.dumps({ \u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;:\u0026#34;host.create\u0026#34;, \u0026#34;params\u0026#34;:{ \u0026#34;host\u0026#34;: hostName, \u0026#34;interfaces\u0026#34;: [ { \u0026#34;type\u0026#34;: 1, \u0026#34;main\u0026#34;: 1, \u0026#34;useip\u0026#34;: 1, \u0026#34;ip\u0026#34;: hostIp, \u0026#34;dns\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;10050\u0026#34; } ], \u0026#34;groups\u0026#34;: group_list, \u0026#34;templates\u0026#34;: template_list, }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;:1 }) request = urllib2.Request(self.url, data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) response = json.loads(result.read()) result.close() print \u0026#34;add host : %s id :%s\u0026#34; % (hostIp, hostName) except URLError as e: print \u0026#34;Error as \u0026#34;, e except KeyError as e: print \u0026#34;\\033[041m 主机添加有误，请检查模板正确性或主机是否添加重复 !\\033[0m\u0026#34;,e print response def host_disable(self,hostip): data=json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;host.update\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;hostid\u0026#34;: self.host_get(hostip), \u0026#34;status\u0026#34;: 1 }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url,data) #opener = urllib2.build_opener(urllib2.HTTPBasicAuthHandler(TerminalPassword())) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) #result = opener.open(request) except URLError as e: print \u0026#34;Error as \u0026#34;, e else: response = json.loads(result.read()) result.close() print \u0026#39;------------主机现在状态------------\u0026#39; print self.host_get(hostip) def host_enable(self,hostip): data=json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;host.update\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;hostid\u0026#34;: self.host_get(hostip), \u0026#34;status\u0026#34;: 0 }, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url,data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) #result = opener.open(request) except URLError as e: print \u0026#34;Error as \u0026#34;, e else: response = json.loads(result.read()) result.close() print \u0026#39;------------主机现在状态------------\u0026#39; print self.host_get(hostip) def host_delete(self,hostNames): hostid_list=[] for hostName in hostNames.split(\u0026#39;,\u0026#39;): hostid = self.host_get(hostName=hostName) if not hostid: print \u0026#34;主机 \\033[041m %s\\033[0m 删除失败 !\u0026#34; % hostName sys.exit() hostid_list.append(hostid) data=json.dumps({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;host.delete\u0026#34;, \u0026#34;params\u0026#34;: hostid_list, \u0026#34;auth\u0026#34;: self.user_login(), \u0026#34;id\u0026#34;: 1 }) request = urllib2.Request(self.url,data) for key in self.header: request.add_header(key, self.header[key]) try: result = urllib2.urlopen(request) result.close() print \u0026#34;主机 \\033[041m %s\\033[0m 已经删除 !\u0026#34; % hostName except Exception,e: print e if __name__ == \u0026#34;__main__\u0026#34;: zabbix=zabbix_api() parser=argparse.ArgumentParser(description=\u0026#39;zabbix api \u0026#39;,usage=\u0026#39;%(prog)s [options]\u0026#39;) parser.add_argument(\u0026#39;-H\u0026#39;,\u0026#39;--host\u0026#39;,nargs=\u0026#39;?\u0026#39;,dest=\u0026#39;listhost\u0026#39;,default=\u0026#39;host\u0026#39;,help=\u0026#39;查询主机\u0026#39;) parser.add_argument(\u0026#39;-G\u0026#39;,\u0026#39;--group\u0026#39;,nargs=\u0026#39;?\u0026#39;,dest=\u0026#39;listgroup\u0026#39;,default=\u0026#39;group\u0026#39;,help=\u0026#39;查询主机组\u0026#39;) parser.add_argument(\u0026#39;-T\u0026#39;,\u0026#39;--template\u0026#39;,nargs=\u0026#39;?\u0026#39;,dest=\u0026#39;listtemp\u0026#39;,default=\u0026#39;template\u0026#39;,help=\u0026#39;查询模板信息\u0026#39;) parser.add_argument(\u0026#39;-A\u0026#39;,\u0026#39;--add-group\u0026#39;,nargs=1,dest=\u0026#39;addgroup\u0026#39;,help=\u0026#39;添加主机组\u0026#39;) parser.add_argument(\u0026#39;-C\u0026#39;,\u0026#39;--add-host\u0026#39;,dest=\u0026#39;addhost\u0026#39;,nargs=4,metavar=(\u0026#39;192.168.2.1\u0026#39;, \u0026#39;groupname\u0026#39;, \u0026#39;Template01,Template02\u0026#39;, \u0026#39;hostName\u0026#39;),help=\u0026#39;添加主机,多个主机组或模板使用逗号\u0026#39;) parser.add_argument(\u0026#39;-d\u0026#39;,\u0026#39;--disable\u0026#39;,dest=\u0026#39;disablehost\u0026#39;,nargs=\u0026#39;+\u0026#39;,metavar=(\u0026#39;sh-aa-01\u0026#39;),help=\u0026#39;禁用主机,填写主机名，多个主机名之间用逗号\u0026#39;) parser.add_argument(\u0026#39;-e\u0026#39;,\u0026#39;--enable\u0026#39;,dest=\u0026#39;enablehost\u0026#39;,nargs=1,metavar=(\u0026#39;sh-aa-01\u0026#39;),help=\u0026#39;开启主机\u0026#39;) parser.add_argument(\u0026#39;-D\u0026#39;,\u0026#39;--delete\u0026#39;,dest=\u0026#39;deletehost\u0026#39;,nargs=\u0026#39;+\u0026#39;,metavar=(\u0026#39;sh-aa-01\u0026#39;),help=\u0026#39;删除主机,多个主机之间用逗号\u0026#39;) parser.add_argument(\u0026#39;-v\u0026#39;,\u0026#39;--version\u0026#39;, action=\u0026#39;version\u0026#39;, version=\u0026#39;%(prog)s 1.0\u0026#39;) if len(sys.argv) == 1: #print parser.print_help() #print zabbix.host_get(hostName=\u0026#39;bbb\u0026#39;) #print zabbix.hostip_get(hostIp=\u0026#39;127.0.0.1\u0026#39;) #print zabbix.hostid_get_hostname(hostId=\u0026#39;10108\u0026#39;) #print zabbix.hostid_get_hostid(hostId=\u0026#39;10105\u0026#39;) #print zabbix.hostgroup_get(hostgroupName=\u0026#39;Linux servers\u0026#39;) #print zabbix.hostgroup_get(hostgroupName=\u0026#39;aaa\u0026#39;) # ... print zabbix.host_delete(\u0026#39;hz-aaa-02\u0026#39;) else: args = parser.parse_args() if args.listhost != \u0026#39;host\u0026#39;: if args.listhost: zabbix.host_get(args.listhost) else: zabbix.host_get() if args.listgroup != \u0026#39;group\u0026#39;: if args.listgroup: zabbix.hostgroup_get(args.listgroup) else: zabbix.hostgroup_get() if args.listtemp != \u0026#39;template\u0026#39;: if args.listtemp: zabbix.template_get(args.listtemp) else: zabbix.template_get() if args.addgroup: zabbix.hostgroup_create(args.addgroup[0]) if args.addhost: zabbix.host_create(args.addhost[0], args.addhost[1], args.addhost[2], args.addhost[3]) if args.disablehost: zabbix.host_disable(args.disablehost) if args.deletehost: zabbix.host_delete(args.deletehost[0]) 用法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt; python zbx_tool.py -h usage: zbx_tool.py [options] zabbix api optional arguments: -h, --help show this help message and exit -H [LISTHOST], --host [LISTHOST] 查询主机 -G [LISTGROUP], --group [LISTGROUP] 查询主机组 -T [LISTTEMP], --template [LISTTEMP] 查询模板信息 -A ADDGROUP, --add-group ADDGROUP 添加主机组 -C 192.168.2.1 groupname Template01,Template02 hostName, --add-host 192.168.2.1 groupname Template01,Template02 hostName 添加主机,多个主机组或模板使用逗号 -d sh-aa-01 [sh-aa-01 ...], --disable sh-aa-01 [sh-aa-01 ...] 禁用主机,填写主机名，多个主机名之间 ¨逗号 -e sh-aa-01, --enable sh-aa-01 开启主机 -D sh-aa-01 [sh-aa-01 ...], --delete sh-aa-01 [sh-aa-01 ...] 删除主机,多个主机之间用逗号 -v, --version show program\u0026#39;s version number and exit 相关博客\n利用zabbix API批量查询主机、模板、添加删除主机 - 代码先锋网\n","date":"2022-03-16T12:50:02Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/202203161250/","title":"Zabbix的API批量添加，删除主机（生产实战）"},{"content":" 容器本身特性： 采集目标多：容器本身的特性导致采集目标多，需要采集容器内日志、容器 stdout。对于容器内部的文件日志采集，现在并没有一个很好的工具能够去动态发现采集。针对每种数据源都有对应的采集软件，但缺乏一站式的工具。 弹性伸缩难：kubernetes 是分布式的集群，服务、环境的弹性伸缩对于日志采集带来了很大的困难，无法像传统虚拟机环境下那样，事先配置好日志的采集路径等信息，采集的动态性以及数据完整性是非常大的挑战。 现有日志工具的一些缺陷： 缺乏动态配置的能力。目前的采集工具都需要事先手动配置好日志采集方式和路径等信息，因为它无法能够自动感知到容器的生命周期变化或者动态漂移，所以它无法动态地去配置。 日志采集重复或丢失的问题。因为现在的一些采集工具基本上是通过 tail 的方式来进行日志采集的，那么这里就可能存在两个方面的问题：一个是可能导致日志丢失，比如采集工具在重启的过程中，而应用依然在写日志，那么就有可能导致这个窗口期的日志丢失；而对于这种情况一般保守的做法就是，默认往前多采集 1M 日志或 2M 的日志，那么这就又会可能引起日志采集重复的问题。 未明确标记日志源。因为一个应用可能有很多个容器，输出的应用日志也是一样的，那么当我们将所有应用日志收集到统一日志存储后端时，在搜索日志的时候，我们就无法明确这条日志具体是哪一个节点上的哪一个应用容器产生的。 本文档将介绍一种 Docker 日志收集工具 log-pilot，结合 Elasticsearch 和 kibana 等工具，形成一套适用于 kubernetes 环境下的一站式日志解决方案。\nGitHub - AliyunContainerService/log-pilot: Collect logs for docker containersCollect logs for docker containers. Contribute to AliyunContainerService/log-pilot development by creating an account on GitHub.https://github.com/AliyunContainerService/log-pilot容器日志采集利器Log-Pilot-阿里云开发者社区容器时代越来越多的传统应用将会逐渐容器化，而日志又是应用的一个关键环节，那么在应用容器化过程中，如何方便快捷高效地来自动发现和采集应用的日志，如何与日志存储系统协同来高效存储和搜索应用日志，本文将主要跟大家分享下如何通过Log-Pilot来采集容器的标准输出日志和容器内文件日志。https://developer.aliyun.com/article/674327\nLog-Pilot 支持容器事件管理，它能够动态地监听容器的事件变化，然后依据容器的标签来进行解析，生成日志采集配置文件，然后交由采集插件来进行日志采集。\n相关特点和优势，可以点击上方的链接学习。\n方案 从日志的采集方式上，在我看来方案大致主要分为两种：\n**（1）POD 里面安装logging agent**\n每个pod里面都要安装一个agent，无论是以放在本container还是以sidecar的方式部署，很明显会占用很多资源，基本不推荐\n**（2）在节点上安装logging agent（推荐）**\n其实容器stdout,stderr的日志最终也是落在宿主机上，而容器内的路径可以通过配置volumeMount 在宿主机上配置映射即可，所以这种方式还是最可行的\n当然应用还可以自己通过代码直接上报给日志服务，但是这种方式不够通用，还增加了业务代码的复杂性\nlog-Pilot是一个智能容器日志采集工具，它不仅能够高效便捷地将容器日志采集输出到多种存储日志后端，同时还能够动态地发现和采集容器内部的日志文件，更多咨询可以移步这里。\nlog-Pilot目前支持两种工具对日志进行收集，Fluentd Plugin 和 Filebeat Plugin。\nLog-Pilot支持容器事件管理，它能够动态地监听容器的事件变化，然后依据容器的标签来进行解析，生成日志采集配置文件，然后交由采集插件来进行日志采集\n以下的的安装配置，是基于上一篇的EFK的配置，只是修改收集端（有必要，可以了解上一篇的文章）\nKubernetes安装EFK日志收集_南宫乘风-Linux运维-虚拟化容器-Python编程 ownit.top-CSDN博客\n准备 参考官方部署文档的基础上使用本项目manifests/efk/部署，以下为几点主要的修改：\n增加 log-Pilot 部署代码（解决 不支持 es7.0 版本以上的痛点） 修改官方docker镜像，方便国内下载加速 修改 es-statefulset.yaml 支持日志存储持久化等 增加自动清理日志 创建 Elasticsearch 集群 1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: logging 1、 es-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kind: Service apiVersion: v1 metadata: name: elasticsearch namespace: logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node 定义了一个名为 elasticsearch 的 Service，指定标签 app=elasticsearch，当我们将 Elasticsearch StatefulSet 与此服务关联时，服务将返回带有标签 app=elasticsearch的 Elasticsearch Pods 的 DNS A 记录，然后设置 clusterIP=None，将该服务设置成无头服务。最后，我们分别定义端口9200、9300，分别用于与 REST API 交互，以及用于节点间通信。 1 2 3 4 5 [root@master01 efk]# kubectl apply -f es-service.yaml service/elasticsearch-logging created [root@master01 efk]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-logging ClusterIP None \u0026lt;none\u0026gt; 9200/TCP 15s es-statefulset.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 apiVersion: apps/v1 kind: StatefulSet metadata: name: es namespace: logging spec: serviceName: elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: nodeSelector: #记得给节点把标签，不然会找到不符合的节点 es: log initContainers: - name: increase-vm-max-map image: busybox command: [\u0026#34;sysctl\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;vm.max_map_count=262144\u0026#34;] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ulimit -n 65536\u0026#34;] securityContext: privileged: true containers: - name: elasticsearch image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2 ports: - name: rest containerPort: 9200 - name: inter containerPort: 9300 resources: limits: cpu: 1000m requests: cpu: 1000m volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: cluster.initial_master_nodes value: \u0026#34;es-0,es-1,es-2\u0026#34; - name: discovery.zen.minimum_master_nodes value: \u0026#34;2\u0026#34; - name: discovery.seed_hosts value: \u0026#34;elasticsearch\u0026#34; - name: ES_JAVA_OPTS value: \u0026#34;-Xms512m -Xmx512m\u0026#34; - name: network.host value: \u0026#34;0.0.0.0\u0026#34; volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: nfsdata resources: requests: storage: 5Gi Pods 部署完成后，我们可以通过请求一个 REST API 来检查 Elasticsearch 集群是否正常运行。使用下面的命令将本地端口9200 转发到 Elasticsearch 节点（如es-0）对应的端口：\n1 2 3 [root@master01 new]# kubectl port-forward es-0 9200:9200 --namespace=logging Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200 1 curl http://localhost:9200/_cluster/state?pretty 看到上面的信息就表明我们名为 k8s-logs 的 Elasticsearch 集群成功创建了3个节点：es-0，es-1，和es-2，当前主节点是 es-0\n创建 Kibana 服务 Elasticsearch 集群启动成功了，接下来我们可以来部署 Kibana 服务，新建一个名为 kibana.yaml 的文件，对应的文件内容如下：\nkibana.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion: v1 kind: Service metadata: name: kibana namespace: logging labels: app: kibana spec: ports: - port: 5601 type: NodePort selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: logging labels: app: kibana spec: selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: nodeSelector: es: log containers: - name: kibana image: docker.elastic.co/kibana/kibana:7.6.2 resources: limits: cpu: 1000m requests: cpu: 1000m env: - name: ELASTICSEARCH_HOSTS value: http://elasticsearch:9200 ports: - containerPort: 5601 上面我们定义了两个资源对象，一个 Service 和 Deployment，为了测试方便，我们将 Service 设置为了 NodePort 类型，Kibana Pod 中配置都比较简单，唯一需要注意的是我们使用 ELASTICSEARCH_HOSTS 这个环境变量来设置Elasticsearch 集群的端点和端口，直接使用 Kubernetes DNS 即可，此端点对应服务名称为 elasticsearch，由于是一个 headless service，所以该域将解析为3个 Elasticsearch Pod 的 IP 地址列表。 配置完成后，直接使用 kubectl 工具创建：\n1 2 3 kubectl create -f kibana.yaml service/kibana created deployment.apps/kibana created 创建完成后，可以查看 Kibana Pod 的运行状态\n1 2 3 4 5 6 [root@master01 new]# kubectl get pods --namespace=logging NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 13m es-1 1/1 Running 0 9m48s es-2 1/1 Running 0 7m46s kibana-8476dc9bbf-6mm6k 1/1 Running 0 3m2s 如果 Pod 已经是 Running 状态了，证明应用已经部署成功了，然后可以通过 NodePort 来访问 Kibana 这个服务，在浏览器中打开http://\u0026lt;任意节点IP\u0026gt;:31838即可，\n出现 这个代表成功\n部署 log-Pilot Log-pilot是一个智能容器日志采集工具，它不仅能够高效便捷地将容器日志采集输出到多种存储日志后端，同时还能够动态地发现和采集容器内部的日志文件。\n针对前面提出的日志采集难题，Log-pilot通过声明式配置实现强大的容器事件管理，可同时获取容器标准输出和内部文件日志，解决了动态伸缩问题，此外，Log-pilot具有自动发现机制、CheckPoint及句柄保持的机制、自动日志数据打标、有效应对动态配置、日志重复和丢失以及日志源标记等问题。\n本质\n通过变量和模版文件生产日志收集配置文件，对日志进行采集。\n容器内文件日志路径需要配置emptyDir\nlog-pilot.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 apiVersion: apps/v1 kind: DaemonSet metadata: name: log-pilot labels: app: log-pilot # 设置期望部署的namespace。 namespace: logging spec: selector: matchLabels: app: log-pilot updateStrategy: type: RollingUpdate template: metadata: labels: app: log-pilot annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39; spec: # 是否允许部署到Master节点上tolerations。 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: log-pilot # 版本请参考https://github.com/AliyunContainerService/log-pilot/releases。 image: heleicool/log-pilot:7.x-filebeat resources: limits: memory: 500Mi requests: cpu: 200m memory: 200Mi env: - name: \u0026#34;NODE_NAME\u0026#34; valueFrom: fieldRef: fieldPath: spec.nodeName - name: \u0026#34;LOGGING_OUTPUT\u0026#34; value: \u0026#34;elasticsearch\u0026#34; # 请确保集群到ES网络可达。 - name: \u0026#34;ELASTICSEARCH_HOSTS\u0026#34; value: \u0026#34;elasticsearch:9200\u0026#34; # 配置ES访问权限。 # - name: \u0026#34;ELASTICSEARCH_USER\u0026#34; # value: \u0026#34;{es_username}\u0026#34; # - name: \u0026#34;ELASTICSEARCH_PASSWORD\u0026#34; # value: \u0026#34;{es_password}\u0026#34; volumeMounts: - name: sock mountPath: /var/run/docker.sock - name: root mountPath: /host readOnly: true - name: varlib mountPath: /var/lib/filebeat - name: varlog mountPath: /var/log/filebeat - name: localtime mountPath: /etc/localtime readOnly: true livenessProbe: failureThreshold: 3 exec: command: - /pilot/healthz initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 securityContext: capabilities: add: - SYS_ADMIN terminationGracePeriodSeconds: 30 volumes: - name: sock hostPath: path: /var/run/docker.sock - name: root hostPath: path: / - name: varlib hostPath: path: /var/lib/filebeat type: DirectoryOrCreate - name: varlog hostPath: path: /var/log/filebeat type: DirectoryOrCreate - name: localtime hostPath: path: /etc/localtime 默认阿里云仓库只支持7.x以下版本es的数据写入，使用如下插件可以实现。\ngit地址：https://github.com/40kuai/log-pilot/tree/filebeat7.x\ndockerhub：heleicool/log-pilot:7.x-filebeat\n服务日志采集 创建一个标准的 Nginx 服务, 主要是 env 添加了两种,上面我们介绍过\n一 是 基于 docker stdout 输出日志 (aliyun_logs_catalina)\n二 是 基于 程序指定输出到指定的目录中 (aliyun_logs_access)\n( elasticsearch )环境变量中的 name表示Index，这里name 表示 Index，这里 name表示Index，这里name 即是 catalina 和 access, 这里用于 Kibana 查询日志\nnginx.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent env: - name: aliyun_logs_nginx-log value: \u0026#34;stdout\u0026#34; - name: aliyun_logs_access value: \u0026#34;/var/log/nginx/*.log\u0026#34; ports: - containerPort: 80 name: http --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx pod.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@master01 new]# cat pod2.yaml apiVersion: v1 kind: Pod metadata: name: counter namespace: logging #可以添加空间名称，也会收集到的 spec: containers: - name: count image: busybox args: [/bin/sh, -c, \u0026#39;i=0; while true; do echo \u0026#34;$i: $(date)\u0026#34;; i=$((i+1)); sleep 1; done\u0026#39;] env: # 1、stdout为约定关键字，表示采集标准输出日志。 # 2、配置标准输出日志采集到ES的catalina索引下。 - name: aliyun_logs_catalina value: \u0026#34;stdout\u0026#34; # 1、配置采集容器内文件日志，支持通配符。 # 2、配置该日志采集到ES的access索引下。 tomcat.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: v1 kind: Pod metadata: name: tomcat spec: containers: - name: tomcat image: \u0026#34;tomcat:7.0\u0026#34; env: # 1、stdout为约定关键字，表示采集标准输出日志。 # 2、配置标准输出日志采集到ES的catalina索引下。 - name: aliyun_logs_catalina value: \u0026#34;stdout\u0026#34; # 1、配置采集容器内文件日志，支持通配符。 # 2、配置该日志采集到ES的access索引下。 - name: aliyun_logs_access value: \u0026#34;/usr/local/tomcat/logs/catalina.*.log\u0026#34; # 容器内文件日志路径需要配置emptyDir。 volumeMounts: - name: tomcat-log mountPath: /usr/local/tomcat/logs volumes: - name: tomcat-log emptyDir: {} k8s 部署 log-pilot 收集容器标准输出日志和指定路径应用日志 - lixinliang - 博客园\n","date":"2022-03-12T19:40:43Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/202203121940/","title":"Kubernetes高效收集生产日志"},{"content":"Kubernetes 中比较流行的日志收集解决方案是 Elasticsearch、Fluentd 和 Kibana（EFK）技术栈，也是官方现在比较推荐的一种方案。\n后续我会更新使用Log-Pilot + ES + Kibana 日志方案\nElasticsearch 是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大量日志数据，也可用于搜索许多不同类型的文档。\nElasticsearch 通常与 Kibana 一起部署，Kibana 是 Elasticsearch 的一个功能强大的数据可视化 Dashboard，Kibana 允许你通过 web 界面来浏览 Elasticsearch 日志数据。\nFluentd是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd，通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储。\n我们先来配置启动一个可扩展的 Elasticsearch 集群，然后在 Kubernetes 集群中创建一个 Kibana 应用，最后通过 DaemonSet 来运行 Fluentd，以便它在每个 Kubernetes 工作节点上都可以运行一个 Pod。\n官方文档：\nhttps://github.com/kubernetes/kubernetes/tree/release-1.22/cluster/addons/fluentd-elasticsearch\n准备 参考官方部署文档的基础上使用本项目manifests/efk/部署，以下为几点主要的修改：\n修改 fluentd-es-configmap.yaml 中的部分 journald 日志源（增加集群组件服务日志搜集） 修改官方docker镜像，方便国内下载加速 修改 es-statefulset.yaml 支持日志存储持久化等 增加自动清理日志 创建 Elasticsearch 集群 1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: logging 1、 es-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kind: Service apiVersion: v1 metadata: name: elasticsearch namespace: logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node 定义了一个名为 elasticsearch 的 Service，指定标签 app=elasticsearch，当我们将 Elasticsearch StatefulSet 与此服务关联时，服务将返回带有标签 app=elasticsearch的 Elasticsearch Pods 的 DNS A 记录，然后设置 clusterIP=None，将该服务设置成无头服务。最后，我们分别定义端口9200、9300，分别用于与 REST API 交互，以及用于节点间通信。 1 2 3 4 5 [root@master01 efk]# kubectl apply -f es-service.yaml service/elasticsearch-logging created [root@master01 efk]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-logging ClusterIP None \u0026lt;none\u0026gt; 9200/TCP 15s es-statefulset.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 apiVersion: apps/v1 kind: StatefulSet metadata: name: es namespace: logging spec: serviceName: elasticsearch replicas: 3 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: nodeSelector: #记得给节点把标签，不然会找到不符合的节点 es: log initContainers: - name: increase-vm-max-map image: busybox command: [\u0026#34;sysctl\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;vm.max_map_count=262144\u0026#34;] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ulimit -n 65536\u0026#34;] securityContext: privileged: true containers: - name: elasticsearch image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2 ports: - name: rest containerPort: 9200 - name: inter containerPort: 9300 resources: limits: cpu: 1000m requests: cpu: 1000m volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: cluster.initial_master_nodes value: \u0026#34;es-0,es-1,es-2\u0026#34; - name: discovery.zen.minimum_master_nodes value: \u0026#34;2\u0026#34; - name: discovery.seed_hosts value: \u0026#34;elasticsearch\u0026#34; - name: ES_JAVA_OPTS value: \u0026#34;-Xms512m -Xmx512m\u0026#34; - name: network.host value: \u0026#34;0.0.0.0\u0026#34; volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: nfsdata resources: requests: storage: 5Gi Pods 部署完成后，我们可以通过请求一个 REST API 来检查 Elasticsearch 集群是否正常运行。使用下面的命令将本地端口9200 转发到 Elasticsearch 节点（如es-0）对应的端口：\n1 2 3 [root@master01 new]# kubectl port-forward es-0 9200:9200 --namespace=logging Forwarding from 127.0.0.1:9200 -\u0026gt; 9200 Forwarding from [::1]:9200 -\u0026gt; 9200 1 curl http://localhost:9200/_cluster/state?pretty 看到上面的信息就表明我们名为 k8s-logs 的 Elasticsearch 集群成功创建了3个节点：es-0，es-1，和es-2，当前主节点是 es-0\n创建 Kibana 服务 Elasticsearch 集群启动成功了，接下来我们可以来部署 Kibana 服务，新建一个名为 kibana.yaml 的文件，对应的文件内容如下：\nkibana.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion: v1 kind: Service metadata: name: kibana namespace: logging labels: app: kibana spec: ports: - port: 5601 type: NodePort selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: logging labels: app: kibana spec: selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: nodeSelector: es: log containers: - name: kibana image: docker.elastic.co/kibana/kibana:7.6.2 resources: limits: cpu: 1000m requests: cpu: 1000m env: - name: ELASTICSEARCH_HOSTS value: http://elasticsearch:9200 ports: - containerPort: 5601 上面我们定义了两个资源对象，一个 Service 和 Deployment，为了测试方便，我们将 Service 设置为了 NodePort 类型，Kibana Pod 中配置都比较简单，唯一需要注意的是我们使用 ELASTICSEARCH_HOSTS 这个环境变量来设置Elasticsearch 集群的端点和端口，直接使用 Kubernetes DNS 即可，此端点对应服务名称为 elasticsearch，由于是一个 headless service，所以该域将解析为3个 Elasticsearch Pod 的 IP 地址列表。 配置完成后，直接使用 kubectl 工具创建：\n1 2 3 kubectl create -f kibana.yaml service/kibana created deployment.apps/kibana created 创建完成后，可以查看 Kibana Pod 的运行状态\n1 2 3 4 5 6 [root@master01 new]# kubectl get pods --namespace=logging NAME READY STATUS RESTARTS AGE es-0 1/1 Running 0 13m es-1 1/1 Running 0 9m48s es-2 1/1 Running 0 7m46s kibana-8476dc9bbf-6mm6k 1/1 Running 0 3m2s 如果 Pod 已经是 Running 状态了，证明应用已经部署成功了，然后可以通过 NodePort 来访问 Kibana 这个服务，在浏览器中打开http://\u0026lt;任意节点IP\u0026gt;:31838即可，\n出现 这个代表成功\n部署 Fluentd Fluentd 是一个高效的日志聚合器，是用 Ruby 编写的，并且可以很好地扩展。对于大部分企业来说，Fluentd 足够高效并且消耗的资源相对较少，另外一个工具Fluent-bit更轻量级，占用资源更少，但是插件相对 Fluentd 来说不够丰富，所以整体来说，Fluentd 更加成熟，使用更加广泛，所以我们这里也同样使用 Fluentd 来作为日志收集工具。\n工作原理 Fluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下：\n首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务去 配置 一般来说我们是通过一个配置文件来告诉 Fluentd 如何采集、处理数据的\n日志源配置\n比如我们这里为了收集 Kubernetes 节点上的所有容器日志，就需要做如下的日志源配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。 path /var/log/containers/*.log # 挂载的服务器Docker容器日志地址 pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* # 设置日志标签 read_from_head true \u0026lt;parse\u0026gt; # 多行格式化成JSON @type multi_format # 使用 multi-format-parser 解析器插件 \u0026lt;pattern\u0026gt; format json # JSON 解析器 time_key time # 指定事件时间的时间字段 time_format %Y-%m-%dT%H:%M:%S.%NZ # 时间格式 \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; 上面配置部分参数说明如下：\nid：表示引用该日志源的唯一标识符，该标识可用于进一步过滤和路由结构化日志数据 type：Fluentd 内置的指令，tail 表示 Fluentd 从上次读取的位置通过 tail 不断获取数据，另外一个是 http 表示通过一个 GET 请求来收集数据。 path：tail 类型下的特定参数，告诉 Fluentd 采集 /var/log/containers 目录下的所有日志，这是 docker 在 Kubernetes 节点上用来存储运行容器 stdout 输出日志数据的目录。 pos_file：检查点，如果 Fluentd 程序重新启动了，它将使用此文件中的位置来恢复日志数据收集。 tag：用来将日志源与目标或者过滤器匹配的自定义字符串，Fluentd 匹配源/目标标签来路由日志数据。 路由配置\n上面是日志源的配置，接下来看看如何将日志数据发送到 Elasticsearch：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true type_name fluentd host \u0026#34;#{ENV[\u0026#39;OUTPUT_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;OUTPUT_PORT\u0026#39;]}\u0026#34; logstash_format true \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size \u0026#34;#{ENV[\u0026#39;OUTPUT_BUFFER_CHUNK_LIMIT\u0026#39;]}\u0026#34; queue_limit_length \u0026#34;#{ENV[\u0026#39;OUTPUT_BUFFER_QUEUE_LIMIT\u0026#39;]}\u0026#34; overflow_action block \u0026lt;/buffer\u0026gt; match：标识一个目标标签，后面是一个匹配日志源的正则表达式，我们这里想要捕获所有的日志并将它们发送给 Elasticsearch，所以需要配置成**。 id：目标的一个唯一标识符。 type：支持的输出插件标识符，我们这里要输出到 Elasticsearch，所以配置成 elasticsearch，这是 Fluentd 的一个内置插件。 log_level：指定要捕获的日志级别，我们这里配置成 info，表示任何该级别或者该级别以上（INFO、WARNING、ERROR）的日志都将被路由到 Elsasticsearch。 host/port：定义 Elasticsearch 的地址，也可以配置认证信息，我们的 Elasticsearch 不需要认证，所以这里直接指定 host 和 port 即可。 logstash_format：Elasticsearch 服务对日志数据构建反向索引进行搜索，将 logstash_format 设置为 true，Fluentd 将会以 logstash 格式来转发结构化的日志数据。 Buffer： Fluentd 允许在目标不可用时进行缓存，比如，如果网络出现故障或者 Elasticsearch 不可用的时候。缓冲区配置也有助于降低磁盘的 IO。 过滤\n由于 Kubernetes 集群中应用太多，也还有很多历史数据，所以我们可以只将某些应用的日志进行收集，比如我们只采集具有 logging=true 这个 Label 标签的 Pod 日志，这个时候就需要使用 filter，如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 删除无用的属性 \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash \u0026lt;/filter\u0026gt; # 只保留具有logging=true标签的Pod日志 \u0026lt;filter kubernetes.**\u0026gt; @id filter_log @type grep \u0026lt;regexp\u0026gt; key $.kubernetes.labels.logging pattern ^true$ \u0026lt;/regexp\u0026gt; \u0026lt;/filter\u0026gt; 安装 要收集 Kubernetes 集群的日志，直接用 DasemonSet 控制器来部署 Fluentd 应用，这样，它就可以从 Kubernetes 节点上采集日志，确保在集群中的每个节点上始终运行一个 Fluentd 容器。当然可以直接使用 Helm 来进行一键安装，为了能够了解更多实现细节，我们这里还是采用手动方法来进行安装。\n首先，我们通过 ConfigMap 对象来指定 Fluentd 配置文件，新建 fluentd-configmap.yaml 文件，文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 kind: ConfigMap apiVersion: v1 metadata: name: fluentd-config namespace: logging data: system.conf: |- \u0026lt;system\u0026gt; root_dir /tmp/fluentd-buffers/ \u0026lt;/system\u0026gt; containers.input.conf: |- \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。 path /var/log/containers/*.log # 挂载的服务器Docker容器日志地址 pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* # 设置日志标签 read_from_head true \u0026lt;parse\u0026gt; # 多行格式化成JSON @type multi_format # 使用 multi-format-parser 解析器插件 \u0026lt;pattern\u0026gt; format json # JSON解析器 time_key time # 指定事件时间的时间字段 time_format %Y-%m-%dT%H:%M:%S.%NZ # 时间格式 \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; # 在日志输出中检测异常，并将其作为一条日志转发 # https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions \u0026lt;match raw.kubernetes.**\u0026gt; # 匹配tag为raw.kubernetes.**日志信息 @id raw.kubernetes @type detect_exceptions # 使用detect-exceptions插件处理异常栈信息 remove_tag_prefix raw # 移除 raw 前缀 message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 \u0026lt;/match\u0026gt; \u0026lt;filter **\u0026gt; # 拼接日志 @id filter_concat @type concat # Fluentd Filter 插件，用于连接多个事件中分隔的多行日志。 key message multiline_end_regexp /\\n$/ # 以换行符“\\n”拼接 separator \u0026#34;\u0026#34; \u0026lt;/filter\u0026gt; # 添加 Kubernetes metadata 数据 \u0026lt;filter kubernetes.**\u0026gt; @id filter_kubernetes_metadata @type kubernetes_metadata \u0026lt;/filter\u0026gt; # 修复 ES 中的 JSON 字段 # 插件地址：https://github.com/repeatedly/fluent-plugin-multi-format-parser \u0026lt;filter kubernetes.**\u0026gt; @id filter_parser @type parser # multi-format-parser多格式解析器插件 key_name log # 在要解析的记录中指定字段名称。 reserve_data true # 在解析结果中保留原始键值对。 remove_key_name_field true # key_name 解析成功后删除字段。 \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format none \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; # 删除一些多余的属性 \u0026lt;filter kubernetes.**\u0026gt; @type record_transformer remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash \u0026lt;/filter\u0026gt; # 只保留具有logging=true标签的Pod日志 \u0026lt;filter kubernetes.**\u0026gt; @id filter_log @type grep \u0026lt;regexp\u0026gt; key $.kubernetes.labels.logging pattern ^true$ \u0026lt;/regexp\u0026gt; \u0026lt;/filter\u0026gt; ###### 监听配置，一般用于日志聚合用 ###### forward.input.conf: |- # 监听通过TCP发送的消息 \u0026lt;source\u0026gt; @id forward @type forward \u0026lt;/source\u0026gt; output.conf: |- \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch port 9200 logstash_format true logstash_prefix k8s # 设置 index 前缀为 k8s request_timeout 30s \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; 上面配置文件中我们只配置了 docker 容器日志目录，收集到数据经过处理后发送到 elasticsearch:9200 服务。\n然后新建一个 fluentd-daemonset.yaml 的文件，文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion: v1 kind: ServiceAccount metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \u0026#34;\u0026#34; resources: - \u0026#34;namespaces\u0026#34; - \u0026#34;pods\u0026#34; verbs: - \u0026#34;get\u0026#34; - \u0026#34;watch\u0026#34; - \u0026#34;list\u0026#34; --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: fluentd-es namespace: logging apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: fluentd-es apiGroup: \u0026#34;\u0026#34; --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: fluentd-es template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; # 此注释确保如果节点被驱逐，fluentd不会被驱逐，支持关键的基于 pod 注释的优先级方案。 annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39; spec: serviceAccountName: fluentd-es containers: - name: fluentd-es image: quay.io/fluentd_elasticsearch/fluentd:v3.0.1 env: - name: FLUENTD_ARGS value: --no-supervisor -q resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: config-volume mountPath: /etc/fluent/config.d nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \u0026#34;true\u0026#34; tolerations: - operator: Exists terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: config-volume configMap: name: fluentd-config 我们将上面创建的 fluentd-config 这个 ConfigMap 对象通过 volumes 挂载到了 Fluentd 容器中，另外为了能够灵活控制哪些节点的日志可以被收集，所以这里还添加了一个 nodSelector 属性：\n1 2 nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \u0026#34;true\u0026#34; 意思就是要想采集节点的日志，那么我们就需要给节点打上上面的标签\n如果你需要在其他节点上采集日志，则需要给对应节点打上标签，使用如下命令：\nkubectl label nodes node名 beta.kubernetes.io/fluentd-ds-ready=true\n1 2 3 4 5 6 [root@master01 ~]# kubectl label nodes 172.17.1.32 beta.kubernetes.io/fluentd-ds-ready=true node/172.17.1.32 labeled [root@master01 ~]# kubectl label nodes 172.17.1.30 beta.kubernetes.io/fluentd-ds-ready=true node/172.17.1.30 labeled [root@master01 ~]# kubectl label nodes 172.17.1.31 beta.kubernetes.io/fluentd-ds-ready=true node/172.17.1.31 labeled 默认情况下 master 节点有污点，所以如果要想也收集 master 节点的日志，则需要添加上容忍：\n1 2 tolerations: - operator: Exists 创建完成后，查看对应的 Pods 列表，检查是否部署成功\nFluentd 启动成功后，这个时候就可以发送日志到 ES 了，但是我们这里是过滤了只采集具有 logging=true 标签的 Pod 日志，所以现在还没有任何数据会被采集。\n下面我们部署一个简单的测试应用， 新建 counter.yaml 文件，文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: name: counter labels: logging: \u0026#34;true\u0026#34; # 一定要具有该标签才会被采集 spec: containers: - name: count image: busybox args: [/bin/sh, -c, \u0026#39;i=0; while true; do echo \u0026#34;$i: $(date)\u0026#34;; i=$((i+1)); sleep 1; done\u0026#39;] 该 Pod 只是简单将日志信息打印到 stdout，所以正常来说 Fluentd 会收集到这个日志数据，在 Kibana 中也就可以找到对应的日志数据了，使用 kubectl 工具创建该 Pod：\nPod 创建并运行后，回到 Kibana Dashboard 页面，点击左侧最下面的 management 图标，然后点击 Kibana 下面的 Index Patterns 开始导入索引数据：\n更详细细节请看阴阳博客\n在 Kubernetes 上搭建 EFK 日志收集系统[更新]-阳明的博客|Kubernetes|Istio|Prometheus|Python|Golang|云原生\n","date":"2022-03-11T11:26:38Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202203111126/","title":"Kubernetes安装EFK日志收集"},{"content":"首先，我们使用Kubernetes的都知道，etcd是k8s的核心所在，会记录各个pod的状态信息。所以重要性极为重要。\netcd是kubernetes集群极为重要的一块服务，存储了kubernetes集群所有的数据信息，如Namespace、Pod、Service、路由等状态信息。如果etcd集群发生灾难或者 etcd 集群数据丢失，都会影响k8s集群数据的恢复。因此，通过备份etcd数据来实现kubernetes集群的灾备环境十分重要。\n我们一点要养成，重要的东西备份、备份、在备份的习惯。\n1 2 3 4 5 6 7 8 9 ETCDCTL_API=3 /opt/kube/bin/etcdctl --endpoints=\u0026#34;https://172.17.1.20:2379,https://172.17.1.21:2379,https://172.17.1.22:2379\u0026#34; \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ endpoint status --write-out=table #状态 （配合替换最后一行使用） member list --write-out=table #列表 endpoint health --write-out=table #健康检查 1 2 3 4 5 6 7 8 [root@master01 ~]# ETCDCTL_API=3 /opt/kube/bin/etcdctl --endpoints=\u0026#34;https://172.17.1.20:2379,https://172.17.1.21:2379,https://172.17.1.22:2379\u0026#34; --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem endpoint status --write-out=table +--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://172.17.1.20:2379 | 20eec45319ea02e1 | 3.4.13 | 4.3 MB | false | false | 5 | 14715 | 14715 | | | https://172.17.1.21:2379 | 395af18f6bbef1a | 3.4.13 | 4.3 MB | false | false | 5 | 14715 | 14715 | | | https://172.17.1.22:2379 | fc24d224af6a71d9 | 3.4.13 | 4.3 MB | true | false | 5 | 14715 | 14715 | | +--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 一、etcd集群备份 etcd不同版本的 etcdctl 命令不一样，但大致差不多，这里备份使用 napshot save进行快照备份。\n需要注意几点：\n备份操作在etcd集群的其中一个节点执行就可以。 这里使用的是etcd v3的api，因为从 k8s 1.13 开始，k8s不再支持 v2 版本的 etcd，即k8s的集群数据都存在了v3版本的etcd中。故备份的数据也只备份了使用v3添加的etcd数据，v2添加的etcd数据是没有做备份的。 本案例使用的是二进制部署的k8s v1.20.2 + Calico 容器环境（下面命令中的\u0026quot;ETCDCTL_API=3 etcdctl\u0026quot; 等同于 \u0026ldquo;etcdctl\u0026rdquo;） 1）开始备份之前，先来查看下etcd数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 [root@master01 ~]# cat /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ ExecStart=/opt/kube/bin/etcd \\ --name=etcd-172.17.1.20 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.17.1.20:2380 \\ --listen-peer-urls=https://172.17.1.20:2380 \\ --listen-client-urls=https://172.17.1.20:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.17.1.20:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd \\ --snapshot-count=50000 \\ --auto-compaction-retention=1 \\ --max-request-bytes=10485760 \\ --auto-compaction-mode=periodic \\ --quota-backend-bytes=8589934592 Restart=always RestartSec=15 LimitNOFILE=65536 OOMScoreAdjust=-999 [root@master01 ~]# tree /var/lib/etcd /var/lib/etcd └── member ├── snap │ └── db └── wal ├── 0000000000000000-0000000000000000.wal └── 0.tmp 3 directories, 3 files 2）执行etcd集群数据备份\n在etcd集群的其中一个节点执行备份操作，然后将备份文件拷贝到其他节点上。\n先在etcd集群的每个节点上创建备份目录\n1 mkdir -p /data/etcd_backup_dir 在etcd集群其中个一个节点（这里在k8s-master01）上执行备份：\n1 2 3 4 5 6 ETCDCTL_API=3 /opt/kube/bin/etcdctl \\ snapshot save /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db \\ --endpoints=https://172.17.1.20:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem 将备份文件拷贝到其他的etcd节点\n1 2 3 4 5 6 [root@master01 ~]# scp /data/etcd_backup_dir/etcd-snapshot-20220310.db 172.17.1.21:/data/etcd_backup_dir/ etcd-snapshot-20220310.db 100% 4208KB 158.6MB/s 00:00 您在 /var/spool/mail/root 中有新邮件 [root@master01 ~]# scp /data/etcd_backup_dir/etcd-snapshot-20220310.db 172.17.1.22:/data/etcd_backup_dir/ etcd-snapshot-20220310.db 100% 4208KB 158.3MB/s 00:00 [root@master01 ~]# 可以将上面k8s-master01节点的etcd备份命令放在脚本里，结合crontab进行定时备份：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@k8s-master01 ~]# cat /data/etcd_backup_dir/etcd_backup.sh #!/usr/bin/bash date; ETCDCTL_API=3 /opt/kube/bin/etcdctl \\ snapshot save /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db \\ --endpoints=https://172.17.1.20:2379 \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem # 备份保留30天 find /data/etcd_backup_dir/ -name \u0026#34;*.db\u0026#34; -mtime +30 -exec rm -f {} \\; # 同步到其他两个etcd节点 scp /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db 172.17.1.21:/data/etcd_backup_dir/ scp /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db 172.17.1.22:/data/etcd_backup_dir/ 设置crontab定时备份任务，每天凌晨5点执行备份：\n1 2 3 4 5 6 [root@master01 ~]# chmod 755 /data/etcd_backup_dir/etcd_backup.sh [root@master01 ~]# crontab -e crontab: installing new crontab [root@master01 ~]# crontab -l */10 * * * * ntpdate time.windows.com 0 5 * * * /bin/bash -x /data/etcd_backup_dir/etcd_backup.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 二、etcd集群恢复 etcd集群备份操作只需要在其中的一个etcd节点上完成，然后将备份文件拷贝到其他节点。\n但etcd集群恢复操作必须要所有的etcd节点上完成！\n1）模拟etcd集群数据丢失\n删除三个etcd集群节点的data数据 (或者直接删除data目录)\n1 2 3 4 5 6 7 8 9 10 11 [root@master01 ~]# ls /var/lib/etcd/ member [root@master01 ~]# ansible master -m shell -a \u0026#34;mv /var/lib/etcd/member /var/lib/etcd/member_bak\u0026#34; 172.17.1.21 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.22 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.20 | CHANGED | rc=0 \u0026gt;\u0026gt; [root@master01 ~]# ls /var/lib/etcd/ member_bak 查看k8s集群状态：\n由于此时etcd集群的三个节点服务还在，过一会儿查看集群状态恢复正常：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [root@master01 ~]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-1 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} etcd-2 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 您在 /var/spool/mail/root 中有新邮件 [root@master01 ~]# [root@master01 ~]# [root@master01 ~]# ETCDCTL_API=3 /opt/kube/bin/etcdctl --endpoints=\u0026#34;https://172.17.1.20:2379,https://172.17.1.21:2379,https://172.17.1.22:2379\u0026#34; \\ \u0026gt; --cacert=/etc/kubernetes/ssl/ca.pem \\ \u0026gt; --cert=/etc/kubernetes/ssl/etcd.pem \\ \u0026gt; --key=/etc/kubernetes/ssl/etcd-key.pem \\ \u0026gt; endpoint status --write-out=table #状态 +--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://172.17.1.20:2379 | 20eec45319ea02e1 | 3.4.13 | 143 kB | false | false | 7 | 191 | 191 | | | https://172.17.1.21:2379 | 395af18f6bbef1a | 3.4.13 | 143 kB | true | false | 7 | 191 | 191 | | | https://172.17.1.22:2379 | fc24d224af6a71d9 | 3.4.13 | 152 kB | false | false | 7 | 191 | 191 | | +--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ [root@master01 ~]# kubectl get svc,pod -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.68.0.1 \u0026lt;none\u0026gt; 443/TCP 18s 但是，k8s集群数据其实已经丢失了。namespace命名空间下的pod等资源都没有了。此时就需要通过etcd集群备份文件来恢复，即通过上面的etcd集群快照文件恢复。\n1 2 3 4 5 6 7 8 [root@master01 ~]# kubectl get ns NAME STATUS AGE default Active 2m20s kube-node-lease Active 2m20s kube-public Active 2m20s kube-system Active 2m20s [root@master01 ~]# kubectl get pod -n kube-system No resources found in kube-system namespace. 2）etcd集群数据恢复，即kubernetes集群数据恢复\n在etcd数据恢复之前，先依次关闭所有master节点的kube-aposerver服务，所有etcd节点的etcd服务：\n1 2 3 4 5 6 [root@master01 ~]# ansible master -m shell -a \u0026#34;systemctl stop kube-apiserver \u0026amp;\u0026amp; systemctl stop etcd\u0026#34; 172.17.1.21 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.22 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.20 | CHANGED | rc=0 \u0026gt;\u0026gt; **特别注意：**在进行etcd集群数据恢复之前，一定要先将所有etcd节点的data和wal旧工作目录删掉，否则，可能会导致恢复失败（恢复命令执行时报错数据目录已存在）。\n在每个etcd节点执行恢复操作：\nmaster01\n1 2 3 4 5 6 7 8 9 10 11 12 [root@master01 ~]# [root@master01 ~]# ETCDCTL_API=3 /opt/kube/bin/etcdctl \\ \u0026gt; --name=etcd-172.17.1.20 \\ \u0026gt; --endpoints=\u0026#34;https://172.17.1.20:2379\u0026#34; \\ \u0026gt; --cacert=/etc/kubernetes/ssl/ca.pem \\ \u0026gt; --cert=/etc/kubernetes/ssl/etcd.pem \\ \u0026gt; --key=/etc/kubernetes/ssl/etcd-key.pem \\ \u0026gt; --initial-cluster-token=etcd-cluster-0 \\ \u0026gt; --initial-advertise-peer-urls=https://172.17.1.20:2380 \\ \u0026gt; --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 \\ \u0026gt; --data-dir=/var/lib/etcd \\ \u0026gt; snapshot restore /data/etcd_backup_dir/etcd-snapshot-20220310.db master02\n1 2 3 4 5 6 7 8 9 10 11 ETCDCTL_API=3 /opt/kube/bin/etcdctl \\ --name=etcd-172.17.1.21 \\ --endpoints=\u0026#34;https://172.17.1.21:2379\u0026#34; \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-advertise-peer-urls=https://172.17.1.21:2380 \\ --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 \\ --data-dir=/var/lib/etcd \\ snapshot restore /data/etcd_backup_dir/etcd-snapshot-20220310.db master03\n1 2 3 4 5 6 7 8 9 10 11 ETCDCTL_API=3 /opt/kube/bin/etcdctl \\ --name=etcd-172.17.1.22 \\ --endpoints=\u0026#34;https://172.17.1.22:2379\u0026#34; \\ --cacert=/etc/kubernetes/ssl/ca.pem \\ --cert=/etc/kubernetes/ssl/etcd.pem \\ --key=/etc/kubernetes/ssl/etcd-key.pem \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-advertise-peer-urls=https://172.17.1.22:2380 \\ --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 \\ --data-dir=/var/lib/etcd \\ snapshot restore /data/etcd_backup_dir/etcd-snapshot-20220310.db 依次启动所有etcd节点的etcd服务：\n1 2 # systemctl start etcd # systemctl status etcd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 [root@master01 ~]# ansible master -m shell -a \u0026#34;systemctl start etcd\u0026#34; 172.17.1.22 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.21 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.20 | CHANGED | rc=0 \u0026gt;\u0026gt; 您在 /var/spool/mail/root 中有新邮件 [root@master01 ~]# ansible master -m shell -a \u0026#34;systemctl status etcd\u0026#34; 172.17.1.21 | CHANGED | rc=0 \u0026gt;\u0026gt; ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since 四 2022-03-10 14:43:41 CST; 8s ago Docs: https://github.com/coreos Main PID: 81855 (etcd) Tasks: 13 Memory: 23.4M CGroup: /system.slice/etcd.service └─81855 /opt/kube/bin/etcd --name=etcd-172.17.1.21 --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem --peer-cert-file=/etc/kubernetes/ssl/etcd.pem --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem --trusted-ca-file=/etc/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem --initial-advertise-peer-urls=https://172.17.1.21:2380 --listen-peer-urls=https://172.17.1.21:2380 --listen-client-urls=https://172.17.1.21:2379,http://127.0.0.1:2379 --advertise-client-urls=https://172.17.1.21:2379 --initial-cluster-token=etcd-cluster-0 --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 --initial-cluster-state=new --data-dir=/var/lib/etcd --snapshot-count=50000 --auto-compaction-retention=1 --max-request-bytes=10485760 --auto-compaction-mode=periodic --quota-backend-bytes=8589934592 3月 10 14:43:41 master02 etcd[81855]: published {Name:etcd-172.17.1.21 ClientURLs:[https://172.17.1.21:2379]} to cluster 948e35f16fc80e25 3月 10 14:43:41 master02 etcd[81855]: ready to serve client requests 3月 10 14:43:41 master02 etcd[81855]: ready to serve client requests 3月 10 14:43:41 master02 systemd[1]: Started Etcd Server. 3月 10 14:43:41 master02 etcd[81855]: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! 3月 10 14:43:41 master02 etcd[81855]: serving client requests on 172.17.1.21:2379 3月 10 14:43:41 master02 etcd[81855]: set the initial cluster version to 3.4 3月 10 14:43:41 master02 etcd[81855]: enabled capabilities for version 3.4 3月 10 14:43:41 master02 etcd[81855]: established a TCP streaming connection with peer 20eec45319ea02e1 (stream Message reader) 3月 10 14:43:41 master02 etcd[81855]: established a TCP streaming connection with peer 20eec45319ea02e1 (stream MsgApp v2 reader) 172.17.1.22 | CHANGED | rc=0 \u0026gt;\u0026gt; ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since 四 2022-03-10 14:43:41 CST; 8s ago Docs: https://github.com/coreos Main PID: 81803 (etcd) Tasks: 13 Memory: 20.7M CGroup: /system.slice/etcd.service └─81803 /opt/kube/bin/etcd --name=etcd-172.17.1.22 --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem --peer-cert-file=/etc/kubernetes/ssl/etcd.pem --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem --trusted-ca-file=/etc/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem --initial-advertise-peer-urls=https://172.17.1.22:2380 --listen-peer-urls=https://172.17.1.22:2380 --listen-client-urls=https://172.17.1.22:2379,http://127.0.0.1:2379 --advertise-client-urls=https://172.17.1.22:2379 --initial-cluster-token=etcd-cluster-0 --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 --initial-cluster-state=new --data-dir=/var/lib/etcd --snapshot-count=50000 --auto-compaction-retention=1 --max-request-bytes=10485760 --auto-compaction-mode=periodic --quota-backend-bytes=8589934592 3月 10 14:43:41 master03 etcd[81803]: ready to serve client requests 3月 10 14:43:41 master03 etcd[81803]: ready to serve client requests 3月 10 14:43:41 master03 systemd[1]: Started Etcd Server. 3月 10 14:43:41 master03 etcd[81803]: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! 3月 10 14:43:41 master03 etcd[81803]: serving client requests on 172.17.1.22:2379 3月 10 14:43:41 master03 etcd[81803]: setting up the initial cluster version to 3.4 3月 10 14:43:41 master03 etcd[81803]: set the initial cluster version to 3.4 3月 10 14:43:41 master03 etcd[81803]: enabled capabilities for version 3.4 3月 10 14:43:41 master03 etcd[81803]: established a TCP streaming connection with peer 20eec45319ea02e1 (stream Message reader) 3月 10 14:43:41 master03 etcd[81803]: established a TCP streaming connection with peer 20eec45319ea02e1 (stream MsgApp v2 reader) 172.17.1.20 | CHANGED | rc=0 \u0026gt;\u0026gt; ● etcd.service - Etcd Server Loaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since 四 2022-03-10 14:43:41 CST; 9s ago Docs: https://github.com/coreos Main PID: 103123 (etcd) Tasks: 13 Memory: 10.7M CGroup: /system.slice/etcd.service └─103123 /opt/kube/bin/etcd --name=etcd-172.17.1.20 --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem --peer-cert-file=/etc/kubernetes/ssl/etcd.pem --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem --trusted-ca-file=/etc/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem --initial-advertise-peer-urls=https://172.17.1.20:2380 --listen-peer-urls=https://172.17.1.20:2380 --listen-client-urls=https://172.17.1.20:2379,http://127.0.0.1:2379 --advertise-client-urls=https://172.17.1.20:2379 --initial-cluster-token=etcd-cluster-0 --initial-cluster=etcd-172.17.1.20=https://172.17.1.20:2380,etcd-172.17.1.21=https://172.17.1.21:2380,etcd-172.17.1.22=https://172.17.1.22:2380 --initial-cluster-state=new --data-dir=/var/lib/etcd --snapshot-count=50000 --auto-compaction-retention=1 --max-request-bytes=10485760 --auto-compaction-mode=periodic --quota-backend-bytes=8589934592 3月 10 14:43:41 master01 etcd[103123]: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged! 3月 10 14:43:41 master01 etcd[103123]: serving client requests on 172.17.1.20:2379 3月 10 14:43:41 master01 systemd[1]: Started Etcd Server. 3月 10 14:43:41 master01 etcd[103123]: 20eec45319ea02e1 initialized peer connection; fast-forwarding 8 ticks (election ticks 10) with 2 active peer(s) 3月 10 14:43:41 master01 etcd[103123]: set the initial cluster version to 3.4 3月 10 14:43:41 master01 etcd[103123]: enabled capabilities for version 3.4 3月 10 14:43:41 master01 etcd[103123]: established a TCP streaming connection with peer fc24d224af6a71d9 (stream Message writer) 3月 10 14:43:41 master01 etcd[103123]: established a TCP streaming connection with peer fc24d224af6a71d9 (stream MsgApp v2 writer) 3月 10 14:43:41 master01 etcd[103123]: established a TCP streaming connection with peer 395af18f6bbef1a (stream Message writer) 3月 10 14:43:41 master01 etcd[103123]: established a TCP streaming connection with peer 395af18f6bbef1a (stream MsgApp v2 writer) 检查 ETCD 集群状态（如下，发现etcd集群里已经成功选主了）\n再依次启动所有master节点的kube-apiserver服务：\n1 2 # systemctl start kube-apiserver # systemctl status kube-apiserver 1 2 3 4 5 6 [root@master01 ~]# ansible master -m shell -a \u0026#34;systemctl start kube-apiserver\u0026#34; 172.17.1.22 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.21 | CHANGED | rc=0 \u0026gt;\u0026gt; 172.17.1.20 | CHANGED | rc=0 \u0026gt;\u0026gt; 查看kubernetes集群状态：\n1 2 3 4 5 6 7 8 [root@master01 ~]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} etcd-2 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 查看kubernetes的资源情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [root@master01 ~]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5677ffd49-t7dbx 1/1 Running 0 3h27m kube-system calico-node-2hrhv 1/1 Running 0 3h27m kube-system calico-node-5kc79 1/1 Running 0 3h27m kube-system calico-node-6v294 1/1 Running 0 3h27m kube-system calico-node-hkm8l 1/1 Running 0 3h27m kube-system calico-node-kkpmd 1/1 Running 0 3h27m kube-system calico-node-tsmwx 1/1 Running 0 3h27m kube-system coredns-5787695b7f-8ql8q 1/1 Running 0 3h27m kube-system metrics-server-8568cf894b-ztcgf 1/1 Running 1 3h27m kube-system node-local-dns-5mbzg 1/1 Running 0 3h27m kube-system node-local-dns-c5d9j 1/1 Running 0 3h27m kube-system node-local-dns-nqnjw 1/1 Running 0 3h27m kube-system node-local-dns-rz565 1/1 Running 0 3h27m kube-system node-local-dns-skmzk 1/1 Running 0 3h27m kube-system node-local-dns-zcncq 1/1 Running 0 3h27m kube-system traefik-79f5f7879c-nwhlq 1/1 Running 0 3h27m 在etcd集群数据恢复后，pod容器也会慢慢恢复到running状态。至此，kubernetes整个集群已经通过etcd备份数据恢复了\n三、最后总结 Kubernetes 集群备份主要是备份 ETCD 集群。而恢复时，主要考虑恢复整个顺序：\n停止kube-apiserver --\u0026gt; 停止ETCD --\u0026gt; 恢复数据 --\u0026gt; 启动ETCD --\u0026gt; 启动kube-apiserve\n特别注意：\n备份ETCD集群时，只需要备份一个ETCD数据，然后同步到其他节点上。 恢复ETCD数据时，拿其中一个节点的备份数据恢复即可 参考\nK8S集群灾备环境部署 - 散尽浮华 - 博客园\nhttps://www.cnblogs.com/kevingrace/p/14616824.html\n","date":"2022-03-10T15:02:58Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202203101502/","title":"Kubernetes的ETCD集群备份、恢复"},{"content":"\n原因：因为突然断电，导致机器关闭\n结果：发现有一台虚拟机无法启动，一直报错 Unmount and run xfs_repair\n分析：主机异常掉电后里面的虚拟机无法启动，主要是损坏的分区\n解决办法：\n原因：看出来应该是dm-0分区损坏，修复就可以了\n1：启动虚拟机E进入单用户模式\n2：在linux16开头的哪一行后面添加rd.break，ctrl+x进入救援模式\n3：分析dm-0\n1 ls -l /dev/mapper 4:卸载目录\n1 umount /dev/mapper/centos-root 5:修复目录\n1 xfs_repair -L /dev/mapper/centos-root 6：重启机器\n1 init 6 修复完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 xfs_repair -h xfs_repair: invalid option -- \u0026#39;h\u0026#39; Usage: xfs_repair [options] device Options: -f The device is a file -L Force log zeroing. Do this as a last resort. -l logdev Specifies the device where the external log resides. -m maxmem Maximum amount of memory to be used in megabytes. -n No modify mode, just checks the filesystem for damage. -P Disables prefetching. -r rtdev Specifies the device where the realtime section resides. -v Verbose output. -c subopts Change filesystem parameters - use xfs_admin. -o subopts Override default behaviour, refer to man page. -t interval Reporting interval in minutes. -d Repair dangerously. -V Reports version and exits. ","date":"2022-03-09T16:39:45Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202203091639/","title":"Linux虚拟机（lvm）报Unmount and run xfs_repair"},{"content":"DNS服务简介： DNS(Domain Name System–域名系统),是因特网的一项服务。它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。是一个应用层的协议DNS使用TCP和UDP端口53。\nDNS是一个分布式数据库,命名系统采用层次的逻辑结构,如同一颗倒置的树,这个逻辑的树形结构称为域名空间,由于DNS划分了域名空间,所以各机构可以使用自己的域名空间创建DNS信息.\n注:DNS域名空间中,树的最大深度不得超过127层,树中每个节点最长可以存储63个字符.\n以上是在公网中的使用\n更详细介绍：DNS服务器搭建与配置 | 曹世宏的博客\nDNS内网使用 首先，我们纯内网中，是不能过连接外网的环境的。\n但是纯内网也会有许多的IP和服务，我们该怎么来区分，总不能记录所有IP，所以我们需要自建内网的DNS，达到内网域名解析的效果。\nDNS使用作用 WHAT：DNS（域名系统）说白了，就是把一个域和IP地址做了一下绑定，如你在里机器里面输入 nslookup \u0026lt;www.qq.com\u0026gt;，出来的Address是一堆IP，IP是不容易记的，所以DNS让IP和域名做一下绑定，这样你输入域名就可以了\nWHY：我们要用ingress，在K8S里要做7层调度，而且无论如何都要用域名（如之前的那个百度页面的域名，那个是host的方式），但是问题是我们怎么给K8S里的容器绑host，所以我们必须做一个DNS，然后容器服从我们的DNS解析调度\n我们会在10.4.7.11 安装dns主服务，10.4.7.12作为备用dns服务\n也可以走一层keepalive虚拟IP。\n可以简单理解成：\n11机器：反向代理\n12机器：反向代理\n21机器：主控+运算节点（即服务群都是跑在21和22上）\n22机器：主控+运算节点（生产上我们会把主控和运算分开）\n200机器：运维主机（放各种文件资源）\n这样控节点有两个，运算节点有两个，就是小型的分布式，现在你可能没办法理解这些内容，我们接着做下去，慢慢的，你就理解了\n初始化 1 2 3 4 5 6 7 8 9 # 查看enforce是否关闭，确保disabled状态，当然可能没有这个命令 ~]# getforce # 查看内核版本，确保在3.8以上版本 ~]# uname -a # 关闭firewalld ~]# systemctl stop firewalld # 安装epel源及相关工具 ~]# yum install epel-release -y ~]# yum install wget net-tools telnet tree nmap sysstat lrzsz dos2unix bind-utils -y uname:显示系统信息\n-a/-all：显示全部 yum：提供了查找、安装、删除某一个、一组甚至全部软件包的命令\ninstall：安装\n-y：当安装过程提示选择全部为\u0026quot;yes\u0026quot;\n安装步骤 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 在11机器： ~]# yum install bind -y ~]# rpm -qa bind # out: bind-9.11.4-9.P2.el7.x86_64 # 配置主配置文件，11机器 ~]# vi /etc/named.conf listen-on port 53 { 10.4.7.11; }; # 原本是127.0.0.1 # listen-on-v6 port 53 { ::1; }; # 需要删掉 allow-query { any; }; # 原本是locall forwarders { 10.4.7.254; }; #另外添加的 dnssec-enable no; # 原本是yes dnssec-validation no; # 原本是yes # 检查修改情况，没有报错即可（即没有信息） ~]# named-checkconf rpm：软件包管理器\n-qa：查看已安装的所有软件包 rpm和yum安装的区别：前者不检查相依性问题，后者检查（即相关依赖包）\nnamed.conf文件内容解析：\nlisten-on：监听端口，改为监听在内网，这样其它机器也可以用\nallow-query：哪些客户端能通过自建的DNS查\nforwarders：上级DNS是什么\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 11机器，经验：主机域一定得跟业务是一点关系都没有，如host.com，而业务用的是od.com，因为业务随时可能变 # 区域配置文件，加在最下面 ~]# vi /etc/named.rfc1912.zones zone \u0026#34;host.com\u0026#34; IN { type master; file \u0026#34;host.com.zone\u0026#34;; allow-update { 10.4.7.11; }; }; zone \u0026#34;od.com\u0026#34; IN { type master; file \u0026#34;od.com.zone\u0026#34;; allow-update { 10.4.7.11; }; }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # 11机器： # 注意serial行的时间，代表今天的时间+第一条记录：20200112+01 7-11 ~]# vi /var/named/host.com.zone $ORIGIN host.com. $TTL 600\t; 10 minutes @ IN SOA\tdns.host.com. dnsadmin.host.com. ( 2020011201 ; serial 10800 ; refresh (3 hours) 900 ; retry (15 minutes) 604800 ; expire (1 week) 86400 ; minimum (1 day) ) NS dns.host.com. $TTL 60\t; 1 minute dns A 10.4.7.11 HDSS7-11 A 10.4.7.11 HDSS7-12 A 10.4.7.12 HDSS7-21 A 10.4.7.21 HDSS7-22 A 10.4.7.22 HDSS7-200 A 10.4.7.200 7-11 ~]# vi /var/named/od.com.zone $ORIGIN od.com. $TTL 600\t; 10 minutes @ IN SOA\tdns.od.com. dnsadmin.od.com. ( 2020011201 ; serial 10800 ; refresh (3 hours) 900 ; retry (15 minutes) 604800 ; expire (1 week) 86400 ; minimum (1 day) ) NS dns.od.com. $TTL 60\t; 1 minute dns A 10.4.7.11 # 看一下有没有报错 7-11 ~]# named-checkconf 7-11 ~]# systemctl start named 7-11 ~]# netstat -luntp|grep 53 TTL 600：指定IP包被路由器丢弃之前允许通过的最大网段数量\n10 minutes：过期时间10分钟 SOA：一个域权威记录的相关信息，后面有5组参数分别设定了该域相关部分\ndnsadmin.od.com. 一个假的邮箱\nserial：记录的时间\n$ORIGIN：即下列的域名自动补充od.com，如dns，外面看来是dns.od.com\nnetstat -luntp：显示 tcp,udp 的端口和进程等相关情况\n1 2 3 4 5 6 7 8 # 11机器，检查主机域是否解析 7-11 ~]# dig -t A hdss7-21.host.com @10.4.7.11 +short # 配置linux客户端和win客户端都能使用这个服务，修改 7-11 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0 DNS1=10.4.7.11 7-11 ~]# systemctl restart network 7-11 ~]# ping www.baidu.com 7-11 ~]# ping hdss7-21.host.com dig -t A：指的是找DNS里标记为A的相关记录，而后面会带上相关的域，如上面的hdss7-21.host.com，为什么外面配了HDSS7-21后面还会自动接上.host.com就是因为$ORIGIN，后面则是对应的IP\n+short：表示只返回IP 1 2 3 # 在所有机器添加search... ，即可使用短域名（我的是自带的） ~]# vi /etc/resolv.conf ~]# ping hdss7-200 1 2 3 4 5 6 7 8 9 # 在非11机器上，全部改成11 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0 DNS1=10.4.7.11 ~]# systemctl restart network # 试下网络是否正常 ~]# ping baidu.com # 其它机器尝试ping7-11机器 7-12 ~]# ping hdss7-11.host.com 让其它机器的DNS全部改成11机器的好处是，全部的机器访问外网就只有通过11端口，更好控制\n# 修改window网络，并ping\n后续域名解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 [root@hdss7-11 ~]# cat /var/named/od.com.zone $ORIGIN od.com. $TTL 600\t; 10 minutes @ IN SOA\tdns.od.com. dnsadmin.od.com. ( 2020011523 ; serial #这个需要每次增加1 10800 ; refresh (3 hours) 900 ; retry (15 minutes) 604800 ; expire (1 week) 86400 ; minimum (1 day) ) NS dns.od.com. $TTL 60\t; 1 minute dns A 10.4.7.11 harbor A 10.4.7.200 k8s-yaml A 10.4.7.200 traefik A 10.4.7.10 dashboard A 10.4.7.10 zk1 A 10.4.7.11 zk2 A 10.4.7.12 zk3 A 10.4.7.21 jenkins A 10.4.7.10 gitlab A 10.4.7.200 dubbo-monitor A 10.4.7.10 demo A 10.4.7.10 config A 10.4.7.10 mysql A 10.4.7.11 portal A 10.4.7.10 zk-test A 10.4.7.11 zk-prod A 10.4.7.12 config-test A 10.4.7.10 config-prod A 10.4.7.10 demo-test A 10.4.7.10 demo-prod A 10.4.7.10 blackbox A 10.4.7.10 prometheus A 10.4.7.10 grafana A 10.4.7.10 km A 10.4.7.10 kibana A 10.4.7.10 ","date":"2022-02-09T15:29:30Z","image":"https://www.ownit.top/title_pic/10.jpg","permalink":"https://www.ownit.top/p/202202091529/","title":"内网DNS重要使用作用"},{"content":"日常我们会使用到到图床来存放图片，但是大致流程是什么样子的来？？\n图床原理 实现一个HTTP服务器,用这个服务器存储图片,针对每个图片提供一个唯一的url,借助这个url就可以将图片展示到其他网页上\n后端代码实现 项目接口 首先，我们需要建立Gin框架 post请求访问，上传图片，判断图片是否符合，在上传图片，返回Url地址，浏览器就可以正常访问\napp.ini 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [app] HttpPort = 8000 RuntimeRootPath = runtime/ ImagePrefixUrl = http://127.0.0.1:8000 ImageSavePath = upload/images/ # MB ImageMaxSize = 5 ImageAllowExts = .jpg,.jpeg,.png,.gif LogSavePath = logs/ LogSaveName = log LogFileExt = log TimeFormat = 20060102 app/upload.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 package app import ( \u0026#34;ImagesUpload/pkg/e\u0026#34; \u0026#34;ImagesUpload/pkg/logging\u0026#34; \u0026#34;ImagesUpload/pkg/upload\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; ) //获取ip func GetRequestIP(c *gin.Context) string { reqIP := c.ClientIP() if reqIP == \u0026#34;::1\u0026#34; { reqIP = \u0026#34;127.0.0.1\u0026#34; } return reqIP } func UploadImage(c *gin.Context) { code := e.SUCCESS data := make(map[string]string) t := time.Now() year := t.Year() // type int month := t.Month() // type time.Month file, image, err := c.Request.FormFile(\u0026#34;image\u0026#34;) if err != nil { logging.Warn(err) code = e.ERROR c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: code, \u0026#34;msg\u0026#34;: e.GetMsg(code), \u0026#34;data\u0026#34;: data, }) } if image == nil { code = e.INVALID_PARAMS } else { imageName := upload.GetImageName(image.Filename) fullPath := upload.GetImageFullPath() + strconv.Itoa(year) + strconv.Itoa(int(month)) savePath := upload.GetImagePath() + strconv.Itoa(year) + strconv.Itoa(int(month)) src := fullPath + \u0026#34;/\u0026#34; + imageName fmt.Println(imageName, fullPath, savePath, src) if !upload.CheckImageExt(imageName) || !upload.CheckImageSize(file) { code = e.ERROR_UPLOAD_CHECK_IMAGE_FORMAT fmt.Println(e.GetMsg(code)) } else { err := upload.CheckImage(fullPath) if err != nil { logging.Warn(err) code = e.ERROR_UPLOAD_CHECK_IMAGE_FAIL } else if err := c.SaveUploadedFile(image, src); err != nil { logging.Warn(err) code = e.ERROR_UPLOAD_SAVE_IMAGE_FAIL } else { logging.Info(\u0026#34;访问者IP:\u0026#34;,GetRequestIP(c),\u0026#34;上传地址连接:\u0026#34;,upload.GetImageFullUrl(imageName)) data[\u0026#34;image_url\u0026#34;] = upload.GetImageFullUrl(imageName) data[\u0026#34;image_save_url\u0026#34;] = savePath + \u0026#34;/\u0026#34; + imageName } } } c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: code, \u0026#34;msg\u0026#34;: e.GetMsg(code), \u0026#34;data\u0026#34;: data, }) } pkg pkg/e/code.go 1 2 3 4 5 6 7 8 9 10 11 12 13 package e const ( SUCCESS = 200 ERROR = 500 INVALID_PARAMS = 400 // 保存图片失败 ERROR_UPLOAD_SAVE_IMAGE_FAIL = 30001 // 检查图片失败 ERROR_UPLOAD_CHECK_IMAGE_FAIL = 30002 // 校验图片错误，图片格式或大小有问题 ERROR_UPLOAD_CHECK_IMAGE_FORMAT = 30003 ) pkg/e/msg.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package e var MsgFlags = map[int]string { SUCCESS: \u0026#34;ok\u0026#34;, ERROR: \u0026#34;fail\u0026#34;, INVALID_PARAMS: \u0026#34;请求参数错误\u0026#34;, // 保存图片失败 ERROR_UPLOAD_SAVE_IMAGE_FAIL: \u0026#34;保存图片失败\u0026#34;, // 检查图片失败 ERROR_UPLOAD_CHECK_IMAGE_FAIL: \u0026#34;检查图片失败\u0026#34;, // 校验图片错误，图片格式或大小有问题 ERROR_UPLOAD_CHECK_IMAGE_FORMAT: \u0026#34;校验图片错误，图片格式或大小有问题\u0026#34;, } func GetMsg(code int) string { msg ,ok := MsgFlags[code] if ok{ return msg } return MsgFlags[ERROR] } pkg/file/file.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 package file import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;mime/multipart\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path\u0026#34; ) /* GetSize：获取文件大小 GetExt：获取文件后缀 CheckNotExist：检查文件是否存在 CheckPermission：检查文件权限 IsNotExistMkDir：如果不存在则新建文件夹 MkDir：新建文件夹 Open：打开文件 */ //GetSize：获取文件大小 func GetSize(f multipart.File) (int, error) { content, err := ioutil.ReadAll(f) return len(content), err } //GetExt：获取文件后缀 func GetExt(fileName string) string { return path.Ext(fileName) } //CheckNotExist：检查文件是否存在 func CheckNotExist(src string) bool { _, err := os.Stat(src) return os.IsNotExist(err) } //CheckPermission：检查文件权限 func CheckPermission(src string) bool { _, err := os.Stat(src) return os.IsPermission(err) } //IsNotExistMkDir：如果不存在则新建文件夹 func IsNotExistMkDir(src string) error { if notExist := CheckNotExist(src); notExist == true { if err := MkDir(src); err != nil { return err } } return nil } //MkDir：新建文件夹 func MkDir(src string) error { err := os.MkdirAll(src, os.ModePerm) if err != nil { return err } return nil } //Open：打开文件 func Open(name string, flag int, perm os.FileMode) (*os.File, error) { f, err := os.OpenFile(name, flag, perm) if err != nil { return nil, err } return f, nil } pkg/logging/file.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 package logging import ( \u0026#34;ImagesUpload/pkg/file\u0026#34; \u0026#34;ImagesUpload/setting\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func getLogFilePath() string { return fmt.Sprintf(\u0026#34;%s%s\u0026#34;, setting.AppSetting.RuntimeRootPath, setting.AppSetting.LogSavePath) } func getLogFileName() string { return fmt.Sprintf(\u0026#34;%s%s.%s\u0026#34;, setting.AppSetting.LogSaveName, time.Now().Format(setting.AppSetting.TimeFormat), setting.AppSetting.LogFileExt, ) } func openLogFile(fileName, filePath string) (*os.File, error) { dir, err := os.Getwd() if err != nil { return nil, fmt.Errorf(\u0026#34;os.Getwd err: %v\u0026#34;, err) } src := dir + \u0026#34;/\u0026#34; + filePath perm := file.CheckPermission(src) if perm == true { return nil, fmt.Errorf(\u0026#34;file.CheckPermission Permission denied src: %s\u0026#34;, src) } err = file.IsNotExistMkDir(src) if err != nil { return nil, fmt.Errorf(\u0026#34;file.IsNotExistMkDir src: %s, err: %v\u0026#34;, src, err) } f, err := file.Open(src+fileName, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { return nil, fmt.Errorf(\u0026#34;Fail to OpenFile :%v\u0026#34;, err) } return f, nil } pkg/logging/log.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package logging import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;runtime\u0026#34; ) type Level int var ( F *os.File DefaultPrefix = \u0026#34;\u0026#34; DefaultCallerDepth = 2 logger *log.Logger logPrefix = \u0026#34;\u0026#34; levelFlags = []string{\u0026#34;DEBUG\u0026#34;, \u0026#34;INFO\u0026#34;, \u0026#34;WARN\u0026#34;, \u0026#34;ERROR\u0026#34;, \u0026#34;FATAL\u0026#34;} ) const ( DEBUG Level = iota INFO WARNING ERROR FATAL ) func Setup() { var err error filePath := getLogFilePath() fileName := getLogFileName() F, err = openLogFile(fileName, filePath) if err != nil { log.Fatalln(err) } logger = log.New(F, DefaultPrefix, log.LstdFlags) } func Debug(v ...interface{}) { setPrefix(DEBUG) logger.Println(v) } func Info(v ...interface{}) { setPrefix(INFO) logger.Println(v) } func Warn(v ...interface{}) { setPrefix(WARNING) logger.Println(v) } func Error(v ...interface{}) { setPrefix(ERROR) logger.Println(v) } func Fatal(v ...interface{}) { setPrefix(FATAL) logger.Fatalln(v) } func setPrefix(level Level) { _, file, line, ok := runtime.Caller(DefaultCallerDepth) if ok { logPrefix = fmt.Sprintf(\u0026#34;[%s][%s:%d]\u0026#34;, levelFlags[level], filepath.Base(file), line) } else { logPrefix = fmt.Sprintf(\u0026#34;[%s]\u0026#34;, levelFlags[level]) } logger.SetPrefix(logPrefix) } pkg/logging/logrus.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 package logging import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; ) func Logger() *logrus.Logger { //now := time.Now() //logFilePath := getLogFilePath() //logFileName := getLogFileFullPath() //日志文件 //fileName := path.Join(logFilePath, logFileName) //if _, err := os.Stat(fileName); err != nil { //\tif _, err := os.Create(fileName); err != nil { //\tfmt.Println(err.Error()) //\t} //} //写入文件 filePath := getLogFilePath() fileName := getLogFileName() F, err := openLogFile(fileName, filePath) if err != nil { fmt.Println(\u0026#34;日志写入失败，请检查\u0026#34;) } //实例化 logger := logrus.New() //设置输出 logger.Out = F //设置日志级别 logger.SetLevel(logrus.DebugLevel) //设置日志格式 logger.SetFormatter(\u0026amp;logrus.TextFormatter{ TimestampFormat: \u0026#34;2006-01-02 15:04:05\u0026#34;, }) return logger } pkg/upload/image.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 package upload import ( \u0026#34;ImagesUpload/pkg/file\u0026#34; \u0026#34;ImagesUpload/pkg/logging\u0026#34; \u0026#34;ImagesUpload/pkg/util\u0026#34; \u0026#34;ImagesUpload/setting\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;mime/multipart\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; ) /* GetImageFullUrl：获取图片完整访问 URL GetImageName：获取图片名称 GetImagePath：获取图片路径 GetImageFullPath：获取图片完整路径 CheckImageExt：检查图片后缀 CheckImageSize：检查图片大小 CheckImage：检查图片 */ //GetImageFullUrl：获取图片完整访问 URL func GetImageFullUrl(name string) string { t := time.Now() year := t.Year() // type int month := t.Month() // type time.Month return setting.AppSetting.ImagePrefixUrl + \u0026#34;/\u0026#34; + GetImagePath() + strconv.Itoa(year) + strconv.Itoa(int(month)) + \u0026#34;/\u0026#34; + name } //GetImageName：获取图片名称 func GetImageName(name string) string { ext := path.Ext(name) fileName := strings.TrimSuffix(name, ext) fileName = util.EncodeMD5(fileName) return fileName + ext } //GetImagePath：获取图片路径 func GetImagePath() string { return setting.AppSetting.ImageSavePath } //GetImageFullPath：获取图片完整路径 func GetImageFullPath() string { return setting.AppSetting.RuntimeRootPath + GetImagePath() } //CheckImageExt：检查图片后缀 func CheckImageExt(fileName string) bool { ext := file.GetExt(fileName) for _, allowExt := range setting.AppSetting.ImageAllowExts { if strings.ToUpper(allowExt) == strings.ToUpper(ext) { return true } } return false } //CheckImageSize：检查图片大小 func CheckImageSize(f multipart.File) bool { size, err := file.GetSize(f) if err != nil { log.Println(err) logging.Warn(err) return false } return size \u0026lt;= setting.AppSetting.ImageMaxSize } //CheckImage：检查图片 func CheckImage(src string) error { dir, err := os.Getwd() if err != nil { return fmt.Errorf(\u0026#34;os.Getwd err: %v\u0026#34;, err) } err = file.IsNotExistMkDir(dir + \u0026#34;/\u0026#34; + src) if err != nil { return fmt.Errorf(\u0026#34;file.IsNotExistMkDir err: %v\u0026#34;, err) } perm := file.CheckPermission(src) if perm == true { return fmt.Errorf(\u0026#34;file.CheckPermission Permission denied src: %s\u0026#34;, src) } return nil } pkg/util/md5.go 1 2 3 4 5 6 7 8 9 10 11 12 package util import ( \u0026#34;crypto/md5\u0026#34; \u0026#34;encoding/hex\u0026#34; ) func EncodeMD5(value string) string { m := md5.New() m.Write([]byte(value)) return hex.EncodeToString(m.Sum(nil)) } routers/router.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package routers import ( \u0026#34;ImagesUpload/app\u0026#34; \u0026#34;ImagesUpload/pkg/upload\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;net/http\u0026#34; ) func InitRouter() *gin.Engine { r := gin.New() r.Use(gin.Logger()) r.Use(gin.Recovery()) r.StaticFS(\u0026#34;/upload/images\u0026#34;, http.Dir(upload.GetImageFullPath())) r.POST(\u0026#34;upload\u0026#34;, app.UploadImage) return r } setting/setting.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package setting import ( \u0026#34;github.com/go-ini/ini\u0026#34; \u0026#34;log\u0026#34; ) type App struct { HttpPort int RuntimeRootPath string ImagePrefixUrl string ImageSavePath string ImageMaxSize int ImageAllowExts []string LogSavePath string LogSaveName string LogFileExt string TimeFormat string } var AppSetting = \u0026amp;App{} func Setup() { Cfg, err := ini.Load(\u0026#34;conf/app.ini\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Fail to parse \u0026#39;conf/app.ini\u0026#39;: %v\u0026#34;, err) } err = Cfg.Section(\u0026#34;app\u0026#34;).MapTo(AppSetting) if err != nil { log.Fatalf(\u0026#34;Cfg.MapTo AppSetting err: %v\u0026#34;, err) } AppSetting.ImageMaxSize = AppSetting.ImageMaxSize * 1024 * 1024 } main.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import ( \u0026#34;ImagesUpload/pkg/logging\u0026#34; \u0026#34;ImagesUpload/routers\u0026#34; \u0026#34;ImagesUpload/setting\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { setting.Setup() logging.Setup() router := routers.InitRouter() fmt.Println(\u0026#34;启动端口：\u0026#34;, setting.AppSetting.HttpPort) s := \u0026amp;http.Server{ Addr: fmt.Sprintf(\u0026#34;:%d\u0026#34;, setting.AppSetting.HttpPort), Handler: router, } if err := s.ListenAndServe(); err != nil { log.Printf(\u0026#34;Listen: %s\\n\u0026#34;, err) } } ","date":"2022-01-12T10:43:15Z","image":"https://www.ownit.top/title_pic/36.jpg","permalink":"https://www.ownit.top/p/202201121043/","title":"Gin编写图床后端-上传图片代码实现"},{"content":"Gin Gin是一个golang的微框架，封装比较优雅，API友好，源码注释比较明确，已经发布了1.0版本。具有快速灵活，容错方便等特点。其实对于golang而言，web框架的依赖要远比Python，Java之类的要小。自身的net/http足够简单，性能也非常不错。框架更像是一些常用函数或者工具的集合。借助框架开发，不仅可以省去很多常用的封装带来的时间，也有助于团队的编码风格和形成规范。\n下面就Gin的用法做一个简单的介绍。\n首先需要安装，安装比较简单，使用go get即可：\n1 go get gopkg.in/gin-gonic/gin.v1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import ( \u0026#34;gopkg.in/gin-gonic/gin.v1\u0026#34; \u0026#34;net/http\u0026#34; ) func main(){ router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;Hello World\u0026#34;) }) router.Run(\u0026#34;:8000\u0026#34;) } 日常我们是使用Gin框架编写接口等等，可能会使用到一下几种组件。\nZap 日志记录\nlumberjack 日志切割\nini 配置文件读取\nini配置读取 下载\n1 go get github.com/go-ini/ini 因为项目中要是到配置文件，我们可以采用ini模块来实现\n首先，创建一个config.ini配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 port = 9000 release = false [mysql] user = db1 password = db1 host = 10.10.10.6 port = 3306 db = db1 [log] level = debug filename = ./logs/info.log maxsize = 1 max_age = 30 max_backups = 5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package setting import ( \u0026#34;gopkg.in/ini.v1\u0026#34; ) var Conf = new(AppConfig) // AppConfig 应用程序配置 type AppConfig struct { Release bool `ini:\u0026#34;release\u0026#34;` Port int `ini:\u0026#34;port\u0026#34;` *MySQLConfig `ini:\u0026#34;mysql\u0026#34;` *LogConfig `ini:\u0026#34;log\u0026#34;` } // MySQLConfig 数据库配置 type MySQLConfig struct { User string `ini:\u0026#34;user\u0026#34;` Password string `ini:\u0026#34;password\u0026#34;` DB string `ini:\u0026#34;db\u0026#34;` Host string `ini:\u0026#34;host\u0026#34;` Port int `ini:\u0026#34;port\u0026#34;` } type LogConfig struct { Level string `ini:\u0026#34;level\u0026#34;` Filename string `ini:\u0026#34;filename\u0026#34;` MaxSize int `ini:\u0026#34;maxsize\u0026#34;` MaxAge int `ini:\u0026#34;max_age\u0026#34;` MaxBackups int `ini:\u0026#34;max_backups\u0026#34;` } //把先关初始的参数加载到全局变量，然后方便调用 func Init(file string) error { return ini.MapTo(Conf, file) } 看一下调用方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // 传入配置文件路径，加载配置文件, if err := setting.Init(\u0026#34;conf/config.ini\u0026#34;); err != nil { fmt.Printf(\u0026#34;load config from file failed, err:%v\\n\u0026#34;, err) return } fmt.Println(\u0026#34;config.ini配置加载成功\u0026#34;, setting.Conf.Port) // 创建数据库 // sql: CREATE DATABASE bubble; // 连接数据库 err := dao.InitMySQL(setting.Conf.MySQLConfig) if err != nil { fmt.Printf(\u0026#34;init mysql failed, err:%v\\n\u0026#34;, err) return } fmt.Println(\u0026#34;数据库配置初始化加载成功\u0026#34;, setting.Conf.MySQLConfig) if err := log.InitLogger(setting.Conf.LogConfig); err != nil { fmt.Printf(\u0026#34;init logger failed, err:%v\\n\u0026#34;, err) return } 已经加载全局变量，可以正常使用\nmysql调用参数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 func InitMySQL(cfg *setting.MySQLConfig) (err error) { dsn := fmt.Sprintf(\u0026#34;%s:%s@tcp(%s:%d)/%s?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34;, cfg.User, cfg.Password, cfg.Host, cfg.Port, cfg.DB) DB, err = gorm.Open(\u0026#34;mysql\u0026#34;, dsn) if err != nil { return } DB.Debug() DB.LogMode(true) DB.SetLogger(\u0026amp;GormLogger{}) return DB.DB().Ping() } Zap和lumberjack Zap 日志记录\nlumberjack 日志切割\n1 2 go get -u go.uber.org/zap go get -u github.com/natefinch/lumberjack 大家可以参考\ngolang开发:类库篇(一) Zap高性能日志类库的使用 - 飞翔码农 - 博客园使用zap接收gin框架默认的日志并配置日志归档 - 兰玉磊的个人博客golang开发:类库篇(一) Zap高性能日志类库的使用 - 飞翔码农 - 博客园\n在Go语言项目中使用Zap日志库 - 知乎\n日志格式\n1 2 {\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-28T15:34:50.934+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;log/logger.go:65\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;/\u0026#34;,\u0026#34;status\u0026#34;:200,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/\u0026#34;,\u0026#34;query\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;127.0.0.1\u0026#34;,\u0026#34;user-agent\u0026#34;:\u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\u0026#34;,\u0026#34;errors\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;cost\u0026#34;:0.0010676} {\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-28T15:34:51.194+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;dao/mysql.go:23\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;sql\u0026#34;,\u0026#34;module\u0026#34;:\u0026#34;gorm\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;sql\u0026#34;,\u0026#34;src\u0026#34;:\u0026#34;D:/桌面/bubble/models/todo.go:24\u0026#34;,\u0026#34;duration\u0026#34;:0.0504676,\u0026#34;sql\u0026#34;:\u0026#34;SELECT * FROM `todos` \u0026#34;,\u0026#34;values\u0026#34;:null,\u0026#34;rows_returned\u0026#34;:3} 我们需要把它集成到Gin框架中。\n我这边的配置，也会读取ini的文件（看上面的代码）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [log] level = debug filename = ./logs/info.log maxsize = 1 max_age = 30 max_backups = 5 type LogConfig struct { Level string `ini:\u0026#34;level\u0026#34;` Filename string `ini:\u0026#34;filename\u0026#34;` MaxSize int `ini:\u0026#34;maxsize\u0026#34;` MaxAge int `ini:\u0026#34;max_age\u0026#34;` MaxBackups int `ini:\u0026#34;max_backups\u0026#34;` } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 package log import ( \u0026#34;bubble/setting\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/natefinch/lumberjack\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httputil\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime/debug\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; ) var Logger *zap.Logger // InitLogger 初始化Logger func InitLogger(cfg *setting.LogConfig) (err error) { writeSyncer := getLogWriter(cfg.Filename, cfg.MaxSize, cfg.MaxBackups, cfg.MaxAge) //fmt.Println(cfg.Filename) encoder := getEncoder() var l = new(zapcore.Level) err = l.UnmarshalText([]byte(cfg.Level)) if err != nil { return } core := zapcore.NewCore(encoder, writeSyncer, l) Logger = zap.New(core, zap.AddCaller()) return } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.TimeKey = \u0026#34;time\u0026#34; encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder encoderConfig.EncodeDuration = zapcore.SecondsDurationEncoder encoderConfig.EncodeCaller = zapcore.ShortCallerEncoder return zapcore.NewJSONEncoder(encoderConfig) } func getLogWriter(filename string, maxSize, maxBackup, maxAge int) zapcore.WriteSyncer { lumberJackLogger := \u0026amp;lumberjack.Logger{ Filename: filename, MaxSize: maxSize, MaxBackups: maxBackup, MaxAge: maxAge, } return zapcore.AddSync(lumberJackLogger) } // GinLogger 接收gin框架默认的日志 func GinLogger(logger *zap.Logger) gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() path := c.Request.URL.Path query := c.Request.URL.RawQuery c.Next() cost := time.Since(start) logger.Info(path, zap.Int(\u0026#34;status\u0026#34;, c.Writer.Status()), zap.String(\u0026#34;method\u0026#34;, c.Request.Method), zap.String(\u0026#34;path\u0026#34;, path), zap.String(\u0026#34;query\u0026#34;, query), zap.String(\u0026#34;ip\u0026#34;, c.ClientIP()), zap.String(\u0026#34;user-agent\u0026#34;, c.Request.UserAgent()), zap.String(\u0026#34;errors\u0026#34;, c.Errors.ByType(gin.ErrorTypePrivate).String()), zap.Duration(\u0026#34;cost\u0026#34;, cost), ) } } // GinRecovery recover掉项目可能出现的panic，并使用zap记录相关日志 func GinRecovery(logger *zap.Logger, stack bool) gin.HandlerFunc { return func(c *gin.Context) { defer func() { if err := recover(); err != nil { // Check for a broken connection, as it is not really a // condition that warrants a panic stack trace. var brokenPipe bool if ne, ok := err.(*net.OpError); ok { if se, ok := ne.Err.(*os.SyscallError); ok { if strings.Contains(strings.ToLower(se.Error()), \u0026#34;broken pipe\u0026#34;) || strings.Contains(strings.ToLower(se.Error()), \u0026#34;connection reset by peer\u0026#34;) { brokenPipe = true } } } httpRequest, _ := httputil.DumpRequest(c.Request, false) if brokenPipe { logger.Error(c.Request.URL.Path, zap.Any(\u0026#34;error\u0026#34;, err), zap.String(\u0026#34;request\u0026#34;, string(httpRequest)), ) // If the connection is dead, we can\u0026#39;t write a status to it. c.Error(err.(error)) // nolint: errcheck c.Abort() return } if stack { logger.Error(\u0026#34;[Recovery from panic]\u0026#34;, zap.Any(\u0026#34;error\u0026#34;, err), zap.String(\u0026#34;request\u0026#34;, string(httpRequest)), zap.String(\u0026#34;stack\u0026#34;, string(debug.Stack())), ) } else { logger.Error(\u0026#34;[Recovery from panic]\u0026#34;, zap.Any(\u0026#34;error\u0026#34;, err), zap.String(\u0026#34;request\u0026#34;, string(httpRequest)), ) } c.AbortWithStatus(http.StatusInternalServerError) } }() c.Next() } } 注册中间件的操作在routes.SetupRouter()中：\n1 2 3 4 5 6 7 8 9 func SetupRouter() *gin.Engine { if setting.Conf.Release { gin.SetMode(gin.ReleaseMode) } #开始调用，中间件使用 r := gin.New() r.Use(log.GinLogger(log.Logger), log.GinRecovery(log.Logger, true)) } main中调用\n1 2 log.Logger.Debug(\u0026#34;大家好，日志展示\u0026#34;) defer log.Logger.Sync() 1 {\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-28T15:34:42.647+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;bubble/main.go:39\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;大家好，日志展示\u0026#34;} 日志格式可以定义json格式，后期可以直接接入elk分析展示日志\nGorm的sql日志记录到文本 Gorm 建立了对 Logger 的支持，默认模式只会在错误发生的时候打印日志。可以通过gorm SetLogger(log logger)方法 改变gorm 打日志的行为。\ngorm 中 logger的接口：\n1 2 3 4 5 6 7 8 9 10 11 type logger interface { Print(ctx context.Context, v ...interface{}) } v 的值为： 1个参数： level，表示这个是个什么请求，可以是“sql” 2个参数：打印sql的代码行号，如/Users/yejianfeng/Documents/gopath/src/gorm-log/main.go:50, 3个参数: 执行时间戳 4个参数: sql语句 5参数：如果有预处理，请求参数，第六个参数是这个sql影响的行数。 zaplog集成示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 DB.Debug() DB.LogMode(true) DB.SetLogger(\u0026amp;GormLogger{}) // GormLogger struct type GormLogger struct{} // Print - Log Formatter func (*GormLogger) Print(v ...interface{}) { switch v[0] { case \u0026#34;sql\u0026#34;: log.Debug( \u0026#34;sql\u0026#34;, zap.String(\u0026#34;module\u0026#34;, \u0026#34;gorm\u0026#34;), zap.String(\u0026#34;type\u0026#34;, \u0026#34;sql\u0026#34;), zap.Any(\u0026#34;src\u0026#34;, v[1]), zap.Any(\u0026#34;duration\u0026#34;, v[2]), zap.Any(\u0026#34;sql\u0026#34;, v[3]), zap.Any(\u0026#34;values\u0026#34;, v[4]), zap.Any(\u0026#34;rows_returned\u0026#34;, v[5]), ) case \u0026#34;log\u0026#34;: log.Debug(\u0026#34;log\u0026#34;, zap.Any(\u0026#34;gorm\u0026#34;, v[2])) } } 日志格式，sql语句已经打印到日志中\n1 {\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-28T15:34:51.194+0800\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;dao/mysql.go:23\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;sql\u0026#34;,\u0026#34;module\u0026#34;:\u0026#34;gorm\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;sql\u0026#34;,\u0026#34;src\u0026#34;:\u0026#34;D:/桌面/bubble/models/todo.go:24\u0026#34;,\u0026#34;duration\u0026#34;:0.0504676,\u0026#34;sql\u0026#34;:\u0026#34;SELECT * FROM `todos` \u0026#34;,\u0026#34;values\u0026#34;:null,\u0026#34;rows_returned\u0026#34;:3} 参考文档 GORM自定义日志配置 - 苍山落暮 - 博客园\n","date":"2021-12-30T16:43:23Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202112301643/","title":"Gin框架组合（Zap、lumberjack、ini）使用手册"},{"content":"近期，快要邻近春节，安全方面更加重要。\n首先要对操作系统的用户做安全监控，防止操作系统账号被爆破泄露，我们也要监控起来。\n（1）Zabbix记录每分钟日志登录失败的次数\n（2）Zabbix记录登录失败用户的信息，方便查看\n首先，我们整个集群日志，通过rsyslog服务，把上千台的日志同步到一台上，所以我们只需要监控这个rsyslog的服务端就可以了。 看效果图（这样一来，十分方便查看记录）\n（1） 登录失败次数 日志格式\n1 2021-12-29T15:04:16.264895+08:00 127.0.0.1 [sshd] notice: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=172.17.9.200 user=deployer 编写代码的脚本\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash LOG_PATH=\u0026#34;/var/log/secure\u0026#34; mon=$(date +%B) h=$(date +%d) ms=$(date +%H:%M) #表示字符开头为0就替换为空 h=${h/#0/\u0026#34;\u0026#34;} k=\u0026#34;T\u0026#34; #我这边有T，有的是空格，根据时间环境使用 count=`grep \u0026#34;$h$k$ms\u0026#34; /var/log/secure | grep -v sudo | grep -c \u0026#34;authentication failure\u0026#34; ` echo $count 修改zabbix客户端配置\n1 2 #====================检查 账号登录失败次数====================== UserParameter=check_failed,sh /usr/local/zabbix-v503/scripts/check_failed.sh 重启zabbix客户端\nzabbix界面配置\n检查配置\n触发器\n（2）失败日志记录 编写脚本\n1 2 3 4 5 6 7 8 9 10 11 12 [root@logserver01 zabbix-v503]# cat scripts/check_failedlog.sh #!/bin/bash LOG_PATH=\u0026#34;/var/log/secure\u0026#34; mon=$(date +%B) h=$(date +%d) #获取前一分钟的爆破日志记录的时间 时:分 ms=$(date -d \u0026#34;1 minute ago\u0026#34; +\u0026#34;%H:%M\u0026#34;) #表示字符开头为0就替换为空 h=${h/#0/\u0026#34;\u0026#34;} k=\u0026#34;T\u0026#34; grep \u0026#34;$h$k$ms\u0026#34; /var/log/secure | grep -v sudo | grep \u0026#34;authentication failure\u0026#34; \u0026gt;\u0026gt; /usr/local/zabbix-v503/scripts/fail.log 开启定时任务（每分钟检查一次）\n1 2 #-------check_fail_user_log----------------- * * * * * sh /usr/local/zabbix-v503/scripts/check_failedlog.sh 如果有登录失败的，会单独过滤出来\nzabbix界面配置\n1 log[/usr/local/zabbix-v503/scripts/fail.log,\u0026#34;sshd\u0026#34;,skip,] 已经完成相关的项目类容，很容易监控。\n根据触发器，可以设置值，如果每分钟爆破登录失败10次，就报警，有爆破的嫌疑\n","date":"2021-12-29T15:28:13Z","image":"https://www.ownit.top/title_pic/73.jpg","permalink":"https://www.ownit.top/p/202112291528/","title":"Zabbix监控集群操作用户“登录失败次数“和“失败日志记录“"},{"content":"相关代码已经放在github：https://github.com/nangongchengfeng/Go_gin/tree/main/Projects\nGO的WEB编程（GIN实现邮件接口报警） Gin编写邮件接口（支持多人发送） Gin编写邮件告警接口（添加优化日志记录） 上面3个已经完成基本功能，但是所有的代码放在一个main，配置也是写死代码里面，不方便修改调整。现在在以上的基础上添加新的功能\n（1）加入配置文件读写管理\n（2）项目拆分\n因为日常范围，我们在操作系统上，需要报警时，只能采用mailx来使用。需要配置账号，密码，和邮箱认证。如果需要多台使用的话，岂不是很麻烦，要配置多台，这个导致密码很不安全，容易泄露。所以，为了安全，有效，更方便，我们可以采用接口发送邮件。\n（1）构建接口\n（2）传入post的json情况\n（3）把相应json转换字符\n（4）发送邮件\n开始下面项目规划\nmain.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package main import ( \u0026#34;code/mail_qq/log\u0026#34; \u0026#34;code/mail_qq/routers\u0026#34; \u0026#34;code/mail_qq/setting\u0026#34; \u0026#34;fmt\u0026#34; ) /* 支持多人发送 curl http://10.10.10.3:7070/send -H \u0026#34;Content-Type:application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;source\u0026#34;:\u0026#34;heian\u0026#34;,\u0026#34;contacts\u0026#34;:[\u0026#34;账号@mail_qq.com\u0026#34;,\u0026#34;账号@mail_qq.com\u0026#34;],\u0026#34;subject\u0026#34;:\u0026#34;多人测试\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;现在进行多人测试\u0026#34;}\u0026#39; */ /* zapcore.Core需要三个配置——Encoder，WriteSyncer，LogLevel Encoder:编码器(如何写入日志)。我们将使用开箱即用的NewJSONEncoder() WriterSyncer ：指定日志将写到哪里去。我们使用zapcore.AddSync() Log Level：哪种级别的日志将被写入。 */ //var sugarLogger *zap.SugaredLogger func main() { log.InitLogger() defer log.SugarLogger.Sync() port := setting.GetPort() r := routers.SetupRouter() r.Run(\u0026#34;:\u0026#34; + fmt.Sprint(port)) log.SugarLogger.Infof(\u0026#34;Success! Port is start\u0026#34;) //r.Run(\u0026#34;:8080\u0026#34;) } conf/config.ini 1 2 3 4 5 6 port = 8080 [mail] user = 账号@qq.com password = 密码 host = smtp.qq.com:25 source = heian util/ GetIP.go 工具类，获取客户端的IP\n1 2 3 4 5 6 7 8 9 10 11 12 package util import \u0026#34;github.com/gin-gonic/gin\u0026#34; //获取ip func GetRequestIP(c *gin.Context) string { reqIP := c.ClientIP() if reqIP == \u0026#34;::1\u0026#34; { reqIP = \u0026#34;127.0.0.1\u0026#34; } return reqIP } ** setting/setting.go** 加入配置文件，给代码返回相应的参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package setting import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/go-ini/ini\u0026#34; ) func GetMail() (user, password, host, source string) { //读取.ini里面的数据库配置 config, err := ini.Load(\u0026#34;conf/config.ini\u0026#34;) if err != nil { //失败 fmt.Printf(\u0026#34;Fail to read file: %v\u0026#34;, err) os.Exit(1) } host = config.Section(\u0026#34;mail\u0026#34;).Key(\u0026#34;host\u0026#34;).String() //port = config.Section(\u0026#34;mail\u0026#34;).Key(\u0026#34;port\u0026#34;).String() user = config.Section(\u0026#34;mail\u0026#34;).Key(\u0026#34;user\u0026#34;).String() password = config.Section(\u0026#34;mail\u0026#34;).Key(\u0026#34;password\u0026#34;).String() source = config.Section(\u0026#34;mail\u0026#34;).Key(\u0026#34;source\u0026#34;).String() //fmt.Println(user, password, host, source) return } func GetPort() (prot string) { config, err := ini.Load(\u0026#34;conf/config.ini\u0026#34;) if err != nil { //失败 fmt.Printf(\u0026#34;Fail to read file: %v\u0026#34;, err) os.Exit(1) } prot = config.Section(\u0026#34;\u0026#34;).Key(\u0026#34;port\u0026#34;).String() return } log/ log.go 日志切割和分割类，主要记录日志\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 package log import ( \u0026#34;github.com/natefinch/lumberjack\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; ) var SugarLogger *zap.SugaredLogger func InitLogger() { writeSyncer := getLogWriter() encoder := getEncoder() core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger := zap.New(core, zap.AddCaller()) SugarLogger = logger.Sugar() } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder return zapcore.NewConsoleEncoder(encoderConfig) } func getLogWriter() zapcore.WriteSyncer { /* Lumberjack Logger采用以下属性作为输入: Filename: 日志文件的位置 MaxSize：在进行切割之前，日志文件的最大大小（以MB为单位） MaxBackups：保留旧文件的最大个数 MaxAges：保留旧文件的最大天数 Compress：是否压缩/归档旧文件 */ lumberJackLogger := \u0026amp;lumberjack.Logger{ Filename: \u0026#34;./logs/info.log\u0026#34;, MaxSize: 10, MaxBackups: 5, MaxAge: 30, Compress: false, } return zapcore.AddSync(lumberJackLogger) } routers/router.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package routers import ( \u0026#34;code/mail_qq/app\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func SetupRouter() *gin.Engine { r := gin.Default() //v1 v1Group := r.Group(\u0026#34;v1\u0026#34;) { //待办事项 //添加 v1Group.POST(\u0026#34;/send\u0026#34;, app.PostMail) } return r } app/ send.go 发送邮件的代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 package app import ( \u0026#34;code/mail_qq/log\u0026#34; \u0026#34;code/mail_qq/setting\u0026#34; \u0026#34;code/mail_qq/util\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) //var sugarLogger *zap.SugaredLogger // 定义接收数据的结构体 type User struct { // binding:\u0026#34;required\u0026#34;修饰的字段，若接收为空值，则报错，是必须字段 Source string `form:\u0026#34;source\u0026#34; json:\u0026#34;source\u0026#34; uri:\u0026#34;source\u0026#34; xml:\u0026#34;source\u0026#34; binding:\u0026#34;required\u0026#34;` Contacts []string `form:\u0026#34;contacts\u0026#34; json:\u0026#34;contacts\u0026#34; uri:\u0026#34;contacts\u0026#34; xml:\u0026#34;contacts\u0026#34; binding:\u0026#34;required\u0026#34;` Subject string `form:\u0026#34;subject\u0026#34; json:\u0026#34;subject\u0026#34; uri:\u0026#34;subject\u0026#34; xml:\u0026#34;subject\u0026#34; binding:\u0026#34;required\u0026#34;` Content string `form:\u0026#34;content\u0026#34; json:\u0026#34;content\u0026#34; uri:\u0026#34;content\u0026#34; xml:\u0026#34;content\u0026#34; binding:\u0026#34;required\u0026#34;` } func SendToMail(user, sendUserName, password, host, to, subject, body, mailtype string) error { hp := strings.Split(host, \u0026#34;:\u0026#34;) //fmt.Println(hp) auth := smtp.PlainAuth(\u0026#34;\u0026#34;, user, password, hp[0]) var content_type string if mailtype == \u0026#34;html\u0026#34; { content_type = \u0026#34;Content-Type: text/\u0026#34; + mailtype + \u0026#34;; charset=UTF-8\u0026#34; } else { content_type = \u0026#34;Content-Type: text/plain\u0026#34; + \u0026#34;; charset=UTF-8\u0026#34; } msg := []byte(\u0026#34;To: \u0026#34; + to + \u0026#34;\\r\\nFrom: \u0026#34; + sendUserName + \u0026#34;\u0026lt;\u0026#34; + user + \u0026#34;\u0026gt;\u0026#34; + \u0026#34;\\r\\nSubject: \u0026#34; + subject + \u0026#34;\\r\\n\u0026#34; + content_type + \u0026#34;\\r\\n\\r\\n\u0026#34; + body) send_to := strings.Split(to, \u0026#34;;\u0026#34;) err := smtp.SendMail(host, auth, user, send_to, msg) //fmt.Println(err) return err } func PostMail(c *gin.Context) { c_ip := util.GetRequestIP(c) fmt.Println(c_ip) log.SugarLogger.Debugf(\u0026#34;调用 PostMail 接口Api，调用者IP： %s \u0026#34;, c_ip) 声明接收的变量 var json User 将request的body中的数据，自动按照json格式解析到结构体 // if err := c.ShouldBindJSON(\u0026amp;json); err != nil { //\t// 返回错误信息 //\t// gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) log.SugarLogger.Errorf(\u0026#34;Error: %s\u0026#34;, err.Error()) return } //fmt.Println(json.Content, json.Contacts) //c.JSON(http.StatusOK, gin.H{\u0026#34;status\u0026#34;: \u0026amp;json}) //sugarLogger.Infof(\u0026#34;info: %s\u0026#34;, json) sources := json.Source user, password, host, source := setting.GetMail() if sources != source { fmt.Println(\u0026#34;Send mail error!,source 认证失败\u0026#34;) log.SugarLogger.Errorf(\u0026#34;Send mail error!,source 认证失败\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,source 认证失败\u0026#34;, }) return } //println(json.Contacts) to := json.Contacts if to[0] == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!,发送人为空\u0026#34;) log.SugarLogger.Errorf(\u0026#34;Send mail error!,发送人为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,发送人为空\u0026#34;, }) return } subject := json.Subject if strings.TrimSpace(subject) == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!标题为空\u0026#34;) log.SugarLogger.Errorf(\u0026#34;Send mail error!标题为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,标题为空\u0026#34;, }) return } body := ` \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;iso-8859-15\u0026#34;\u0026gt; \u0026lt;title\u0026gt;MMOGA POWER\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ` + fmt.Sprintf(json.Content) + `\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` sendUserName := \u0026#34;告警平台\u0026#34; //发送邮件的人名称 fmt.Println(\u0026#34;send email\u0026#34;) for _, s := range to { //fmt.Println(i, s) err := SendToMail(user, sendUserName, password, host, s, subject, body, \u0026#34;html\u0026#34;) //log.Printf(\u0026#34;接收人：\u0026#34;, s+\u0026#34;\\n\u0026#34;+\u0026#34;标题:\u0026#34;, json.Subject+\u0026#34;\\n\u0026#34;, \u0026#34;发送内容：\u0026#34;, json.Content+\u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;接收人:%s \\n 标题: %s \\n 内容: %s \\n\u0026#34;, s, json.Subject, json.Content) log.SugarLogger.Infof(\u0026#34;接收人: %s ,标题: %s, 内容: %s\u0026#34;, s, json.Subject, json.Content) fmt.Println(err) if err != nil { fmt.Println(\u0026#34;Send mail error!\\n\u0026#34;) log.SugarLogger.Errorf(\u0026#34;Error 调用者IP: %s ,Send mail error! !\u0026#34;, c_ip) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error! !\\n\u0026#34;, }) //fmt.Println(err) } else { fmt.Println(\u0026#34;Send mail success!\\n\u0026#34;) log.SugarLogger.Infof(\u0026#34;success 调用者IP: %s ,Send mail success! !\u0026#34;, c_ip) c.JSON(http.StatusOK, gin.H{ \u0026#34;success\u0026#34;: \u0026#34;Send mail success! !\\n\u0026#34;, }) } } } 功能实现\n日志记录\n","date":"2021-12-25T14:13:07Z","image":"https://www.ownit.top/title_pic/71.jpg","permalink":"https://www.ownit.top/p/202112251413/","title":"Gin编写邮件告警接口（添加配置，项目拆分）"},{"content":"根据我开发的邮件接口上调用操作，触发告警\nGO的WEB编程（GIN实现邮件接口报警） Gin编写邮件接口（支持多人发送） Gin编写邮件告警接口（添加优化日志记录） 首先，我们Linux操作系统可以创建多个用户账号。\n但是为了系统安全考虑，我们会给账号密码设置有效期和复杂难度，防止非法操作爆破我们的机器。\n但是每次修改完，到规定时间需要修改账号密码，这个每次人工来看，比较麻烦，所以做个账号密码到期的警告。当密码快要过期时，我们可以发邮件告警。\n账号密码过期设置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 function Check_Password_Policy(){ #查看系统账户策略:密码失效时间90天、密码到期提醒时间14天 echo \u0026#34;========正在检查账户密码失效时间========\u0026#34; Check_Pass_Poli=`grep -E \u0026#34;^PASS_MAX_DAYS|^PASS_WARN_AGE\u0026#34; /etc/login.defs | wc -l` cp /etc/login.defs{,_bak$Date_Time} if [ $Check_Pass_Poli -lt 2 ];then echo -e \u0026#34;\\033[31;40m当前系统未对账户密码进行失效时间设置、密码到期提醒设置\\033[0m\u0026#34; sed -i \u0026#39;$iPASS_MAX_DAYS 90\u0026#39; /etc/login.defs \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; sed -i \u0026#39;$iPASS_WARN_AGE 14\u0026#39; /etc/login.defs \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; else PAMAX=`grep -E \u0026#34;^PASS_MAX_DAYS\u0026#34; /etc/login.defs | awk -F\u0026#34; \u0026#34; \u0026#39;{ print $2 }\u0026#39;` PAWARN=`grep -E \u0026#34;^PASS_WARN_AGE\u0026#34; /etc/login.defs | awk -F\u0026#34; \u0026#34; \u0026#39;{ print $2 }\u0026#39;` if [ $PAMAX -le 90 ];then echo -e \u0026#34;\\033[32;40m密码失效时间为$PAMAX天，符合标准\\033[0m\u0026#34;; else echo -e \u0026#34;\\033[31;40m密码失效时间为$PAMAX天，不符合标准\\033[0m\u0026#34;; sed -i \u0026#39;s/^PASS_MAX_DAYS.*/PASS_MAX_DAYS 90/\u0026#39; /etc/login.defs \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; fi if [ $PAWARN -ge 14 ];then echo -e \u0026#34;\\033[32;40m密码到期提醒时间为$PAWARN天，符合标准\\033[0m\u0026#34;; else echo -e \u0026#34;\\033[31;40m密码到期提醒时间为$PAWARN天，不符合标准\\033[0m\u0026#34;; sed -i \u0026#39;s/^PASS_WARN_AGE.*/PASS_WARN_AGE 14/\u0026#39; /etc/login.defs \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; fi fi #chage --warndays 14 root \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; #chage --maxdays 90 root \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; } 密码复杂度设置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 function Check_User_Policy(){ #查看系统账户策略:密码最小长度12位、密码复杂度为大小写英文字母、数字、特殊字符 echo \u0026#34;========正在检查账户密码策略========\u0026#34; Check_User_Poli=`grep -E \u0026#34;^minlen|^minclass\u0026#34; /etc/security/pwquality.conf |wc -l` cp /etc/security/pwquality.conf{,_bak$Date_Time} if [ $Check_User_Poli -lt 2 ];then echo -e \u0026#34;\\033[31;40m当前系统未对账户密码复杂度及密码最小长度设置\\033[0m\u0026#34; sed -i \u0026#39;$iminlen = 12\u0026#39; /etc/security/pwquality.conf \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; sed -i \u0026#39;$iminclass = 4\u0026#39; /etc/security/pwquality.conf \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; else PACLS=`grep -E \u0026#34;^minclass\u0026#34; /etc/security/pwquality.conf | awk -F\u0026#34;=| \u0026#34; \u0026#39;{ print $NF }\u0026#39;` PALEN=`grep -E \u0026#34;^minlen\u0026#34; /etc/security/pwquality.conf | awk -F\u0026#34;=| \u0026#34; \u0026#39;{ print $NF }\u0026#39;` if [ $PACLS -eq 4 ];then echo -e \u0026#34;\\033[32;40m密码负责度为$PACLS种类型，符合标准\\033[0m\u0026#34;; else echo -e \u0026#34;\\033[31;40m密码负责度为$PACLS种类型，不符合标准\\033[0m\u0026#34;; sed -i \u0026#39;s/^minclass.*/minclass = 4/\u0026#39; /etc/login.defs \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; fi if [ $PALEN -ge 12 ];then echo -e \u0026#34;\\033[32;40m密码长度为$PALEN位，符合标准\\033[0m\u0026#34;; else echo -e \u0026#34;\\033[31;40m密码长度为$PALEN位，不符合标准\\033[0m\u0026#34;; sed -i \u0026#39;s/^minlen.*/minlen = 12/\u0026#39; /etc/login.defs \u0026amp;\u0026amp; echo -e \u0026#34;\\033[32;40m 修改成功 \\033[0m\u0026#34; || echo -e \u0026#34;\\033[31;40m 修改失败 \\033[0m\u0026#34; fi fi } 用户认证失败次数设置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 function Check_Auth_Failed(){ echo \u0026#34;========正在检查用户登陆认证失败次数========\u0026#34; Check_Auth_Failsystem=`grep pam_faillock.so /etc/pam.d/system-auth | wc -l` Check_Auth_Failpasswd=`grep pam_faillock.so /etc/pam.d/password-auth | wc -l` cp /etc/pam.d/system-auth{,_bak$Date_Time} cp /etc/pam.d/password-auth{,_bak$Date_Time} if [ $Check_Auth_Failsystem -ge 3 ];then if [ $Check_Auth_Failpasswd -ge 3 ];then echo -e \u0026#34;\\033[32;40m 用户登陆连续认证失败锁定策略设置成功，符合标准 \\033[0m\u0026#34; else echo -e \u0026#34;\\033[31;40m 用户登陆连续认证失败锁定策略设置不完全，不符合标准 \\033[0m\u0026#34; fi else echo -e \u0026#34;\\033[31;40m 用户登陆连续认证失败锁定策略设置不正确，不符合标准 \\033[0m\u0026#34; sed -i \u0026#39;/auth required pam_env.so/i auth required pam_faillock.so preauth audit silent deny=5 unlock_time=900\u0026#39; /etc/pam.d/system-auth sed -i \u0026#39;/auth required pam_deny.so/a auth [default=die] pam_faillock.so authfail audit deny=5 unlock_time=900\u0026#39; /etc/pam.d/system-auth sed -i \u0026#39;/account required pam_unix.so/i account required pam_faillock.so\u0026#39; /etc/pam.d/system-auth sed -i \u0026#39;/auth required pam_env.so/i auth required pam_faillock.so preauth audit silent deny=5 unlock_time=900\u0026#39; /etc/pam.d/password-auth sed -i \u0026#39;/auth required pam_deny.so/a auth [default=die] pam_faillock.so authfail audit deny=5 unlock_time=900\u0026#39; /etc/pam.d/password-auth sed -i \u0026#39;/account required pam_unix.so/i account required pam_faillock.so\u0026#39; /etc/pam.d/password-auth fi } 账号密码过期设置 设置定时任务，可以每天自己执行判断，如果快到期会发邮件告警，及时修改，防止过期导致crontab任务不可使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #!/bin/bash #定义时间变量，用于告警发送 check_time=`date +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;` #日志位置 log=${HOME}/user-info.log user_name=`whoami` end_year=` chage -l ${user_name} | head -2| tail -1 | awk -F: \u0026#39;{print $2}\u0026#39;| awk -F\u0026#39;,\u0026#39; \u0026#39;{print $2}\u0026#39;| awk \u0026#39;{print $1}\u0026#39;` if [ \u0026#34;${end_year}\u0026#34; == \u0026#34;\u0026#34; ];then echo \u0026#34;99999\u0026#34; exit 0 fi end_mounth=`chage -l ${user_name} | head -2| tail -1 | awk -F: \u0026#39;{print $2}\u0026#39;| awk -F\u0026#39;,\u0026#39; \u0026#39;{print $1}\u0026#39;| awk \u0026#39;{print $1}\u0026#39;` case ${end_mounth} in \u0026#39;Jan\u0026#39;) end_mounth=1;; \u0026#39;Feb\u0026#39;) end_mounth=2;; \u0026#39;Mar\u0026#39;) end_mounth=3;; \u0026#39;Apr\u0026#39;) end_mounth=4;; \u0026#39;May\u0026#39;) end_mounth=5;; \u0026#39;Jun\u0026#39;) end_mounth=6;; \u0026#39;Jul\u0026#39;) end_mounth=7;; \u0026#39;Aug\u0026#39;) end_mounth=8;; \u0026#39;Sep\u0026#39;) end_mounth=9;; \u0026#39;Oct\u0026#39;) end_mounth=10;; \u0026#39;Nov\u0026#39;) end_mounth=11;; \u0026#39;Dec\u0026#39;) end_mounth=12;; esac end_day=`chage -l ${user_name} | head -2| tail -1 | awk -F: \u0026#39;{print $2}\u0026#39;| awk -F\u0026#39;,\u0026#39; \u0026#39;{print $1}\u0026#39;| awk \u0026#39;{print $2}\u0026#39;` end_date_s=`/bin/date -d \u0026#34;${end_year}\u0026#34;-\u0026#34;${end_mounth}\u0026#34;-\u0026#34;${end_day}\u0026#34; +%s` star_date_s=`/bin/date +%s` let diffday=(${end_date_s}-${star_date_s})/86400 echo ${diffday} #过期时间判断，如果小于15天，开始发邮件 if [ ${diffday} -gt 15 ] then curl http://mail.ownit.top/send -H \u0026#34;Content-Type:application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;source\u0026#34; : \u0026#34;game\u0026#34;,\u0026#34;contacts\u0026#34; : [\u0026#34;1794748404@qq.com\u0026#34;],\u0026#34;subject\u0026#34; : \u0026#34;\u0026#39;\u0026#34; ${user_name} 账号过期警告\u0026#34;\u0026#39;\u0026#34;,\u0026#34;content\u0026#34; : \u0026#34;\u0026#39;\u0026#34; ${user_name} 账号过期警告 \u0026lt;br\u0026gt; ${user_name} 即将在 ${diffday} 后过期，请及时修改 \u0026lt;br\u0026gt; 注意: 账号密码过期后,用户的Crontab中的执行任务会失效 \u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt; \u0026#34;\u0026#39;\u0026#34;}\u0026#39; echo \u0026#34; time: $check_time 账号 ${user_name} 即将 在 ${diffday} 天后 密码过期 \u0026#34; \u0026gt;\u0026gt; ${log} else echo \u0026#34; time: $check_time 账号 ${user_name} 还有 ${diffday} 天使用期 \u0026#34; \u0026gt;\u0026gt; ${log} fi ","date":"2021-12-23T15:39:37Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202112231539/","title":"Linux操作系统账号密码失效检测"},{"content":"GO的WEB编程（GIN实现邮件接口报警） Gin编写邮件接口（支持多人发送） 这个代码基于上面两个已经完成的功能上实现。\n实现下面功能\n日志分割\n日志记录\n效果图\n因为日常范围，我们在操作系统上，需要报警时，只能采用mailx来使用。需要配置账号，密码，和邮箱认证。如果需要多台使用的话，岂不是很麻烦，要配置多台，这个导致密码很不安全，容易泄露。所以，为了安全，有效，更方便，我们可以采用接口发送邮件。\n（1）构建接口\n（2）传入post的json情况\n（3）把相应json转换字符\n（4）发送邮件\n代码放在一个main。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/natefinch/lumberjack\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; ) /* 支持多人发送 curl http://10.10.10.3:7070/send -H \u0026#34;Content-Type:application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;source\u0026#34;:\u0026#34;heian\u0026#34;,\u0026#34;contacts\u0026#34;:[\u0026#34;账号@qq.com\u0026#34;,\u0026#34;账号@qq.com\u0026#34;],\u0026#34;subject\u0026#34;:\u0026#34;多人测试\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;现在进行多人测试\u0026#34;}\u0026#39; */ /* zapcore.Core需要三个配置——Encoder，WriteSyncer，LogLevel Encoder:编码器(如何写入日志)。我们将使用开箱即用的NewJSONEncoder() WriterSyncer ：指定日志将写到哪里去。我们使用zapcore.AddSync() Log Level：哪种级别的日志将被写入。 */ var sugarLogger *zap.SugaredLogger func InitLogger() { writeSyncer := getLogWriter() encoder := getEncoder() core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger := zap.New(core, zap.AddCaller()) sugarLogger = logger.Sugar() } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder return zapcore.NewConsoleEncoder(encoderConfig) } func getLogWriter() zapcore.WriteSyncer { /* Lumberjack Logger采用以下属性作为输入: Filename: 日志文件的位置 MaxSize：在进行切割之前，日志文件的最大大小（以MB为单位） MaxBackups：保留旧文件的最大个数 MaxAges：保留旧文件的最大天数 Compress：是否压缩/归档旧文件 */ lumberJackLogger := \u0026amp;lumberjack.Logger{ Filename: \u0026#34;./logs/info.log\u0026#34;, MaxSize: 10, MaxBackups: 5, MaxAge: 30, Compress: false, } return zapcore.AddSync(lumberJackLogger) } // 定义接收数据的结构体 type User struct { // binding:\u0026#34;required\u0026#34;修饰的字段，若接收为空值，则报错，是必须字段 Source string `form:\u0026#34;source\u0026#34; json:\u0026#34;source\u0026#34; uri:\u0026#34;source\u0026#34; xml:\u0026#34;source\u0026#34; binding:\u0026#34;required\u0026#34;` Contacts []string `form:\u0026#34;contacts\u0026#34; json:\u0026#34;contacts\u0026#34; uri:\u0026#34;contacts\u0026#34; xml:\u0026#34;contacts\u0026#34; binding:\u0026#34;required\u0026#34;` Subject string `form:\u0026#34;subject\u0026#34; json:\u0026#34;subject\u0026#34; uri:\u0026#34;subject\u0026#34; xml:\u0026#34;subject\u0026#34; binding:\u0026#34;required\u0026#34;` Content string `form:\u0026#34;content\u0026#34; json:\u0026#34;content\u0026#34; uri:\u0026#34;content\u0026#34; xml:\u0026#34;content\u0026#34; binding:\u0026#34;required\u0026#34;` } func SendToMail(user, sendUserName, password, host, to, subject, body, mailtype string) error { hp := strings.Split(host, \u0026#34;:\u0026#34;) //fmt.Println(hp) auth := smtp.PlainAuth(\u0026#34;\u0026#34;, user, password, hp[0]) var content_type string if mailtype == \u0026#34;html\u0026#34; { content_type = \u0026#34;Content-Type: text/\u0026#34; + mailtype + \u0026#34;; charset=UTF-8\u0026#34; } else { content_type = \u0026#34;Content-Type: text/plain\u0026#34; + \u0026#34;; charset=UTF-8\u0026#34; } msg := []byte(\u0026#34;To: \u0026#34; + to + \u0026#34;\\r\\nFrom: \u0026#34; + sendUserName + \u0026#34;\u0026lt;\u0026#34; + user + \u0026#34;\u0026gt;\u0026#34; + \u0026#34;\\r\\nSubject: \u0026#34; + subject + \u0026#34;\\r\\n\u0026#34; + content_type + \u0026#34;\\r\\n\\r\\n\u0026#34; + body) send_to := strings.Split(to, \u0026#34;;\u0026#34;) err := smtp.SendMail(host, auth, user, send_to, msg) //fmt.Println(err) return err } //获取ip func GetRequestIP(c *gin.Context) string { reqIP := c.ClientIP() if reqIP == \u0026#34;::1\u0026#34; { reqIP = \u0026#34;127.0.0.1\u0026#34; } return reqIP } func PostMail(c *gin.Context) { c_ip := GetRequestIP(c) //fmt.Println(c_ip) sugarLogger.Debugf(\u0026#34;调用 PostMail 接口Api，调用者IP： %s \u0026#34;, c_ip) 声明接收的变量 var json User 将request的body中的数据，自动按照json格式解析到结构体 // if err := c.ShouldBindJSON(\u0026amp;json); err != nil { //\t// 返回错误信息 //\t// gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) sugarLogger.Errorf(\u0026#34;Error: %s\u0026#34;, err.Error()) return } //fmt.Println(json.Content, json.Contacts) //c.JSON(http.StatusOK, gin.H{\u0026#34;status\u0026#34;: \u0026amp;json}) sugarLogger.Infof(\u0026#34;info: %s\u0026#34;, json) user := \u0026#34;账号@qq.com\u0026#34; password := \u0026#34;密码\u0026#34; host := \u0026#34;smtp.qq.com:25\u0026#34; source := json.Source if source != \u0026#34;heian\u0026#34; { fmt.Println(\u0026#34;Send mail error!,source 认证失败\u0026#34;) sugarLogger.Errorf(\u0026#34;Send mail error!,source 认证失败\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,source 认证失败\u0026#34;, }) return } //println(json.Contacts) to := json.Contacts if to[0] == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!,发送人为空\u0026#34;) sugarLogger.Errorf(\u0026#34;Send mail error!,发送人为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,发送人为空\u0026#34;, }) return } subject := json.Subject if strings.TrimSpace(subject) == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!标题为空\u0026#34;) sugarLogger.Errorf(\u0026#34;Send mail error!标题为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,标题为空\u0026#34;, }) return } body := ` \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;iso-8859-15\u0026#34;\u0026gt; \u0026lt;title\u0026gt;MMOGA POWER\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ` + fmt.Sprintf(json.Content) + `\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` sendUserName := \u0026#34;告警平台\u0026#34; //发送邮件的人名称 fmt.Println(\u0026#34;send email\u0026#34;) for _, s := range to { //fmt.Println(i, s) err := SendToMail(user, sendUserName, password, host, s, subject, body, \u0026#34;html\u0026#34;) //log.Printf(\u0026#34;接收人：\u0026#34;, s+\u0026#34;\\n\u0026#34;+\u0026#34;标题:\u0026#34;, json.Subject+\u0026#34;\\n\u0026#34;, \u0026#34;发送内容：\u0026#34;, json.Content+\u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;接收人:%s \\n 标题: %s \\n 内容: %s \\n\u0026#34;, s, json.Subject, json.Content) sugarLogger.Infof(\u0026#34;接收人: %s ,标题: %s, 内容: %s\u0026#34;, s, json.Subject, json.Content) if err != nil { fmt.Println(\u0026#34;Send mail error!\\n\u0026#34;) sugarLogger.Errorf(\u0026#34;Error 调用者IP: %s ,Send mail error! !\u0026#34;, c_ip) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error! !\\n\u0026#34;, }) //fmt.Println(err) } else { fmt.Println(\u0026#34;Send mail success!\\n\u0026#34;) sugarLogger.Infof(\u0026#34;success 调用者IP: %s ,Send mail success! !\u0026#34;, c_ip) c.JSON(http.StatusOK, gin.H{ \u0026#34;success\u0026#34;: \u0026#34;Send mail success! !\\n\u0026#34;, }) } } } func main() { // 1.创建路由 // 默认使用了2个中间件Logger(), Recovery() InitLogger() defer sugarLogger.Sync() r := gin.Default() // JSON绑定 r.POST(\u0026#34;send\u0026#34;, PostMail) sugarLogger.Infof(\u0026#34;Success! Port is start\u0026#34;) r.Run(\u0026#34;:7070\u0026#34;) } ","date":"2021-12-17T18:31:58Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202112171831/","title":"Gin编写邮件告警接口（添加优化日志记录）"},{"content":"GO的WEB编程（GIN实现邮件接口报警） 上面吧基本功能实现，但是不支持多人发送。\n下面做个小改动，支持多人发送\n因为日常范围，我们在操作系统上，需要报警时，只能采用mailx来使用。需要配置账号，密码，和邮箱认证。如果需要多台使用的话，岂不是很麻烦，要配置多台，这个导致密码很不安全，容易泄露。所以，为了安全，有效，更方便，我们可以采用接口发送邮件。\n（1）构建接口\n（2）传入post的json情况\n（3）把相应json转换字符\n（4）发送邮件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) /* 支持多人发送 curl http://10.10.10.3:7070/send -H \u0026#34;Content-Type:application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;source\u0026#34;:\u0026#34;heian\u0026#34;,\u0026#34;contacts\u0026#34;:[\u0026#34;账号@qq.com\u0026#34;,\u0026#34;账号@qq.com\u0026#34;],\u0026#34;subject\u0026#34;:\u0026#34;多人测试\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;现在进行多人测试\u0026#34;}\u0026#39; */ // 定义接收数据的结构体 type User struct { // binding:\u0026#34;required\u0026#34;修饰的字段，若接收为空值，则报错，是必须字段 Source string `form:\u0026#34;source\u0026#34; json:\u0026#34;source\u0026#34; uri:\u0026#34;source\u0026#34; xml:\u0026#34;source\u0026#34; binding:\u0026#34;required\u0026#34;` Contacts []string `form:\u0026#34;contacts\u0026#34; json:\u0026#34;contacts\u0026#34; uri:\u0026#34;contacts\u0026#34; xml:\u0026#34;contacts\u0026#34; binding:\u0026#34;required\u0026#34;` Subject string `form:\u0026#34;subject\u0026#34; json:\u0026#34;subject\u0026#34; uri:\u0026#34;subject\u0026#34; xml:\u0026#34;subject\u0026#34; binding:\u0026#34;required\u0026#34;` Content string `form:\u0026#34;content\u0026#34; json:\u0026#34;content\u0026#34; uri:\u0026#34;content\u0026#34; xml:\u0026#34;content\u0026#34; binding:\u0026#34;required\u0026#34;` } func SendToMail(user, sendUserName, password, host, to, subject, body, mailtype string) error { hp := strings.Split(host, \u0026#34;:\u0026#34;) //fmt.Println(hp) auth := smtp.PlainAuth(\u0026#34;\u0026#34;, user, password, hp[0]) var content_type string if mailtype == \u0026#34;html\u0026#34; { content_type = \u0026#34;Content-Type: text/\u0026#34; + mailtype + \u0026#34;; charset=UTF-8\u0026#34; } else { content_type = \u0026#34;Content-Type: text/plain\u0026#34; + \u0026#34;; charset=UTF-8\u0026#34; } msg := []byte(\u0026#34;To: \u0026#34; + to + \u0026#34;\\r\\nFrom: \u0026#34; + sendUserName + \u0026#34;\u0026lt;\u0026#34; + user + \u0026#34;\u0026gt;\u0026#34; + \u0026#34;\\r\\nSubject: \u0026#34; + subject + \u0026#34;\\r\\n\u0026#34; + content_type + \u0026#34;\\r\\n\\r\\n\u0026#34; + body) send_to := strings.Split(to, \u0026#34;;\u0026#34;) err := smtp.SendMail(host, auth, user, send_to, msg) //fmt.Println(err) return err } func PostMail(c *gin.Context) { 声明接收的变量 var json User 将request的body中的数据，自动按照json格式解析到结构体 // if err := c.ShouldBindJSON(\u0026amp;json); err != nil { //\t// 返回错误信息 //\t// gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } //fmt.Println(json.Content, json.Contacts) //c.JSON(http.StatusOK, gin.H{\u0026#34;status\u0026#34;: \u0026amp;json}) user := \u0026#34;账号@qq.com\u0026#34; password := \u0026#34;密码\u0026#34; host := \u0026#34;smtp.qq.com:25\u0026#34; source := json.Source if source != \u0026#34;heian\u0026#34; { fmt.Println(\u0026#34;Send mail error!,source 认证失败\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,source 认证失败\u0026#34;, }) return } //println(json.Contacts) to := json.Contacts if to[0] == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!,发送人为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,发送人为空\u0026#34;, }) return } subject := json.Subject if strings.TrimSpace(subject) == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!标题为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,标题为空\u0026#34;, }) return } body := ` \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;iso-8859-15\u0026#34;\u0026gt; \u0026lt;title\u0026gt;MMOGA POWER\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ` + fmt.Sprintf(json.Content) + `\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` sendUserName := \u0026#34;告警平台\u0026#34; //发送邮件的人名称 fmt.Println(\u0026#34;send email\u0026#34;) for _, s := range to { //fmt.Println(i, s) err := SendToMail(user, sendUserName, password, host, s, subject, body, \u0026#34;html\u0026#34;) //log.Printf(\u0026#34;接收人：\u0026#34;, s+\u0026#34;\\n\u0026#34;+\u0026#34;标题:\u0026#34;, json.Subject+\u0026#34;\\n\u0026#34;, \u0026#34;发送内容：\u0026#34;, json.Content+\u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;接收人:%s \\n 标题: %s \\n 内容: %s \\n\u0026#34;, s, json.Subject, json.Content) if err != nil { fmt.Println(\u0026#34;Send mail error!\\n\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error! !\\n\u0026#34;, }) //fmt.Println(err) } else { fmt.Println(\u0026#34;Send mail success!\\n\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;success\u0026#34;: \u0026#34;Send mail success! !\\n\u0026#34;, }) } } } func main() { // 1.创建路由 // 默认使用了2个中间件Logger(), Recovery() r := gin.Default() // JSON绑定 r.POST(\u0026#34;send\u0026#34;, PostMail) r.Run(\u0026#34;:7070\u0026#34;) } ","date":"2021-12-13T19:36:02Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202112131936/","title":"Gin编写邮件接口（支持多人发送）"},{"content":"为什么要写这个邮件告警接口？？？？？？\n因为日常范围，我们在操作系统上，需要报警时，只能采用mailx来使用。需要配置账号，密码，和邮箱认证。如果需要多台使用的话，岂不是很麻烦，要配置多台，这个导致密码很不安全，容易泄露。所以，为了安全，有效，更方便，我们可以采用接口发送邮件。\n（1）构建接口\n（2）传入post的json情况\n（3）把相应json转换字符\n（4）发送邮件\n** 调用方式（已做脱敏）**\n1 curl http://mawnit.top/send -H \u0026#34;Content-Type:application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;source\u0026#34;:\u0026#34;heiassns\u0026#34;,\u0026#34;contacts\u0026#34;:\u0026#34;17947@qq.com\u0026#34;,\u0026#34;subject\u0026#34;:\u0026#34;geian\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;精确吗,我很好\u0026#34;} \u0026#39; 代码很简单，实现基本功能\n自定义标题，内容，发送人（只支持单个）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) // 定义接收数据的结构体 type User struct { // binding:\u0026#34;required\u0026#34;修饰的字段，若接收为空值，则报错，是必须字段 Source string `form:\u0026#34;source\u0026#34; json:\u0026#34;source\u0026#34; uri:\u0026#34;source\u0026#34; xml:\u0026#34;source\u0026#34; binding:\u0026#34;required\u0026#34;` Contacts string `form:\u0026#34;contacts\u0026#34; json:\u0026#34;contacts\u0026#34; uri:\u0026#34;contacts\u0026#34; xml:\u0026#34;contacts\u0026#34; binding:\u0026#34;required\u0026#34;` Subject string `form:\u0026#34;subject\u0026#34; json:\u0026#34;subject\u0026#34; uri:\u0026#34;subject\u0026#34; xml:\u0026#34;subject\u0026#34; binding:\u0026#34;required\u0026#34;` Content string `form:\u0026#34;content\u0026#34; json:\u0026#34;content\u0026#34; uri:\u0026#34;content\u0026#34; xml:\u0026#34;content\u0026#34; binding:\u0026#34;required\u0026#34;` } func SendToMail(user, sendUserName, password, host, to, subject, body, mailtype string) error { hp := strings.Split(host, \u0026#34;:\u0026#34;) fmt.Println(hp) auth := smtp.PlainAuth(\u0026#34;\u0026#34;, user, password, hp[0]) var content_type string if mailtype == \u0026#34;html\u0026#34; { content_type = \u0026#34;Content-Type: text/\u0026#34; + mailtype + \u0026#34;; charset=UTF-8\u0026#34; } else { content_type = \u0026#34;Content-Type: text/plain\u0026#34; + \u0026#34;; charset=UTF-8\u0026#34; } msg := []byte(\u0026#34;To: \u0026#34; + to + \u0026#34;\\r\\nFrom: \u0026#34; + sendUserName + \u0026#34;\u0026lt;\u0026#34; + user + \u0026#34;\u0026gt;\u0026#34; + \u0026#34;\\r\\nSubject: \u0026#34; + subject + \u0026#34;\\r\\n\u0026#34; + content_type + \u0026#34;\\r\\n\\r\\n\u0026#34; + body) send_to := strings.Split(to, \u0026#34;;\u0026#34;) err := smtp.SendMail(host, auth, user, send_to, msg) fmt.Println(err) return err } func main() { // 1.创建路由 // 默认使用了2个中间件Logger(), Recovery() r := gin.Default() // JSON绑定 r.POST(\u0026#34;send\u0026#34;, func(c *gin.Context) { 声明接收的变量 var json User 将request的body中的数据，自动按照json格式解析到结构体 // if err := c.ShouldBindJSON(\u0026amp;json); err != nil { //\t// 返回错误信息 //\t// gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } //fmt.Println(json.Content, json.Contacts) //c.JSON(http.StatusOK, gin.H{\u0026#34;status\u0026#34;: \u0026amp;json}) user := \u0026#34;发件人邮箱\u0026#34; password := \u0026#34;密码\u0026#34; host := \u0026#34;smtp.qq.com:25\u0026#34; source := json.Source if source != \u0026#34;heian\u0026#34; { fmt.Println(\u0026#34;Send mail error!,source 认证失败\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,source 认证失败\u0026#34;, }) return } to := json.Contacts if strings.TrimSpace(to) == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!,发送人为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,发送人为空\u0026#34;, }) return } subject := json.Subject if strings.TrimSpace(subject) == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Send mail error!标题为空\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error!,标题为空\u0026#34;, }) return } body := ` \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;iso-8859-15\u0026#34;\u0026gt; \u0026lt;title\u0026gt;MMOGA POWER\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ` + fmt.Sprintf(json.Content) + `\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;` //log.Printf(\u0026#34;接收人：\u0026#34;, json.Contacts+\u0026#34;\\n\u0026#34;+\u0026#34;标题:\u0026#34;, json.Subject+\u0026#34;\\n\u0026#34;, \u0026#34;发送内容：\u0026#34;, json.Content+\u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;接收人:%s \\n 标题: %s \\n 内容: %s \\n\u0026#34;, json.Contacts, json.Subject, json.Content) sendUserName := \u0026#34;告警平台\u0026#34; //发送邮件的人名称 fmt.Println(\u0026#34;send email\u0026#34;) err := SendToMail(user, sendUserName, password, host, to, subject, body, \u0026#34;html\u0026#34;) if err != nil { fmt.Println(\u0026#34;Send mail error!\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;Send mail error! !\u0026#34;, }) //fmt.Println(err) } else { fmt.Println(\u0026#34;Send mail success!\u0026#34;) c.JSON(http.StatusOK, gin.H{ \u0026#34;success\u0026#34;: \u0026#34;Send mail success! !\u0026#34;, }) } }) r.Run(\u0026#34;:7070\u0026#34;) } 后续慢慢改进，把相应的发送信息，保存到数据库中，支持多人发送，支持文件发送，等等\n","date":"2021-12-10T11:12:36Z","image":"https://www.ownit.top/title_pic/59.jpg","permalink":"https://www.ownit.top/p/202112101112/","title":"GO的WEB编程（GIN实现邮件接口报警）"},{"content":"\nTCP扫描增强器 TCP扫描增强器实现原理，主要是使用TCP三次握手原理\nTCP是比我们介绍的要复杂的多，但是我们只介绍一点基础知识。TCP的握手有三个过程。\n首先，客户端发送一个 syn 的包，表示建立回话的开始。如果客户端收到超时，说明端口可能在防火墙后面，或者没有启用服务器\n第二，如果服务端应答 syn-ack 包，意味着这个端口是打开的，否则会返回 rst 包。最后，客户端需要另外发送一个 ack 包。从这时起，连接就已经建立。\n我们TCP扫描器第一步先实现单个端口的测试。使用标准库中的 net.Dial 函数，该函数接收两个参数：协议和测试地址（带端口号）。\n版本一（单端口） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func main() { _, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;www.baidu.com:80\u0026#34;) if err == nil { fmt.Println(\u0026#34;Connection successful\u0026#34;) } else { fmt.Println(err) } } 版本二（多端口） 为了不一个一个地测试每个端口，我们将添加一个简单的循环来简化整个测试过程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; ) func main() { for port := 80; port \u0026lt; 100; port++ { conn, err := net.Dial(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;www.baidu.com:%d\u0026#34;, port)) if err == nil { conn.Close() fmt.Println(\u0026#34;Connection successful\u0026#34;) } else { fmt.Println(err) } } } 这种处理方式有个很大的问题，极度的慢。我们可以通过两个操作来处理一下：并行的执行及为每个连接添加超时控制。\n我们来看下如何实现并行。第一步先把扫描功能拆分为一个独立函数。这样会使我们的代码看起来清晰。\n版本三（并发执行） 我们会引入一个新的方法 WaitGroup ，详细用法信息可以参考标准库文档。在主函数中，我们可以拆分为协程去执行，然后等待执行结束\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func isOpen(host string, port int) bool { time.Sleep(time.Millisecond * 3) conn, err := net.Dial(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, host, port)) if err == nil { _ = conn.Close() return true } return false } func main() { ports := []int{} wg := \u0026amp;sync.WaitGroup{} for port := 1; port \u0026lt; 50000; port++ { wg.Add(1) port := port go func() { opened := isOpen(\u0026#34;www.baidu.com\u0026#34;, port) if opened { ports = append(ports, port) } wg.Done() }() } wg.Wait() fmt.Printf(\u0026#34;opened ports: %v\\n\u0026#34;, ports) } 版本四（等待超时） 我们的代码已经执行的很快了，但是由于超时的原因，我们需要等待很久才能收到返回的错误信息。我们可以假设如果我们200毫秒内没有收到服务器的回应，就不再继续等待。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func isOpen(host string, port int, timeout time.Duration) bool { time.Sleep(time.Millisecond * 1) conn, err := net.DialTimeout(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, host, port), timeout) if err == nil { _ = conn.Close() return true } return false } func main() { ports := []int{} wg := \u0026amp;sync.WaitGroup{} timeout := time.Millisecond * 200 for port := 1; port \u0026lt; 100; port++ { wg.Add(1) go func(p int) { opened := isOpen(\u0026#34;www.baidu.com\u0026#34;, p, timeout) if opened { ports = append(ports, p) } wg.Done() }(port) } wg.Wait() fmt.Printf(\u0026#34;opened ports: %v\\n\u0026#34;, ports) } 版本五（添加锁） 为什么要添加锁，因为并发执行的话，在往ports数组写的话，会有影响。\n现在这个程序会有竞争条件。在只扫描少数端口时，速度比较慢，可能不会出现，但确实存在这个问题。所以我们需要使用 mutex 来修复它。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup var mutex sync.Mutex func isOpen(host string, port int, timeout time.Duration) bool { time.Sleep(time.Millisecond * 1) conn, err := net.DialTimeout(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, host, port), timeout) if err == nil { _ = conn.Close() return true } return false } func main() { startTime := time.Now() ports := []int{} timeout := time.Millisecond * 500 for port := 1; port \u0026lt;= 65000; port++ { go func(p int) { opened := isOpen(\u0026#34;www.baidu.com\u0026#34;, p, timeout) if opened { mutex.Lock() ports = append(ports, p) log.Printf(\u0026#34;端口: %d 已经开通\u0026#34;, p) mutex.Unlock() } }(port) } time.Since(startTime) cost := int(time.Since(startTime) / time.Second) fmt.Printf(\u0026#34;opened ports: %v\\n\u0026#34;, ports) fmt.Printf(\u0026#34;代码运行时长: %d S\u0026#34;, cost) } 版本六（并发控制） 为什么并发控制，不控制的话，在运行时会卡，有时间会导致竞争条件。会影响接口，为了数值的准确性，有必要控制一下并发数量\n这里面并发控制，我采用channel，有兴趣可以谷歌一下。\ngolimit.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package golimit type GoLimit struct { ch chan int } func NewGoLimit(max int) *GoLimit { return \u0026amp;GoLimit{ch: make(chan int, max)} } func (g *GoLimit) Add() { g.ch \u0026lt;- 1 } func (g *GoLimit) Done() { \u0026lt;-g.ch } tcp.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package main import ( \u0026#34;code/Projects/tcp_Scanning/golimit\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup var mutex sync.Mutex func isOpen(host string, port int, timeout time.Duration) bool { time.Sleep(time.Millisecond * 1) conn, err := net.DialTimeout(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, host, port), timeout) if err == nil { _ = conn.Close() return true } return false } func main() { startTime := time.Now() ports := []int{} timeout := time.Millisecond * 500 g := golimit.NewGoLimit(2000) for port := 1; port \u0026lt;= 65000; port++ { g.Add() go func(g *golimit.GoLimit, p int) { opened := isOpen(\u0026#34;10.10.10.1\u0026#34;, p, timeout) if opened { mutex.Lock() ports = append(ports, p) log.Printf(\u0026#34;端口: %d 已经开通\u0026#34;, p) mutex.Unlock() } g.Done() }(g, port) } time.Since(startTime) cost := int(time.Since(startTime) / time.Second) fmt.Printf(\u0026#34;opened ports: %v\\n\u0026#34;, ports) fmt.Printf(\u0026#34;代码运行时长: %d S\u0026#34;, cost) } 版本七（参数定制） 我们就得到了一个简单的端口扫描器。但有些不好的是，不能很方便的修改域名地址以及端口号范围，我们必须要重新编译代码才可以。Go还有一个很不错的包叫做 flag 。\nflag 包可以帮助我们编写命令行程序。我们可以配置每个字符串或数字。我们为主机名及要测试的端口范围和连接超时添加参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package main import ( \u0026#34;code/Projects/tcp_Scanning/golimit\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup var mutex sync.Mutex func isOpen(host string, port int, timeout time.Duration) bool { time.Sleep(time.Millisecond * 1) conn, err := net.DialTimeout(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, host, port), timeout) if err == nil { _ = conn.Close() return true } return false } func main() { startTime := time.Now() hostname := flag.String(\u0026#34;hostname\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;hostname to test\u0026#34;) startPort := flag.Int(\u0026#34;start-port\u0026#34;, 80, \u0026#34;the port on which the scanning starts\u0026#34;) endPort := flag.Int(\u0026#34;end-port\u0026#34;, 100, \u0026#34;the port from which the scanning ends\u0026#34;) timeout := flag.Duration(\u0026#34;timeout\u0026#34;, time.Millisecond*200, \u0026#34;timeout\u0026#34;) golimits := flag.Int(\u0026#34;golimit\u0026#34;, 1000, \u0026#34;the Program Concurrency\u0026#34;) flag.Parse() ports := []int{} //timeout := time.Millisecond * 500 g := golimit.NewGoLimit(*golimits) for port := *startPort; port \u0026lt;= *endPort; port++ { g.Add() go func(g *golimit.GoLimit, p int) { opened := isOpen(*hostname, p, *timeout) if opened { mutex.Lock() ports = append(ports, p) log.Printf(\u0026#34;端口: %d 已经开通\u0026#34;, p) mutex.Unlock() } g.Done() }(g, port) } time.Since(startTime) cost := int(time.Since(startTime) / time.Second) fmt.Printf(\u0026#34;opened ports: %v\\n\u0026#34;, ports) fmt.Printf(\u0026#34;代码运行时长: %d S\u0026#34;, cost) } 如果我们想要显示如何使用，我们可以添加一个 -h 参数，来显示使用说明。整个项目不到50行的代码，我们使用到了并行、flag 及 net 包。\n速度测试还不错，\n并发 2000，扫描65000端口，只需要16s，而且很准确。\n如果设置4000的并发，扫描出来的结果可能缺少，只需要8S 。\n大家可以多试试。\n","date":"2021-12-02T17:25:11Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202112021725/","title":"TCP扫描增强器实现65000端口，10S完成，快准狠（Go语言编程）"},{"content":"Go语言的协程会并发，执行，可以大大提高效率。\n列如，我们通过 ping 来检测网络的主机的话。\n如果使用shell的话，会检查一个IP，在检查下一个IP，速度很慢。\n如果我们使用Python 的话，可以使用多线程。\n我们这里使用Go的协程来操作，速度是刚刚的。\n一个网段，10S中，相当于，一秒钟处理25个左右的IP，因为ping检查，有延时性 此脚本，只能在Linux上执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup func main() { start := time.Now() ip := \u0026#34;10.10.10.\u0026#34; wg.Add(254) for i := 1; i \u0026lt;= 254; i++ { //fmt.Println(ip + strconv.Itoa(i)) true_ip := ip + strconv.Itoa(i) go ping(true_ip) } wg.Wait() cost := time.Since(start) fmt.Println(\u0026#34;执行时间:\u0026#34;, cost) } func ping(ip string) { var beaf = \u0026#34;false\u0026#34; Command := fmt.Sprintf(\u0026#34;ping -c 1 %s \u0026gt; /dev/null \u0026amp;\u0026amp; echo true || echo false\u0026#34;, ip) output, err := exec.Command(\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, Command).Output() if err != nil { fmt.Println(err) return } real_ip := strings.TrimSpace(string(output)) if real_ip == beaf { fmt.Printf(\u0026#34;IP: %s 失败\\n\u0026#34;, ip) } else { fmt.Printf(\u0026#34;IP: %s 成功 ping通\\n\u0026#34;, ip) } wg.Done() } 不得不说，GO语言的并发，是真的香啊，\n","date":"2021-11-30T11:17:32Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/202111301117/","title":"golang的ping检测主机存活"},{"content":"\nRsyslog同步集群服务器的网络连接状态 上篇文件，主要是把集群服务器状态同步到一台机器上，然后通过grep，awk 什么的比较方便。但是考虑到更简单，方便的操作，那就是接入elk日志管理平台\n来来，大致思路很简单\n（1）有个完成的elk集群\n（2）使用filebeat收集汇总的tcp.log（只要一个filebeat就可以）\n（3）把filebeat数据发送到logstash中，进行日志切割转换（靠，正则很难受）\n（4）把logstash的数据存储到es\n（5）kibana展示es中的数据日志\n（1）filebeat收集tcp.log 来来，看配置，很简单\n配置文件 1 2 3 4 5 6 7 8 9 10 11 [root@logserver01 filebeat]# cat tcp_listen.yml #=========================== Filebeat inputs ============================= filebeat.inputs: - type: log enabled: true tail_files: true paths: - /var/log/history/tcp.log #=========================== Filebeat outppp_id: messuts ============================= output.logstash: hosts: [\u0026#34;127.0.0.1:5516\u0026#34;] 启动命令 1 nohup /usr/local/filebeat/filebeat -e -c /usr/local/filebeat/tcp_listen.yml -path.data=/usr/local/filebeat/tcp_listen \u0026amp; （2）logstash收集清洗日志 清洗日志，比较麻烦，需要grok正则来实现\ngrok正则\nhttp://grokdebug.herokuapp.com/\n1 2021-11-22T10:51:41+08:00 172.17.42.101 storm-42-101 [storm] info: 172.21.5.22 ESTABLISHED 1 grok的正则 1 ^(?\u0026lt;atime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s+)(?\u0026lt;hostip\u0026gt;\\d+\\.\\d+\\.\\d+\\.\\d+)(?:\\s)(?\u0026lt;hostname\u0026gt;[^ ]+)(?:\\s+)(?\u0026lt;hostuser\u0026gt;[^ ]+)(?:\\s+)(?\u0026lt;names\u0026gt;[^ ]+)(?:\\s)%{HOSTNAME:connect_IP}(?:\\s)(?\u0026lt;connect_state\u0026gt;[^ ]+)(?:\\s)%{HOSTNAME:connect_num} 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 [root@logserver01 config]# cat tcp_listen.conf input { beats { port =\u0026gt; 5516 type =\u0026gt; syslog } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;^(?\u0026lt;atime\u0026gt;\\d+-\\d+-\\d+)(?:[^\\d]+)(?\u0026lt;hhmmss\u0026gt;\\d+:\\d+:\\d+)(?:[^\\d]+\\d+:\\d+)(?:\\s+)(?\u0026lt;hostip\u0026gt;\\d+\\.\\d+\\.\\d+\\.\\d+)(?:\\s)(?\u0026lt;hostname\u0026gt;[^ ]+)(?:\\s+)(?\u0026lt;hostuser\u0026gt;[^ ]+)(?:\\s+)(?\u0026lt;names\u0026gt;[^ ]+)(?:\\s)%{HOSTNAME:connect_IP}(?:\\s)(?\u0026lt;connect_state\u0026gt;[^ ]+)(?:\\s)%{HOSTNAME:connect_num}\u0026#34; } overwrite =\u0026gt; [\u0026#34;message\u0026#34;] } mutate { split =\u0026gt; [\u0026#34;type\u0026#34;,\u0026#34;,\u0026#34;] } mutate{remove_field =\u0026gt; [ \u0026#34;tags\u0026#34;,\u0026#34;agent\u0026#34;,\u0026#34;host\u0026#34;,\u0026#34;log\u0026#34;,\u0026#34;ecs\u0026#34;,\u0026#34;type\u0026#34; ]} ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;index_date\u0026#39;, event.get(\u0026#39;@timestamp\u0026#39;).time.localtime + 8*60*60)\u0026#34; } mutate { convert =\u0026gt; [\u0026#34;index_date\u0026#34;, \u0026#34;string\u0026#34;] gsub =\u0026gt; [\u0026#34;index_date\u0026#34;, \u0026#34;-\\d{2}T([\\S\\s]*?)Z\u0026#34;, \u0026#34;\u0026#34;] gsub =\u0026gt; [\u0026#34;index_date\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;] } date { match =\u0026gt; [\u0026#34;time\u0026#34;, \u0026#34;yyyy-MM-dd HH:mm:ss,SSS\u0026#34;, \u0026#34;UNIX\u0026#34;] target =\u0026gt; \u0026#34;@timestamp\u0026#34; locale =\u0026gt; \u0026#34;cn\u0026#34; } } output { stdout { # codec=\u0026gt; rubydebug } elasticsearch { hosts =\u0026gt; [\u0026#34;http://127.0.0.1:9200\u0026#34;] index =\u0026gt; \u0026#34;tcp_listen_%{index_date}\u0026#34; } } 启动命令 1 nohup /usr/local/logstash/bin/logstash -f /usr/local/logstash/config/tcp_listen.conf --path.data=/usr/local/logstash/data/tcp_listen \u0026amp; （3）kibana展示数据 创建索引 查看数据 脚本为15分钟拉去一次数据的。所有在kibana展示也是和脚本时间同步的 ","date":"2021-11-22T15:36:12Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/202111221536/","title":"集群服务器的网络连接状态接入ELK（可视化操作）"},{"content":"目前集群机器比较多，因为下载业务比较多，网络状态不好监控，涉及的状态比较多。有几次出问题，对方的锅，还要扔给我们。有必要做一下监控。防止背锅\n目前业务方式是把日志写入本地，到时间那台有问题，再去分析那一台。\n我一看，这还的行吗。如果出问题业务量大，分析半天能累死人，太累了，不太建议。\n看来的拿出来看家本领，好歹也是管理过上K台机器的菜鸡，还是有点方法的。\n（1）使用脚本分析本地网络的问题\n（2）写入rsyslog\n（3）使用rsyslog的同步，把日志接入一台机器\n（4）然后把数据接入elk，使用可视化界面操作（逼格是否一下上来了）\n1、分析网络的脚本 1 2 3 4 5 6 7 [heian@game ~]$ netstat -n | awk \u0026#39;/^tcp/ {print $0}\u0026#39; | awk \u0026#39;{print $(NF-1),$NF}\u0026#39; | awk -F \u0026#39;:| \u0026#39; \u0026#39;{print $1,$NF}\u0026#39; |awk \u0026#39;{a[$1\u0026#34; \u0026#34;$2]++}END{for(i in a)print i,a[i]}\u0026#39; |sort -nk 3 10.10.10.3 ESTABLISHED 1 127.0.0.1 ESTABLISHED 6 172.18.0.2 ESTABLISHED 1 192.168.1.100 ESTABLISHED 6 192.168.1.15 ESTABLISHED 6 192.168.1.210 ESTABLISHED 1 脚本实现，把数据写入rsyslog\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [root@game heian]# cat tcp_listen.sh #!/bin/bash date_time=`date +\u0026#34;%Y%m%d%H%M\u0026#34;` today=`date +\u0026#34;%Y-%m-%d\u0026#34;` date_hour=`date +\u0026#34;%Y-%m-%d_%H\u0026#34;` log_dir=\u0026#34;/app/bighead/scripts/monitorNetstat_log\u0026#34; log_path=$log_dir/$today/${date_hour}.log #检查文件夹是否存在，不存在则创建 [ ! -d $log_dir/$today ] \u0026amp;\u0026amp; mkdir -p $log_dir/$today #获取数据 netstat -n | awk \u0026#39;/^tcp/ {print $0}\u0026#39; | awk \u0026#39;{print $(NF-1),$NF}\u0026#39; | awk -F \u0026#39;:| \u0026#39; \u0026#39;{print $1,$NF}\u0026#39; |awk -v date_time=$date_time \u0026#39;{a[$1\u0026#34; \u0026#34;$2]++}END{for(i in a)print date_time,i,a[i]}\u0026#39; |sort -nk 4 \u0026gt;\u0026gt; $log_path tcp_listen2=$(netstat -n | awk \u0026#39;/^tcp/ {print $0}\u0026#39; | awk \u0026#39;{print $(NF-1),$NF}\u0026#39; | awk -F \u0026#39;:| \u0026#39; \u0026#39;{print $1,$NF}\u0026#39; |awk \u0026#39;{a[$1\u0026#34; \u0026#34;$2]++}END{for(i in a)print i,a[i]}\u0026#39; |sort -nk 3) #写入本地文件 while read tcp do echo \u0026#34;$tcp\u0026#34; logger -p local5.info \u0026#34;$tcp\u0026#34; done \u0026lt;\u0026lt;\u0026lt; \u0026#34;$tcp_listen2\u0026#34; #删除5天前的文件夹 [ -d $log_dir ] \u0026amp;\u0026amp; find $log_dir -type d -mtime +5 | xargs rm -rf 此脚本会把数据在本地保存一份，也会把数据写入rsyslog\n为什么要这么做，防止rsyslog服务器宕机，数据丢失喽（别说，考虑全面点）\n2、分发脚本，执行定时任务 把这个脚本分发到集群机器上，设置定时任务，10分钟执行一次，会同步一次网络状态。\n至于分发，和执行命令创建，ansible喽，自己写了命令，一个copy 一个cron 两个命令搞定\n1 2 3 4 5 #大致这格式 ansible p3 -m copy -a \u0026#34;src=/root/node_monitor dest=/root\u0026#34; ansible p3 -m shell -a \u0026#34;chmod a+x /root/node_monitor/*\u0026#34; -f 10 ansible p3 -m cron -a \u0026#34;minute=* hour=* day=* month=* weekday=* name=\u0026#39;chia\u0026#39; job=\u0026#39;/usr/bin/python3 /root/node_monitor/node_monitor.py\u0026#39; user=\u0026#39;root\u0026#39; state=\u0026#39;present\u0026#39;\u0026#34; 3、同步集群网络状态到rsyslog rsyslog 服务器配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@jenkins ~]# cat /etc/rsyslog.conf $ModLoad imuxsock # provides support for local system logging (e.g. via logger command) $ModLoad imklog # provides kernel logging support (previously done by rklogd) $ModLoad imudp $UDPServerRun 514 $ModLoad imtcp $InputTCPServerRun 514 $template myFormat,\u0026#34;%timestamp:::date-rfc3339% %fromhost-ip% %HOSTNAME% [%programname%] %syslogseverity-text%:%msg%\\n\u0026#34; $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat $IncludeConfig /etc/rsyslog.d/*.conf *.info;mail.none;authpriv.none;cron.none /var/log/messages;myFormat authpriv.* /var/log/secure;myFormat mail.* -/var/log/maillog cron.* /var/log/cron *.emerg :omusrmsg:* uucp,news.crit /var/log/spooler local7.* /var/log/boot.log local5.* /var/log/history/tcp.log;myFormat #将local类型5的日志存放到 /var/log/history/tcp.log下 使用myFormat模板 rsyslog 客户端配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [heian@game ~]$ cat /etc/rsyslog.conf | grep -Ev \u0026#34;^$|^#\u0026#34; $ModLoad imuxsock # provides support for local system logging (e.g. via logger command) $ModLoad imjournal # provides access to the systemd journal $WorkDirectory /var/lib/rsyslog $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat $IncludeConfig /etc/rsyslog.d/*.conf $OmitLocalLogging on $IMJournalStateFile imjournal.state *.info;mail.none;authpriv.none;cron.none /var/log/messages authpriv.* /var/log/secure mail.* -/var/log/maillog cron.* /var/log/cron *.emerg :omusrmsg:* uucp,news.crit /var/log/spooler local7.* /var/log/boot.log *.* @@192.168.1.210 #主要是最后一行啊啊啊啊啊啊啊啊啊，注意 配置文件后，记得重启客户端啊\n4、查看日志 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@jenkins history]# pwd /var/log/history [root@jenkins history]# ls tcp.log [root@jenkins history]# cat tcp.log |wc -l 28 [root@jenkins history]# tail -f tcp.log 2021-11-19T16:32:21+08:00 192.168.1.100 game [heian] info: 172.18.0.2 ESTABLISHED 1 2021-11-19T16:32:21+08:00 192.168.1.100 game [heian] info: 192.168.1.100 ESTABLISHED 6 2021-11-19T16:32:21+08:00 192.168.1.100 game [heian] info: 192.168.1.15 ESTABLISHED 6 2021-11-19T16:32:21+08:00 192.168.1.100 game [heian] info: 192.168.1.210 ESTABLISHED 1 2021-11-19T16:32:40+08:00 192.168.1.100 game [heian] info: 10.10.10.3 ESTABLISHED 1 2021-11-19T16:32:40+08:00 192.168.1.100 game [heian] info: 127.0.0.1 ESTABLISHED 6 2021-11-19T16:32:40+08:00 192.168.1.100 game [heian] info: 172.18.0.2 ESTABLISHED 1 2021-11-19T16:32:40+08:00 192.168.1.100 game [heian] info: 192.168.1.100 ESTABLISHED 6 2021-11-19T16:32:40+08:00 192.168.1.100 game [heian] info: 192.168.1.15 ESTABLISHED 6 2021-11-19T16:32:40+08:00 192.168.1.100 game [heian] info: 192.168.1.210 ESTABLISHED 1 集群的数据已经同步过来，后面直接接入elk就可以了。\nRsyslog同步集群服务器的网络连接状态 集群服务器的网络连接状态接入ELK（可视化操作） ","date":"2021-11-19T23:13:08Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/202111192313/","title":"Rsyslog同步集群服务器的网络连接状态"},{"content":"后续补充\n1 2 3 4 sed -n \u0026#39;/2021-02-13 21:00:00/,/2021-02-13 22:00:00/p\u0026#39; /usr/local/apache-tomcat-6.0.45/logs/crm.log \u0026gt; /opt/crm-`date +%Y-%m-%d-%H-%M`.log 截取某个时间段到某个时间段的日志 sed -n \u0026#39;/2021-01-18 21:10:00/,$p\u0026#39; /usr/local/apache-tomcat-6.0.45/logs/crm.log \u0026gt; /opt/crm-`date +%Y-%m-%d-%H-%M`.log 截取某个时间段到现在的 ","date":"2021-11-03T16:53:26Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/202111031653/","title":"截取日志范围（sed）"},{"content":"在工作中，我们经常会使用mysql的主从，来去报数据的安全性。\n首先目前主流的备份工作mysqldump和Xtrabackup，都可以实现数据库的备份。\n关于那个用来备份数据，来做mysql主从，毫无疑问就是 Xtrabackup了。\n首先MySQL做主从时，需要使用到binlog日志 和 pos号，关键就是这个pos。\n如果是mysqldump来备份，需要进行锁表才行，如果是Xtrabackup 完全可以在任务运行中，进行备份，完全不需要锁表，更不会影响业务。所以我们首选就是Xtrabackup\n相关原理，可以进行谷歌。\n在这个只暂时先关步骤\nMysql日志选择 （1）mysql的binlog日志格式 mysql的binlog有3种日志格式。\nmysql binlog日志有三种格式，分别为Statement,MiXED,以及ROW！\n查看binlog的格式的脚本：\n（2）binlog 的不同模式有什么区别呢？ 1.Statement：每一条会修改数据的sql都会记录在binlog中。\n优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。(相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。)\n缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以及user-defined functions(udf)会出现问题).\n2.Row:不记录sql语句上下文相关信息，仅保存哪条记录被修改。\n优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题\n缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。\n3.Mixedlevel: 是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog,MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种.新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。\n（3）Binlog基本配制与格式设定 1.基本配制\nMysql BInlog日志格式可以通过mysql的my.cnf文件的属性binlog_format指定。如以下：\nbinlog_format = MIXED //binlog日志格式\nlog_bin =目录/mysql-bin.log //binlog日志名\nexpire_logs_days = 7 //binlog过期清理时间\nmax_binlog_size 100m //binlog每个日志文件大小\n2.Binlog日志格式选择\nMysql默认是使用Statement日志格式，推荐使用MIXED.\n由于一些特殊使用，可以考虑使用ROWED，如自己通过binlog日志来同步数据的修改，这样会节省很多相关操作。对于binlog数据处理会变得非常轻松,相对mixed，解析也会很轻松(当然前提是增加的日志量所带来的IO开销在容忍的范围内即可)。 1 2 3 4 5 6 7 1.先修改从库的binlog格式 set global binlog_format=MIXED; 2.再修改主库的binlog格式 set global binlog_format=MIXED; 3.修改主库的my.cnf(避免重启失效) binlog_format=MIXED 4.重启stop slave，start slave 2、主备库均安装Xtrabackup开源工具在线热备份搭建备库 1 2 3 4 5 6 7 #下载工具包percona-xtrabackup-2.4.14-Linux-x86_64.libgcrypt183.tar.gz tar -xf percona-xtrabackup-2.4.14-Linux-x86_64.libgcrypt183.tar.gz mv percona-xtrabackup-2.4.14-Linux-x86_64 xtrabackup mv xtrabackup/ /usr/local/ echo \u0026#34;export PATH=$PATH:/usr/local/xtrabackup/bin\u0026#34; \u0026gt;\u0026gt; /etc/profile source /etc/profile 3、主库在线备份(不影响业务) 1 2 3 4 5 6 7 8 9 10 11 12 13 innobackupex --user=\u0026#39;root\u0026#39; --password=\u0026#39;xxxxxx\u0026#39; --slave-info /data1/backup/20210927 --no-timestamp --socket=/tmp/mysql.sock #以下是输出结果: xtrabackup: recognized server arguments: --server-id=1 --log_bin=binlog --datadir=/data1/mysql/data/ --tmpdir=/tmp xtrabackup: recognized client arguments: --server-id=1 --log_bin=binlog --datadir=/data1/mysql/data/ --tmpdir=/tmp 200615 16:26:27 innobackupex: Starting the backup operation IMPORTANT: Please check that the backup run completes successfully. At the end of a successful backup run innobackupex prints \u0026#34;completed OK!\u0026#34;. .......... xtrabackup: Transaction log of lsn (5055292518) to (5055292527) was copied. 200615 16:26:30 completed OK! 4、检查全备的POS 1 2 cat /data/backup/20200615/xtrabackup_binlog_info binlog.000002 184668420 5、将备份文件压缩传输到备机上 1 2 3 cd /data1/backup/ tar -zcf 20210927.tar.gz 20210927 scp 20210927.tar.gz dam@dam02:/home/dam/ 6、停止备库,将原来的备库datadir的路径重命名并创建新的数据data目录 1 2 3 4 /etc/init.d/mysqld stop cd /data1/mysql mv data data3 mkdir data 7、解压缩备份文件并恢复数据到新的data目录 1 2 3 4 5 6 7 8 9 tar -xf 20210927.tar.gz -C /data1/mysql/ #数据一致性恢复 innobackupex --apply-log /data1/mysql/20210927 #数据copy到dir目录 innobackupex --copy-back /data1/mysql/20210927 chown -R mysql.mysql /data1/mysql/data chmod -R 775 /data1/mysql/data 8、重启备机228的mysql 1 2 /etc/init.d/mysqld stop /etc/init.d/mysqld start 9、配置主从同步 1 2 3 4 5 6 7 8 9 10 11 12 #227主机创建同步用户: create USER \u0026#39;xxxx\u0026#39;@\u0026#39;172.17.8.%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;xxxxxx\u0026#39;; grant replication slave on *.* to \u0026#39;syncuser\u0026#39;@\u0026#39;172.17.8.%\u0026#39;; flush privileges; #228配置备机： reset slave all; stop slave; change master to master_host=\u0026#39;172.17.8.227\u0026#39;,master_port=3306,master_user=\u0026#39;syncuser\u0026#39;,master_password=\u0026#39;xxxxx\u0026#39;,master_log_file=\u0026#39;binlog.000002\u0026#39;,master_log_pos=184668420; start slave; show slave status\\G; show master status\\G; 10、为解决msyql主从延迟问题进行的相关调优： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #主从并行复制参数： vim my.cnf master_info_repository=TABLE #master信息放到表里 relay_log_info_repository=TABLE #relay日志信息放到表里 slave_parallel_type = LOGICAL_CLOCK #基于组提交的并行复制方式 slave_parallel_workers=8 #sql并行量 slave_preserve_commit_order=ON #relay顺序与master并行顺序保持一致 relay_log_recovery=ON #宕机时重新从master获取日志，保证relay完整性 replicate-wild-ignore-table=scheduler.tb_qa% #不同步scheduler.tb_qa开头的表 replicate-wild-ignore-table=scheduler.tb_qc% #不同步scheduler.tb_qc开头的表\tstop slave; set global master_info_repository=\u0026#39;table\u0026#39;; set global relay_log_info_repository=\u0026#39;table\u0026#39;; set global slave_parallel_type=\u0026#39;logical_clock\u0026#39;; set global slave_parallel_workers=8; set global slave_preserve_commit_order=\u0026#39;on\u0026#39;; set global relay_log_recovery=\u0026#39;on\u0026#39;; start slave; ","date":"2021-11-01T12:01:06Z","image":"https://www.ownit.top/title_pic/38.jpg","permalink":"https://www.ownit.top/p/202111011201/","title":"Xtrabackup工具进行在线主从搭建"},{"content":"近期因为业务系统等保，发现ssh有好多的漏洞，需要更新升级。\n负责的服务器有点多，不能单个手动编译，所以采用 ansible + 脚本方式批量编译安装\n相关下载软件地址：https://download.csdn.net/download/heian_99/35589407\n本次升级主要解决上次升级造成的隐患，并添加12222端口，\n隐患：\n（1）sshd无法开机自启 主机重启后，sshd无法起来，需要他们手动重启 （2）ssh的配置文件目录不对 ，没办法统一管理 （3）sshd无法开机自启，导致rc-local.service 无法自启, 再次升级基础上\n（1）添加ssh_banner_change 隐藏版本信息\n（2）编译是修改版本号，禁止telnet显示版本号\n（3）禁止root的密码登录\n端口添加\n（注：集群有些业务是需要22端口，暂时不能直接去掉22端口）\n隐藏效果\nsshup.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 #!/bin/bash # 作者：南宫乘风 # 作用：升级ssh 到 OpenSSH_8.6p1 版本，并添加12222端口 #日期和升级过程中步骤 log=\u0026#34;/opt/backup/update_log\u0026#34; datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) hostname=`hostname` echo \u0026#34;==========主机名：${hostname} 开始升级==============\u0026#34; \u0026gt;\u0026gt;${log} yum -y install openssl-devel zlib-devel gcc gcc-c++ pam-devel perl #步骤 ：备份 解压 编译 重启 #备份，主要是/etc/ssh 目录 和 先关升级的包 backup=/opt/backup/update_2021_11_01 mkdir -p ${backup} echo \u0026#34;${backup} 创建成功\u0026#34; \u0026gt;\u0026gt;${log} #备份文件 主要是移除 echo \u0026#34;=======time: ${datetime}=======开始备份文件==============\u0026#34; \u0026gt;\u0026gt;${log} mv /etc/ssh ${backup} mv /usr/sbin/sshd ${backup} mv /usr/bin/ssh ${backup}/ssh_bak mv /usr/bin/ssh-keygen ${backup} datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;${datetime} /etc/ssh 备份成功\u0026#34; \u0026gt;\u0026gt;${log} echo \u0026#34;${datetime} /usr/sbin/sshd 备份成功\u0026#34; \u0026gt;\u0026gt;${log} echo \u0026#34;${datetime} /usr/bin/ssh 备份成功\u0026#34; \u0026gt;\u0026gt;${log} echo \u0026#34;${datetime} /usr/bin/ssh-keygen 备份成功\u0026#34; \u0026gt;\u0026gt;${log} FILE=/lib/systemd/system/sshd.service if [ -f \u0026#34;$FILE\u0026#34; ]; then rm -rf /lib/systemd/system/sshd.service echo \u0026#34;sshd.service 已清除\u0026#34; \u0026gt;\u0026gt;${log} fi datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======备份程序编译文件==============\u0026#34; \u0026gt;\u0026gt;${log} mv /usr/local/src/openssh* ${backup} mv /usr/local/src/openssl* ${backup} mv /usr/local/src/zlib* ${backup} datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======开始解压文件==============\u0026#34; \u0026gt;\u0026gt;${log} #解压 cd /opt/backup/ssh_tar tar xf openssh-8.6p1.tar.gz -C /usr/local/src/ tar xf openssl-1.1.1i.tar.gz -C /usr/local/src/ tar xf zlib-1.2.11.tar.gz -C /usr/local/src/ datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======开始编译zlib==============\u0026#34; \u0026gt;\u0026gt;${log} #移除 mv /usr/local/zlib ${backup} cd /usr/local/src/zlib-1.2.11/ ./configure --prefix=/usr/local/zlib \u0026amp;\u0026amp; make -j 4 \u0026amp;\u0026amp; make install datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======开始编译openssl==============\u0026#34; \u0026gt;\u0026gt;${log} #openssl mv /usr/local/ssl ${backup} cd /usr/local/src/openssl-1.1.1i/ ./config --prefix=/usr/local/ssl -d shared make -j 4 \u0026amp;\u0026amp; make install echo \u0026#39;/usr/local/ssl/lib\u0026#39; \u0026gt;\u0026gt;/etc/ld.so.conf ldconfig -v datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======开始编译openssh==============\u0026#34; \u0026gt;\u0026gt;${log} #openssh mv /usr/local/openssh ${backup} cd /usr/local/src/openssh-8.6p1/ sed -i \u0026#39;s/OpenSSH_8.6/Prohibit_detection/g\u0026#39; /usr/local/src/openssh-8.6p1/version.h ./configure --prefix=/usr/local/openssh --sysconfdir=/etc/ssh --with-pam --with-ssl-dir=/usr/local/ssl --with-zlib=/usr/local/zlib make -j 4 \u0026amp;\u0026amp; make install echo \u0026#39;Version is empty\u0026#39;\u0026gt;\u0026gt; /etc/ssh_banner_change echo \u0026#39;Banner /etc/ssh_banner_change\u0026#39; \u0026gt;\u0026gt; /etc/ssh/sshd_config echo \u0026#39;Port 22\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config echo \u0026#39;Port 12222\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config echo \u0026#39;PermitRootLogin without-password\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config echo \u0026#39;PubkeyAuthentication yes\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config echo \u0026#39;PasswordAuthentication yes\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config echo \u0026#39;UsePAM yes\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config echo \u0026#39;KexAlgorithms curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group14-sha1\u0026#39; \u0026gt;\u0026gt;/etc/ssh/sshd_config datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======开始写入pam_sshd==============\u0026#34; \u0026gt;\u0026gt;${log} cp /etc/pam.d/sshd ${backup}/pam_sshd if [ ! -e /etc/pam.d/sshd ]; then cat \u0026gt;/etc/pam.d/sshd \u0026lt;\u0026lt;EOF #%PAM-1.0 auth required pam_sepermit.so auth substack password-auth auth include postlogin # Used with polkit to reauthorize users in remote sessions -auth optional pam_reauthorize.so prepare account required pam_nologin.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session include password-auth session include postlogin # Used with polkit to reauthorize users in remote sessions -session optional pam_reauthorize.so prepare EOF chmod 644 /etc/pam.d/sshd else cp -pf /etc/pam.d/{login,sshd} fi datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======开始覆盖sshd，ssh，ssh-keygen==============\u0026#34; \u0026gt;\u0026gt;${log} cp -rf /usr/local/openssh/sbin/sshd /usr/sbin/sshd cp -rf /usr/local/openssh/bin/ssh /usr/bin/ssh cp -rf /usr/local/openssh/bin/ssh-keygen /usr/bin/ssh-keygen chmod 755 /usr/sbin/sshd chmod 755 /usr/bin/ssh chmod 755 /usr/bin/ssh-keygen datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======设置开机自启sshd==============\u0026#34; \u0026gt;\u0026gt;${log} \\cp /usr/local/src/openssh-8.6p1/contrib/redhat/sshd.init /etc/init.d/sshd chmod u+x /etc/init.d/sshd systemctl daemon-reload /etc/init.d/sshd restart chkconfig --add sshd chkconfig sshd on chkconfig --list sshd datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======sshd升级成功==============\u0026#34; \u0026gt;\u0026gt;${log} echo -e \u0026#34;本次升级使用: $SECONDS seconds\u0026#34; datetime=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) echo \u0026#34;=======time: ${datetime}=======本次升级使用: ${SECONDS} seconds ==============\u0026#34; \u0026gt;\u0026gt;${log} 配合ansible 批量升级\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [heian@zabbix ssh]$ cat ssh-update.yml - hosts: test remote_user: heian gather_facts: false become: yes become_user: root become_method: sudo vars: - ssh_tar: /opt/backup/ssh_tar tasks: - name: 创建ssh_tar目录 file: path: \u0026#34;{{ssh_tar}}\u0026#34; state: directory mode: \u0026#39;0755\u0026#39; - name: 分发编译包 copy: src: \u0026#34;{{ item }}\u0026#34; dest: \u0026#34;{{ ssh_tar }}\u0026#34; mode: 0755 with_items: - /home/heian/ssh/openssh-8.6p1.tar.gz - /home/heian/ssh/openssl-1.1.1i.tar.gz - /home/heian/ssh/zlib-1.2.11.tar.gz - /home/heian/ssh/sshup.sh - name: 执行sshd升级脚本 shell: sh /opt/backup/ssh_tar/sshup.sh - name: 查看升级结果 shell: cat /opt/backup/update_log register: ssh_log - name: Debug number debug: msg: \u0026#34; IP : {{ inventory_hostname }} 升级结果 : {{ ssh_log.stdout }} \u0026#34; 参考文档：CentOS7.x升级openssh8.4p1详解 - chillax1314 - 博客园\n","date":"2021-10-30T14:55:04Z","image":"https://www.ownit.top/title_pic/47.jpg","permalink":"https://www.ownit.top/p/202110301455/","title":"openssh-8.6p1批量编译安装升级"},{"content":"业务要求，近期集群要使用zookeeper集群，准备在Kubernetes实现。\n方法一： 使用pv和pvc 来挂载\n这个同时创建pv和pvc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [root@k8s-master k8s-yaml]# cat zookeeper-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: zk-01 labels: name: zk-cluster type: nfs spec: nfs: path: /home/installapp/nfs/data/zk-01 server: 192.168.1.210 accessModes: - \u0026#34;ReadWriteOnce\u0026#34; capacity: storage: 1Gi persistentVolumeReclaimPolicy: Recycle --- apiVersion: v1 kind: PersistentVolume metadata: name: zk-02 labels: name: zk-cluster type: nfs spec: nfs: path: /home/installapp/nfs/data/zk-02 server: 192.168.1.210 accessModes: - \u0026#34;ReadWriteOnce\u0026#34; capacity: storage: 1Gi persistentVolumeReclaimPolicy: Recycle --- apiVersion: v1 kind: PersistentVolume metadata: name: zk-03 labels: name: zk-cluster type: nfs spec: nfs: path: /home/installapp/nfs/data/zk-03 server: 192.168.1.210 accessModes: - \u0026#34;ReadWriteOnce\u0026#34; capacity: storage: 1Gi persistentVolumeReclaimPolicy: Recycle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 [root@k8s-master k8s-yaml]# cat statefulset-zk.yaml apiVersion: v1 kind: Service metadata: name: zk-hs labels: app: zk spec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zk --- apiVersion: v1 kind: Service metadata: name: zk-cs labels: app: zk spec: ports: - port: 2181 name: client selector: app: zk --- apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: zk-pdb spec: selector: matchLabels: app: zk maxUnavailable: 1 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: zk spec: selector: matchLabels: app: zk serviceName: zk-hs replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: zk spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \u0026#34;app\u0026#34; operator: In values: - zk topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: kubernetes-zookeeper imagePullPolicy: IfNotPresent image: k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10 resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;0.5\u0026#34; ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election command: - sh - -c - \u0026#34;start-zookeeper \\ --servers=3 \\ --data_dir=/var/lib/zookeeper/data \\ --data_log_dir=/var/lib/zookeeper/data/log \\ --conf_dir=/opt/zookeeper/conf \\ --client_port=2181 \\ --election_port=3888 \\ --server_port=2888 \\ --tick_time=2000 \\ --init_limit=10 \\ --sync_limit=5 \\ --heap=512M \\ --max_client_cnxns=60 \\ --snap_retain_count=3 \\ --purge_interval=12 \\ --max_session_timeout=40000 \\ --min_session_timeout=4000 \\ --log_level=INFO\u0026#34; readinessProbe: exec: command: - sh - -c - \u0026#34;zookeeper-ready 2181\u0026#34; initialDelaySeconds: 10 timeoutSeconds: 5 livenessProbe: exec: command: - sh - -c - \u0026#34;zookeeper-ready 2181\u0026#34; initialDelaySeconds: 10 timeoutSeconds: 5 volumeMounts: - name: datadir mountPath: /var/lib/zookeeper securityContext: # runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: datadir spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi 方法二 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 apiVersion: v1 kind: Service metadata: name: zk-hs namespace: uat labels: app: zk spec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zk --- apiVersion: v1 kind: Service metadata: name: zk-cs namespace: uat labels: app: zk spec: ports: - port: 2181 name: client selector: app: zk --- apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: zk-pdb namespace: uat spec: selector: matchLabels: app: zk maxUnavailable: 1 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: zk namespace: uat spec: selector: matchLabels: app: zk serviceName: zk-hs replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: zk spec: nodeSelector: env: uat containers: - name: kubernetes-zookeeper imagePullPolicy: IfNotPresent image: k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10 resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;0.5\u0026#34; ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election command: - sh - -c - \u0026#34;start-zookeeper \\ --servers=3 \\ --data_dir=/var/lib/zookeeper/data/${HOSTNAME} \\ --data_log_dir=/var/lib/zookeeper/data/${HOSTNAME}/log \\ --conf_dir=/opt/zookeeper/conf \\ --client_port=2181 \\ --election_port=3888 \\ --server_port=2888 \\ --tick_time=2000 \\ --init_limit=10 \\ --sync_limit=5 \\ --heap=512M \\ --max_client_cnxns=60 \\ --snap_retain_count=3 \\ --purge_interval=12 \\ --max_session_timeout=40000 \\ --min_session_timeout=4000 \\ --log_level=INFO\u0026#34; readinessProbe: exec: command: - sh - -c - \u0026#34;zookeeper-ready 2181\u0026#34; initialDelaySeconds: 10 timeoutSeconds: 5 livenessProbe: exec: command: - sh - -c - \u0026#34;zookeeper-ready 2181\u0026#34; initialDelaySeconds: 10 timeoutSeconds: 5 volumeMounts: - name: datadir mountPath: /var/lib/zookeeper - name: host-time mountPath: /etc/localtime # securityContext: # runAsUser: 0 # fsGroup: 0 volumes: - name: datadir hostPath: path: /store/logs/uat/zk type: DirectoryOrCreate - name: host-time hostPath: path: /etc/localtime ","date":"2021-10-14T18:07:59Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202110141807/","title":"Kubernetes安装zookeeper集群"},{"content":"背景环境：\n开发频繁查看日志，登录服务器导出日志比较耗费时间，搭建一款轻量又简单的日志查询工具供开\n发查询容器日志。\n方案选型：\n（1）ELK（EFK）适用于日志体系比较大的场景，且比较消耗服务器资源。\n（2）Loki\u0026mdash;-\u0026gt;轻量级（特点：简洁明了，安装部署简单）\u0026mdash;\u0026ndash;\u0026gt;适用于当前环境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 #1.安装helm 首先安装helm管理工具（官网）：https://helm.sh/docs/intro/install/ [root@k8s-node01 ~]# wget https://get.helm.sh/helm-v3.3.3-linux-amd64.tar.gz [root@k8s-node01 ~]# tar xf helm-v3.3.3-linux-amd64.tar.gz [root@k8s-node01 ~]# mv linux-amd64/helm /usr/local/bin/helm [root@k8s-node01 ~]# helm version version.BuildInfo{Version:\u0026#34;v3.3.3\u0026#34;, GitCommit:\u0026#34;55e3ca022e40fe200fbc855938995f40b2a68ce0\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.14.9\u0026#34;} [root@k8s-node01 ~]# mkdir loki [root@k8s-node01 ~]# ls anaconda-ks.cfg helm-v3.3.3-linux-amd64.tar.gz linux-amd64 loki thirdservice [root@k8s-node01 ~]# cd loki/ [root@k8s-node01 loki]# helm repo add loki https://grafana.github.io/loki/charts \u0026amp;\u0026amp; helm repo update [root@k8s-node01 loki]# helm pull loki/loki-stack [root@k8s-node01 loki]# tar xf loki-stack-2.1.2.tgz [root@k8s-node01 loki]# helm install loki -n loki loki-stack/ #创建grafana文件 [root@k8s-node01 loki]# cat grangfan.yaml apiVersion: apps/v1 kind: Deployment metadata: name: grafana labels: app: grafana spec: replicas: 1 selector: matchLabels: app: grafana template: metadata: labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:latest volumeMounts: - name: timezone mountPath: /etc/localtime volumes: - name: timezone hostPath: path: /usr/share/zoneinfo/Asia/Shanghai --- apiVersion: v1 kind: Service metadata: name: grafana-svc #namespace: test spec: ports: - port: 3000 targetPort: 3000 nodePort: 3303 type: NodePort selector: app: grafana 浏览器访问 IP:3003 账号密码：admin admin\n数据源配置\nloki:3100 容器日志查询 依次点击实时查看容器日志 效果图\n","date":"2021-10-12T17:58:22Z","image":"https://www.ownit.top/title_pic/40.jpg","permalink":"https://www.ownit.top/p/202110121758/","title":"Kubernetes收集pod的日志"},{"content":"**！！！中文社区中说的config.json的方式已经打算被废弃了，所以现在最好通过minio客户端的方式进行处理了**\n· 在目标mysql创建数据库miniodb（要用默认字符集，不然会报错）\n· 在minio client的可执行目录下执行如下命令，查看是否已经存在配置\n1 mc --insecure admin config get minio notify_mysql 添加Mysql通知配置\n1 mc admin config set minio notify_mysql:mysql table=\u0026#34;minio_log\u0026#34; dsn_string=\u0026#34;minio:minio@tcp(192.168.1.200:3306)/minio\u0026#34; 步骤：先创建minio的数据库，编码默认一下，在执行这条命令，但是会报错。\n发现报错，错误如下：\n1 Error: Error 1071: Specified key was too long; max key length is 3072 bytes 解决办法：手动创建记录表就可以了\n1 2 3 4 CREATE TABLE `minio_log` ( `key_name` varchar(1000) DEFAULT NULL, `value` longtext ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 关联event事件\n1 2 3 4 mc event add minio/k8s/ arn:minio:sqs::mysql:mysql 查看 mc event list minio/k8s/ 重启server服务，直接命令重启\n1 mc admin service restart minio/k8s/ ","date":"2021-09-22T15:59:58Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/202109221559/","title":"Minio--Mysql存储桶通知配置"},{"content":"集群内部构建一套Kubernetes集群，纯内网没有公网环境，无法拉取镜像，不是很方便\n前期做法，本地拉去镜像，保存，上传内网，加载。这个是针对每台机器的步骤。\n步骤比较繁琐，和机械化。\n所以准备在内部构建harbor，走域名绑定。docker加载配置，后期可以正常使用。\n1、环境 Docker version 20.10.7\ndocker-compose version 1.29.2\nharbor-offline-installer-v2.3.1.tgz\n内网安装，我选择离线版本的\nRelease v2.3.2 · goharbor/harbor · GitHub\n2、生成https证书 docker 和 docker-compose 已经安装好下进行\n1 2 3 4 5 6 7 8 9 10 11 cd /opt/cert #输入密码 openssl genrsa -des3 -out server.pass.key 2048 #去除密码 openssl rsa -in server.pass.key -out server.key #生成域名证书 openssl req -new -key server.key -out server.csr -subj \u0026#34;/C=CN/ST=Jiangsu/L=Nanjing/O=Company/OU=group/CN=hub.docker.com\u0026#34; #生成时间多久 openssl x509 -req -days 3650 -in server.csr -signkey server.key -out server.crt #生成pem的（可以跳过） openssl x509 -in server.crt -out server.pem -outform PEM 3、安装harbor 解压文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 #复制新的配置 [root@hdpv3test08 harbor]# cat harbor.yml # Configuration file of Harbor # The IP address or hostname to access admin UI and registry service. # DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients. hostname: hub.docker.com # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /opt/cert/server.crt private_key: /opt/cert/server.key # # Uncomment following will enable tls communication between all harbor components # internal_tls: # # set enabled to true means internal tls is enabled # enabled: true # # put your cert and key files on dir # dir: /etc/harbor/tls/internal # Uncomment external_url if you want to enable external proxy # And when it enabled the hostname will no longer used # external_url: https://reg.mydomain.com:8433 # The initial password of Harbor admin # It only works in first time to install harbor # Remember Change the admin password from UI after launching Harbor. harbor_admin_password: Harbor12345 # Harbor DB configuration database: # The password for the root user of Harbor DB. Change this before any production use. password: root123 # The maximum number of connections in the idle connection pool. If it \u0026lt;=0, no idle connections are retained. max_idle_conns: 100 # The maximum number of open connections to the database. If it \u0026lt;= 0, then there is no limit on the number of open connections. # Note: the default number of connections is 1024 for postgres of harbor. max_open_conns: 900 # The default data volume data_volume: /data/service/harbor # Harbor Storage settings by default is using /data dir on local filesystem # Uncomment storage_service setting If you want to using external storage # storage_service: # # ca_bundle is the path to the custom root ca certificate, which will be injected into the truststore # # of registry\u0026#39;s and chart repository\u0026#39;s containers. This is usually needed when the user hosts a internal storage with self signed certificate. # ca_bundle: # # storage backend, default is filesystem, options include filesystem, azure, gcs, s3, swift and oss # # for more info about this configuration please refer https://docs.docker.com/registry/configuration/ # filesystem: # maxthreads: 100 # # set disable to true when you want to disable registry redirect # redirect: # disabled: false # Trivy configuration # # Trivy DB contains vulnerability information from NVD, Red Hat, and many other upstream vulnerability databases. # It is downloaded by Trivy from the GitHub release page https://github.com/aquasecurity/trivy-db/releases and cached # in the local file system. In addition, the database contains the update timestamp so Trivy can detect whether it # should download a newer version from the Internet or use the cached one. Currently, the database is updated every # 12 hours and published as a new release to GitHub. trivy: # ignoreUnfixed The flag to display only fixed vulnerabilities ignore_unfixed: false # skipUpdate The flag to enable or disable Trivy DB downloads from GitHub # # You might want to enable this flag in test or CI/CD environments to avoid GitHub rate limiting issues. # If the flag is enabled you have to download the `trivy-offline.tar.gz` archive manually, extract `trivy.db` and # `metadata.json` files and mount them in the `/home/scanner/.cache/trivy/db` path. skip_update: false # # insecure The flag to skip verifying registry certificate insecure: false # github_token The GitHub access token to download Trivy DB # # Anonymous downloads from GitHub are subject to the limit of 60 requests per hour. Normally such rate limit is enough # for production operations. If, for any reason, it\u0026#39;s not enough, you could increase the rate limit to 5000 # requests per hour by specifying the GitHub access token. For more details on GitHub rate limiting please consult # https://developer.github.com/v3/#rate-limiting # # You can create a GitHub token by following the instructions in # https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line # # github_token: xxx jobservice: # Maximum number of job workers in job service max_job_workers: 10 notification: # Maximum retry count for webhook job webhook_job_max_retry: 10 chart: # Change the value of absolute_url to enabled can enable absolute url in chart absolute_url: disabled # Log configurations log: # options are debug, info, warning, error, fatal level: info # configs for logs in local storage local: # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated. rotate_count: 50 # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes. # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G # are all valid. rotate_size: 200M # The directory on your host that store log location: /var/log/harbor # Uncomment following lines to enable external syslog endpoint. # external_endpoint: # # protocol used to transmit log to external endpoint, options is tcp or udp # protocol: tcp # # The host of external endpoint # host: localhost # # Port of external endpoint # port: 5140 #This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY! _version: 2.3.0 # Uncomment external_database if using external database. # external_database: # harbor: # host: harbor_db_host # port: harbor_db_port # db_name: harbor_db_name # username: harbor_db_username # password: harbor_db_password # ssl_mode: disable # max_idle_conns: 2 # max_open_conns: 0 # notary_signer: # host: notary_signer_db_host # port: notary_signer_db_port # db_name: notary_signer_db_name # username: notary_signer_db_username # password: notary_signer_db_password # ssl_mode: disable # notary_server: # host: notary_server_db_host # port: notary_server_db_port # db_name: notary_server_db_name # username: notary_server_db_username # password: notary_server_db_password # ssl_mode: disable # Uncomment external_redis if using external Redis server # external_redis: # # support redis, redis+sentinel # # host for redis: \u0026lt;host_redis\u0026gt;:\u0026lt;port_redis\u0026gt; # # host for redis+sentinel: # # \u0026lt;host_sentinel1\u0026gt;:\u0026lt;port_sentinel1\u0026gt;,\u0026lt;host_sentinel2\u0026gt;:\u0026lt;port_sentinel2\u0026gt;,\u0026lt;host_sentinel3\u0026gt;:\u0026lt;port_sentinel3\u0026gt; # host: redis:6379 # password: # # sentinel_master_set must be set to support redis+sentinel # #sentinel_master_set: # # db_index 0 is for core, it\u0026#39;s unchangeable # registry_db_index: 1 # jobservice_db_index: 2 # chartmuseum_db_index: 3 # trivy_db_index: 5 # idle_timeout_seconds: 30 # Uncomment uaa for trusting the certificate of uaa instance that is hosted via self-signed cert. # uaa: # ca_file: /path/to/ca # Global proxy # Config http proxy for components, e.g. http://my.proxy.com:3128 # Components doesn\u0026#39;t need to connect to each others via http proxy. # Remove component from `components` array if want disable proxy # for it. If you want use proxy for replication, MUST enable proxy # for core and jobservice, and set `http_proxy` and `https_proxy`. # Add domain to the `no_proxy` field, when you want disable proxy # for some special registry. proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy # metric: # enabled: false # port: 9090 # path: /metrics 安装\n1 2 ./prepare ./install.sh --with-notary --with-trivy --with-chartmuseum 安装完就成功\n4、配置本地域名 由于是内网，我们需要写入hosts文件，配置本地域名\n1 2 3 4 [root@hdpv3test08 harbor]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.17.9.47 hub.docker.com 5、配置docker 给dock添加仓库，我们自建的，需要添加到配置里面\n1 2 3 4 5 [root@hdpv3test08 harbor]# cat /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;:[\u0026#34;hub.docker.com\u0026#34;] } [ 填加完需要重启，但是由于已经容器，重启会照成影响，我们可以热加载\n热加载\n1 2 3 4 [root@hdpv3test08 harbor]# ps -ef | grep docker root 81995 1 0 9月15 ? 00:04:03 /usr/bin/dockerd [root@hdpv3test08 harbor]# kill -HUP 81995 这个就是热加载 6、测试 1 docker login hub.docker.com -u admin -p Harbor12345 ","date":"2021-09-16T14:18:55Z","image":"https://www.ownit.top/title_pic/66.jpg","permalink":"https://www.ownit.top/p/202109161418/","title":"企业级-纯内网构建Harbor （HTTPS认证）"},{"content":"错误 今天不知道怎么回事，一台机器的calico-node报错，也就是无法初始化正常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 45s default-scheduler Successfully assigned kube-system/calico-node-pkbkv to k8s-node2 Normal Started 45s kubelet Started container install-cni Normal Pulled 45s kubelet Container image \u0026#34;docker.io/calico/cni:v3.20.0\u0026#34; already present on machine Normal Started 45s kubelet Started container upgrade-ipam Normal Pulled 45s kubelet Container image \u0026#34;docker.io/calico/cni:v3.20.0\u0026#34; already present on machine Normal Created 45s kubelet Created container install-cni Normal Created 45s kubelet Created container upgrade-ipam Normal Started 44s kubelet Started container flexvol-driver Normal Pulled 44s kubelet Container image \u0026#34;docker.io/calico/pod2daemon-flexvol:v3.20.0\u0026#34; already present on machine Normal Created 44s kubelet Created container flexvol-driver Normal Pulled 43s kubelet Container image \u0026#34;docker.io/calico/node:v3.20.0\u0026#34; already present on machine Normal Created 43s kubelet Created container calico-node Normal Started 43s kubelet Started container calico-node Warning Unhealthy 40s kubelet Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused Warning Unhealthy 30s kubelet Readiness probe failed: 2021-09-15 02:36:49.282 [INFO][417] confd/health.go 180: Number of node(s) with BGP peering established = 0 calico/node is not ready: BIRD is not ready: BGP not established with 172.17.6.120,172.17.6.121,172.17.6.122 Warning Unhealthy 20s kubelet Readiness probe failed: 2021-09-15 02:36:59.282 [INFO][497] confd/health.go 180: Number of node(s) with BGP peering established = 0 calico/node is not ready: BIRD is not ready: BGP not established with 172.17.6.120,172.17.6.121,172.17.6.122 Warning Unhealthy 10s kubelet Readiness probe failed: 2021-09-15 02:37:09.280 [INFO][567] confd/health.go 180: Number of node(s) with BGP peering established = 0 calico/node is not ready: BIRD is not ready: BGP not established with 172.17.6.120,172.17.6.121,172.17.6.122 解决办法 1 2 3 4 5 Remove interfaces related to docker and flannel: ip link For each interface for docker or flannel, do the following ifconfig \u0026lt;name of interface from ip link\u0026gt; down ip link delete \u0026lt;name of interface from ip link\u0026gt; 移除这台主机多余的docker网卡和calico\n然后从重新删除这个错误pod的，就会恢复正常\n","date":"2021-09-15T10:56:40Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202109151056/","title":"calico-node 报错calico/node is not ready- BIRD is not ready- BGP not established with"},{"content":"prometheus报警规则，是由promsql语句编写组合的，但是有时语句会很长，我们看还好，但是有时间业务组那边也会使用promsql来看主机偏高的指标，这边只能设置别名，方便他们使用。\n别名设置：\n很简单，也是和报警规则一样，但是语法可能不一样\n示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@hdpv3test08 rules]# cat prometheus_rules_name.yml groups: - name: alive rules: - record: node:ping:total expr: up - name: cpu rules: - record: node:cpu_usage:ratio #别的文件使用，直接使用这个 expr: ((100 - (avg by(instance,ip,hostname) (irate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100))) - name: mem rules: - record: node:memory_usage:ratio expr: (100 -(node_memory_MemTotal_bytes -node_memory_MemFree_bytes+node_memory_Buffers_bytes+node_memory_Cached_bytes ) / node_memory_MemTotal_bytes * 100 ) node:cpu_usage:ratio 就是查看cpu使用率的指标\n下面两张图就是区别\n我们正常使用，就是直接采用这个别名指标了\n业务组使用\nprometheus支持promsql语法，我们可以通过相关语句，很快定位到集群，资源使用情况\n如：高CPU 高内存，出入流量大， tcp连接数多等等一些列问题。\n主机重启\ndelta(node_boot_time_seconds[5m]) != 0\n文件只读异常\nnode_filesystem_readonly == 1\nCPU****使用率\n((100 - (avg by(instance,ip,hostname) (irate(node_cpu_seconds_total{mode=\u0026ldquo;idle\u0026rdquo;}[5m])) * 100)))\n内存使用率\n(100 -(node_memory_MemTotal_bytes -node_memory_MemFree_bytes+node_memory_Buffers_bytes+node_memory_Cached_bytes ) / node_memory_MemTotal_bytes * 100 )\nIO****性能\n100-(avg(irate(node_disk_io_time_seconds_total[5m])) by(instance,hostname)* 100) \u0026lt; 40\n磁盘使用率\n100-(node_filesystem_free_bytes{fstype=~\u0026ldquo;ext4|xfs\u0026rdquo;}/node_filesystem_size_bytes {fstype=~\u0026ldquo;ext4|xfs\u0026rdquo;}*100) \u0026gt; 80\n主机网络IO速率\n入速率(MiB/s)\nirate(node_network_receive_bytes_total{}[5m]) / 1024 / 1024\n出速率(MiB/s)\nirate(node_network_transmit_bytes_total{}[5m]) / 1024 / 1024\n主机磁盘IO\n写速率(MiB/s)\nirate(node_disk_written_bytes_total{}[5m]) / 1024 / 1024\n读速率(MiB/s)\nirate(node_disk_read_bytes_total{}[5m]) / 1024 / 1024\nTCP****连接数\nnode_netstat_Tcp_CurrEstab\n","date":"2021-09-12T13:02:43Z","image":"https://www.ownit.top/title_pic/04.jpg","permalink":"https://www.ownit.top/p/202109121302/","title":"Prometheus报警规则别名设置"},{"content":"以下是生产环境中prometheus.rules.yml告警规则用例\nprometheus默认的instance是ip:port格式的，无法知道主机名。\n方法一：node_uname_info获取 参考链接：https://blog.csdn.net/CHEndorid/article/details/106820612\n主机名（nodename）在指标node_uname_info中，且node_uname_info的值恰巧为1，所以我们可以在PromQL中通过node_uname_info提取，只需要在原有PromQL后添加\n1 * on(instance) group_left(nodename) (node_uname_info) 这样，在prometheus告警的labels中，就可以通过nodename获取主机名了\n特别的，up==0的值是0，做乘法是不会得到结果的\n示例：\n1 2 3 4 5 6 7 8 - alert: cpu使用率过高告警 expr: (100 - (avg(irate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) by(instance)* 100))* on(instance) group_left(nodename) (node_uname_info) \u0026gt; 85 for: 5m labels: region: 成都 annotations: summary: \u0026#34;{{$labels.instance}}（{{$labels.nodename}}）CPU使用率过高！\u0026#34; description: \u0026#39;服务器{{$labels.instance}}（{{$labels.nodename}}）CPU使用率超过85%(目前使用:{{printf \u0026#34;%.2f\u0026#34; $value}}%)\u0026#39; 方法二：relabel_configs增加主机名标签 此方法，可以适应所有报警的规则\n在 prometheus增加主机配置是，添加配置即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - job_name: \u0026#39;hadoop-test-exporter\u0026#39; consul_sd_configs: - server: \u0026#39;localhost:8500\u0026#39; services: [hadoop-test-exporter] relabel_configs: #把__meta_consul_service_id 映射主机名 - source_labels: [__meta_consul_service_id] separator: ; regex: (.*) target_label: hostname replacement: $1 action: replace - source_labels: [__meta_consul_service_address] #映射主机IP separator: ; regex: (.*) target_label: ip replacement: $1 action: replace - source_labels: [\u0026#39;__meta_consul_tags\u0026#39;] #根据tag来匹配分组 regex: \u0026#39;^.*,hadoop-test,.*$\u0026#39; action: keep ","date":"2021-09-12T12:51:59Z","image":"https://www.ownit.top/title_pic/04.jpg","permalink":"https://www.ownit.top/p/202109121251/","title":"Prometheus监控，生产可用告警规则（可获取主机名）"},{"content":"前文链接：https://blog.csdn.net/heian_99/article/details/119874180\n需求：prometheus监控多台主机时，基于自动发现consul模块，主机安装采集器注册到那台consul，consul识别到。promethues获取consul地址上的监控主机列表，实现多台主机自动发现。\n思路：\nweb机器安装node_exporter采集器\n2.注册node_exporter为系统服务\n3.使用curl -x PUT \u0026hellip; 注册到consul机器中\n4.consul监控到后，prometheus配置consul地址。\n1-3步都可用ansible批量完成。 node_exoporter软件包 ，system服务配置文件，注册脚本，注册接口\n如果有密码验证还需要一个config.yml\nconsul-register.sh 注册脚本(传参方式注册)：\n1 2 3 4 5 6 7 #!/bin/bash service_name=$1 instance_id=$2 ip=$3 port=$4 curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$instance_id\u0026#34;\u0026#39;\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$service_name\u0026#34;\u0026#39;\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$ip\u0026#34;\u0026#39;\u0026#34;,\u0026#34;port\u0026#34;: \u0026#39;\u0026#34;$port\u0026#34;\u0026#39;,\u0026#34;tags\u0026#34;: [\u0026#34;\u0026#39;\u0026#34;$service_name\u0026#34;\u0026#39;\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://\u0026#39;\u0026#34;$ip\u0026#34;\u0026#39;:\u0026#39;\u0026#34;$port\u0026#34;\u0026#39;\u0026#34;,\u0026#34;interval\u0026#34;: \u0026#34;5s\u0026#34;}]}\u0026#39; http://10.10.10.27:8500/v1/agent/service/register node_exporter.service node_exporter系统服务配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [heian@zabbix ~]$ cat node_exporter.service [Unit] Description=node_exporter After=network.target [Service] ExecStart=/usr/local/node_exporter/node_exporter\\ --web.listen-address=:9100\\ --collector.systemd\\ --collector.systemd.unit-whitelist=(sshd|nginx).service\\ --collector.processes\\ --collector.tcpstat\\ --collector.supervisord [Install] WantedBy=multi-user.target --collector.systemd --collector.systemd.unit-include=(docker|portal|sshd).service 配置的意思是只收集docker,portal,sshd服务的数据 --web.config=/usr/local/jiankong/node_exporter/config.yml 如果有密码验证接口需要指定这个config.yml，里面保存的用户名和密码。需要把这条配在启动execstart行的node_proter后。 ansible-playbook 1 2 3 4 cat hosts [blog] 10.10.10.15 name=nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 cat playbook.yml - hosts: blog remote_user: heian become: yes become_user: root become_method: sudo gather_facts: no tasks: - name: 推送采集器安装包 unarchive: src=node_exporter-1.2.2.linux-amd64.tar.gz dest=/usr/local/ - name: 重命名 shell: | cd /usr/local/ if [ ! -d node_exporter ];then mv node_exporter-1.2.2.linux-amd64 node_exporter fi - name: 推送system文件 copy: src=node_exporter.service dest=/usr/lib/systemd/system - name: 启动服务 systemd: name=node_exporter state=started enabled=yes - name: 推送注册脚本 copy: src=consul-register.sh dest=/usr/local/node_exporter - name: 注册当前节点 shell: /bin/sh /usr/local/node_exporter/consul-register.sh {{ group_names[0] }} {{ name }} {{ inventory_hostname }} 9100 {{ group_names[0] }} \u0026ndash;ansible内置变量 代表hosts中自定义组名，数组形式，[0]取第一个\n{{ name }} \u0026ndash; hosts文件中定义主机的名字，如name=web1\n{{ inventory_hostname }} 当前执行主机的ip\n执行：\n看一下consul中，服务已经注册进来了 prometheus自动发现 参考文档地址：https://www.jianshu.com/p/f243a3aec18e\n","date":"2021-08-23T19:25:59Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202108231925/","title":"Ansible批量部署客户端并注册consul自动发现"},{"content":" 一、 简介 prometheus配置文件 prometheus.yml 里配置需要监听的服务时，是按服务名写死的，如果后面增加了节点或者组件信息，就得手动修改此配置，并重启 promethues；那么能否动态的监听微服务呢？Prometheus 提供了多种动态服务发现的功能，这里以 consul 为例。\n二、引入 consul 的好处 在没有使用 consul 服务自动发现的时候，我们需要频繁对 Prometheus 配置文件进行修改，无疑给运维人员带来很大的负担。引入consul之后，只需要在consul中维护监控组件配置，prometheus就能够动态发现配置了。\n三、Prometheus 支持的多种服务发现机制 1 2 3 4 5 6 7 8 9 #Prometheus数据源的配置主要分为静态配置和动态发现, 常用的为以下几类: 1）static_configs: #静态服务发现 2）file_sd_configs: #文件服务发现 3）dns_sd_configs: DNS #服务发现 4）kubernetes_sd_configs: #Kubernetes 服务发现 5）consul_sd_configs: Consul #服务发现 ... #在监控kubernetes的应用场景中，频繁更新的pod，svc，等等资源配置应该是最能体现Prometheus监控目标自动发现服务的好处 四、安装单节点consul 下载consul文件\nhttps://www.consul.io/downloads\n1 2 3 4 5 6 7 8 9 10 11 解压 移动 给权限 查看版 consul --version Consul v1.10.1 Revision db839f18b Protocol 2 spoken by default, understands 2 to 3 (agent will automatically use protocol \u0026gt;2 when speaking to compatible agents) 数据持久化\n1 mkdir -p /app/consul/data 启动命令\n1 2 启动命令 nohup consul agent -server -data-dir=/app/consul/data -node=agent-one -bind=172.17.9.47 -bootstrap-expect=1 -client=0.0.0.0 -ui \u0026gt; /app/consul/consul.log 2\u0026gt;\u0026amp;1 \u0026amp; 简单单机版已经完成。\n复杂点的有集群版，账号口令认证等等，相关参考文档\nhttps://www.k8stech.net/post/prom-discovery-consul/\n大佬版本讲解真的是好\n五、注册主机到consul 由于机器比较多，需要批量添加\nconsul主要是添加和删除命令，都是使用接口调用\n1 2 3 4 5 6 删除 curl -X PUT http://172.17.9.47:8500/v1/agent/service/deregister/dam02 添加 curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;\u0026#39;${host_name}\u0026#39;\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;node-exporter\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;\u0026#39;${host_addr}\u0026#39;\u0026#34;,\u0026#34;port\u0026#34;:9100,\u0026#34;tags\u0026#34;: [\u0026#34;dam\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://\u0026#39;${host_addr}\u0026#39;:9100/\u0026#34;,\u0026#34;interval\u0026#34;: \u0026#34;5s\u0026#34;}]}\u0026#39; http://172.17.9.47:8500/v1/agent/service/register 批量添加可以使用下面脚本\n1 2 3 4 hosts文件格式 dam01 172.17.8.227 dam02 172.17.8.228 1 2 3 4 5 6 $ cat registry.sh # 脚本内容如下 #!/bin/bash while read host_name host_addr do curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;\u0026#39;${host_name}\u0026#39;\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;node-exporter\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;\u0026#39;${host_addr}\u0026#39;\u0026#34;,\u0026#34;port\u0026#34;:9100,\u0026#34;tags\u0026#34;: [\u0026#34;dam\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://\u0026#39;${host_addr}\u0026#39;:9100/\u0026#34;,\u0026#34;interval\u0026#34;: \u0026#34;15s\u0026#34;}]}\u0026#39; http://172.17.9.47:8500/v1/agent/service/register done \u0026lt; hosts 执行这个脚本，就可以批量添加主机到consul里面去。\n六、配置prometheus自动发现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat /usr/local/prometheus/prometheus.yml #类似下面格式 这个server为consul的地址，根据标签匹配组 - job_name: \u0026#39;dam-exporter\u0026#39; consul_sd_configs: - server: \u0026#39;localhost:8500\u0026#39; services: [dam-exporter] #复杂一点，需要正则表达式 - job_name: dam-exporter honor_labels: true metrics_path: /metrics scheme: http consul_sd_configs: - server: 172.17.9.47:8500 services: [dam-exporter] relabel_configs: - source_labels: [\u0026#39;__meta_consul_tags\u0026#39;] target_label: \u0026#39;product\u0026#39; - source_labels: [\u0026#39;__meta_consul_dc\u0026#39;] target_label: \u0026#39;idc\u0026#39; - source_labels: [\u0026#39;product\u0026#39;] regex: \u0026#34;,dam-exporter,\u0026#34; action: keep 重启prometheus就自动发现成功，并且可以采集数据。\n","date":"2021-08-23T17:49:43Z","image":"https://www.ownit.top/title_pic/73.jpg","permalink":"https://www.ownit.top/p/202108231749/","title":"Prometheus基于consul中心自动发现注册监控"},{"content":"前提： 近期业务做了集群的流量汇总，整体没有问题。后面慢慢优化一些参数项。但是这两天发现，集群流量数据增大，业务正常。\n问题： zabbix和prometheus 监控网卡，流量异常增大，超出限制，每次2分钟，偶尔性触发\n看图，这个和7月对比，简介翻了几倍，但是业务没有增长，这就很奇怪了\n上面介绍相关截图。\n解决： 首先以为业务导致网卡过大加载，导致流量增大，我们使用 ifstat-1.1.tar.gz 工具记录每一秒的网卡速度，记录一晚上再看。\n分析上图，虽然流量有超过100M的，但是网卡是能够支撑的。没有zabbix和prometheus显示的那么恐怖。\n思考 我和大佬分交流一下。说是zabbix的单位转换，要加8倍，我也是添加了的\n嗯。。。。。。。。。。。。。。。。。。。。。正常\n大佬建议让我使用snmp监控对比一下，思路不错，可以搞。直接部署上去对比了\n分析 snmp对比一下\nzabbix的（还是这么高）\nsnmp的（这个是正常的）\n很明显，这是snmp是准确的。但是为什么会这样\n结果： 首先，业务正常，服务器正常，现在就是zabbix不正常，怀疑是zabbix的问题\n回想一下，在数据量增加前做了什么操作。\n之前，我当时增加一批监控指标，因为监控点比较重要，所有设置抓取时间为10s。谁知道这个10s就是罪魁祸首。集群内部机器较多，可能会产生数据积压。\n后续 取消关联模板，监控整体流量，确定无虚假流量\n调整监控项的抓取指标为：1m 再次关联模板正常\n","date":"2021-08-20T16:01:39Z","image":"https://www.ownit.top/title_pic/53.jpg","permalink":"https://www.ownit.top/p/202108201601/","title":"Zabbix监控流量异常（偶尔超出交换机限制）"},{"content":"问题需求 公司有台业务服务器，上面有多个用户，但是这台机器无法使用scp ，sftp ，和ftp等传输工具（因为安全问题，不能对外公开传输数据渠道）\n但是，这些功能禁用后，怎么往上面传输文件，偶尔有些用户需要往上面传程序，这个就是很头疼的问题。\n解决方案： 采用vsftp工具，构建本机用户登录，使用白名单，对特定用户使用，运行上传，不能下载\n实现方法： 环境是纯内网：首先找vsftp安装包\n1 2 3 4 yum install yum-utils -y #下载rpm包的工具 yumdownloader vsftpd --resolve --destdir=/root/rpm #使用这个下载vsftp包和依赖 下载完成，上传服务器直接安装\n1 rpm -ivh vsftpd-3.0.2-29.el7_9.x86_64.rpm 1 2 3 4 主配置文件：/etc/vsftpd/vsftpd.conf 配置文件目录：/etc/vsftpd/*.conf 服务启动脚本：/etc/rc.d/init.d/vsftpd 用户认证配置文件：/etc/pam.d/vsftpd 配置参数\n常用配置参数都为主配置文件，/etc/vsftpd/vsftpd.conf 的常用配置。\n通用基础配置\n1 2 3 4 5 6 7 8 9 listen=[YES|NO] #是否以独立运行的方式监听服务 listen_address=IP地址 #设置要监听的 IP 地址 listen_port=21 #设置 FTP 服务的监听端口 download_enable＝[YES|NO] #是否允许下载文件 max_clients=0 #最大客户端连接数，0 为不限制 max_per_ip=0 #同一 IP 地址的最大连接数，0 为不限制 chown_uploads=[YES|NO] #是否允许改变上传文件的属主 chown_username=whoever #改变上传文件的属主为 whoever pam_service_name=vsftpd #让 vsftpd 使用 pam 完成用户认证，使用的文件为/etc/pam.d/vsftpd 系统用户的配置\n1 2 3 4 5 6 7 8 9 10 11 anonymous_enable=NO #禁止匿名访问模式 local_enable=[YES|NO] #是否允许本地用户登录 FTP write_enable=[YES|NO] #是否开放本地用户的其他写入权限 local_umask=022 #本地用户上传文件的 umask 值 local_root=/var/ftp #本地用户的 FTP 根目录 local_max_rate=0 #本地用户最大传输速率（字节/秒），0 为不限制 userlist_enable=[YES|NO] #开启用户作用名单文件功能 userlist_deny=[YES|NO] #启用禁止用户名单，名单文件为 ftpusers 和/etc/vsftpd/user_list chroot_local_user=[YES|NO] #是否将用户权限禁锢在 FTP 家目录中，以确保安全 chroot_list_enable=[YES|NO] #禁锢文件中指定的 FTP 本地用户于其家目录中 chroot_list_file=/etc/vsftpd/chroot_list #指定禁锢文件位置，需要和 chroot_list_enable 一同 vsftp配置 我这边为了实现对应功能，配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 anonymous_enable=NO #禁止匿名用户登录 local_enable=YES #本地用户 write_enable=YES download_enable=NO local_umask=022 chroot_list_enable=YES chroot_local_user=yes #固定传输文件在家目录 allow_writeable_chroot=YES userlist_enable=YES userlist_deny=NO userlist_file=/etc/vsftpd/user_list #登录白名单 dirmessage_enable=YES connect_from_port_20=YES listen=NO listen_ipv6=YES pam_service_name=vsftpd userlist_enable=YES tcp_wrappers=YES xferlog_enable=YES xferlog_std_format=YES /etc/vsftpd/user_list 这个文件里面写入允许登录的用户即可\n（1）允许特定用户登录\n（2）允许上传，禁止下载\n（3）禁止跳转目录，只允许自己家目录\n参考地址：\nhttps://zhuanlan.zhihu.com/p/354583347\nhttps://blog.csdn.net/weixin_30514745/article/details/98155033\nhttps://www.huaweicloud.com/articles/2cc1b07fa2841e3874e7b4f430b038af.html\n","date":"2021-08-10T21:58:25Z","image":"https://www.ownit.top/title_pic/16.jpg","permalink":"https://www.ownit.top/p/202108102158/","title":"vsftp禁止下载，允许上传文件"},{"content":"目录\n企业纯内网二进制完美部署Docker（20.10.7版本）\nDocker下载\n上传解压\nsystemd管理docker\n普通用户管理Docker\n自定义网段\n更改存储路径\n如果有数据\n无数据\n启动并设置开机启动\ndocker命令补全方法\n1.复制文件\n2.安装bash-completion 3.刷新生效\n4、测试\n企业纯内网二进制完美部署Docker（20.10.7版本） 近期由于公司业务需求，需要使用到Docker。\n平常有网环境，直接yum可以安装完成。但是由于服务器在纯内网环境，无法访问公网，所有无法在线直接安装Docker，需要另想方法完成部署安装。\n（1）找一台有网机器，使用yum（yumdownloader）把docker包和依赖下载下来，上传在安装 （2）二进制安装Docker（比较复杂，但是能更好理解和管理Docker，我选择后者）\nDocker下载 Docker版本：20.10.7\nDocker版本下载地址：https://download.docker.com/linux/static/stable/x86_64/\n上传解压 1 2 tar zxvf docker-20.10.7.tgz mv docker/* /usr/bin systemd管理docker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat \u0026gt; /usr/lib/systemd/system/docker.service \u0026lt;\u0026lt; EOF [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target Requires=docker.socket [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process OOMScoreAdjust=-500 [Install] WantedBy=multi-user.target EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026gt; /usr/lib/systemd/system/docker.socket \u0026lt;\u0026lt; EOF [Unit] Description=Docker Socket for the API [Socket] ListenStream=/var/run/docker.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target EOF 普通用户管理Docker 1 2 3 4 groupadd docker #添加docker组，二进制不会自动添加的，yum会 usermod -a -G docker user1 #把要管理的用户添加到组里面就行 这样在docker组的用户，也有权限管理docker，这边是因为在docker.sock定义了以docker组启动\n自定义网段 1 2 3 4 5 6 7 mkdir /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;bip\u0026#34;:\u0026#34;10.10.10.1/24\u0026#34; } EOF 纯内网Docker-hub无法使用，这里就定义网段，启用初始就会直接定义网段\n更改存储路径 docker的默认路径是/var/lib/docker\n但是有时间这个空间很小，我们需要把目录迁移到足够大的磁盘下\n如果有数据 1 2 3 4 5 6 systemctl stop docker mkdir /data/service/docker -p mv /var/lib/docker/* /data/service/docker/ #迁移数据 vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --graph /data/service/docker 无数据 1 2 3 4 mkdir /data/service/docker -p vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --graph /data/service/docker 启动并设置开机启动 1 2 3 systemctl daemon-reload systemctl start docker systemctl enable docker docker命令补全方法 1.复制文件 通过yum安装相同版本的docker。将 /usr/share/bash-completion/completions/docker 文件拷贝到二进制安装的docker服务器上的 /usr/share/bash-completion/completions/ 目录下\n2.安装bash-completion 1 yum install -y bash-completion 3.刷新生效 1 2 source /usr/share/bash-completion/completions/docker source /usr/share/bash-completion/bash_completion 4、测试 1 2 3 4 5 6 7 [root@localhost ~]# docker attach context exec import logout port rm service system version build cp export info logs ps rmi stack tag volume builder create help inspect network pull run start top wait commit diff history kill node push save stats trust config engine image load pause rename search stop unpause container events images login plugin restart secret swarm update ","date":"2021-08-03T21:43:26Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/202108032143/","title":"企业纯内网二进制完美部署Docker（20.10.7版本）"},{"content":"布署完metircs-server后，但是访问会报错，此处有两个坑\n1、API聚合功能 通过二进制方式部署完成 kubernetes 后，部署 Metrics Server 后，查看日志出现下面错误信息：\n1 2 3 4 5 6 E1231 10:33:31.978715 1 configmap_cafile_content.go:243] key failed with: missing content for CA bundle \u0026#34;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\u0026#34; E1231 10:34:22.710836 1 configmap_cafile_content.go:243] kube-system/extension-apiserver-authentication failed with: missing content for CA bundle \u0026#34;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\u0026#34; E1231 10:34:31.978769 1 configmap_cafile_content.go:243] key failed with: missing content for CA bundle \u0026#34;client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file\u0026#34; 根据错误日志信息，可以知道是缺少认证的证书文件，导致不能访问 kube-apiserver 而出现的问题。\n出现这个错误是因为 kube-apiserver 没有开启 API 聚合功能。所以需要配置 kube-apiserver 参数，开启聚合功能即可。\n为了能够将用户自定义的 API 注册到 Master 的 API Server 中，首先需要在 Master 节点所在服务器，配置 kube-apiserver 应用的启动参数来启用 API 聚合 功能，参数如下：\n1 2 3 4 5 6 7 8 --runtime-config=api/all=true --requestheader-allowed-names=aggregator --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-client-ca-file=/etc/kubernetes/pki/ca.pem --proxy-client-cert-file=/etc/kubernetes/pki/proxy-client.pem --proxy-client-key-file=/etc/kubernetes/pki/proxy-client-key.pem 如果 kube-apiserver 所在的主机上没有运行 kube-proxy，即无法通过服务的 ClusterIP 进行访问，那么还需要设置以下启动参数：\n1 --enable-aggregator-routing=true 在设置完成重启 kube-apiserver 服务，就启用 API 聚合 功能了。\n1 systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kube-apiserver 开启API聚合功能 按照上面的解决问题思路，我们可以开启 API 聚合功能，然后重启 Metrics Server 服务，步骤如下：\n安装 cfssl 工具\n1 2 3 4 5 6 7 8 ## 下载三个组件 $ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O cfssl $ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O cfssljson $ wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O cfssl-certinfo ## 复制到 bin 目录下 $ chmod +x ./cfssl* $ mv ./cfssl* /usr/local/bin/ 创建 cfssl 配置文件 创建 proxy-client-csr.json 文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;CN\u0026#34;: \u0026#34;aggregator\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } 生成证书和秘钥：\n1 cfssl gencert -profile=kubernetes -ca=/etc/kubernetes/pki/ca.pem -ca-key=/etc/kubernetes/pki/ca-key.pem proxy-client-csr.json | cfssljson -bare kube-proxy 1 2 3 4 5 6 ls -l -rw-r--r-- 1 root root 1017 12月 31 11:20 proxy-client.csr -rw-r--r-- 1 root root 236 12月 31 11:07 proxy-client-csr.json -rw------- 1 root root 1675 12月 31 11:20 proxy-client-key.pem -rw-r--r-- 1 root root 1411 12月 31 11:20 proxy-client.pem 将证书访问指定的目录下，这里我将其放到 /etc/kubernetes/pki 下：\n1 cp * /etc/kubernetes/pki/ 修改 kube-apiserver 参数\n1 2 3 4 5 6 7 8 --runtime-config=api/all=true \\ --requestheader-allowed-names=aggregator \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-client-ca-file=/etc/kubernetes/pki/ca.pem \\ --proxy-client-cert-file=/etc/kubernetes/pki/proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/pki/proxy-client-key.pem \\ 1 2 3 4 5 6 7 8 9 10 11 参数说明： –requestheader-client-ca-file： 客户端 CA 证书。 –requestheader-allowed-names： 允许访问的客户端 common names 列表，通过 header 中 –requestheader-username-headers 参数指定的字段获取。客户端 common names 的名称需要在 client-ca-file 中进行设置，将其设置为空值时，表示任意客户端都可访问。 –requestheader-username-headers： 参数指定的字段获取。 –requestheader-extra-headers-prefix： 请求头中需要检查的前缀名。 –requestheader-group-headers 请求头中需要检查的组名。 –requestheader-username-headers 请求头中需要检查的用户名。 –proxy-client-cert-file： 在请求期间验证 Aggregator 的客户端 CA 证书。 –proxy-client-key-file： 在请求期间验证 Aggregator 的客户端私钥。 –requestheader-allowed-names： 允许访问的客户端 common names 列表，通过 header 中 –requestheader-username-headers 参数指定的字段获取。客户端 common names 的名称需要在 client-ca-file 中进行设置，将其设置为空值时，表示任意客户端都可访问。 重启 kube-apiserver 组件\n1 systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kube-apiserver 2、用户无权限访问资源pod 报错\n1 2 3 4 ^C[root@k8s-master cfg]# kubectl top pod -A Error from server (Forbidden): pods.metrics.k8s.io is forbidden: User \u0026#34;system:kube-proxy\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;metrics.k8s.io\u0026#34; at the cluster scope [root@k8s-master cfg]# kubectl top pod Error from server (Forbidden): pods.metrics.k8s.io is forbidden: User \u0026#34;system:kube-proxy\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;metrics.k8s.io\u0026#34; in the namespace \u0026#34;default\u0026#34; 报错提示为RBAC权限问题，给kubernetes用户授权如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: metrics-reader rules: - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User name: system:kube-proxy #用户名称 apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: metrics-reader apiGroup: rbac.authorization.k8s.io --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: metrics-reader rules: - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: metrics-reader subjects: - kind: User name: system:kube-proxy #用户名称 apiGroup: rbac.authorization.k8s.io 问题就解决\n","date":"2021-08-01T23:23:37Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/202108012323/","title":"解决二进制K8S布署的metrics-server查看集群资源报错权限问题"},{"content":"最近公司服务器的一个磁盘出现Read Only，导致数据不可写，但此服务器安装的zabbix监控并未报警，所以针对此情况，新增了监控系统磁盘读写状态的监控。\n下面是效果图\n如果返回值0代表磁盘都是rw状态可以正常读写，返回值1的话，代表磁盘是ro状态，会报警。\n如何实现：\n目标 使用zabbix监控磁盘ro指标，如果发现ro模式，及时报警\n1 mount | grep -v \u0026#39;/sys/fs/cgroup\u0026#39; |awk \u0026#39;{print $NF}\u0026#39; |cut -c 2-3 |awk \u0026#39;{if($1~/ro/) {print 1}}\u0026#39;|wc -l|awk \u0026#39;{if($1\u0026lt;=0) {print 0 } else {print 1}}\u0026#39; 使用这个监控所有磁盘，判断是否只读模式\n修改zabbix_agentd.conf文件 添加\n1 UserParameter=check_disk_status,/bin/mount | grep -v \u0026#39;/sys/fs/cgroup\u0026#39; |awk \u0026#39;{print $NF}\u0026#39; |cut -c 2-3 |awk \u0026#39;{if($1~/ro/) {print 1}}\u0026#39;|wc -l|awk \u0026#39;{if($1\u0026lt;=0) {print 0 } else {print 1}}\u0026#39; 重启zabbix客户端 1 sh startup_zabbix_agentd.sh restart zabbix服务端添加监控项 zabbix服务端添加触发器 这个触发器是最近3次检测都出现ro状态，就会报警。\n测试 ","date":"2021-08-01T12:34:05Z","image":"https://www.ownit.top/title_pic/68.jpg","permalink":"https://www.ownit.top/p/202108011234/","title":"zabbix企业应用之监控磁盘读写状态"},{"content":"今天机器重启，但是以前加的自启任务的脚本没有执行\n原因：\n在centos7中,/etc/rc.d/rc.local文件的权限被降低了,没有执行权限,需要给它添加可执行权限。\n1 chmod +x /etc/rc.d/rc.local 然后就可以在里面添加你要开机自启的命令了\n1 vi /etc/rc.d/rc.local ","date":"2021-07-26T16:39:52Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/202107261639/","title":"Centos7 自启没有起作用？？（rc.local）"},{"content":"什么是Portainer Portainer是Docker的图形化管理工具，提供状态显示面板、应用模板快速部署、容器镜像网络数据卷的基本操作（包括上传下载镜像，创建容器等操作）、事件日志显示、容器控制台操作、Swarm集群和服务等集中管理和操作、登录用户管理和控制等功能。功能十分全面，基本能满足中小型单位对容器管理的全部需求。\n汉化界面 安装Docker 如果已经安装了Docker环境直接跳过本步骤即可\n1 2 3 4 5 6 7 8 9 10 11 #CentOS 6 rpm -iUvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm yum update -y yum -y install docker-io service docker start chkconfig docker on #CentOS 7、Debian、Ubuntu curl -sSL https://get.docker.com/ | sh systemctl start docker systemctl enable docker.service 安装Portainer版本： portainer-ce:2.0.1\n汉化地址：https://github.com/eysp/public/releases\n此版本已经测试，暂无bug，可以正常汉化显示，推荐\nPortainer中文汉化 1 2 docker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:2.0.1 进入这个目录把汉化的文件全部解压覆盖到： public 目录下\n","date":"2021-07-25T23:24:21Z","image":"https://www.ownit.top/title_pic/23.jpg","permalink":"https://www.ownit.top/p/202107252324/","title":"Docker集群可视化管理平台（Portainer）"},{"content":"在Python中实现文件的读写操作其实非常简单，通过Python内置的open函数，我们可以指定文件名、操作模式、编码信息等来获得操作文件的对象，接下来就可以对文件进行读写操作了。这里所说的操作模式是指要打开什么样的文件（字符文件还是二进制文件）以及做什么样的操作（读、写还是追加），具体的如下表所示。\n操作模式 具体含义 'r' 读取 （默认） 'w' 写入（会先截断之前的内容） 'x' 写入，如果文件已经存在会产生异常 'a' 追加，将内容写入到已有文件的末尾 'b' 二进制模式 't' 文本模式（默认） '+' 更新（既可以读又可以写） Python文件处理，正确格式，防止有异常，我们需要加上try except\n标准格式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def main(): f = None try: f = open(\u0026#39;文件.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) print(f.read()) except FileNotFoundError: print(\u0026#39;无法打开指定的文件!\u0026#39;) except LookupError: print(\u0026#39;指定了未知的编码!\u0026#39;) except UnicodeDecodeError: print(\u0026#39;读取文件时解码错误!\u0026#39;) finally: if f: f.close() if __name__ == \u0026#39;__main__\u0026#39;: main() 在Python中，我们可以将那些在运行时可能会出现状况的代码放在try代码块中，在try代码块的后面可以跟上一个或多个except来捕获可能出现的异常状况。例如在上面读取文件的过程中，文件找不到会引发FileNotFoundError，指定了未知的编码会引发LookupError，而如果读取文件时无法按指定方式解码会引发UnicodeDecodeError，我们在try后面跟上了三个except分别处理这三种不同的异常状况。最后我们使用finally代码块来关闭打开的文件，释放掉程序中获取的外部资源，由于finally块的代码不论程序正常还是异常都会执行到（甚至是调用了sys模块的exit函数退出Python环境，finally块都会被执行，因为exit函数实质上是引发了SystemExit异常），因此我们通常把finally块称为“总是执行代码块”，它最适合用来做释放外部资源的操作。如果不愿意在finally代码块中关闭文件对象释放资源，也可以使用上下文语法，通过with关键字指定文件对象的上下文环境并在离开上下文环境时自动释放文件资源，代码如下所示。\n除了使用文件对象的read方法读取文件之外，还可以使用for-in循环逐行读取或者用readlines方法将文件按行读取到一个列表容器中，代码如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time def main(): # 一次性读取整个文件内容 with open(\u0026#39;文件.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: print(f.read()) # 通过for-in循环逐行读取 with open(\u0026#39;文件.txt\u0026#39;, mode=\u0026#39;r\u0026#39;) as f: for line in f: print(line, end=\u0026#39;\u0026#39;) time.sleep(0.5) print() # 读取文件按行读取到列表中 with open(\u0026#39;文件.txt\u0026#39;) as f: lines = f.readlines() print(lines) if __name__ == \u0026#39;__main__\u0026#39;: main() 通过上面的讲解，我们已经知道如何将文本数据和二进制数据保存到文件中，那么这里还有一个问题，如果希望把一个列表或者一个字典中的数据保存到文件中又该怎么做呢？答案是将数据以JSON格式进行保存。JSON是“JavaScript Object Notation”的缩写，它本来是JavaScript语言中创建对象的一种字面量语法，现在已经被广泛的应用于跨平台跨语言的数据交换，原因很简单，因为JSON也是纯文本，任何系统任何编程语言处理纯文本都是没有问题的。目前JSON基本上已经取代了XML作为异构系统间交换数据的事实标准。关于JSON的知识，更多的可以参考JSON的官方网站，从这个网站也可以了解到每种语言处理JSON数据格式可以使用的工具或三方库，下面是一个JSON的简单例子。\n上面的JSON跟Python中的字典其实是一样一样的，事实上JSON的数据类型和Python的数据类型是很容易找到对应关系的，如下面两张表所示。\nJSON Python object dict array list string str number (int / real) int / float true / false True / False null None 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json def main(): mydict = { \u0026#39;name\u0026#39;: \u0026#39;小风\u0026#39;, \u0026#39;age\u0026#39;: 20, \u0026#39;qq\u0026#39;: 95849158, \u0026#39;friends\u0026#39;: [\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;], \u0026#39;cars\u0026#39;: [ {\u0026#39;brand\u0026#39;: \u0026#39;BYD\u0026#39;, \u0026#39;max_speed\u0026#39;: 180}, {\u0026#39;brand\u0026#39;: \u0026#39;Audi\u0026#39;, \u0026#39;max_speed\u0026#39;: 280}, {\u0026#39;brand\u0026#39;: \u0026#39;Benz\u0026#39;, \u0026#39;max_speed\u0026#39;: 320} ] } try: with open(\u0026#39;data.json\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as fs: json.dump(mydict, fs) except IOError as e: print(e) print(\u0026#39;保存数据完成!\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() json模块主要有四个比较重要的函数，分别是：\ndump - 将Python对象按照JSON格式序列化到文件中 dumps - 将Python对象处理成JSON格式的字符串 load - 将文件中的JSON数据反序列化成对象 loads - 将字符串的内容反序列化成Python对象 ","date":"2021-07-21T15:00:49Z","image":"https://www.ownit.top/title_pic/09.jpg","permalink":"https://www.ownit.top/p/202107211500/","title":"Python文件处理"},{"content":"zabbix调用api查询机器资源利用率，导出execl表格\n背景：平常我们工作中，需要知道机器的资源利用率多少，我们可以手动查看，但是有1000多台改怎么办？\n方案：我们机器如果是zabbix监控的，我们可以直接调用zabbix的api，来获取所有主机的相关信息，生成execl表格并且导出\n我们可以设置定时任务，每周定时导出，查看集群服务器的利用率。 github地址：https://github.com/nangongchengfeng/zabbix-api.git\n1 2 3 4 5 6 if host[\u0026#39;Number of CPUs\u0026#39;] == \u0026#34;0\u0026#34;: HostItemValues.append(\u0026#34;已停用\u0026#34;) if float(host[\u0026#39;Load average (15m avg)\u0026#39;]) \u0026gt;= 10 or float(round(float(host[\u0026#39;CPU utilization\u0026#39;]), 2)) \u0026gt;= 20 : HostItemValues.append(\u0026#34;否\u0026#34;) else: HostItemValues.append(\u0026#34;是\u0026#34;) 最后面添加是否低负载机器\n判断条件：CPU平均负载/15min \u0026gt;= 10 or CPU使用率 \u0026gt;=20\nZabbix-api_v1 Zabbix-api_v1版本是初级版本，需要Python环境下操作\nZabbix-api_v2 Zabbix-api_v2版本可以打包成exe程序，可以方便执行，不需要依赖\n这里我贴版本2的代码，可以生成exe\nmain.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #! /usr/bin/env python import configparser import os,sys import time from GetItems import Zabbix from SaveToExcel import WriteExcel import datetime path = os.path.dirname(os.path.abspath(__file__)) sys.intern(path) if __name__ == \u0026#34;__main__\u0026#34;: print(\u0026#34;start\u0026#34;.center(60,\u0026#34;*\u0026#34;)) print(\u0026#34;zabbix统计机器资源使用情况\u0026#34;.center(60)) config = configparser.ConfigParser() config.read(os.path.join(os.getcwd(), \u0026#39;config.ini\u0026#39;), encoding=\u0026#39;utf-8\u0026#39;) # 实例化一个zabbix对象 #api调用地址 zabbix_api=\u0026#39;http://10.190.5.237/api_jsonrpc.php\u0026#39; zabbix_user=input(\u0026#34;请输入您zabbix的账号：\u0026#34;) zabbix_passwd=input(\u0026#39;请输入您zabbix的密码：\u0026#39;) file_name=\u0026#39;服务器资源使用情况分析\u0026#39; zabbix = Zabbix( zabbix_api, zabbix_user, zabbix_passwd ) starttime = datetime.datetime.now() # 调用GetItemValue方法获取每台监控主机的监控数据 zabbix_data = zabbix.GetItemValue() if len(zabbix_data) == 2: print(zabbix_data[\u0026#39;errmsg\u0026#39;]) print(\u0026#34;end\u0026#34;.center(60, \u0026#34;*\u0026#34;)) else: date_time = time.strftime(\u0026#39;%Y-%m-%d_%H-%M\u0026#39;) #print(zabbix_data) file_name = os.path.join(os.getcwd(), file_name + date_time + \u0026#39;.xlsx\u0026#39;) WriteExcel(file_name, zabbix_data) endtime = datetime.datetime.now() run_time=endtime - starttime print(f\u0026#34;程序运行：{run_time.seconds} s 生成文件：{file_name}\u0026#34;) print(\u0026#34;end\u0026#34;.center(60, \u0026#34;*\u0026#34;)) GetItems.py 在这里可以定制值。切记，这里代码只适合我的环境，有些值为特定值，如有需要，需要更改值\n列如：\n\u0026ldquo;io.usedgen[*]\u0026rdquo;, # 根目录使用率监控\n\u0026ldquo;disk_capacity.[disk_all_Usage]\u0026rdquo;,#服务器总使用率\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 #! /usr/bin/env python # _*_ coding: utf-8 _*_ # @Desc :调用zabbix api接口，获取监控数据，zabbix-版本为5.0以上 import requests import json import re class Zabbix(object): def __init__(self, ApiUrl, User, Pwd): self.ApiUrl = ApiUrl self.User = User self.Pwd = Pwd self.__Headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json-rpc\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36\u0026#39; } self.Message = { 1001: {\u0026#34;errcode\u0026#34;: \u0026#34;1001\u0026#34;, \u0026#34;errmsg\u0026#34;: \u0026#34;请求路径错误，请检查API接口路径是否正确.\u0026#34;}, 1002: {\u0026#34;errcode\u0026#34;: \u0026#34;1002\u0026#34;, \u0026#34;errmsg\u0026#34;: \u0026#34;Login name or password is incorrect.\u0026#34;}, 1003: {\u0026#34;errcode\u0026#34;: \u0026#34;1003\u0026#34;, \u0026#34;errmsg\u0026#34;: \u0026#34;未获取到监控主机，请检查server端是否监控有主机.\u0026#34;}, 1004: {\u0026#34;errcode\u0026#34;: \u0026#34;1004\u0026#34;, \u0026#34;errmsg\u0026#34;: \u0026#34;未知错误.\u0026#34;}, } def __Login(self): \u0026#39;\u0026#39;\u0026#39; 登陆zabbix，获取认证的秘钥 Returns: 返回认证秘钥 \u0026#39;\u0026#39;\u0026#39; # 登陆zabbix,接口的请求数据 LoginApiData = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;user.login\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;user\u0026#34;: self.User, \u0026#34;password\u0026#34;: self.Pwd }, \u0026#34;id\u0026#34;: 1 } # 向登陆接口发送post请求，获取result LoginRet = requests.post(url=self.ApiUrl, data=json.dumps(LoginApiData), headers=self.__Headers) # 判断请求是否为200 if LoginRet.status_code is not 200: return 1001 else: # 如果是200状态，则进行数据格式化 try: LoginRet = LoginRet.json() except: return 1001 # 如果result在返回数据中，那么表示请求成功，则获取认证key if \u0026#39;result\u0026#39; in LoginRet: Result = LoginRet[\u0026#39;result\u0026#39;] return Result # 否则返回用户或密码错误 else: return 1002 def __GetMonitorHost(self): # 调用登陆函数，获取auth，并判断是否登陆成功 Auth = self.__Login() if Auth == 1001: return 1001 elif Auth == 1002: return 1002 else: HostApiData = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;host.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: [\u0026#34;hostid\u0026#34;, \u0026#34;host\u0026#34;, \u0026#34;name\u0026#34;], \u0026#34;selectInterfaces\u0026#34;: [\u0026#34;interfaces\u0026#34;, \u0026#34;ip\u0026#34;], }, \u0026#34;auth\u0026#34;: Auth, \u0026#34;id\u0026#34;: 1 } # 向host.get接口发起请求，获取所有监控主机 HostRet = requests.post(url=self.ApiUrl, data=json.dumps(HostApiData), headers=self.__Headers).json() if \u0026#39;result\u0026#39; in HostRet: if len(HostRet[\u0026#39;result\u0026#39;]) != 0: # 循环处理每一条记录，进行结构化,最终将所有主机加入到all_host字典中 Allhost = {} for host in HostRet[\u0026#39;result\u0026#39;]: # host = {\u0026#39;hostid\u0026#39;: \u0026#39;10331\u0026#39;, \u0026#39;host\u0026#39;: \u0026#39;172.24.125.24\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;TBDS测试版172.24.125.24\u0026#39;, \u0026#39;interfaces\u0026#39;: [{\u0026#39;ip\u0026#39;: \u0026#39;172.24.125.24\u0026#39;}]} # 进行结构化，提取需要的信息 HostInfo = {\u0026#39;host\u0026#39;: host[\u0026#39;host\u0026#39;], \u0026#39;hostid\u0026#39;: host[\u0026#39;hostid\u0026#39;], \u0026#39;ip\u0026#39;: host[\u0026#39;interfaces\u0026#39;][0][\u0026#39;ip\u0026#39;], \u0026#39;name\u0026#39;: host[\u0026#39;name\u0026#39;]} # host_info = {\u0026#39;host\u0026#39;: \u0026#39;172.24.125.24\u0026#39;, \u0026#39;hostid\u0026#39;: \u0026#39;10331\u0026#39;, \u0026#39;ip\u0026#39;: \u0026#39;172.24.125.24\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;TBDS测试版172.24.125.24\u0026#39;} # 加入到all_host中 Allhost[host[\u0026#39;hostid\u0026#39;]] = HostInfo #print(Allhost)主机结构化列表 return {\u0026#34;Auth\u0026#34;:Auth, \u0026#34;Allhost\u0026#34;:Allhost} else: return 1003 else: return 1001 def GetItemValue(self): \u0026#39;\u0026#39;\u0026#39; # 调用item.get接口，获取监控项（监控项中带有每个监控项的最新监控数据） 接口说明文档：https://www.zabbix.com/documentation/4.0/zh/manual/api/reference/item/get Returns: 返回所有监控主机监控信息， \u0026#39;\u0026#39;\u0026#39; # 获取所有的主机 HostRet = self.__GetMonitorHost() # 判断HostRet是否有主机和认证key存在，这里如果是类型如果是字段，那边表示一定获取到的有主机信息，如果不是，则表示没有获取到值 if type(HostRet) is dict: # 首先拿到认证文件和所有主机信息 Auth, AllHost = HostRet[\u0026#39;Auth\u0026#39;], HostRet[\u0026#39;Allhost\u0026#39;] # 定义一个新的allhost，存放所有主机新的信息 NewAllHost = {} # 循环向每个主机发起请求，获取监控项的值 for k in AllHost: ItemData = { \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;item.get\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;output\u0026#34;: [\u0026#34;extend\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;key_\u0026#34;, \u0026#34;lastvalue\u0026#34;], \u0026#34;hostids\u0026#34;: str(k), \u0026#34;search\u0026#34;: { \u0026#34;key_\u0026#34;: [ \u0026#34;system.hostname\u0026#34;, # 主机名 \u0026#34;system.uptime\u0026#34;, # 系统开机时长 \u0026#34;io.usedgen[*]\u0026#34;, # 根目录使用率监控 \u0026#34;disk_capacity.[disk_all_Usage]\u0026#34;,#服务器总使用率 \u0026#34;system.cpu.util\u0026#34;, # cpu使用率 \u0026#34;system.cpu.num\u0026#34;, # cpu核数 \u0026#34;system.cpu.load\u0026#34;, # cpu平均负载 \u0026#34;system.cpu.util[,idle]\u0026#34;, # cpu空闲时间 \u0026#34;vm.memory.utilization\u0026#34;, # 内存使用率 \u0026#34;vm.memory.size[total]\u0026#34;, # 内存总大小 \u0026#34;vm.memory.size[available]\u0026#34;, # 可用内存 \u0026#34;net.if.in\u0026#34;, # 网卡每秒流入的比特(bit)数 \u0026#34;net.if.out\u0026#34; # 网卡每秒流出的比特(bit)数 ] }, \u0026#34;searchByAny\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;sortfield\u0026#34;: \u0026#34;name\u0026#34; }, \u0026#34;auth\u0026#34;: Auth, \u0026#34;id\u0026#34;: 1 } # 向每一台主机发起请求，获取监控项 Ret = requests.post(url=self.ApiUrl, data=json.dumps(ItemData), headers=self.__Headers).json() #print(Ret) if \u0026#39;result\u0026#39; in Ret: # 判断每台主机是否有获取到监控项，如果不等于0表示获取到有监控项 if len(Ret[\u0026#39;result\u0026#39;]) != 0: # 从所有主机信息中取出目前获取信息的这台主机信息存在host_info中 HostInfo = AllHost[k] #{\u0026#39;host\u0026#39;: \u0026#39;Zabbix server\u0026#39;, \u0026#39;hostid\u0026#39;: \u0026#39;10084\u0026#39;, \u0026#39;ip\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;Zabbix server\u0026#39;} # 循环处理每一台主机的所有监控项 #print(HostInfo) for host in Ret[\u0026#39;result\u0026#39;]: #print(str(host.values())) # 匹配所有分区挂载目录使用率的正则表达式 DiskUtilization = re.findall(r\u0026#39;根目录使用率监控\u0026#39;, str(host.values())) #print(DiskUtilization) if len(DiskUtilization) == 1: #如果匹配到了分区目录，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] # 匹配网卡进出流量的正则表达式 NetworkBits = re.findall(r\u0026#39;Interface.*: Bits [a-z]{4,8}\u0026#39;, str(host.values())) #print(host.values()) if len(NetworkBits) == 1: HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;System name\u0026#39; in host.values(): # 匹配主机名，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;System uptime\u0026#39; in host.values(): # 匹配系统开机运行时长，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Number of CPUs\u0026#39; in host.values(): # 匹配CPU核数，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Total memory\u0026#39; in host.values(): # 匹配内存总大小，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;/: Total space\u0026#39; in host.values(): # 匹配根目录总量，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;/: Used space\u0026#39; in host.values(): # 匹配根目录使用量，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;/: Space utilization\u0026#39; in host.values(): # 匹配根目录使用量，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Load average (1m avg)\u0026#39; in host.values(): # 匹配CPU平均1分钟负载，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Load average (5m avg)\u0026#39; in host.values(): # 匹配CPU平均5分钟负载，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Load average (15m avg)\u0026#39; in host.values(): # 匹配CPU平均15分钟负载，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;idle time\u0026#39; in host.values(): # 匹配CPU空闲时间，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;CPU utilization\u0026#39; in host.values(): # 匹配CPU使用率，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Memory utilization\u0026#39; in host.values(): # 匹配内存使用率，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;Available memory\u0026#39; in host.values(): # 匹配可用内存大小，进行保存 HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] elif \u0026#39;服务器硬盘总使用率\u0026#39; in host.values(): HostInfo[host[\u0026#39;name\u0026#39;]] = host[\u0026#39;lastvalue\u0026#39;] #print(HostInfo) NewAllHost[HostInfo[\u0026#39;hostid\u0026#39;]] = HostInfo #print(NewAllHost) else: return {\u0026#34;errcode\u0026#34;: \u0026#34;1001\u0026#34;, \u0026#34;errmess\u0026#34;: \u0026#34;Login name or password is incorrect.\u0026#34;} return NewAllHost #print(NewAllHost) elif HostRet == 1001: return self.Message[1001] elif HostRet == 1002: return self.Message[1002] elif HostRet == 1003: return self.Message[1003] else: return self.Message[1004] SaveToExcel.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 #! /usr/bin/env python # _*_ coding: utf-8 _*_ import re from openpyxl import Workbook from openpyxl.styles import Font, Alignment, Side, Border, PatternFill def WriteExcel(FilaPath, ZabbixData): WorkBook = Workbook() Sheet = WorkBook.active Sheet.title = \u0026#39;服务器资源使用情况\u0026#39; # 除去 : \u0026#39;根目录总量/G\u0026#39;,\u0026#39;根目录使用量/G\u0026#39;, TableTitle = [\u0026#39;IP\u0026#39;,\u0026#39;主机名\u0026#39;,\u0026#39;运行时长/天\u0026#39;,\u0026#39;CPU/核\u0026#39;,\u0026#39;内存/GB\u0026#39;,\u0026#39;根目录使用率/%\u0026#39;,\u0026#39;CPU平均负载/1min\u0026#39;,\u0026#39;CPU平均负载/5min\u0026#39;,\u0026#39;CPU平均负载/15min\u0026#39;,\u0026#39;CPU空闲时间\u0026#39;,\u0026#39;CPU使用率/%\u0026#39;,\u0026#39;内存使用率/%\u0026#39;,\u0026#39;可用内存/G\u0026#39;,\u0026#39;磁盘使用率/%\u0026#39;,\u0026#39;低负载（是/否）\u0026#39;] TitleColumn = {} #存放每个title值所对应的列{\u0026#39;IP\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;主机名\u0026#39;: \u0026#39;B\u0026#39;, \u0026#39;运行时长\u0026#39;: \u0026#39;C\u0026#39;, \u0026#39;CPU/核\u0026#39;: \u0026#39;D\u0026#39;, \u0026#39;内存/GB\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;根目录总量\u0026#39;: \u0026#39;F\u0026#39;,...} AllHostItemValues = [] #存放所有主机的监控项值 列表信息。 # 维护表头，写入表头数据 for row in range(len(TableTitle)): Col = row + 1 Column = Sheet.cell(row=1, column=Col) #获取单元格的位置 Column.value = TableTitle[row] #写入数据 TitleCol = Column.coordinate.strip(\u0026#39;1\u0026#39;) #获取Title所在的列 TitleColumn[TableTitle[row]] = TitleCol #加入到TitleColumn # 整理Zabbix 监控数据逐行写入到表格中 #print(ZabbixData) for host in ZabbixData.values(): # 1.首先要对分区目录使用率进行一个整合，将除/目录外的分区目录使用率整合为一个值 DiskItems = \u0026#39;\u0026#39; #定义一个空值，用于存放除根目录空间使用率外所有的分区目录使用率 DelItems = [] #定义一个空列表，用于存放除根目录空间使用率外所有的分区目录使用率的键值 for item in host: DiskItem = re.findall(r\u0026#39;^/[a-z0-9]{1,50}: Space utilization\u0026#39;, item) if len(DiskItem) == 1: DiskItem = DiskItem[0] #获取监控项的名字 /boot: Space utilization NewDiskItem = DiskItem.strip(\u0026#39;Space utilization\u0026#39;) # 将名字格式化，/boot: Space utilization 格式化为：/boot: DiskItemValue = str(round(float(host[item]), 2)) + \u0026#39;%\u0026#39; # 取出对应监控项的值，并格式化保留两位小数 # 将所有分区目录使用率组合为一个整的磁盘使用率 if DiskItems == \u0026#39;\u0026#39;: DiskItemData = str(NewDiskItem) + \u0026#39; \u0026#39; + str(DiskItemValue) else: DiskItemData = \u0026#39;\\n\u0026#39; + str(NewDiskItem) + \u0026#39; \u0026#39; + str(DiskItemValue) DiskItems += DiskItemData # 将处理完的磁盘使用率加入到DelItems列表中，供后续删除使用 DelItems.append(DiskItem) #print(host) # 2.将已经整合过的分区目录使用率监控项在原来的主机监控项中删除 for delitem in DelItems: host.pop(delitem) # 3.将整合好的分区目录使用率，重新加入到主机监控项的字典中 host[\u0026#39;Disk utilization\u0026#39;] = DiskItems #print(host) # 4.将每台主机监控项的值取出来组成一个列表 # 最终得到一条一条这样的数据： #\u0026#39;IP\u0026#39;,\u0026#39;主机名\u0026#39;,\u0026#39;运行时长/天\u0026#39;,\u0026#39;CPU/核\u0026#39;,\u0026#39;内存/GB\u0026#39;,\u0026#39;根目录使用率/%\u0026#39;,\u0026#39;CPU平均负载/1min\u0026#39;,\u0026#39;CPU平均负载/5min\u0026#39;,\u0026#39;CPU平均负载/15min\u0026#39;,\u0026#39;CPU空闲时间\u0026#39;,\u0026#39;CPU使用率/%\u0026#39;,\u0026#39;内存使用率/%\u0026#39;,\u0026#39;可用内存/G\u0026#39;,\u0026#39;磁盘使用率/%\u0026#39; # [\u0026#39;172.24.125.12\u0026#39;, \u0026#39;tbds-172-24-125-12\u0026#39;, \u0026#39;245.87d\u0026#39;, \u0026#39;16\u0026#39;, 64, \u0026#39;50G\u0026#39;, 7.43, \u0026#39;14.87%\u0026#39;, \u0026#39;0.1\u0026#39;, \u0026#39;0.18\u0026#39;, \u0026#39;0.32\u0026#39;, 97.79, \u0026#39;2.21%\u0026#39;, \u0026#39;35.52%\u0026#39;, 40.45, \u0026#39;/boot: 14.23%\\n/data: 6.24%\\n/home: 0.03%\u0026#39;] #print(host[\u0026#39;System uptime\u0026#39;]) if \u0026#39;System uptime\u0026#39; in host: HostItemValues = [] #定义一个空列表，用于存放主机的监控项的值 HostItemValues.append(host[\u0026#39;ip\u0026#39;]) HostItemValues.append(host[\u0026#39;name\u0026#39;]) try: HostItemValues.append(str(round(int(host[\u0026#39;System uptime\u0026#39;]) / 24 / 60 / 60, 2)) + \u0026#39;d\u0026#39;) # 首先将运行时长换算为天数，然后再加入到列表中 except IndexError as e: print(\u0026#34;IndexError Details : \u0026#34; + str(e)) pass HostItemValues.append(host[\u0026#39;Number of CPUs\u0026#39;]) TotalMemory = int(int(host[\u0026#39;Total memory\u0026#39;]) / 1024 / 1024 / 1024) if TotalMemory == 7: TotalMemory = 8 elif TotalMemory == 15: TotalMemory = 16 elif TotalMemory == 31: TotalMemory = 32 elif TotalMemory == 62: TotalMemory = 64 elif TotalMemory == 251: TotalMemory = 256 elif TotalMemory == 503: TotalMemory = 512 HostItemValues.append(TotalMemory) # 内存总大小 #HostItemValues.append(str(round(int(host[\u0026#39;/: Total space\u0026#39;]) / 1024 / 1024 / 1024)) + \u0026#39;G\u0026#39;) # 根目录总共大小 #HostItemValues.append(str(round(int(host[\u0026#39;/: Used space\u0026#39;]) / 1024 / 1024 / 1024, 2)) + \u0026#39;G\u0026#39;) # 根目录使用量 HostItemValues.append(str(round(float(host[\u0026#39;根目录使用率监控\u0026#39;]), 2)) + \u0026#39;%\u0026#39;) # 根目录使用率 HostItemValues.append(host[\u0026#39;Load average (1m avg)\u0026#39;]) HostItemValues.append(host[\u0026#39;Load average (5m avg)\u0026#39;]) HostItemValues.append(host[\u0026#39;Load average (15m avg)\u0026#39;]) HostItemValues.append(round(float(host[\u0026#39;idle time\u0026#39;]), 2)) # CPU空闲时间 HostItemValues.append(str(round(float(host[\u0026#39;CPU utilization\u0026#39;]), 2)) + \u0026#39;%\u0026#39;) # CPU使用率 HostItemValues.append(str(round(float(host[\u0026#39;Memory utilization\u0026#39;]), 2)) + \u0026#39;%\u0026#39;) # 内存使用率 HostItemValues.append(str(round(int(host[\u0026#39;Available memory\u0026#39;]) / 1024 / 1024 / 1024, 2)) + \u0026#39;G\u0026#39;) # 可用内存 HostItemValues.append(host[\u0026#39;服务器硬盘总使用率\u0026#39;]) # 磁盘使用率 if host[\u0026#39;Number of CPUs\u0026#39;] == \u0026#34;0\u0026#34;: HostItemValues.append(\u0026#34;已停用\u0026#34;) #print(type(float(host[\u0026#39;Load average (15m avg)\u0026#39;])),type(float(round(float(host[\u0026#39;CPU utilization\u0026#39;]), 2)))) if float(host[\u0026#39;Load average (15m avg)\u0026#39;]) \u0026gt;= 10 or float(round(float(host[\u0026#39;CPU utilization\u0026#39;]), 2)) \u0026gt;= 20 : #print(\u0026#34;负载: 是\u0026#34; + host[\u0026#39;Load average (15m avg)\u0026#39;]) #print(\u0026#34;cpu: 是\u0026#34; + str(round(float(host[\u0026#39;CPU utilization\u0026#39;]), 2))) HostItemValues.append(\u0026#34;否\u0026#34;) else: #print(\u0026#34;负载: 否\u0026#34; + host[\u0026#39;Load average (15m avg)\u0026#39;]) # print(\u0026#34;cpu: 否\u0026#34; + str(round(float(host[\u0026#39;CPU utilization\u0026#39;]), 2))) HostItemValues.append(\u0026#34;是\u0026#34;) # 将每一台主机的所有监控项信息添加到AllHostItems列表中 AllHostItemValues.append(HostItemValues) #print(AllHostItemValues) # 将所有信息写入到表格中 for HostValue in range(len(AllHostItemValues)): Sheet.append(AllHostItemValues[HostValue]) #print(HostValue) ############ 设置单元格样式 ############ # 字体样式 TitleFont = Font(name=\u0026#34;宋体\u0026#34;, size=12, bold=True, italic=False, color=\u0026#34;000000\u0026#34;) TableFont = Font(name=\u0026#34;宋体\u0026#34;, size=11, bold=False, italic=False, color=\u0026#34;000000\u0026#34;) # 对齐样式 alignment = Alignment(horizontal=\u0026#34;center\u0026#34;, vertical=\u0026#34;center\u0026#34;, text_rotation=0, wrap_text=True) # 边框样式 side1 = Side(style=\u0026#39;thin\u0026#39;, color=\u0026#39;000000\u0026#39;) border = Border(left=side1, right=side1, top=side1, bottom=side1) # 填充样式 pattern_fill = PatternFill(fill_type=\u0026#39;solid\u0026#39;, fgColor=\u0026#39;99ccff\u0026#39;) # 设置列宽 column_width = {\u0026#39;A\u0026#39;: 15, \u0026#39;B\u0026#39;: 30, \u0026#39;C\u0026#39;: 14, \u0026#39;D\u0026#39;: 10, \u0026#39;E\u0026#39;: 10, \u0026#39;F\u0026#39;: 16, \u0026#39;G\u0026#39;: 18, \u0026#39;H\u0026#39;: 18, \u0026#39;I\u0026#39;: 22, \u0026#39;J\u0026#39;: 22, \u0026#39;K\u0026#39;: 23, \u0026#39;L\u0026#39;: 15, \u0026#39;M\u0026#39;: 16, \u0026#39;N\u0026#39;: 16, \u0026#39;O\u0026#39;: 14, \u0026#39;P\u0026#39;: 16} for i in column_width: Sheet.column_dimensions[i].width = column_width[i] # 设置首行的高度 Sheet.row_dimensions[1].height = 38 # 冻结窗口 Sheet.freeze_panes = \u0026#39;A2\u0026#39; # 添加筛选器 Sheet.auto_filter.ref = Sheet.dimensions # 设置单元格字体及样式 for row in Sheet.rows: for cell in row: if cell.coordinate.endswith(\u0026#39;1\u0026#39;) and len(cell.coordinate) == 2: cell.alignment = alignment #设置对齐样式 cell.font = TitleFont #设置字体 cell.border = border #设置边框样式 cell.fill = pattern_fill #设置填充样式 else: cell.font = TableFont cell.alignment = alignment cell.border = border WorkBook.save(filename=FilaPath) 多文件打包。这边百度一下，很简单的\n","date":"2021-07-19T21:19:19Z","image":"https://www.ownit.top/title_pic/52.jpg","permalink":"https://www.ownit.top/p/202107192119/","title":"zabbix-api查询机器资源利用率，导出execl表格"},{"content":"Python实现查询剪贴板自动匹配信息 前提：业务IP太多，每个有不同的主机名和不同的功能。\n不想每次都要去查execl，想更方便点，更快一点。\n通俗点思路：点击exe，Python 自动监控剪贴板的内容，然后正则取出IP，接着根据IP对比业务文档，获取相应的信息，然后把查询出来的内容，弹出提示，把查询出的内容写入剪贴板。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 \u0026#39;\u0026#39;\u0026#39; 功能作用：对比剪贴板类容 \u0026#39;\u0026#39;\u0026#39; import win32clipboard as w import win32con import xlrd from tkinter import messagebox import win32api, win32con import pyperclip import re import sys import os # print(__file__) path = os.path.dirname(os.path.abspath(__file__)) sys.intern(path) # print(path) # 获取剪贴板中的内容 def getText(): w.OpenClipboard() d = w.GetClipboardData(win32con.CF_TEXT) w.CloseClipboard() return (d).decode(\u0026#39;GBK\u0026#39;) # 设置剪贴板的类容 def set_text(aString): w.OpenClipboard() w.EmptyClipboard() w.SetClipboardData(win32con.CF_TEXT, aString) w.CloseClipboard() # 生成资源文件目录访问路径 def resource_path(relative_path): if getattr(sys, \u0026#39;frozen\u0026#39;, False): # 是否Bundle Resource base_path = sys._MEIPASS else: base_path = os.path.abspath(\u0026#34;.\u0026#34;) return os.path.join(base_path, relative_path) # 获取剪贴板中的ip,并判断是否正常 def get_ip(ss_ip): ipList = re.findall(r\u0026#39;[0-9]+(?:\\.[0-9]+){3}\u0026#39;, ss_ip) # print(ipList) if ipList: return ipList else: win32api.MessageBox(0, \u0026#34;请您检查复制是否带有IP，请重新测试\u0026#34;, \u0026#34;提醒\u0026#34;, win32con.MB_OK) sys.exit(0) # 获取xls中的数据，和之前剪贴板的数据对比 def host(ss_ip): # 获取execl的内容，这边是根据业务来分析 filename = resource_path(os.path.join(\u0026#34;res\u0026#34;, \u0026#34;hosts.xls\u0026#34;)) # print(filename) # execl_hosts = \u0026#39;./hosts.xls\u0026#39; data1 = xlrd.open_workbook(filename) page = data1.sheet_by_index(2) nrows1 = page.nrows ncols1 = page.ncols # 获取ip host_ip = page.col_values(10) app = page.col_values(1) # 功能集群 purpose = page.col_values(2) # 用途 hostname = page.col_values(11) # 主机名称 # print(host_ip) # 开始对比数据 start = 0 count = 1 # print(ss_ip) if str(ss_ip[0]) not in host_ip: win32api.MessageBox(0, f\u0026#34;暂无设备{ss_ip[0]}的信息\u0026#34;, \u0026#34;未知设备\u0026#34;, win32con.MB_OK) sys.exit(0) for k, item in enumerate(host_ip, start): # print(k,item,ss_ip[0]) if str(ss_ip[0]) == str(item): # print(\u0026#34;正常:\u0026#34; + item, k) win32api.MessageBox(0, f\u0026#34;\\t\\t注意\\n 主机ip：{item} 主机名称：{hostname[k]} \\n 功能集群：{app[k]} 主机用途：{purpose[k]}\u0026#34;, \u0026#34;发现设备\u0026#34;, win32con.MB_OK) pyperclip.copy(f\u0026#34;主机ip：{item} 主机名称：{hostname[k]} \\n 功能集群：{app[k]} 主机用途：{purpose[k]}\u0026#34;) sys.exit(0) count = count + 1 def main(): ss_ip = getText() one_ip = get_ip(ss_ip) host(one_ip) if __name__ == \u0026#39;__main__\u0026#39;: main() 测试效果：\n打包资源生成exe Python打包.exe的方法大致有四种：py2exe, pyinstaller,cx_Freeze和nuitka。其中最常用的是pyinstaller。Pyinstaller本身不是python库，但依旧可以安装python库安装方式安装，生成的.exe可以跨多平台使用，也能指定图标。\n我们需要把使用到的资源文件都放在一个文件夹里。本文在当前目录下新建了一个名为res的子文件夹来存放资源文件，本文假设res内的资源文件为hosts.xls\n修改完.py文件后可以先运行一下，保证无误。然后通过cmd指令：\n1 pyi-makespec -F beloved.py 生成.spec文件。如果要添加Icon等可以在这里就使用pyi-makespec --icon abc.jpg -F beloved.py语句生成spec文件。\n接下来，修改.spec文件：\n修改前datas=[]，本文这里把它改成上图所示，意思是\n将beloved.py当前目录下的res目录（及其目录中的文件）加入目标exe中，在运行时放在零时文件的根目录下，名称为res。\n生成.exe文件以及其他相关文件 接下来，我们便可以放心的生成.exe文件了。执行cmd指令\n1 pyinstaller -F beloved.spec .exe文件生成在子文件dict中。到此便可以把.exe发给其他电脑端运行了。.exe运行比较慢，建议多等待，只要没出现错误提示就OK。\n参考地址：https://blog.csdn.net/qq_44685030/article/details/105096338\n","date":"2021-07-08T18:20:52Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/202107081820/","title":"Python实现查询剪贴板自动匹配信息"},{"content":"简介 写过shell脚本的人都知道，即便出现一些简单的语法错误，运行的时候也可能没有办法发现。有些看似运行正确的脚本，实际上可能在某些分支，某些场景下仍然出现错误，而有的写法可能运行正常，但是却不符合POSIX标准，不具备可移植性。\n诚然，shell脚本是解释运行，没有办法向C/C++那样严格检查，但是我们仍然可以借助一些工具帮助我们提前发现一些错误。\nshellcheck github地址：https://github.com/koalaman/shellcheck\nshellcheck 是一款实用的 shell脚本静态检查工具。\n首先，可以帮助你提前发现并修复简单的语法错误，节约时间。每次都需要运行才发现写错了一个小地方，确实非常浪费时间。\n其次，可以针对你当前不够完善不够健壮的写法，提供建议，帮助你提前绕开一些坑，避免等问题真的发生了才去调试处理。\n在其介绍中，目标是针对所有用户的，从初学者到高手，都用得上\n指出并澄清典型的初学者的语法问题，那通常会shell提供神秘的错误消息。 指出并澄清典型的中级的语义问题，这些问题会导致shell出现奇怪且反直觉的行为。 指出可能导致高级用户的脚本中，可能在未来某种情况下失败的陷阱。 在线使用 非常简单，在网页 https://www.shellcheck.net 上，贴入你的脚本，运行检查即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/sh for n in {1..$RANDOM} do str=\u0026#34;\u0026#34; if (( n % 3 == 0 )) then str=\u0026#34;fizz\u0026#34; fi if [ $[n%5] == 0 ] then str=\u0026#34;$strbuzz\u0026#34; fi if [[ ! $str ]] then str=\u0026#34;$n\u0026#34; fi echo \u0026#34;$str\u0026#34; done shell\n它会给出错误提示或者建议：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ shellcheck myscript Line 2: for n in {1..$RANDOM} ^-- SC3009: In POSIX sh, brace expansion is undefined. ^-- SC3028: In POSIX sh, RANDOM is undefined. Line 5: if (( n % 3 == 0 )) ^-- SC3006: In POSIX sh, standalone ((..)) is undefined. Line 9: if [ $[n%5] == 0 ] ^-- SC3007: In POSIX sh, $[..] in place of $((..)) is undefined. ^-- SC2007: Use $((..)) instead of deprecated $[..] ^-- SC3014: In POSIX sh, == in place of = is undefined. Line 11: str=\u0026#34;$strbuzz\u0026#34; ^-- SC2154: strbuzz is referenced but not assigned. Line 13: if [[ ! $str ]] ^-- SC3010: In POSIX sh, [[ ]] is undefined. $ 每个可能的错误都提示了。新手写shell出现莫名的报错时，可以尝试使用奥。当然例子中很多并不是真的错误，而是某种写法不符合POSIX标准，这种情况也应该避免。\n安装方式 在大多数发行版的包管理中，已经有shellcheck了，如在基于debian的机器上\n1 apt-get install shellcheck 其他系统的具体安装方式，可以查阅 shellcheck 的github首页介绍\n当然，也可以选择自行从源码安装。\n我选择的就是二进制安装，直接放在/usr/bin/目录下，给权限就可以了\n下载：https://download.csdn.net/download/heian_99/19597695\ngithub：https://github.com/koalaman/shellcheck/releases/download/stable/shellcheck-stable.linux.x86_64.tar.xz\n测试\n1 2 3 4 5 6 7 8 #!/bin/bash if[ $# -eq 0 ] then echo \u0026#34;no para\u0026#34; else echo \u0026#34;$# para\u0026#34; fi exit 0 结果：\n1 2 3 4 5 6 7 8 9 10 11 12 [root@redhat heian]# sh test.sh test.sh:行2: if[ 0 -eq 0 ]: 未找到命令 test.sh:行3: 未预期的符号 `then\u0026#39; 附近有语法错误 test.sh:行3: `then\u0026#39; [root@redhat heian]# shellcheck test.sh In test.sh line 2: if[ $# -eq 0 ] ^-- SC1069: You need a space before the [. For more information: https://www.shellcheck.net/wiki/SC1069 -- You need a space before the [. 相关的报错已经给了解决的办法，也可以根据连接去查询更详细的原因。\n问题列表 那么shellcheck具体会检查一些什么问题呢，以下给出一个不完整的问题检查列表。\n可以看下，你是否都能意识到这样的写法时有错误或隐患的。\n如果发现有自己不知道的或自己容易错漏的，那么也许你也应该花点时间，装上shellcheck。\n引号问题 1 2 3 4 5 6 7 8 9 10 echo $1 # Unquoted variables #变量未加引号 find . -name *.ogg # Unquoted find/grep patterns #find/grep 的匹配模式未加引号 rm \u0026#34;~/my file.txt\u0026#34; # Quoted tilde expansion #引号中的波浪符扩展 v=\u0026#39;--verbose=\u0026#34;true\u0026#34;\u0026#39;; cmd $v # Literal quotes in variables # 变量中的字面引号 for f in \u0026#34;*.ogg\u0026#34; # Incorrectly quoted \u0026#39;for\u0026#39; loops # 错误的for循环 touch $@ # Unquoted $@ # $@未加引号 echo \u0026#39;Don\u0026#39;t forget to restart!\u0026#39; # Singlequote closed by apostrophe # 单引号被撇号意外关闭了 echo \u0026#39;Don\\\u0026#39;t try this at home\u0026#39; # Attempting to escape \u0026#39; in \u0026#39;\u0026#39; #试图在单引号括起来的部分中加上一个单引号 echo \u0026#39;Path is $PATH\u0026#39; # Variables in single quotes # 将变量用单引号括起来 trap \u0026#34;echo Took ${SECONDS}s\u0026#34; 0 # Prematurely expanded trap #过早扩展陷阱 条件判断 ShellCheck 可以识别大多数不正确的条件判断语句\n1 2 3 4 5 6 7 8 9 10 11 [[ n != 0 ]] # Constant test expressions # 常量测试表达式 [[ -e *.mpg ]] # Existence checks of globs # 对文件是否存在进行检查时，使用通配符 [[ $foo==0 ]] # Always true due to missing spaces #由于缺乏空格，结果总是为真 [[ -n \u0026#34;$foo \u0026#34; ]] # Always true due to literals #由于字面值存在，结果总是为真 [[ $foo =~ \u0026#34;fo+\u0026#34; ]] # Quoted regex in =~ # 在 =~ 中使用正则表达式 [ foo =~ re ] # Unsupported [ ] operators # 不支持的[]运算符 [ $1 -eq \u0026#34;shellcheck\u0026#34; ] # Numerical comparison of strings # 比较数字和字符串 [ $n \u0026amp;\u0026amp; $m ] # \u0026amp;\u0026amp; in [ .. ] # 在[]中使用\u0026amp;\u0026amp;运算符 [ grep -q foo file ] # Command without $(..) #命令缺少了$(..) [[ \u0026#34;$$file\u0026#34; == *.jpg ]] # Comparisons that can\u0026#39;t succeed #无法成功的比较 (( 1 -lt 2 )) # Using test operators in ((..)) #在((..))中使用比较 常见的对命令的错误使用 ShellCheck 可以识别对一些命令的错误使用\n1 2 3 4 5 6 7 8 9 10 11 grep \u0026#39;*foo*\u0026#39; file # Globs in regex contexts #在grep的正则表达式中前后使用通配符 find . -exec foo {} \u0026amp;\u0026amp; bar {} \\; # Prematurely terminated find -exec # 使find -exec 过早结束 sudo echo \u0026#39;Var=42\u0026#39; \u0026gt; /etc/profile # Redirecting sudo # 重定向sudo time --format=%s sleep 10 # Passing time(1) flags to time builtin # 将time(1)的标志传递给内建的time while read h; do ssh \u0026#34;$h\u0026#34; uptime # Commands eating while loop input # 一个获取输入的while循环中，使用同样会获取输入的命令 alias archive=\u0026#39;mv $1 /backup\u0026#39; # Defining aliases with arguments # 定义使用参数的alias tr -cd \u0026#39;[a-zA-Z0-9]\u0026#39; # [] around ranges in tr # 在tr的参数范围外使用[] exec foo; echo \u0026#34;Done!\u0026#34; # Misused \u0026#39;exec\u0026#39; # 错误地使用exec find -name \\*.bak -o -name \\*~ -delete # Implicit precedence in find # 在find中的隐式优先级 # find . -exec foo \u0026gt; bar \\; # Redirections in find #find中的重定向 f() { whoami; }; sudo f # External use of internal functions #在外部使用内部函数 初学者的常见错误 ShellCheck 识别很多初学者的语法错误\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var = 42 # Spaces around = in assignments #等号两边的空格 $foo=42 # $ in assignments # 对变量赋值时使用了$ for $var in *; do ... # $ in for loop variables # 在循环变量处使用$ var$n=\u0026#34;Hello\u0026#34; # Wrong indirect assignment #错误的变量 echo ${var$n} # Wrong indirect reference #错误的引用 var=(1, 2, 3) # Comma separated arrays #逗号分割数组 array=( [index] = value ) # Incorrect index initialization #错误的索引初始化 echo $var[14] # Missing {} in array references #引用数组缺少{} echo \u0026#34;Argument 10 is $10\u0026#34; # Positional parameter misreference #错误的位置参数引用 if $(myfunction); then ..; fi # Wrapping commands in $() #在命令外加上$() else if othercondition; then .. # Using \u0026#39;else if\u0026#39; #使用else if f; f() { echo \u0026#34;hello world; } # Using function before definition 在函数定义之前使用函数 [ false ] # \u0026#39;false\u0026#39; being true # 此处false为true if ( -f file ) # Using (..) instead of test #使用()取代测试条件 数据和拼写错误 ShellCheck 可以识别一些数据和拼写错误\n1 2 3 4 5 6 7 8 9 10 args=\u0026#34;$@\u0026#34; # Assigning arrays to strings # 将数组赋值给字符串 files=(foo bar); echo \u0026#34;$files\u0026#34; # Referencing arrays as strings # 把数字当成字符串引用 declare -A arr=(foo bar) # Associative arrays without index # 不带索引组合数组 printf \u0026#34;%s\\n\u0026#34; \u0026#34;Arguments: $@.\u0026#34; # Concatenating strings and arrays # 连接字符串和数组 [[ $# \u0026gt; 2 ]] # Comparing numbers as strings # 把数字当成字符串比较 var=World; echo \u0026#34;Hello \u0026#34; var # Unused lowercase variables # 未使用的小写变量 echo \u0026#34;Hello $name\u0026#34; # Unassigned lowercase variables # 未赋值的小写变量 cmd | read bar; echo $bar # Assignments in subshells # 在subshells中进行赋值 cat foo | cp bar # Piping to commands that don\u0026#39;t read # 通过管道传递数据给一个不会做读取的程序 printf \u0026#39;%s: %s\\n\u0026#39; foo # Mismatches in printf argument count # pirintf参数数量不匹配 其他杂七杂八的问题 ShellCheck 可以识别到一些其他问题\n1 2 3 4 5 6 7 8 9 10 11 PS1=\u0026#39;\\e[0;32m\\$\\e[0m \u0026#39; # PS1 colors not in \\[..\\] # PS1 的颜色不在\\[..\\] 中 PATH=\u0026#34;$PATH:~/bin\u0026#34; # Literal tilde in $PATH # $PATH中的波浪号 rm “file” # Unicode quotes #Unicode 引号 echo \u0026#34;Hello world\u0026#34; # Carriage return / DOS line endings # 传输返回DOS行结束符/ echo hello \\ # Trailing spaces after \\ # \\后面的行尾空格 var=42 echo $var # Expansion of inlined environment # 展开内联环境变量 #!/bin/bash -x -e # Common shebang errors # shebang 命令错误 echo $((n/180*100)) # Unnecessary loss of precision # 不必要的精度丢失 ls *[:digit:].txt # Bad character class globs # 不好的通配符 sed \u0026#39;s/foo/bar/\u0026#39; file \u0026gt; file # Redirecting to input # 重定向到输入 while getopts \u0026#34;a\u0026#34; f; do case $f in \u0026#34;b\u0026#34;) # Unhandled getopts flags # 未处理的getopts标志 总结 以上就是shellcheck的介绍了，主要来自其github 的readme ，源码在 github https://github.com/koalaman/shellcheck\n简单实用，只要配置好了，就可以持续为你提供帮助。而且这个是建议性的，可以自己根据实际情况决定是否采纳。即用即弃的临时脚本，那兼容性等就不用太care。长期使用的，就还是完善一下比较稳妥。\n参考博客：https://blog.csdn.net/whatday/article/details/105070638\n微信：https://mp.weixin.qq.com/s/jPZocFgl5OiDochx7rvI6w\n","date":"2021-06-12T21:33:26Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/202106122133/","title":"再也不用担心Shell脚本出错-ShellCheck"},{"content":"Shell帮你掌管上千台服务（多线程） 日常服务器运维时，我们都会批量管理多台服务器。\n脚本，批量化，使我们工作中必不可少的。\n首先我们需要一台堡垒机，负责免秘钥的登录和分发任务等等\n堡垒机到主机\n需要一套ssh-keygen来管理\nauthorized_keys： 授权密钥 id_rsa ：私钥 id_rsa.pub ：公钥 known_hosts：连接过机器记录信息，不需要属于“yes” 这些都是前面基础环境和工作。\n默认脚本 这个脚本是最初始化的，可以远程过去执行任务，并打印结果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #/bin/bash START_TIME=`date +%s` for i in `cat /opt/wei/pam_ip.txt` #ip存放的文件 do cc=\u0026#34;ssh deployer@$i \\\u0026#34;sudo cat /etc/ssh/sshd_config | grep -Ev \u0026#39;^#\u0026#39; | grep UsePAM\\\u0026#34; \u0026#34; kk=`echo $cc|bash` if [ ! -n \u0026#34;$kk\u0026#34; ]; then echo \u0026#34;IP: $i UsePAM no seting\u0026#34; sudo echo \u0026#34;IP: $i UsePAM no seting\u0026#34; \u0026gt;\u0026gt; /opt/wei/pam_no.txt else echo \u0026#34;IP: $i $kk\u0026#34; sudo echo \u0026#34;IP: $i $kk\u0026#34; \u0026gt;\u0026gt; /opt/wei/pam_yes.txt fi done END_TIME=`date +%s` EXECUTING_TIME=`expr $END_TIME - $START_TIME` echo \u0026#34;================end=====================\u0026#34; echo \u0026#34;程序运行时长：$EXECUTING_TIME S\u0026#34; 脚本优化（加入线程概念） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #/bin/bash START_TIME=`date +%s` for i in `cat /opt/wei/pam_ip.txt` do { cc=\u0026#34;ssh deployer@$i \\\u0026#34;sudo cat /etc/ssh/sshd_config | grep -Ev \u0026#39;^#\u0026#39; | grep UsePAM\\\u0026#34; \u0026#34; kk=`echo $cc|bash` if [ ! -n \u0026#34;$kk\u0026#34; ]; then echo \u0026#34;IP: $i UsePAM no seting\u0026#34; sudo echo \u0026#34;IP: $i UsePAM no seting\u0026#34; \u0026gt;\u0026gt; /opt/wei/pam_no.txt else echo \u0026#34;IP: $i $kk\u0026#34; sudo echo \u0026#34;IP: $i $kk\u0026#34; \u0026gt;\u0026gt; /opt/we/pam_yes.txt fi }\u0026amp; done wait END_TIME=`date +%s` EXECUTING_TIME=`expr $END_TIME - $START_TIME` echo \u0026#34;================end=====================\u0026#34; echo \u0026#34;程序运行：$EXECUTING_TIME S\u0026#34; 使用**\u0026rsquo;\u0026amp;\u0026rsquo;+wait** 实现**“多进程”**实现\n运行很快，而且很不老实（顺序都乱了,大概是因为expr运算所花时间不同）\n解析：这一个脚本的变化是在命令后面增加了\u0026amp;标记，意思是将进程扔到后台。在shell中，后台命令之间是不区分先来后到关系的。所以各后台子进程会抢夺资源进行运算。\nwait命令：\nwait [n]\nn 表示当前shell中某个执行的后台命令的pid，wait命令会等待该后台进程执行完毕才允许下一个shell语句执行；如果没指定则代表当前shell后台执行的语句，wait会等待到所有的后台程序执行完毕为止。\n如果没有wait，后面的shell语句是不会等待后台进程的，一些对前面后台进程有依赖关系的命令执行就不正确了\n自定义线程数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #!/bin/bash Nproc=20 #最大并发进程数 function PushQue { #将PID值追加到队列中 Que=\u0026#34;$Que $1\u0026#34; Nrun=$(($Nrun+1)) } function GenQue { #更新队列信息，先清空队列信息，然后检索生成新的队列信息 OldQue=$Que Que=\u0026#34;\u0026#34;; Nrun=0 for PID in $OldQue; do if [[ -d /proc/$PID ]]; then PushQue $PID fi done } function ChkQue { #检查队列信息，如果有已经结束了的进程的PID，那么更新队列信息 OldQue=$Que for PID in $OldQue; do if [[ ! -d /proc/$PID ]]; then GenQue; break fi done } for i in `cat /opt/wei/pam_ip.txt` do { cc=\u0026#34;ssh deployer@$i \\\u0026#34;sudo cat /etc/ssh/sshd_config | grep -Ev \u0026#39;^#\u0026#39; | grep UsePAM\\\u0026#34; \u0026#34; kk=`echo $cc|bash` if [ ! -n \u0026#34;$kk\u0026#34; ]; then echo \u0026#34;IP: $i UsePAM no seting\u0026#34; sudo echo \u0026#34;IP: $i UsePAM no seting\u0026#34; \u0026gt;\u0026gt; /opt/wei/pam_no.txt else echo \u0026#34;IP: $i $kk\u0026#34; sudo echo \u0026#34;IP: $i $kk\u0026#34; \u0026gt;\u0026gt; /opt/wei/pam_yes.txt fi }\u0026amp; sleep 0.1 #考虑有序，开启这个参数，速度优先，则注释掉 PID=$! PushQue $PID while [[ $Nrun -ge $Nproc ]]; do # 如果Nrun大于Nproc，就一直ChkQue ChkQue sleep 0.1 done done wait echo -e \u0026#34;time-consuming: $SECONDS seconds\u0026#34; #显示脚本执行耗时#!/bin/bash 使用模拟队列来控制进程数量\n要控制后台同一时刻的进程数量，需要在原有循环的基础上增加管理机制。\n一个方法是以for循环的子进程PID做为队列元素，模拟一个限定最大进程数的队列（只是一个长度固定的数组，并不是真实的队列）。队列的初始长度为0，循环每创建一个进程，就让队列长度+1。当队列长度到达设置的并发进程限制数之后，每隔一段时间检查队列，如果队列长度还是等于限制值，那么不做操作，继续轮询；如果检测到有并发进程执行结束了，那么队列长度-1，轮询检测到队列长度小于限制值后，会启动下一个待执行的进程，直至所有等待执行的并发进程全部执行完。\n定义20线程，速度从原来的311S，降到40S，大大提高效率\n","date":"2021-06-09T20:42:08Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202106092042/","title":"Shell帮你掌管上千台服务（多线程）"},{"content":"博客：https://ownit.top/ Centos7脚本自动安装并注册zabbix服务端 此脚本可以自动安装zabbix客户端并且配置（注意修改）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #zabbix-agent4.0.4 #centos7系列安装zabbix监控 #获取影院IP #ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk \u0026#39;{print $2}\u0026#39;|tr -d \u0026#34;addr:\u0026#34; #首先ip获取要测试一下，双ip，多ip都会有影响 localname=`hostname` conf=\u0026#34;/etc/init.d/zabbix-agent\u0026#34; installzabbix(){ if [[ -f $conf ]]; then echo \u0026#34;zabbix_agentd already install\u0026#34; else /etc/init.d/zabbix_agentd stop rm -rf /etc/init.d/zabbix_agentd rm -rf /usr/local/zabbix yum -y install https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/4.2/rhel/7/x86_64/zabbix-agent-4.2.8-1.el7.x86_64.rpm #delconf sed -i \u0026#39;/^Server/d\u0026#39; /etc/zabbix/zabbix_agentd.conf sed -i \u0026#39;/^Hostname/d\u0026#39; /etc/zabbix/zabbix_agentd.conf sed -i \u0026#39;/^HostMetadata/d\u0026#39; /etc/zabbix/zabbix_agentd.conf #addconf echo \u0026#34;Server=10.10.10.1\u0026#34; \u0026gt;\u0026gt; /etc/zabbix/zabbix_agentd.conf echo \u0026#34;ServerActive=10.10.10.1\u0026#34; \u0026gt;\u0026gt; /etc/zabbix/zabbix_agentd.conf echo \u0026#34;HostMetadata=ownit\u0026#34; \u0026gt;\u0026gt; /etc/zabbix/zabbix_agentd.conf echo Hostname=$localname \u0026gt;\u0026gt; /etc/zabbix/zabbix_agentd.conf fi } installzabbix average(){ #负载监控average echo \u0026#34;UserParameter=average[*],uptime|awk \u0026#39;{print \\$NF}\u0026#39;\u0026#34; \u0026gt; /etc/zabbix/zabbix_agentd.d/average.conf } average restzabbix(){ chkconfig zabbix-agent on systemctl restart zabbix-agent.service } restzabbix zabbix的基本配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #zabbix-agent运行的pid PidFile=/var/run/zabbix/zabbix_agentd.pid #zabbix-aget运行的日志 LogFile=/var/log/zabbix/zabbix_agentd.log #zabbix-agent运行的日志大小，当设置为0时，表示不进行日志轮询 LogFileSize=0 #zabbix-agent的目录路径或扩展配置文件路径 Include=/etc/zabbix/zabbix_agentd.d/*.conf #zabbix server的ip地址，多个ip使用逗号分隔 Server=10.150.11.205,10.50.11.205 #zabbix 主动监控server的ip地址，使用逗号分隔多IP，如果注释这个选项，那么当前服务器的主动监控就被禁用了 ServerActive=10.150.11.205,10.50.11.205 #zabbix-agent的数据源，仅用于主机自动注册功能 HostMetadata=ownit #zabbix-agent的主机名，必须唯一，区分大小写。Hostname必须和zabbix web上配置的一直，否则zabbix主动监控无法正常工作 Hostname=10.10.3.240 #zabbix-agent是否启用用户自定义监控脚本，1启用，0不启用 UnsafeUserParameters=1 Zabbix服务端配置 ","date":"2021-05-22T15:52:23Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202105221552/","title":"自动安装Zabbix-agent 自动注册"},{"content":"1、向各主机分发秘钥 方法一\n1 2 3 4 5 6 7 8 9 10 --- - name: 配置ssh免秘钥连接 hosts: new gather_facts: false connection: local #本地连接 tasks: - name: configure ssh connection shell: | ssh-keyscan {{inventory_hostname}} \u0026gt;\u0026gt;~/.ssh/known_hosts sshpass -p\u0026#39;密码\u0026#39; ssh-copy-id root@{{inventory_hostname}} 方法二 1 2 3 4 5 6 7 8 9 --- - name: configure ssh connection hosts: new gather_facts: false tasks: - authorized_key: key: \u0026#34;{{lookup(\u0026#39;file\u0026#39;,\u0026#39;~/.ssh/id_rsa.pub\u0026#39;)}}\u0026#34; state: present user: root 执行该playbook，主机加上了-k选项，它会提示用户输入ssh连接密码。如果所有目标主机的密码都相同，则只需输入一次即可：\n1 2 3 4 5 6 7 8 9 $ ansible-playbook -k anth_key.yml SSH password: PLAY [configure ssh connection] *********** TASK [authorized_key] ********************* changed: [192.168.200.34] changed: [192.168.200.35] ...... 2、循环创建文件 方法一\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- - name: play1 hosts: localhost gather_facts: false tasks: - name: create /tmp/test1 file: path: /tmp/test1 state: directory - name: create /tmp/test2 file: path: /tmp/test2 state: directory 方法二\n1 2 3 4 5 6 7 8 9 10 11 12 --- - name: play1 hosts: localhost gather_facts: false tasks: - name: create directories file: path: \u0026#34;{{item}}\u0026#34; state: directory loop: - /tmp/test1 - /tmp/test2 3、设置多个主机名 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- - name: set hostname hosts: new gather_facts: false vars: hostnames: - host: 192.168.200.34 name: new1 - host: 192.168.200.35 name: new2 tasks: - name: set hostname hostname: name: \u0026#34;{{item.name}}\u0026#34; when: item.host == inventory_hostname loop: \u0026#34;{{hostnames}}\u0026#34; 4、主机之间相互添加DNS 1 2 3 4 5 6 7 8 9 10 11 --- - name: add DNS for each hosts: new gather_facts: true tasks: - name: add DNS lineinfile: path: \u0026#34;/etc/hosts\u0026#34; line: \u0026#34;{{item}} {{hostvars[item].ansible_hostname}}\u0026#34; when: item != inventory_hostname loop: \u0026#34;{{ play_hosts }}\u0026#34; 5、添加yum源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 --- - name: config yum repo and install software hosts: new gather_facts: false tasks: - name: backup origin yum repos shell: cmd: \u0026#34;mkdir bak; mv *.repo bak\u0026#34; chdir: /etc/yum.repos.d creates: /etc/yum.repos.d/bak - name: add os repo and epel repo yum_repository: name: \u0026#34;{{item.name}}\u0026#34; description: \u0026#34;{{item.name}} repo\u0026#34; baseurl: \u0026#34;{{item.baseurl}}\u0026#34; file: \u0026#34;{{item.name}}\u0026#34; enabled: 1 gpgcheck: 0 reposdir: /etc/yum.repos.d loop: - name: os baseurl: \u0026#34;https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch\u0026#34; - name: epel baseurl: \u0026#34;https://mirrors.tuna.tsinghua.edu.cn/epel/$releasever/$basearch\u0026#34; - name: install pkgs yum: name: lrzsz,vim,dos2unix,wget,curl state: present 6、时间同步 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- - name: sync time hosts: new gather_facts: false tasks: - name: install and sync time block: - name: install ntpdate yum: name: ntpdate state: present - name: ntpdate to sync time shell: | ntpdate ntp1.aliyun.com hwclock -w - name: date_show shell: | date +%F-%T 7、关闭selinux 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- - name: disable selinux hosts: new gather_facts: false tasks: - block: - name: disable on the fly shell: setenforce 0 - name: disable forever in config lineinfile: path: /etc/selinux/config line: \u0026#34;SELINUX=disabled\u0026#34; regexp: \u0026#39;^SELINUX=\u0026#39; ignore_errors: true 8、配置防火墙 方法一\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 --- - name: set firewall hosts: new gather_facts: false tasks: - name: set iptables rule shell: | # 备份已有规则 iptables-save \u0026gt; /tmp/iptables.bak$(date +\u0026#34;%F-%T\u0026#34;) # 给它三板斧 iptables -X iptables -F iptables -Z # 放行lo网卡和允许ping iptables -A INPUT -i lo -j ACCEPT iptables -A INPUT -p icmp -j ACCEPT # 放行关联和已建立连接的包，放行22、443、80端口 iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT iptables -A INPUT -p tcp -m tcp --dport 443 -j ACCEPT iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT # 配置filter表的三链默认规则，INPUT链丢弃所有包 iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT ACCEPT 方法二\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 --- - name: set firewall hosts: new gather_facts: false vars: allowed_tcp_ports: [22,80,443] default_policies: INPUT: DROP FORWARD: DROP OUTPUT: ACCEPT user_iptables_rule: - iptables -A INPUT -p tcp -m tcp --dport 8000 -j ACCEPT - iptables -A INPUT -p tcp -m tcp --dport 8080 -j ACCEPT tasks: - block: - name: backup and empty rules shell: | # 备份已有规则，并清空规则等 iptables-save \u0026gt; /tmp/iptables.bak$(date +\u0026#34;%F-%T\u0026#34;) iptables -X iptables -F iptables -Z - name: green light for lo interface and icmp protocol shell: | # 放行lo接口、ping和已建立连接的包 iptables -A INPUT -i lo -j ACCEPT iptables -A INPUT -p icmp -j ACCEPT iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 放行用户指定的tcp端口列表 - name: allow for given tcp port shell: iptables -A INPUT -p tcp -m tcp --dport {{item}} -j ACCEPT loop: \u0026#34;{{ allowed_tcp_ports | default([]) }}\u0026#34; # 执行用户自定义的iptables命令 - name: execute user iptables command shell: \u0026#34;{{item}}\u0026#34; loop: \u0026#34;{{user_iptables_rule | default([]) }}\u0026#34; # 设置filter表三链的默认规则 - name: default policies for filter table shell: iptables -P {{item.key}} {{item.value}} loop: \u0026#34;{{ query(\u0026#39;dict\u0026#39;, default_policies | default({})) }}\u0026#34; 9、远程修改sshd配置文件并重启 采用lineinfile模块去修改配置文件，要修改的内容只有两项： 1.将PermitRootLogin指令设置为no，禁止root用户直接登录 2.将PasswordAuthentication指令设置为no，不允许使用密码认证的方式登录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 --- - name: modify sshd_config hosts: new gather_facts: false tasks: # 1. 备份/etc/ssh/sshd_config文件 - name: backup sshd config shell: /usr/bin/cp -f {{path}} {{path}}.bak vars: - path: /etc/ssh/sshd_config # 2. 设置PermitRootLogin no - name: disable root login lineinfile: path: \u0026#34;/etc/ssh/sshd_config\u0026#34; line: \u0026#34;PermitRootLogin no\u0026#34; insertafter: \u0026#34;^#PermitRootLogin\u0026#34; regexp: \u0026#34;^PermitRootLogin\u0026#34; notify: \u0026#34;restart sshd\u0026#34; # 3. 设置PasswordAuthentication no - name: disable password auth lineinfile: path: \u0026#34;/etc/ssh/sshd_config\u0026#34; line: \u0026#34;PasswordAuthentication no\u0026#34; regexp: \u0026#34;^PasswordAuthentication yes\u0026#34; notify: \u0026#34;restart sshd\u0026#34; handlers: - name: \u0026#34;restart sshd\u0026#34; service: name: sshd state: restarted 通过一个入口文件引入所有这些任务文件将它们组织起来。假设入口文件名为main.yaml，其内容为：\n1 2 3 4 5 6 7 8 9 --- - import_playbook: \u0026#34;init_server/sshkey.yaml\u0026#34; - import_playbook: \u0026#34;init_server/hostname.yaml\u0026#34; - import_playbook: \u0026#34;init_server/add_dns.yaml\u0026#34; - import_playbook: \u0026#34;init_server/add_repos.yaml\u0026#34; - import_playbook: \u0026#34;init_server/synctime.yaml\u0026#34; - import_playbook: \u0026#34;init_server/disable_selinux.yaml\u0026#34; - import_playbook: \u0026#34;init_server/iptables.yaml\u0026#34; - import_playbook: \u0026#34;init_server/sshd_config.yaml\u0026#34; ","date":"2021-05-05T00:07:08Z","image":"https://www.ownit.top/title_pic/56.jpg","permalink":"https://www.ownit.top/p/202105050007/","title":"Ansible小实例"},{"content":"Shell监控公网IP-变化邮件报警\n公司用的网线IP，但是有时IP会改变，导致部分业务有问题，我们又不能及时发现，会造成一定的影响。\n现在使用shell监控公网的IP，如发生变化，立即邮件报警。\n企业级-Shell案例2——发送告警邮件 centos邮件报警可以参考这个，默认是mailx\n脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash dirfile=\u0026#39;/home/ip_change\u0026#39; new_ip=`curl icanhazip.com` #获取新公网ip mail_user=1794@qq.com #接收收邮件邮箱 mail_subject=\u0026#34;IP已经发生变化，及时处理\u0026#34; #邮件主题 log=\u0026#34;/var/log/tool.log\u0026#34; datetime=`date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;` #判断文件是否存在 if [ ! -f \u0026#34;$dirfile\u0026#34; ]; then touch \u0026#34;$file\u0026#34; echo \u0026#34;1.1.1.1\u0026#34; \u0026gt; $dirfile fi #判断new_ip是否获取 if [ ! -n \u0026#34;$new_ip\u0026#34; ]; then echo \u0026#34;$datetime 公网IP获取失败，检查\u0026#39;curl icanhazip.com\u0026#39; \u0026#34; \u0026gt;\u0026gt; $log exit 1 fi old_ip=`cat $dirfile` #查看旧ip # 判断两个IP是否相等 发邮件 if [ \u0026#34;$new_ip\u0026#34; = \u0026#34;$old_ip\u0026#34; ]; then echo \u0026#34;$datetime IP正常 - true \u0026#34; \u0026gt;\u0026gt; $log else echo $new_ip \u0026gt; $dirfile echo \u0026#34;IP已经发生变化, 新IP: $new_ip 旧IP： $old_ip !!! \u0026#34; | mail -s \u0026#34;$mail_subject\u0026#34; \u0026#34;$mail_user\u0026#34; echo \u0026#34;$datetime IP已经发生变化 - error 新IP ：$new_ip 旧IP： $old_ip\u0026#34; \u0026gt;\u0026gt; $log fi ","date":"2021-05-01T13:58:35Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/202105011358/","title":"Shell监控公网IP-变化邮件报警"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） Kubernetes实战模拟五（wordpress的HPA自动扩缩容） Kubernetes实战模拟六（wordpress的账号信息加密） 源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\nKubernetes实战模拟六，已经构建wordpress的账号信息加密，和时间校准。\n版本6 思路：想mysql和wordpress的数据持久化，总不能把数据放到容器中，如果容器重启或者删除，那么数据将都会消失的。接下来我们采用nfs实践（其实后端存储使用ceph比较好，安全性和高可用。这边方便演示采用nfs）\nNFS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #安装 yum install -y nfs-utils rpcbind mkdir -p /data/nfsdata # 修改配置 $ vim /etc/exports /data/nfsdata 192.168.31.* (rw,async,no_root_squash) # 使配置生效 $ exportfs -r # 服务端查看下是否生效 $ showmount -e localhost Export list for localhost: /data/nfsdata (everyone) helm安装nfs-client 1 2 stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm添加这个源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 下载helm包 helm pull aliyuncs/nfs-client-provisioner 解压 tar -zxvf nfs-client-provisioner-1.2.8.tgz 修复values.yaml 三处 image: repository: quay.io/external_storage/nfs-client-provisioner tag: v3.1.0-k8s1.11 pullPolicy: IfNotPresent nfs: server: 192.168.31.73 path: /data/nfsdata reclaimPolicy: Retain 1 2 3 4 5 安装 helm install nfs-client-provisioner -n nfs . 卸载 helm uninstall -n nfs nfs-client-provisioner mysql数据持久化 mysql-nfs.yaml.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-nfs namespace: kube-example labels: app: mysql spec: storageClassName: \u0026#34;nfs-client\u0026#34; #存储后端 accessModes: - ReadWriteOnce #允许一个容器连接，读写 resources: requests: storage: 1G #存储量 mysql.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 apiVersion: v1 kind: Service metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress spec: selector: app: wordpress tier: mysql ports: - port: 3306 targetPort: dbport --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress tier: mysql spec: replicas: 1 template: metadata: name: wordpress-mysql labels: app: wordpress tier: mysql spec: containers: - name: mysql image: mysql:5.6 args: - --character-set-server=utf8mb4 - --collation-server=utf8mb4_unicode_ci volumeMounts: - mountPath: /var/lib/mysql name: mysql-nfs ports: - containerPort: 3306 name: dbport env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: key: MYSQL_ROOT_PASSWORD name: db.conf - name: MYSQL_DATABASE valueFrom: secretKeyRef: key: MYSQL_DATABASE name: db.conf - name: MYSQL_USER valueFrom: secretKeyRef: key: WORDPRESS_DB_USER name: db.conf - name: MYSQL_PASSWORD valueFrom: secretKeyRef: key: WORDPRESS_DB_PASSWORD name: db.conf imagePullPolicy: IfNotPresent resources: limits: cpu: 1000m memory: 400Mi requests: cpu: 1000m memory: 400Mi startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） tcpSocket: port: 3306 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 10 # 执行探测的频率。默认是10秒，最小1秒。 restartPolicy: Always volumes: - name: mysql-nfs persistentVolumeClaim: claimName: mysql-nfs selector: matchLabels: app: wordpress tier: mysql wordpress数据持久化 wordpress-nfs.yaml 但是由于 Wordpress 应用是多个副本，所以需要同时在多个节点进行读写，也就是 accessModes 需要 ReadWriteMany 模式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: wordpress-nfs namespace: kube-example labels: app: wordpress spec: storageClassName: \u0026#34;nfs-client\u0026#34; accessModes: - ReadWriteMany resources: requests: storage: 2G wordpress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 3 #多副本+pod的反亲合力可以实现pod的高可用 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport #pvc挂载到容器目录 volumeMounts: - mountPath: /var/www/html name: wordpress-nfs env: - name: WORDPRESS_DB_HOST valueFrom: secretKeyRef: key: WORDPRESS_DB_HOST name: db.conf - name: WORDPRESS_DB_USER valueFrom: secretKeyRef: key: WORDPRESS_DB_USER name: db.conf - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: key: WORDPRESS_DB_PASSWORD name: db.conf imagePullPolicy: IfNotPresent resources: limits: cpu: 800m memory: 150Mi requests: cpu: 800m memory: 150Mi startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） httpGet: port: 80 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 5 # 执行探测的频率。默认是10秒，最小1秒。 readinessProbe: # （就绪检查） # 如果检查失败，kubernetes会把Pod从service endpoints中剔除 tcpSocket: port: 80 initialDelaySeconds: 10 timeoutSeconds: 5 failureThreshold: 5 periodSeconds: 5 successThreshold: 3 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - wordpress restartPolicy: Always #挂载的pvc volumes: - name: wordpress-nfs persistentVolumeClaim: claimName: wordpress-nfs 测试 现在删除或者重启pod的数据都不会丢失，我们已经把数据放在固定的节点，通过网络进行访问\n但是，NFS只适合测试环境，或者数据不重要的前提下。如果数据重要，我们需要使用ceph来作为Kubernetes的存储后端\n好了，我们wordpress模拟接近尾声，但是我这些只完成一版，生产环境远远比这复杂和安全，我们后期可以不断慢慢的完善\n","date":"2021-04-08T00:01:28Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202104080001/","title":"Kubernetes实战模拟七（wordpress的数据持久化）"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） Kubernetes实战模拟五（wordpress的HPA自动扩缩容） 源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\nKubernetes实战模拟五，已经构建wordpress的HPA自动扩缩容，可以面对业务并发\n版本6 思路：想mysql和wordpress，交互连接时，需要账号和密码，我们不希望明文显示，这里我们可以使用secrets加密，然后挂载到env环境中。\n安全性这个和具体的业务应用有关系，比如我们这里的 Wordpress 也就是数据库的密码属于比较私密的信息，我们可以使用 Kubernetes 中的 Secret 资源对象来存储比较私密的信息\nsecrets db-secrets.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 data: WORDPRESS_DB_HOST: d29yZHByZXNzLW15c3FsOjMzMDY= WORDPRESS_DB_PASSWORD: d29yZHByZXNz WORDPRESS_DB_USER: d29yZHByZXNz MYSQL_ROOT_PASSWORD: cm9vdFBhc3NXMHJk MYSQL_DATABASE: d29yZHByZXNz kind: Secret metadata: name: db.conf namespace: kube-example type: Opaque 这边当时遇见一个问题 https://blog.csdn.net/heian_99/article/details/115486589 已经解决，可以参照这个。\n然后将Deployment 资源对象中的数据库密码环境变量通过 Secret 对象读取\nmysql.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 apiVersion: v1 kind: Service metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress spec: selector: app: wordpress tier: mysql ports: - port: 3306 targetPort: dbport --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress tier: mysql spec: replicas: 1 template: metadata: name: wordpress-mysql labels: app: wordpress tier: mysql spec: containers: - name: mysql image: mysql:5.7 args: - --default_authentication_plugin=mysql_native_password - --character-set-server=utf8mb4 - --collation-server=utf8mb4_unicode_ci ports: - containerPort: 3306 name: dbport #环境注入 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: key: MYSQL_ROOT_PASSWORD name: db.conf - name: MYSQL_DATABASE valueFrom: secretKeyRef: key: MYSQL_DATABASE name: db.conf - name: MYSQL_USER valueFrom: secretKeyRef: key: WORDPRESS_DB_USER name: db.conf - name: MYSQL_PASSWORD valueFrom: secretKeyRef: key: WORDPRESS_DB_PASSWORD name: db.conf imagePullPolicy: IfNotPresent resources: limits: cpu: 1000m memory: 400Mi requests: cpu: 1000m memory: 400Mi startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） tcpSocket: port: 3306 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 10 # 执行探测的频率。默认是10秒，最小1秒。 restartPolicy: Always selector: matchLabels: app: wordpress tier: mysql wordpress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 3 #多副本+pod的反亲合力可以实现pod的高可用 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport # 环境注入 env: - name: WORDPRESS_DB_HOST valueFrom: secretKeyRef: key: WORDPRESS_DB_HOST name: db.conf - name: WORDPRESS_DB_USER valueFrom: secretKeyRef: key: WORDPRESS_DB_USER name: db.conf - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: key: WORDPRESS_DB_PASSWORD name: db.conf imagePullPolicy: IfNotPresent resources: limits: cpu: 800m memory: 150Mi requests: cpu: 800m memory: 150Mi startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） httpGet: port: 80 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 5 # 执行探测的频率。默认是10秒，最小1秒。 readinessProbe: # （就绪检查） # 如果检查失败，kubernetes会把Pod从service endpoints中剔除 tcpSocket: port: 80 initialDelaySeconds: 10 timeoutSeconds: 5 failureThreshold: 5 periodSeconds: 5 successThreshold: 3 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - wordpress restartPolicy: Always 这样我们就不会在 YAML 文件中看到明文的数据库密码了，当然安全性都是相对的，Secret 资源对象也只是简单的将密码做了一次 Base64 编码而已，对于一些特殊场景安全性要求非常高的应用，就需要使用其他功能更加强大的密码系统来进行管理了，比如 Vault。\n我们的配置也让Secret管理，也方便修改和配置\n","date":"2021-04-07T16:57:04Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/202104071657/","title":"Kubernetes实战模拟六（wordpress的账号信息加密）"},{"content":"错误问题 原因：把MySQL的用户密码配置成secrets，然后挂载mysql，mysql会一直报错，无法完成初始\n1 2 3 4 5 6 7 8 9 Version: \u0026#39;5.7.33\u0026#39; socket: \u0026#39;/var/run/mysqld/mysqld.sock\u0026#39; port: 0 MySQL Community Server (GPL) 2021-04-07 15:06:32+08:00 [Note] [Entrypoint]: Temporary server started. Warning: Unable to load \u0026#39;/usr/share/zoneinfo/iso3166.tab\u0026#39; as time zone. Skipping it. Warning: Unable to load \u0026#39;/usr/share/zoneinfo/leap-seconds.list\u0026#39; as time zone. Skipping it. 2021-04-07T07:06:58.526144Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 24159ms. The settings might not be optimal. (flushed=200 and evicted=0, during the time.) Warning: Unable to load \u0026#39;/usr/share/zoneinfo/zone.tab\u0026#39; as time zone. Skipping it. Warning: Unable to load \u0026#39;/usr/share/zoneinfo/zone1970.tab\u0026#39; as time zone. Skipping it. 2021-04-07 15:06:58+08:00 [Note] [Entrypoint]: Creating database wordpress mysql: [ERROR] unknown option \u0026#39;--\u0026#34;\u0026#39; 解决问题 问题是我使用该echo 'secret' | base64命令创建了我的机密，而echo命令自动插入了结尾的换行符。\n使用echo \\-n 'secret' | base64代替。✔️\n因此我没有注意到日志输出中有换行符。希望这可以帮助一些也使用echo命令对base64进行编码的人。\n参考文章： https://stackoverflow.com/questions/62985541/mysql-unknown-option-in-kubernetes#\n","date":"2021-04-07T15:28:32Z","image":"https://www.ownit.top/title_pic/49.jpg","permalink":"https://www.ownit.top/p/202104071528/","title":"mysql-unknown-option-in-kubernetes错误"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） 源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\nKubernetes实战模拟四，已经构建wordpress的更新升级策略，确保后期升级时，系统能够稳定的提供业务，不被升级带来影响\n版本5 思路：类似wordpress是静态服务，当数据访问过大时，内存和cpu都会上升，这时，我们可以通过提高pod的数量，来分解压力。我们有不可能实时监控数据，手动扩缩容、所有我们采用HPA的自动扩缩容来实现动态控制。\nHPA是kubernetes里面pod弹性伸缩的实现,它能根据设置的监控阀值进行pod的弹性扩缩容，目前默认HPA只能支持cpu和内存的阀值检测扩缩容，但也可以通过custom metric api 调用prometheus实现自定义metric 来更加灵活的监控指标实现弹性伸缩。但hpa不能用于伸缩一些无法进行缩放的控制器如DaemonSet。这里我们用的是resource metric api.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: php-apache namespace: default spec: # HPA的伸缩对象描述，HPA会动态修改该对象的pod数量 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache # HPA的最小pod数量和最大pod数量 minReplicas: 1 maxReplicas: 10 # 监控的指标数组，支持多种类型的指标共存 metrics: # Object类型的指标 - type: Object object: metric: # 指标名称 name: requests-per-second # 监控指标的对象描述，指标数据来源于该对象 describedObject: apiVersion: networking.k8s.io/v1beta1 kind: Ingress name: main-route # Value类型的目标值，Object类型的指标只支持Value和AverageValue类型的目标值 target: type: Value value: 10k # Resource类型的指标 - type: Resource resource: name: cpu # Utilization类型的目标值，Resource类型的指标只支持Utilization和AverageValue类型的目标值 target: type: Utilization averageUtilization: 50 # Pods类型的指标 - type: Pods pods: metric: name: packets-per-second # AverageValue类型的目标值，Pods指标类型下只支持AverageValue类型的目标值 target: type: AverageValue averageValue: 1k # External类型的指标 - type: External external: metric: name: queue_messages_ready # 该字段与第三方的指标标签相关联，（此处官方文档有问题，正确的写法如下） selector: matchLabels: env: \u0026#34;stage\u0026#34; app: \u0026#34;myapp\u0026#34; # External指标类型下只支持Value和AverageValue类型的目标值 target: type: AverageValue averageValue: 30 autoscaling/v1版本将metrics字段放在了annotation中进行处理。\ntarget共有3种类型：Utilization、Value、AverageValue。Utilization表示平均使用率；Value表示裸值；AverageValue表示平均值。\nmetrics中的type字段有四种类型的值：Object、Pods、Resource、External。\nResource指的是当前伸缩对象下的pod的cpu和memory指标，只支持Utilization和AverageValue类型的目标值。\nObject指的是指定k8s内部对象的指标，数据需要第三方adapter提供，只支持Value和AverageValue类型的目标值。\nPods指的是伸缩对象（statefulSet、replicaController、replicaSet）底下的Pods的指标，数据需要第三方的adapter提供，并且只允许AverageValue类型的目标值。\nExternal指的是k8s外部的指标，数据同样需要第三方的adapter提供，只支持Value和AverageValue类型的目标值。 HPA动态伸缩的原理\nHPA在k8s中也由一个controller控制，controller会间隔循环HPA，检查每个HPA中监控的指标是否触发伸缩条件，默认的间隔时间为15s。一旦触发伸缩条件，controller会向k8s发送请求，修改伸缩对象（statefulSet、replicaController、replicaSet）子对象scale中控制pod数量的字段。k8s响应请求，修改scale结构体，然后会刷新一次伸缩对象的pod数量。伸缩对象被修改后，自然会通过list/watch机制增加或减少pod数量，达到动态伸缩的目的。 HPA伸缩过程叙述\nHPA的伸缩主要流程如下： 判断当前pod数量是否在HPA设定的pod数量区间中，如果不在，过小返回最小值，过大返回最大值，结束伸缩。 判断指标的类型，并向api server发送对应的请求，拿到设定的监控指标。一般来说指标会根据预先设定的指标从以下三个aggregated APIs中获取：metrics.k8s.io、custom.metrics.k8s.io、 external.metrics.k8s.io。其中metrics.k8s.io一般由k8s自带的metrics-server来提供，主要是cpu，memory使用率指标，另外两种需要第三方的adapter来提供。custom.metrics.k8s.io提供自定义指标数据，一般跟k8s集群有关，比如跟特定的pod相关。external.metrics.k8s.io同样提供自定义指标数据，但一般跟k8s集群无关。许多知名的第三方监控平台提供了adapter实现了上述api（如prometheus），可以将监控和adapter一同部署在k8s集群中提供服务，甚至能够替换原来的metrics-server来提供上述三类api指标，达到深度定制监控数据的目的。 根据获得的指标，应用相应的算法算出一个伸缩系数，并乘以目前pod数量获得期望pod数量。系数是指标的期望值与目前值的比值，如果大于1表示扩容，小于1表示缩容。指标数值有平均值（AverageValue）、平均使用率（Utilization）、裸值（Value）三种类型，每种类型的数值都有对应的算法。以下几点值得注意：如果系数有小数点，统一进一；系数如果未达到某个容忍值，HPA认为变化太小，会忽略这次变化，容忍值默认为0.1。\nHPA扩容算法是一个非常保守的算法。如果出现获取不到指标的情况，扩容时算最小值，缩容时算最大值；如果需要计算平均值，出现pod没准备好的情况，平均数的分母不计入该pod。\n一个HPA支持多个指标的监控，HPA会循环获取所有的指标，并计算期望的pod数量，并从期望结果中获得最大的pod数量作为最终的伸缩的pod数量。一个伸缩对象在k8s中允许对应多个HPA，但是只是k8s不会报错而已，事实上HPA彼此不知道自己监控的是同一个伸缩对象，在这个伸缩对象中的pod会被多个HPA无意义地来回修改pod数量，给系统增加消耗，如果想要指定多个监控指标，可以如上述所说，在一个HPA中添加多个监控指标。 检查最终的pod数量是否在HPA设定的pod数量范围的区间，如果超过最大值或不足最小值都会修改为最大值或最小值。然后向k8s发出请求，修改伸缩对象的子对象scale的pod数量，结束一个HPA的检查，获取下一个HPA，完成一个伸缩流程。 参照博客:https://www.cnblogs.com/yuhaohao/p/14109787.html\nhttps://zhuanlan.zhihu.com/p/89453704\nHPA 现在应用是固定的3个副本，但是往往在生产环境流量是不可控的，很有可能一次活动就会有大量的流量，3个副本很有可能抗不住大量的用户请求，这个时候我们就希望能够自动对 Pod 进行伸缩，直接使用前面我们学习的 HPA 这个资源对象就可以满足我们的需求了。\n直接使用kubectl autoscale命令来创建一个 HPA 对象\n1 2 3 4 5 [root@k8s-master1 ~]# kubectl autoscale deployment wordpress --namespace kube-example --cpu-percent=60 --min=3 --max=6 # cpu超过60%会扩容副本 horizontalpodautoscaler.autoscaling/wordpress autoscaled [root@k8s-master1 ~]# kubectl get hpa -n kube-example NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE wordpress Deployment/wordpress 1%/60% 3 6 4 20s hpa.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: wordpress namespace: kube-example spec: maxReplicas: 6 minReplicas: 3 scaleTargetRef: # 目标作用对象 apiVersion: apps/v1 kind: Deployment name: wordpress targetCPUUtilizationPercentage: 60 #该使用率基于Pod设置的CPU Request值进行计算 scaleTargetRef：目标作用对象，可以是Deployment、ReplicationController或ReplicaSet。\ntargetCPUUtilizationPercentage：期望每个Pod的CPU使用率都为50%，该使用率基于Pod设置的CPU Request值进行计算，例如该值为200m，那么系统将维持Pod的实际CPU使用值为100m。\nminReplicas和maxReplicas：Pod副本数量的最小值和最大值，系统将在这个范围内进行自动扩缩容操作， 并维持每个Pod的CPU使用率为50%。\n为了使用autoscaling/v1版本的HorizontalPodAutoscaler，需要预先安装Heapster组件或Metrics Server，用于采集Pod的CPU使用率。\n此命令创建了一个关联资源 wordpress 的 HPA，最小的 Pod 副本数为3，最大为6。HPA 会根据设定的 cpu 使用率（60%）动态的增加或者减少 Pod 数量。同样，使用上面的 Fortio 工具来进行压测一次，看下能否进行自动的扩缩容：\n测试 1 fortio load -a -c 8 -qps 1000 -t 60s \u0026#34;http://wordpress.heian.com/\u0026#34; 在压测的过程中我们可以看到 HPA 的状态变化以及 Pod 数量也变成了6个：\n当压测停止以后正常5分钟后就会自动进行缩容，变成最小的3个 Pod 副本。 问题 （1）数据持久化\n（2）mysql账号密码注入等\n","date":"2021-04-07T11:17:25Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/202104071117/","title":"Kubernetes实战模拟五（wordpress的HPA自动扩缩容）"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） 源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\nKubernetes实战模拟三，已经构建wordpress的健康检查和服务质量，对后期的稳定运行有一定的帮助\n（1）startupProbe，首次启动探测，如果没有成功，不会运行下面的检测。配置有restartPolicy：Always 会自动的重启\n（2）健康检查，采取readinessProbe方式，如果检查失败，kubernetes会把Pod从service endpoints中剔除，不在提供流量，错误现场保留，方便排查问题\n（3）fortio 压测，模拟生产环境，确定resources的数值。采用Guaranteed(有保证的)，防止宿主机资源不够被优先驱逐\n版本4 思路：类似wordpress是静态服务，后期可能有版本的变更，我们都需要更新和升级pod，我们接下来优化升级更新操作\nKubernetes官方推荐使用Deployment来取代Rep\nlication Controller(rc) ，两者间主要相同点包括确保处在服务状态的Pod数量(replicas)能满足先前所设定的值以及支援滚动升级(Rolling update)，前者额外支持回滚(Roll back)的机制，因此接下来会介绍如何利用Deployment来进行滚动升级。\n**图中可以看到一个Deployment掌管一或多个Replica Set ，而一个Replica Set掌管一或多个Pod **\n版本升级 1 2 3 4 5 6 7 strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 2 minReadySeconds: 5 revisionHistoryLimit: 10 # 最多保留10个升级副本，便于回滚 minReadySeconds:\nKubernetes在等待设置的时间后才进行升级 (容器内应用的启动时间, pod变为run状态, 会在minReadySeconds后继续更新下一个pod.)\n如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了\n如果没有设置该值，在某些极端情况下可能会造成服务不正常运行\nmaxSurge:\n升级过程中最多可以比原先设置多出的POD数量\n例如：maxSurage=1，replicas=5,则表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。\nmaxUnavaible:\n升级过程中最多有多少个POD处于无法提供服务的状态\n当maxSurge不为0时，该值也不能为0\n例如：maxUnavaible=1，则表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。\nDeployment 控制器默认的就是滚动更新的更新策略，该策略可以在任何时间点更新应用的时候保证某些实例依然可以正常运行来防止应用 down 掉，当新部署的 Pod 启动并可以处理流量之后，才会去杀掉旧的 Pod。在使用过程中我们还可以指定 Kubernetes 在更新期间如何处理多个副本的切换方式，比如我们有一个3副本的应用，在更新的过程中是否应该立即创建这3个新的 Pod 并等待他们全部启动，或者杀掉一个之外的所有旧的 Pod，或者还是要一个一个的 Pod 进行替换？\n如果我们从旧版本到新版本进行滚动更新，只是简单的通过输出显示来判断哪些 Pod 是存活并准备就绪的，那么这个滚动更新的行为看上去肯定就是有效的，但是往往实际情况就是从旧版本到新版本的切换的过程并不总是十分顺畅的，应用程序很有可能会丢弃掉某些客户端的请求。比如我们在 Wordpress 应用中添加上如下的滚动更新策略，随便更改以下 Pod Template 中的参数，比如容器名更改为 blog：\n这里修改wordpress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 4 #多副本+pod的反亲合力可以实现pod的高可用 strategy: # 滚动更新的更新策略 type: RollingUpdate rollingUpdate: maxSurge: 1 # 表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。 maxUnavailable: 1 # 表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。 minReadySeconds: 5 # 容器内应用的启动时间, pod变为run状态, 会在minReadySeconds后继续更新下一个pod. revisionHistoryLimit: 10 # 最多保留10个升级副本，便于回滚 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport env: - name: WORDPRESS_DB_HOST value: wordpress-mysql:3306 - name: WORDPRESS_DB_USER value: wordpress - name: WORDPRESS_DB_PASSWORD value: wordpress imagePullPolicy: IfNotPresent resources: limits: cpu: 800m memory: 150Mi requests: cpu: 800m memory: 150Mi startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） httpGet: port: 80 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 5 # 执行探测的频率。默认是10秒，最小1秒。 readinessProbe: # （就绪检查） # 如果检查失败，kubernetes会把Pod从service endpoints中剔除 tcpSocket: port: 80 initialDelaySeconds: 10 timeoutSeconds: 5 failureThreshold: 5 periodSeconds: 5 successThreshold: 3 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - wordpress restartPolicy: Always 测试 运行时压测\n更新时压测（显示500）\n问题分析 从上面的输出可以看出有部分请求处理失败了（502），要弄清楚失败的原因就需要弄明白当应用在滚动更新期间重新路由流量时，从旧的 Pod 实例到新的实例究竟会发生什么，首先让我们先看看 Kubernetes 是如何管理工作负载连接的。\n失败原因\n我们这里通过 NodePort 去访问应用，实际上也是通过每个节点上面的 kube-proxy 通过更新 iptables 规则来实现的。\nKubernetes 会根据 Pods 的状态去更新 Endpoints 对象，这样就可以保证 Endpoints 中包含的都是准备好处理请求的 Pod。一旦新的 Pod 处于活动状态并准备就绪后，Kubernetes 就将会停止就的 Pod，从而将 Pod 的状态更新为 “Terminating”，然后从 Endpoints 对象中移除，并且发送一个 SIGTERM 信号给 Pod 的主进程。SIGTERM 信号就会让容器以正常的方式关闭，并且不接受任何新的连接。Pod 从 Endpoints 对象中被移除后，前面的负载均衡器就会将流量路由到其他（新的）Pod 中去。因为在负载均衡器注意到变更并更新其配置之前，终止信号就会去停用 Pod，而这个重新配置过程又是异步发生的，并不能保证正确的顺序，所以就可能导致很少的请求会被路由到已经终止的 Pod 上去了，也就出现了上面我们说的情况。\n问题解决 那么如何增强我们的应用程序以实现真正的零宕机迁移更新呢？\n首先，要实现这个目标的先决条件是我们的容器要正确处理终止信号，在 SIGTERM 信号上实现优雅关闭。下一步需要添加 readiness 可读探针，来检查我们的应用程序是否已经准备好来处理流量了。为了解决 Pod 停止的时候不会阻塞并等到负载均衡器重新配置的问题，我们还需要使用 preStop 这个生命周期的钩子，在容器终止之前调用该钩子。\n生命周期钩子函数是同步的，所以必须在将最终停止信号发送到容器之前完成，在我们的示例中，我们使用该钩子简单的等待，然后 SIGTERM 信号将停止应用程序进程。同时，Kubernetes 将从 Endpoints 对象中删除该 Pod，所以该 Pod 将会从我们的负载均衡器中排除，基本上来说我们的生命周期钩子函数等待的时间可以确保在应用程序停止之前重新配置负载均衡器\nlifecycle\n创建资源对象时，可以使用lifecycle来管理容 器在运行前和关闭前的一些动作。\nlifecycle有两种回调函数:\nPostStart: 容器创建成功后，运行前的任务，用于资源部署、环境准备等。\nPreStop: 在容器被终止前的任务，用于优雅关闭应用程序、通知其他系统等等。\n我们这里使用 preStop 设置了一个 20s 的宽限期，Pod 在真正销毁前会先 sleep 等待 20s，这就相当于留了时间给 Endpoints 控制器和 kube-proxy 更新去 Endpoints 对象和转发规则，这段时间 Pod 虽然处于 Terminating 状态，即便在转发规则更新完全之前有请求被转发到这个 Terminating 的 Pod，依然可以被正常处理，因为它还在 sleep，没有被真正销毁。\n现在，当我们去查看滚动更新期间的 Pod 行为时，我们将看到正在终止的 Pod 处于 Terminating 状态，但是在等待时间结束之前不会关闭的，如果我们使用 Fortio 重新测试下，则会看到零失败请求的理想状态。\n1 2 3 4 lifecycle: preStop: # 在容器被终止前的任务，用于优雅关闭应用程序 exec: command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 20\u0026#34;] #睡眠20s，优雅关闭 完整wordpress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 4 #多副本+pod的反亲合力可以实现pod的高可用 strategy: # 滚动更新的更新策略 type: RollingUpdate rollingUpdate: maxSurge: 1 # 表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。 maxUnavailable: 1 # 表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。 minReadySeconds: 5 # 容器内应用的启动时间, pod变为run状态, 会在minReadySeconds后继续更新下一个pod. revisionHistoryLimit: 10 # 最多保留10个升级副本，便于回滚 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: blog image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport env: - name: WORDPRESS_DB_HOST value: wordpress-mysql:3306 - name: WORDPRESS_DB_USER value: wordpress - name: WORDPRESS_DB_PASSWORD value: wordpress imagePullPolicy: IfNotPresent resources: limits: cpu: 800m memory: 150Mi requests: cpu: 800m memory: 150Mi startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） httpGet: port: 80 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 5 # 执行探测的频率。默认是10秒，最小1秒。 readinessProbe: # （就绪检查） # 如果检查失败，kubernetes会把Pod从service endpoints中剔除 tcpSocket: port: 80 initialDelaySeconds: 10 timeoutSeconds: 5 failureThreshold: 5 periodSeconds: 5 successThreshold: 3 lifecycle: preStop: # 在容器被终止前的任务，用于优雅关闭应用程序 exec: command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 20\u0026#34;] #睡眠20s，优雅关闭 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - wordpress restartPolicy: Always 再次测试\n版本回滚 博客参照：https://www.ipcpu.com/2017/09/kubernetes-rolling-update/\n查询升级状态、暂停和恢复\n1 2 3 4 5 6 #@查询升级状况 $ kubectl rollout status deployment \u0026lt;deployment\u0026gt; #@暂停滚动升级 $ kubectl rollout pause deployment \u0026lt;deployment\u0026gt; #@恢复滚动升级 $ kubectl rollout resume deployment \u0026lt;deployment\u0026gt; 操作都加了–record 的参数，這参数主要是告知 Kubernetes 记录此次下达的指令，所以我们可以通过以下命令来查看\n1 2 3 4 5 6 7 8 9 [root@k8s-master1 v4]# kubectl rollout history deployment -n kube-example wordpress deployment.apps/wordpress REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; 3 \u0026lt;none\u0026gt; 4 \u0026lt;none\u0026gt; 5 \u0026lt;none\u0026gt; 6 \u0026lt;none\u0026gt; 这些个记录能保存多少个呢？默认是全部保存的。可以通过设置.spec.revisionHistoryLimit 来决定记录的个数，一般生产环境记录10个左右就差不多了。要不然可能会被刷屏。\n1 2 minReadySeconds: 5 revisionHistoryLimit: 10 可以通过以下命令，来回滚到上一个版本或者特定版本\n1 2 3 4 5 6 # to previous revision $ kubectl rollout undo deployment \u0026lt;deployment\u0026gt; # to specific revision $ kubectl rollout undo deployment \u0026lt;deployment\u0026gt; --to-revision=\u0026lt;revision\u0026gt; # exmaple $ kubectl rollout undo deployment buniess --to-revision=3 问题 （1）自动扩缩容\n（2）数据持久化\n（3）mysql账号密码注入等\n期待下个版本更加优化完美\n","date":"2021-04-06T21:35:06Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/202104062135/","title":"Kubernetes实战模拟四（wordpress升级更新）"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） 源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\nKubernetes实战模拟二，已经优化架构，已经采取分离，可以实现高可用，并优化软策略，防止单点故障\n接下来，慢慢实现健康检查和服务质量\n版本3 思路：分别对2个pod的进行端口检查，判断pod是否正常和流量提供，测试性能，设置资源限制\n健康检查：主要检查pod提供正常服务\n资源限制：后期可以根据这个设置HPA。防止宿主资源不足，被驱逐\n1、Pod的健康检查，也叫做探针，探针的种类有两种。\n1）、livenessProbe，健康状态检查，周期性检查服务是否存活，检查结果失败，将重启容器。\n2）、readinessProbe，可用性检查，周期性检查服务是否可用，不可用将从service的endpoints中移除。\nlivenessProbe (存活检查） # 如果检查失败，将杀死容器，根据Pod的restartPolicy来操作\nreadinessProbe（就绪检查） # 如果检查失败，kubernetes会把Pod从service endpoints中剔除\n2、探针的检测方法。\n1）、exec，执行一段命令。\n2）、httpGet，检测某个http请求的返回状态码。\n3）、tcpSocket，测试某个端口是否能够连接。\n3、startupProbe检测\n判断容器内的应用程序是否已启动。如果提供了启动探测，则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet将杀死容器，容器将服从其重启策略。如果容器没有提供启动探测，则默认状态为成功。\n注意：不要将startupProbe和readinessProbe混淆。\n配置Probe Probe中有很多精确和详细的配置，通过它们你能准确的控制liveness和readiness检查：\ninitialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。\nperiodSeconds：执行探测的频率。默认是10秒，最小1秒。\ntimeoutSeconds：探测超时时间。默认1秒，最小1秒。\nsuccessThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。\nfailureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。\nHTTP probe中可以给 httpGet设置其他配置项：\nhost：连接的主机名，默认连接到pod的IP。你可能想在http header中设置”Host”而不是使用IP。\nscheme：连接使用的schema，默认HTTP。\npath: 访问的HTTP server的path。\nhttpHeaders：自定义请求的header。HTTP运行重复的header。\nport：访问的容器的端口名字或者端口号。端口号必须介于1和65525之间。\n命名空间：(namespace.yaml) 1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: kube-example mysql资源清单 mysql.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 apiVersion: v1 kind: Service metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress spec: selector: app: wordpress tier: mysql ports: - port: 3306 targetPort: dbport --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress tier: mysql spec: replicas: 1 template: metadata: name: wordpress-mysql labels: app: wordpress tier: mysql spec: containers: - name: mysql image: mysql:5.7 args: - --default_authentication_plugin=mysql_native_password - --character-set-server=utf8mb4 - --collation-server=utf8mb4_unicode_ci ports: - containerPort: 3306 name: dbport env: - name: MYSQL_ROOT_PASSWORD value: rootPassW0rd - name: MYSQL_DATABASE value: wordpress - name: MYSQL_USER value: wordpress - name: MYSQL_PASSWORD value: wordpress imagePullPolicy: IfNotPresent startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） tcpSocket: port: 3306 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 20 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 10 # 执行探测的频率。默认是10秒，最小1秒。 restartPolicy: Always selector: matchLabels: app: wordpress tier: mysql 因为这个是模拟环境，现在数据没有持续化，如果加上下面的检测，如果pod的有问题，直接重启，数据都会丢失，这个后期数据持久化，我们可以加上面\n1 2 3 4 5 6 7 8 livenessProbe: tcpSocket: port: 3306 # 访问的容器的端口名字或者端口号 initialDelaySeconds: 10 timeoutSeconds: 5 failureThreshold: 5 periodSeconds: 5 successThreshold: 3 #探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1。 Wordpress 清单 wordpress.yaml\n我们的应用现在还有一个非常重要的功能没有提供，那就是健康检查，我们知道健康检查是提高应用健壮性非常重要的手段，当我们检测到应用不健康的时候我们希望可以自动重启容器，当应用还没有准备好的时候我们也希望暂时不要对外提供服务，所以我们需要添加我们前面经常提到的 liveness probe 和 rediness probe 两个健康检测探针，检查探针的方式有很多，我们这里当然可以认为如果容器的 80 端口可以成功访问那么就是健康的，对于一般的应用提供一个健康检查的 URL 会更好，这里我们添加一个如下所示的可读性探针，为什么不添加存活性探针呢？这里其实是考虑到线上错误排查的一个问题，如果当我们的应用出现了问题，然后就自动重启去掩盖错误的话，可能这个错误就会被永远忽略掉了，所以其实这是一个折衷的做法，不使用存活性探针，而是结合监控报警，保留错误现场，方便错误排查，但是可读写探针是一定需要添加的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 4 #多副本+pod的反亲合力可以实现pod的高可用 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport env: - name: WORDPRESS_DB_HOST value: wordpress-mysql:3306 - name: WORDPRESS_DB_USER value: wordpress - name: WORDPRESS_DB_PASSWORD value: wordpress imagePullPolicy: IfNotPresent startupProbe: #首次启动探测（如果没有成功，不会运行下面livenessProbe） httpGet: port: 80 failureThreshold: 2 #探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1。 initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 timeoutSeconds: 10 # 探测超时时间。默认1秒，最小1秒。 periodSeconds: 5 # 执行探测的频率。默认是10秒，最小1秒。 readinessProbe: # （就绪检查） # 如果检查失败，kubernetes会把Pod从service endpoints中剔除 httpGet: port: 80 initialDelaySeconds: 10 timeoutSeconds: 5 failureThreshold: 5 periodSeconds: 5 successThreshold: 3 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - wordpress restartPolicy: Always 上面为什么不用livenessProbe检测，首先如果livenessProbe检测失败，会直接重启pod的，会毁坏错误的信息，不方便我们排查问题。我们有多个副本，如果一个出问题，其余会正常工作\n1 2 3 4 5 6 7 livenessProbe: #(存活检查） # 如果检查失败，将杀死容器，根据Pod的restartPolicy来操作 httpGet: port: 80 initialDelaySeconds: 15 timeoutSeconds: 5 periodSeconds: 5 failureThreshold: 5 PDB策略 pdb.yaml\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: wordpress-pdb namespace: kube-example spec: maxUnavailable: 1 selector: matchLabels: app: wordpress tier: frontend 服务质量 QoS QoS 是 Quality of Service 的缩写，即服务质量。为了实现资源被有效调度和分配的同时提高资源利用率，Kubernetes 针对不同服务质量的预期，通过 QoS 来对 Pod 进行服务质量管理。对于一个 Pod 来说，服务质量体现在两个具体的指标：CPU 和内存。当节点上内存资源紧张时，Kubernetes 会根据预先设置的不同 QoS 类别进行相应处理。\nQoS 主要分为 Guaranteed、Burstable 和 Best-Effort三类，优先级从高到低。我们先分别来介绍下这三种服务类型的定义。\nGuaranteed(有保证的)\n属于该级别的 Pod 有以下两种：\nPod 中的所有容器都且仅设置了 CPU 和内存的 limits Pod 中的所有容器都设置了 CPU 和内存的 requests 和 limits ，且单个容器内的requests==limits（requests不等于0） Burstable(不稳定的)\nPod 中只要有一个容器的 requests 和 limits 的设置不相同，那么该 Pod 的 QoS 即为 Burstable。\nBest-Effort(尽最大努力)\n如果 Pod 中所有容器的 resources 均未设置 requests 与 limits，该 Pod 的 QoS 即为 Best-Effort。\nfortio 部署 1 2 3 4 wget https://github.com/fortio/fortio/releases/download/v1.4.4/fortio-1.4.4-1.x86_64.rpm rpm -ivh fortio-1.4.4-1.x86_64.rpm nohup fortio server \u0026amp; tail -f nohup.out 开始测压\n命令行\nfortio load -c 5 -n 20 -qps 0 http://www.baidu.com\n#命令解释如下：\n-c 表示并发数\n-n 一共多少请求\n-qps 每秒查询数，0 表示不限制\nWeb控制台\nweb 控制台方式就是提供给习惯使用 web 界面操作的同学一个途径来使用 Fortio。因为是基于 web 方式，所以就需要首先启动一个 web server，这样客户端浏览器才可以访问到 web server 提供的操作界面进行负载压测。默认情况 fortio server 会启动 8080 端口，如下图所示：\n打开浏览器，输入 http://IP:8080/fortio，访问 Fortio server：\n根据实现生成环境访问人数和次数模拟\n测试前\n测试后\n资源限制 采用Guaranteed(有保证的)，防止宿主机资源不够被优先驱逐\nwordpress\n1 2 3 4 5 6 7 resources: limits: cpu: 800m memory: 150Mi requests: cpu: 800m memory: 150Mi mysql\n1 2 3 4 5 6 7 resources: limits: cpu: 1000m memory: 400Mi requests: cpu: 1000m memory: 400Mi 此版本实现健康检查和服务质量QoS\n问题 （1）自动扩缩容\n（2）滚动更新策略\n（3）数据持久化\n（4）mysql账号密码注入等\n这些一些列问题，都会在后面的版本架构中优化\n","date":"2021-04-05T01:05:46Z","image":"https://www.ownit.top/title_pic/04.jpg","permalink":"https://www.ownit.top/p/202104050105/","title":"Kubernetes实战模拟三（wordpress健康检查和服务质量QoS）"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） 源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\n上一篇文件我们使用pod的构建两个容器，但是问题也是一大堆，根本不适合生产方面。\n所以我们慢慢解决上面的问题，优化架构。\n版本2 思路：将 Pod 中的两个容器进行拆分，将 Wordpress 和 MySQL 分别部署\nWordpress 用多个副本进行部署就可以实现应用的高可用了\n由于 MySQL 是有状态应用，一般来说需要用 StatefulSet 来进行管理，但是我们这里部署的 MySQL 并不是集群模式，而是单副本的，所以用 Deployment 也是没有问题的。如果生产环境的话，一般都是集群的，分别在宿主机上部署。\nmysql资源清单 mysql.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion: v1 kind: Service metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress spec: selector: app: wordpress tier: mysql ports: - port: 3306 targetPort: dbport --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-mysql namespace: kube-example labels: app: wordpress tier: mysql spec: replicas: 1 template: metadata: name: wordpress-mysql labels: app: wordpress tier: mysql spec: containers: - name: mysql image: mysql:5.7 args: - --default_authentication_plugin=mysql_native_password - --character-set-server=utf8mb4 - --collation-server=utf8mb4_unicode_ci ports: - containerPort: 3306 name: dbport env: - name: MYSQL_ROOT_PASSWORD value: rootPassW0rd - name: MYSQL_DATABASE value: wordpress - name: MYSQL_USER value: wordpress - name: MYSQL_PASSWORD value: wordpress imagePullPolicy: IfNotPresent restartPolicy: Always selector: matchLabels: app: wordpress tier: mysql 这里给 MySQL 应用添加了一个 Service 对象，是因为 Wordpress 应用需要来连接数据库，之前在同一个 Pod 中用 localhost 即可，现在需要通过 Service 的 DNS 形式的域名进行连接。直接创建上面资源对象\n1 2 3 # kubectl apply -f mysql.yaml service/wordpress-mysql created deployment.apps/wordpress-mysql created Wordpress 清单 wordpress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: NodePort --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 4 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport env: - name: WORDPRESS_DB_HOST value: wordpress-mysql:3306 - name: WORDPRESS_DB_USER value: wordpress - name: WORDPRESS_DB_PASSWORD value: wordpress imagePullPolicy: IfNotPresent restartPolicy: Always 注意这里的环境变量 WORDPRESS_DB_HOST 的值将之前的 localhost 地址更改成了上面 MySQL 服务的 DNS 地址，完整的域名应该是 wordpress-mysql.kube-example.svc.cluster.local:3306，由于这两个应该都处于同一个命名空间，所以直接简写成 wordpress-mysql:3306 也是可以的。\n1 2 3 4 5 6 7 8 9 [root@k8s-master1 v2]# kubectl apply -f wordpress.yaml service/wordpress created deployment.apps/wordpress created [root@k8s-master1 v2]# kubectl get pods -l app=wordpress -n kube-example NAME READY STATUS RESTARTS AGE wordpress-ddb4ff6cf-g2xf8 1/1 Running 0 9s wordpress-ddb4ff6cf-jjt5k 1/1 Running 0 9s wordpress-ddb4ff6cf-zmbc4 1/1 Running 0 9s wordpress-mysql-d9b4b8985-bqv2b 1/1 Running 0 32s 测试 可以看到都已经是 Running 状态了，然后我们需要怎么来验证呢？是不是我们能想到的就是去访问下我们的 Wordpress 服务就可以了，我们这里还是使用的一个 NodePort 类型的 Service 来暴露服务：\n1 2 3 4 5 6 7 8 [root@k8s-master1 v2]# kubectl get svc,ep -n kube-example NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wordpress NodePort 10.0.0.49 \u0026lt;none\u0026gt; 80:30455/TCP 92s service/wordpress-mysql ClusterIP 10.0.0.142 \u0026lt;none\u0026gt; 3306/TCP 115s NAME ENDPOINTS AGE endpoints/wordpress 10.244.0.5:80,10.244.1.135:80,10.244.1.136:80 92s endpoints/wordpress-mysql 10.244.1.134:3306 115s 可以看到 wordpress 服务产生了一个 30455的端口，现在我们就可以通过 http://\u0026lt;任意节点的NodeIP\u0026gt;:30012 访问我们的应用了，在浏览器中打开，如果看到 wordpress 跳转到了安装页面，证明我们的安装是正确的，如果没有出现预期的效果，那么就需要去查看下 Pod 的日志来排查问题了，根据页面提示，填上对应的信息，点击“安装”即可，最终安装成功后\n避免单点故障 为什么会有单点故障的问题呢？\n我们不是部署了多个副本的 Wordpress 应用吗？\n当我们设置 replicas=1 的时候肯定会存在单点故障问题，如果大于 1 但是所有副本都调度到了同一个节点的是不是同样就会存在单点问题了\n这个节点挂了所有副本就都挂了，所以我们不仅需要设置多个副本数量，还需要让这些副本调度到不同的节点上，来打散避免单点故障\nPod 反亲和性来实现了，我们可以防止单点故障出现。\n但是要考虑要用弱策略还是硬策略\n我们首先弱的，如果硬策略，如果节点有问题，pod就不会被创建\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress tier: frontend ports: - port: 80 name: web targetPort: wdport type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress tier: frontend spec: selector: matchLabels: app: wordpress tier: frontend replicas: 4 #多副本+pod的反亲合力可以实现pod的高可用 template: metadata: name: wordpress labels: app: wordpress tier: frontend spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport env: - name: WORDPRESS_DB_HOST value: wordpress-mysql:3306 - name: WORDPRESS_DB_USER value: wordpress - name: WORDPRESS_DB_PASSWORD value: wordpress imagePullPolicy: IfNotPresent affinity: #pod的反亲和力 podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchExpressions: - key: app operator: In values: - wordpress restartPolicy: Always PDB策略 PDB能够限制同时中断的pod的数量,以保证集群的高可用性\n有些时候线上的某些节点需要做一些维护操作，比如要升级内核，这个时候我们就需要将要维护的节点进行驱逐操作，驱逐节点首先是将节点设置为不可调度，这样可以避免有新的 Pod 调度上来，然后将该节点上的 Pod 全部删除，ReplicaSet 控制器检测到 Pod 数量减少了就会重新创建一个新的 Pod，调度到其他节点上面的，这个过程是先删除，再创建，并非是滚动更新，因此更新过程中，如果一个服务的所有副本都在被驱逐的节点上，则可能导致该服务不可用。\n如果服务本身存在单点故障，所有副本都在同一个节点，驱逐的时候肯定就会造成服务不可用了，这种情况我们使用上面的反亲和性和多副本就可以解决这个问题。但是如果我们的服务本身就被打散在多个节点上，这些节点如果都被同时驱逐的话，那么这个服务的所有实例都会被同时删除，这个时候也会造成服务不可用了，这种情况下我们可以通过配置 PDB（PodDisruptionBudget）对象来避免所有副本同时被删除，比如我们可以设置在驱逐的时候 wordpress 应用最多只有一个副本不可用，其实就相当于逐个删除并在其它节点上重建\npdb.yaml\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: wordpress-pdb namespace: kube-example spec: maxUnavailable: 1 selector: matchLabels: app: wordpress tier: frontend 1 2 3 4 5 [root@k8s-master1 v2]# kubectl apply -f pdb.yaml poddisruptionbudget.policy/wordpress-pdb created [root@k8s-master1 v2]# kubectl get pdb -n kube-example NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE wordpress-pdb N/A 1 1 15s PDB 的更多详细信息可以查看官方文档：https://kubernetes.io/docs/tasks/run-application/configure-pdb/。\n此版本实现高可用，但是还有一些问题\n问题 （1）pod的健康检查，如果没有这些，pod额度重启策略都无法执行\n（2）内存，cpu等资源限制，服务质量Qos\n（3）数据持久化\n（4）mysql账号密码注入等\n这些一些列问题，都会在后面的版本架构中优化\n","date":"2021-04-03T23:50:52Z","image":"https://www.ownit.top/title_pic/10.jpg","permalink":"https://www.ownit.top/p/202104032350/","title":"Kubernetes实战模拟二（wordpress高可用）"},{"content":"Kubernetes实战模拟一（wordpress基础版） Kubernetes实战模拟二（wordpress高可用） Kubernetes实战模拟三（wordpress健康检查和服务质量QoS） Kubernetes实战模拟四（wordpress升级更新） Kubernetes是现在比较流行的容器化软件。我们日常也比较使用的多，我们都是慢慢的从陌生到熟悉的进阶，只有不断的学习，才能有收获。\nKubernetes专栏：https://blog.csdn.net/heian_99/category_9652886.html\nKubernetes官网地址：https://kubernetes.io/\n这个专栏，模拟Kubernetes的日常发布流程。\n源码地址：https://github.com/nangongchengfeng/Kubernetes/tree/main/wordpress-example\n里面包含多个版本，现在是演示v1版本。\n环境 Kubernetes：v1.18.3\ndocker：19.03.9\nmysql：5.7\nwordpress：5.3.2-apache\n版本1 思路：由于wordpress和mysql需要进行交互。版本1就把wordpress和mysql集成到一个pod运行，测试访问效果\n原理 Wordpress Wordpress 是一个基于 PHP 和 MySQL 的流行的开源内容管理系统，拥有丰富的插件和模板系统。一个能够解析 PHP 的程序和 MySQL 数据库。官方提供了镜像 https://hub.docker.com/_/wordpress\n可以通过一系列环境变量去指定 MySQL 数据库的配置，只需要将这些参数配置上直接运行即可。\n我们知道 Wordpress 应用本身会频繁的和 MySQL 数据库进行交互，这种情况下如果将二者用容器部署在同一个 Pod 下面是不是要高效很多\n因为一个 Pod 下面的所有容器是共享同一个 network namespace 的，下面我们就来部署我们的应用\n命名空间：(namespace.yaml) 1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: kube-example 应用清单：（deployment.yaml） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 apiVersion: apps/v1 kind: Deployment metadata: name: wordpress namespace: kube-example labels: app: wordpress spec: selector: matchLabels: app: wordpress template: metadata: labels: app: wordpress spec: containers: - name: wordpress image: wordpress:5.3.2-apache ports: - containerPort: 80 name: wdport env: - name: WORDPRESS_DB_HOST value: localhost:3306 - name: WORDPRESS_DB_USER value: wordpress - name: WORDPRESS_DB_PASSWORD value: wordpress - name: mysql image: mysql:5.7 imagePullPolicy: IfNotPresent args: # 新版本镜像有更新，需要使用下面的认证插件环境变量配置才会生效 - --default_authentication_plugin=mysql_native_password - --character-set-server=utf8mb4 - --collation-server=utf8mb4_unicode_ci ports: - containerPort: 3306 name: dbport env: - name: MYSQL_ROOT_PASSWORD value: rootPassW0rd - name: MYSQL_DATABASE value: wordpress - name: MYSQL_USER value: wordpress - name: MYSQL_PASSWORD value: wordpress 由于我们这里 MySQL 和 Wordpress 在同一个 Pod 下面，所以在 Wordpress 中我们指定数据库地址的时候是用的 localhost:3306，因为这两个容器已经共享同一个 network namespace 了，这点很重要，然后如果我们要想把这个服务暴露给外部用户还得创建一个 Service 或者 Ingress 对象\nNodePort 类型的 Service：(service.yaml) 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: wordpress namespace: kube-example spec: selector: app: wordpress type: NodePort ports: - name: web port: 80 targetPort: wdport 因为只需要暴露 Wordpress 这个应用，所以只匹配了一个名为 wdport 的端口，现在我们来创建上面的几个资源对象\n1 2 3 kubectl apply -f namespace.yaml kubectl apply -f deployment.yaml kubectl apply -f service.yaml 接下来就是等待拉取镜像，启动 Pod:\n1 2 3 4 5 6 $ kubectl get pods -n kube-example NAME READY STATUS RESTARTS AGE wordpress-77dcdb64c6-zdlb8 2/2 Running 0 12m $ kubectl get svc -n kube-example NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress NodePort 10.106.237.157 \u0026lt;none\u0026gt; 80:30892/TCP 2m2s 测试 当 Pod 启动完成后，可以通过上面的 http://\u0026lt;任意节点IP\u0026gt;:30892 这个 NodePort 端口来访问应用了\n安装界面\n问题 （1）pod中没有先后顺序\n（2）单节点问题，没有高可用\n（3）Wordpress 是无状态服务器，如果增加节点，mysql也会增加，数据不独立\n（4）数据没有持久化\n等等一些列的问题。\n此版本1，为最简单的问题，我们将在此版本上改进架构，不断的优化\n","date":"2021-04-03T23:47:45Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202104032347/","title":"Kubernetes实战模拟一（wordpress基础版）"},{"content":"环境 Rook Ceph 需要使用 RBD 内核模块，我们可以通过运行 modprobe rbd 来测试 Kubernetes 节点是否有该模块，如果没有，则需要更新下内核版本。\n另外需要在节点上安装 lvm2 软件包：\n1 2 3 4 5 # Centos sudo yum install -y lvm2 # Ubuntu sudo apt-get install -y lvm2 安装 我们这里部署最新的 release-1.2 版本的 Rook，部署清单文件地址：https://github.com/rook/rook/tree/release-1.2/cluster/examples/kubernetes/ceph。\n从上面链接中下载 common.yaml 与 operator.yaml 两个资源清单文件：https://github.com/nangongchengfeng/Kubernetes/tree/main/Ceph（这个是已经整合好的，方便测试）\n1 2 3 4 # 会安装crd、rbac相关资源对象 $ kubectl apply -f common.yaml # 安装 rook operator $ kubectl apply -f operator.yaml 在继续操作之前，验证 rook-ceph-operator 是否处于“Running”状态： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@k8s-master1 ~]# [root@k8s-master1 ~]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-79wcl 3/3 Running 0 7h28m csi-cephfsplugin-jmrgs 3/3 Running 0 7h28m csi-cephfsplugin-provisioner-85979bcd-cfgqx 4/4 Running 22 7h28m csi-cephfsplugin-provisioner-85979bcd-pzjsr 4/4 Running 10 7h28m csi-rbdplugin-j26wk 3/3 Running 0 7h28m csi-rbdplugin-pdbjl 3/3 Running 0 7h28m csi-rbdplugin-provisioner-66f64ff49c-qwhh9 5/5 Running 18 7h28m csi-rbdplugin-provisioner-66f64ff49c-sq57h 5/5 Running 37 7h28m rook-ceph-crashcollector-k8s-node1-fcb4cfc96-4fn26 1/1 Running 0 7h26m rook-ceph-crashcollector-k8s-node2-d97b797b5-mnc2b 1/1 Running 0 7h26m rook-ceph-mgr-a-5dcbf79d8b-n9j68 1/1 Running 0 4h49m rook-ceph-mon-a-75978c9d5b-vhmkx 1/1 Running 1 7h27m rook-ceph-mon-b-7955589ff4-fhbb5 1/1 Running 0 7h26m rook-ceph-operator-658dfb6cc4-9z5lw 1/1 Running 13 7h29m rook-ceph-osd-0-546d474d95-wkn4k 1/1 Running 0 7h26m rook-ceph-osd-1-5c7cc4b9d6-g4g7t 1/1 Running 0 7h26m rook-ceph-osd-prepare-k8s-node1-6nttm 0/1 Completed 0 140m rook-ceph-osd-prepare-k8s-node2-jmddp 0/1 Completed 0 140m rook-ceph-tools-f56f69476-8mnjf 1/1 Running 0 7h16m rook-discover-bgcth 1/1 Running 0 7h29m rook-discover-km45k 1/1 Running 0 7h29m 我们可以看到 Operator 运行成功后，还会有一个 DaemonSet 控制器运行得 rook-discover 应用，当 Rook Operator 处于 Running 状态，我们就可以创建 Ceph 集群了。为了使集群在重启后不受影响，请确保设置的 dataDirHostPath 属性值为有效得主机路径。更多相关设置，可以查看集群配置相关文档。\n创建如下的资源清单文件：(cluster.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: # 最新得 ceph 镜像, 可以查看 https://hub.docker.com/r/ceph/ceph/tags image: ceph/ceph:v14.2.5 dataDirHostPath: /var/lib/rook # 主机有效目录 mon: count: 3 dashboard: enabled: true storage: useAllNodes: true useAllDevices: false # 重要: Directories 应该只在预生产环境中使用 directories: - path: /data/rook 其中有几个比较重要的字段：\ndataDirHostPath：宿主机上的目录，用于每个服务存储配置和数据。如果目录不存在，会自动创建该目录。由于此目录在主机上保留，因此在删除 Pod 后将保留该目录，另外不得使用以下路径及其任何子路径：/etc/ceph、/rook 或 /var/log/ceph。 useAllNodes：用于表示是否使用集群中的所有节点进行存储，如果在 nodes 字段下指定了各个节点，则必须将useAllNodes设置为 false。\nuseAllDevices：表示 OSD 是否自动使用节点上的所有设备，一般设置为 false，这样可控性较高\ndirectories：一般来说应该使用一块裸盘来做存储，有时为了测试方便，使用一个目录也是可以的，当然生成环境不推荐使用目录。\n除了上面这些字段属性之外还有很多其他可以细粒度控制得参数，可以查看集群配置相关文档。\n现在直接创建上面的 CephCluster 对象即可：\n1 2 $ kubectl apply -f cluster.yaml cephcluster.ceph.rook.io/rook-ceph created 创建完成后，Rook Operator 就会根据我\n验证 要验证集群是否处于正常状态，我们可以使用 Rook 工具箱 来运行 ceph status 命令查看。\nRook 工具箱是一个用于调试和测试 Rook 的常用工具容器，该工具基于 CentOS 镜像，所以可以使用 yum 来轻松安装更多的工具包。我们这里用 Deployment 控制器来部署 Rook 工具箱，部署的资源清单文件如下所示：（toolbox.yaml）\n们的描述信息去自动创建 Ceph 集群了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion: apps/v1 kind: Deployment metadata: name: rook-ceph-tools namespace: rook-ceph labels: app: rook-ceph-tools spec: replicas: 1 selector: matchLabels: app: rook-ceph-tools template: metadata: labels: app: rook-ceph-tools spec: dnsPolicy: ClusterFirstWithHostNet containers: - name: rook-ceph-tools image: rook/ceph:v1.2.1 command: [\u0026#34;/tini\u0026#34;] args: [\u0026#34;-g\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;/usr/local/bin/toolbox.sh\u0026#34;] imagePullPolicy: IfNotPresent env: - name: ROOK_ADMIN_SECRET valueFrom: secretKeyRef: name: rook-ceph-mon key: admin-secret securityContext: privileged: true volumeMounts: - mountPath: /dev name: dev - mountPath: /sys/bus name: sysbus - mountPath: /lib/modules name: libmodules - name: mon-endpoint-volume mountPath: /etc/rook # 如果设置 hostNetwork: false, \u0026#34;rbd map\u0026#34; 命令会被 hang 住, 参考 https://github.com/rook/rook/issues/2021 hostNetwork: true volumes: - name: dev hostPath: path: /dev - name: sysbus hostPath: path: /sys/bus - name: libmodules hostPath: path: /lib/modules - name: mon-endpoint-volume configMap: name: rook-ceph-mon-endpoints items: - key: data path: mon-endpoints 然后直接创建这个 Pod：\n1 2 $ kubectl apply -f toolbox.yaml deployment.apps/rook-ceph-tools created 一旦 toolbox 的 Pod 运行成功后，我们就可以使用下面的命令进入到工具箱内部进行操作：\n1 $ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \u0026#34;app=rook-ceph-tools\u0026#34; -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) bash 工具箱中的所有可用工具命令均已准备就绪，可满足您的故障排除需求。例如：\n1 2 3 4 ceph status ceph osd status ceph df rados df 比如现在我们要查看集群的状态，需要满足下面的条件才认为是健康的：\n所有 mons 应该达到法定数量 mgr 应该是激活状态 至少有一个 OSD 处于激活状态 如果不是 HEALTH_OK 状态，则应该查看告警或者错误信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ ceph status ceph status cluster: id: dae083e6-8487-447b-b6ae-9eb321818439 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c (age 15m) mgr: a(active, since 2m) osd: 31 osds: 2 up (since 6m), 2 in (since 6m) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 79 GiB used, 314 GiB / 393 GiB avail pgs: 如果群集运行不正常，可以查看 Ceph 常见问题以了解更多详细信息和可能的解决方案。\nDashboard Ceph 有一个 Dashboard 工具，我们可以在上面查看集群的状态，包括总体运行状态，mgr、osd 和其他 Ceph 进程的状态，查看池和 PG 状态，以及显示守护进程的日志等等。\n我们可以在上面的 cluster CRD 对象中开启 dashboard，设置dashboard.enable=true即可，这样 Rook Operator 就会启用 ceph-mgr dashboard 模块，并将创建一个 Kubernetes Service 来暴露该服务，将启用端口 7000 进行 https 访问，如果 Ceph 集群部署成功了，我们可以使用下面的命令来查看 Dashboard 的 Service：\n1 2 3 4 $ kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rook-ceph-mgr ClusterIP 10.99.87.1 \u0026lt;none\u0026gt; 9283/TCP 3m6s rook-ceph-mgr-dashboard ClusterIP 10.111.195.180 \u0026lt;none\u0026gt; 7000/TCP 3m29s 这里的 rook-ceph-mgr 服务用于报告 Prometheus metrics 指标数据的，而后面的的 rook-ceph-mgr-dashboard 服务就是我们的 Dashboard 服务，如果在集群内部我们可以通过 DNS 名称 http://rook-ceph-mgr-dashboard.rook-ceph:7000或者 CluterIP http://10.111.195.180:7000 来进行访问，但是如果要在集群外部进行访问的话，我们就需要通过 Ingress 或者 NodePort 类型的 Service 来暴露了，为了方便测试我们这里创建一个新的 NodePort 类型的服务来访问 Dashboard，资源清单如下所示：（dashboard-external.yaml）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Service metadata: name: rook-ceph-mgr-dashboard-external namespace: rook-ceph labels: app: rook-ceph-mgr rook_cluster: rook-ceph spec: ports: - name: dashboard port: 7000 protocol: TCP targetPort: 7000 selector: app: rook-ceph-mgr rook_cluster: rook-ceph type: NodePort 同样直接创建即可：\n1 $ kubectl apply -f dashboard-external.yaml 创建完成后我们可以查看到新创建的 rook-ceph-mgr-dashboard-external 这个 Service 服务：\n1 2 3 4 5 $ kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rook-ceph-mgr ClusterIP 10.96.49.29 \u0026lt;none\u0026gt; 9283/TCP 23m rook-ceph-mgr-dashboard ClusterIP 10.109.8.98 \u0026lt;none\u0026gt; 7000/TCP 23m rook-ceph-mgr-dashboard-external NodePort 10.109.53.223 \u0026lt;none\u0026gt; 7000:31361/TCP 14s 现在我们需要通过 http://\u0026lt;NodeIp\u0026gt;:31361 就可以访问到 Dashboard 了。\n但是在访问的时候需要我们登录才能够访问，Rook 创建了一个默认的用户 admin，并在运行 Rook 的命名空间中生成了一个名为 rook-ceph-dashboard-admin-password 的 Secret，要获取密码，可以运行以下命令：\n1 2 $ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\u0026#34;{[\u0026#39;data\u0026#39;][\u0026#39;password\u0026#39;]}\u0026#34; | base64 --decode \u0026amp;\u0026amp; echo xxxx（登录密码） 用上面获得的密码和用户名 admin 就可以登录 Dashboard 了，在 Dashboard 上面可以查看到整个集群的状态：\n使用 现在我们的 Ceph 集群搭建成功了，我们就可以来使用存储了。首先我们需要创建存储池，可以用 CRD 来定义 Pool。Rook 提供了两种机制来维持 OSD：\n副本：缺省选项，每个对象都会根据 spec.replicated.size 在多个磁盘上进行复制。建议非生产环境至少 2 个副本，生产环境至少 3 个。 Erasure Code：是一种较为节约的方式。EC 把数据拆分 n 段（spec.erasureCoded.dataChunks），再加入 k 个代码段（spec.erasureCoded.codingChunks），用分布的方式把 n+k 段数据保存在磁盘上。这种情况下 Ceph 能够隔离 k 个 OSD 的损失。 我们这里使用副本的方式，创建如下所示的 RBD 类型的存储池：(pool.yaml)\n1 2 3 4 5 6 7 8 9 apiVersion: ceph.rook.io/v1 kind: CephBlockPool metadata: name: k8s-test-pool # operator会监听并创建一个pool，执行完后界面上也能看到对应的pool namespace: rook-ceph spec: failureDomain: host # 数据块的故障域: 值为host时，每个数据块将放置在不同的主机上;值为osd时，每个数据块将放置在不同的osd上 replicated: size: 3 # 池中数据的副本数,1就是不保存任何副本 直接创建上面的资源对象：\n1 2 $ kubectl apply -f pool.yaml cephblockpool.ceph.rook.io/k8s-test-pool created 存储池创建完成后我们在 Dashboard 上面的确可以看到新增了一个 pool，但是会发现集群健康状态变成了 WARN，我们可以查看到有如下日志出现：\n1 Health check update: too few PGs per OSD (6 \u0026lt; min 30) (TOO_FEW_PGS) 这是因为每个 osd 上的 pg 数量小于最小的数目30个。pgs 为8，因为是3副本的配置，所以当有4个 osd 的时候，每个 osd 上均分了8/4 *3=6个pgs，也就是出现了如上的错误小于最小配置30个，集群这种状态如果进行数据的存储和操作，集群会卡死，无法响应io，同时会导致大面积的 osd down。\n我们可以进入 toolbox 的容器中查看上面存储的 pg 数量：\n1 2 $ ceph osd pool get k8s-test-pool pg_num pg_num: 8 我们可以通过增加 pg_num 来解决这个问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ ceph osd pool set k8s-test-pool pg_num 64 set pool 1 pg_num to 64 $ ceph -s cluster: id: 7851387c-5d18-489a-8c04-b699fb9764c0 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c (age 33m) mgr: a(active, since 32m) osd: 4 osds: 4 up (since 32m), 4 in (since 32m) data: pools: 1 pools, 64 pgs objects: 0 objects, 0 B usage: 182 GiB used, 605 GiB / 787 GiB avail pgs: 64 active+clean 这个时候我们再查看就可以看到现在就是健康状态了。不过需要注意的是我们这里的 pool 上没有数据，所以修改 pg 影响并不大，但是如果是生产环境重新修改 pg 数，会对生产环境产生较大影响。因为 pg 数变了，就会导致整个集群的数据重新均衡和迁移，数据越大响应 io 的时间会越长。所以，最好在一开始就设置好 pg 数。\n现在我们来创建一个 StorageClass 来进行动态存储配置，如下所示我们定义一个 Ceph 的块存储的 StorageClass：(storageclass.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-block provisioner: rook-ceph.rbd.csi.ceph.com parameters: # clusterID 是 rook 集群运行的命名空间 clusterID: rook-ceph # 指定存储池 pool: k8s-test-pool # RBD image (实际的存储介质) 格式. 默认为 \u0026#34;2\u0026#34;. imageFormat: \u0026#34;2\u0026#34; # RBD image 特性. CSI RBD 现在只支持 `layering` . imageFeatures: layering # Ceph 管理员认证信息，这些都是在 clusterID 命名空间下面自动生成的 csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # 指定 volume 的文件系统格式，如果不指定, csi-provisioner 会默认设置为 `ext4` csi.storage.k8s.io/fstype: ext4 # uncomment the following to use rbd-nbd as mounter on supported nodes # **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi # issue that causes the mount to be disconnected. You will need to follow special upgrade steps # to restart your application pods. Therefore, this option is not recommended. #mounter: rbd-nbd reclaimPolicy: Delete 直接创建上面的 StorageClass 资源对象：\n1 2 3 4 5 $ kubectl apply -f storageclass.yaml storageclass.storage.k8s.io/rook-ceph-block created $ kubectl get storageclass NAME PROVISIONER AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com 35s 然后创建一个 PVC 来使用上面的 StorageClass 对象：(pvc.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim labels: app: wordpress spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 同样直接创建上面的 PVC 资源对象：\n1 2 3 4 5 $ kubectl apply -f pvc.yaml persistentvolumeclaim/mysql-pv-claim created $ kubectl get pvc -l app=wordpress NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-1eab82e3-d214-4d8e-8fcc-ed379c24e0e3 20Gi RWO rook-ceph-block 32m 创建完成后我们可以看到我们的 PVC 对象已经是 Bound 状态了，自动创建了对应的 PV，然后我们就可以直接使用这个 PVC 对象来做数据持久化操作了。\n这个时候可能集群还会出现如下的健康提示：\n1 2 3 4 5 6 7 $ ceph health detail HEALTH_WARN application not enabled on 1 pool(s) POOL_APP_NOT_ENABLED application not enabled on 1 pool(s) application not enabled on pool \u0026#39;k8s-test-pool\u0026#39; use \u0026#39;ceph osd pool application enable \u0026lt;pool-name\u0026gt; \u0026lt;app-name\u0026gt;\u0026#39;, where \u0026lt;app-name\u0026gt; is \u0026#39;cephfs\u0026#39;, \u0026#39;rbd\u0026#39;, \u0026#39;rgw\u0026#39;, or freeform for custom applications. $ ceph osd pool application enable k8s-test-pool k8srbd enabled application \u0026#39;k8srbd\u0026#39; on pool \u0026#39;k8s-test-pool\u0026#39; 根据提示启用一个 application 即可。\n在官方仓库 cluster/examples/kubernetes 目录下，官方给了个 wordpress 的例子，可以直接运行测试即可：\n1 2 $ kubectl apply -f mysql.yaml $ kubectl apply -f wordpress.yaml 官方的这个示例里面的 wordpress 用的 Loadbalancer 类型，我们可以改成 NodePort：\n1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl get pvc -l app=wordpress NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-1eab82e3-d214-4d8e-8fcc-ed379c24e0e3 20Gi RWO rook-ceph-block 12h wp-pv-claim Bound pvc-237932ed-5ca7-468c-bd16-220ebb2a1ce3 20Gi RWO rook-ceph-block 25s $ kubectl get pods -l app=wordpress NAME READY STATUS RESTARTS AGE wordpress-5b886cf59b-4xwn8 1/1 Running 0 24m wordpress-mysql-b9ddd6d4c-qhjd4 1/1 Running 0 24m $ kubectl get svc -l app=wordpress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress NodePort 10.106.253.225 \u0026lt;none\u0026gt; 80:30307/TCP 80s wordpress-mysql ClusterIP None \u0026lt;none\u0026gt; 3306/TCP 87s 当应用都处于 Running 状态后，我们可以通过 http://\u0026lt;任意节点IP\u0026gt;:30307 去访问 wordpress 应用：\n比如我们在第一篇文章中更改下内容，然后我们将应用 Pod 全部删除重建：\n1 2 3 4 5 6 7 $ kubectl delete pod wordpress-mysql-b9ddd6d4c-qhjd4 wordpress-5b886cf59b-4xwn8 pod \u0026#34;wordpress-mysql-b9ddd6d4c-qhjd4\u0026#34; deleted pod \u0026#34;wordpress-5b886cf59b-4xwn8\u0026#34; deleted $ kubectl get pods -l app=wordpress NAME READY STATUS RESTARTS AGE wordpress-5b886cf59b-kwxk4 1/1 Running 0 2m52s wordpress-mysql-b9ddd6d4c-kkcr7 1/1 Running 0 2m52s 当 Pod 重建完成后再次访问 wordpress 应用的主页我们可以发现之前我们添加的数据仍然存在，这就证明我们的数据持久化是正确的。\n原文地址：https://www.qikqiak.com/k8strain/storage/ceph/\n","date":"2021-04-02T18:01:08Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202104021801/","title":"Rook部署测试Ceph和wordpress实战应用"},{"content":"Kubernetes 1.18.3 部署 Traefik2.2 Traefik的中间件，灰度发布，流量复制基于上篇文章环境。\n1.部署whoami 应用和SVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: v1 kind: Service metadata: name: whoami spec: ports: - protocol: TCP name: web port: 80 selector: app: whoami --- kind: Deployment apiVersion: apps/v1 metadata: name: whoami labels: app: whoami spec: replicas: 2 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - name: whoami image: containous/whoami ports: - name: web containerPort: 80 2.部署 IngressRoute 对象 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: simpleingressroute spec: entryPoints: - web routes: - match: Host(`who.heian.com`) \u0026amp;\u0026amp; PathPrefix(`/notls`) kind: Rule services: - name: whoami port: 80 3.配置 HTTPS 路由 1 2 3 4 5 6 7 8 [root@k8s-master1 who]# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=who.heian.com\u0026#34; Generating a 2048 bit RSA private key ..........................................................................+++ ..........+++ writing new private key to \u0026#39;tls.key\u0026#39; ----- [root@k8s-master1 who]# kubectl create secret tls who-tls --cert=tls.crt --key=tls.key secret/who-tls created 创建一个 HTTPS 访问应用的 IngressRoute 对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroutetls spec: entryPoints: - websecure routes: - match: Host(`who.heian.com`) \u0026amp;\u0026amp; PathPrefix(`/tls`) kind: Rule services: - name: whoami port: 80 tls: secretName: who-tls 4 . 中间件 中间件是 Traefik2.0 中一个非常有特色的功能，我们可以根据自己的各种需求去选择不同的中间件来满足服务，Traefik 官方已经内置了许多不同功能的中间件，其中一些可以修改请求，头信息，一些负责重定向，一些添加身份验证等等，而且中间件还可以通过链式组合的方式来适用各种情况。\n同样比如上面我们定义的 whoami 这个应用，我们可以通过 https://who.heian.com/tls 来访问到应用，但是如果我们用 http 来访问的话呢就不行了，就会404了，因为我们根本就没有简单80端口这个入口点，\n所以要想通过 http 来访问应用的话自然我们需要监听下 web 这个入口点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroutetls-http spec: entryPoints: - web routes: - match: Host(`who.heian.com`) \u0026amp;\u0026amp; PathPrefix(`/tls`) kind: Rule services: - name: whoami port: 80 注意这里我们创建的 IngressRoute 的 entryPoints 是 web，然后创建这个对象，这个时候我们就可以通过 http 访问到这个应用了。\n但是我们如果只希望用户通过 https 来访问应用的话呢？按照以前的知识，我们是不是可以让 http 强制跳转到 https 服务去，对的，在 Traefik 中也是可以配置强制跳转的，只是这个功能现在是通过中间件来提供的了。\n如下所示，我们使用 redirectScheme 中间件来创建提供强制跳转服务：\n1 2 3 4 5 6 7 apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: redirect-https spec: redirectScheme: scheme: https 然后将这个中间件附加到 http 的服务上面去，因为 https 的不需要跳转：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: ingressroutetls-http spec: entryPoints: - web routes: - match: Host(`who.heian.com`) \u0026amp;\u0026amp; PathPrefix(`/tls`) kind: Rule services: - name: whoami port: 80 middlewares: - name: redirect-https 这个时候我们再去访问 http 服务可以发现就会自动跳转到 https 去了。关于更多中间件的用法可以查看文档 Traefik Docs。\n5. 灰度发布 Traefik2.0 的一个更强大的功能就是灰度发布，灰度发布我们有时候也会称为金丝雀发布（Canary），主要就是让一部分测试的服务也参与到线上去，经过测试观察看是否符号上线要求。\n比如现在我们有两个名为 appv1 和 appv2 的服务，我们希望通过 Traefik 来控制我们的流量，将 3⁄4 的流量路由到 appv1，¼ 的流量路由到 appv2 去，这个时候就可以利用 Traefik2.0 中提供的**带权重的轮询（WRR）**来实现该功能\n首先在 Kubernetes 集群中部署上面的两个服务。\n为了对比结果我们这里提供的两个服务一个是 heian99/myapp:v2，一个是heian99/myapp:v1，方便测试。\nappv1 服务的资源清单如下所示：（appv1.yaml）heian99/myapp:v1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: apps/v1 kind: Deployment metadata: name: appv1 spec: selector: matchLabels: app: appv1 template: metadata: labels: use: test app: appv1 spec: containers: - name: myappv1 image: heian99/myapp:v1 ports: - containerPort: 80 name: portv1 --- apiVersion: v1 kind: Service metadata: name: appv1 spec: selector: app: appv1 ports: - name: http port: 80 targetPort: portv1 appv2 服务的资源清单如下所示：（appv2.yaml）heian99/myapp:v2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: apps/v1 kind: Deployment metadata: name: appv2 spec: selector: matchLabels: app: appv2 template: metadata: labels: use: test app: appv2 spec: containers: - name: myappv2 image: heian99/myapp:v2 ports: - containerPort: 80 name: portv2 --- apiVersion: v1 kind: Service metadata: name: appv2 spec: selector: app: appv2 ports: - name: http port: 80 targetPort: portv2 直接创建上面两个服务：\n1 2 3 4 5 6 7 8 9 10 11 [root@k8s-master1 test]# kubectl apply -f . deployment.apps/appv1 created service/appv1 created deployment.apps/appv2 created service/appv2 created [root@k8s-master1 test]# kubectl get pods -l use=test NAME READY STATUS RESTARTS AGE appv1-597bbc966f-chwpj 1/1 Running 0 69s appv2-5f5489b4c5-7gsnd 1/1 Running 0 69s 在 Traefik2.1 中新增了一个 TraefikService 的 CRD 资源，我们可以直接利用这个对象来配置 WRR，之前的版本需要通过 File Provider，比较麻烦，新建一个描述 WRR 的资源清单：(wrr.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: app-wrr spec: weighted: services: - name: appv1 weight: 3 # 定义权重 port: 80 kind: Service # 可选，默认就是 Service - name: appv2 weight: 1 port: 80 然后为我们的灰度发布的服务创建一个 IngressRoute 资源对象：(ingressroute.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: wrringressroute namespace: default spec: entryPoints: - web routes: - match: Host(`app.heian.com`) kind: Rule services: - name: app-wrr kind: TraefikService 不过需要注意的是现在我们配置的 Service 不再是直接的 Kubernetes 对象了，而是上面我们定义的 TraefikService 对象，直接创建上面的两个资源对象，这个时候我们对域名 app.heian.com 做上解析，去浏览器中连续访问 4 次，我们可以观察到 appv1 这应用会收到 3 次请求，而 appv2 这个应用只收到 1 次请求，符合上面我们的 3:1 的权重配置。\n6. 流量复制 除了灰度发布之外，Traefik 2.0 还引入了流量镜像服务，是一种可以将流入流量复制并同时将其发送给其他服务的方法，镜像服务可以获得给定百分比的请求同时也会忽略这部分请求的响应。\n同样的在 2.0 中只能通过 FileProvider 进行配置，在 2.1 版本中我们已经可以通过 TraefikService 资源对象来进行配置了，现在我们部署两个 whoami 的服务，资源清单文件如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 apiVersion: v1 kind: Service metadata: name: v1 spec: ports: - protocol: TCP name: web port: 80 selector: app: v1 --- kind: Deployment apiVersion: apps/v1 metadata: name: v1 labels: app: v1 spec: selector: matchLabels: app: v1 template: metadata: labels: app: v1 spec: containers: - name: v1 image: heian99/myapp:v1 ports: - name: web containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: v2 spec: ports: - protocol: TCP name: web port: 80 selector: app: v2 --- kind: Deployment apiVersion: apps/v1 metadata: name: v2 labels: app: v2 spec: selector: matchLabels: app: v2 template: metadata: labels: app: v2 spec: containers: - name: v2 image: heian99/myapp:v2 ports: - name: web containerPort: 80 现在我们创建一个 IngressRoute 对象，将服务 v1 的流量复制 50% 到服务 v2，如下资源对象所示：(mirror-ingress-route.yaml)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: app-mirror spec: mirroring: name: v1 # 发送 100% 的请求到 K8S 的 Service \u0026#34;v1\u0026#34; port: 80 mirrors: - name: v2 # 然后复制 50% 的请求到 v2 percent: 50 port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: mirror-ingress-route namespace: default spec: entryPoints: - web routes: - match: Host(`mirror.heian.com`) kind: Rule services: - name: app-mirror kind: TraefikService # 使用声明的 TraefikService 服务，而不是 K8S 的 Service 这个时候我们在浏览器中去连续访问4次 mirror.heian.com 可以发现有一半的请求也出现在了 v2 这个服务中：\n","date":"2021-03-29T20:31:53Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/202103292031/","title":"Traefik的中间件，灰度发布，流量复制"},{"content":"Kubernetes 1.18.3 部署 Traefik2.0 Centos 3.10.0-693.el7.x86_64\nKubernetes 1.18.3\nTraefik 2.2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@k8s-master1 traefik]# kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} etcd-2 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} etcd-1 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} [root@k8s-master1 traefik]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready \u0026lt;none\u0026gt; 6d1h v1.18.3 k8s-node1 Ready \u0026lt;none\u0026gt; 6d v1.18.3 k8s-node2 Ready \u0026lt;none\u0026gt; 6d v1.18.3 [root@k8s-master1 traefik]# uname -a Linux k8s-master1 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux Traefik 2.0 官方文档：https://docs.traefik.io/v2.0/\n1.Traefik 介绍 traefik 是一款反向代理、负载均衡服务，使用 golang 实现的。和 nginx 最大的不同是，它支持自动化更新反向代理和负载均衡配置。在微服务架构越来越流行的今天，一个业务恨不得有好几个数据库、后台服务和 webapp，开发团队拥有一款 “智能” 的反向代理服务，为他们简化服务配置。traefik 就是为了解决这个问题而诞生的。\nTraefik 2.2新增的功能如下：\n1. 支持了udp\n2. traefik2.2 支持使用K/V存储做为动态配置的源，分别是 consul, etcd, Redis, zookeeper\n3. 能够使用kubernetes CRD自定义资源定义UDP负载平衡 IngressRouteUDP。\n4. 能够使用 rancher， consul catalog， docker和 marathon中的标签定义UDP的负载平衡\n5. 增加了对ingress注解的主持\n6. 将TLS存储功能 TLSStores添加到Kubernetes CRD中，使kubernetes用户无需使用配置文件和安装证书即可提供默认证书。\n7. 在日志中增加了http的请求方式,是http还是https\n8. 因为TLS的配置可能会影响CPU的使用率，因此增加了 TLS version和 TLS cipher使用的指标信息\n9. 当前的WRR算法对于权重不平衡端点存在严重的偏差问题，将EDF调度算法用于WeightedRoundRobin， Envoy也是使用了 EOF调度算法\n10. 支持请求主体用于流量镜像\n11. 增加了 ElasticAPM作为traefik的tracing系统。\n12. Traefik的Dashboard增加了UDP的页面\n13. Traefik也增加了黑暗主题\n2.部署 Traefik 2.0 在 traefik v2.0 版本后，开始使用 CRD（Custom Resource Definition）来完成路由配置等，所以需要提前创建 CRD 资源。\n下面进行安装过程。\n注：我们这里是将traefik部署在ingress-traefik命名空间，如果你需要部署在其他命名空间，需要更改资源清单，如果你是部署在和我同样的命令空间中，你需要创建该命名空间。\n创建命名空间：\n1 kubectl create ns ingress-traefik 创建CRD资源\nTraefik 2.0版本后开始使用CRD来对资源进行管理配置，所以我们需要先创建CRD资源。\ntraefik-crd.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 ## IngressRoute apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ingressroutes.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: IngressRoute plural: ingressroutes singular: ingressroute --- ## IngressRouteTCP apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ingressroutetcps.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: IngressRouteTCP plural: ingressroutetcps singular: ingressroutetcp --- ## Middleware apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: middlewares.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: Middleware plural: middlewares singular: middleware --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: tlsoptions.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: TLSOption plural: tlsoptions singular: tlsoption --- ## TraefikService apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: traefikservices.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: TraefikService plural: traefikservices singular: traefikservice --- ## TraefikTLSStore apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: tlsstores.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: TLSStore plural: tlsstores singular: tlsstore --- ## IngressRouteUDP apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ingressrouteudps.traefik.containo.us spec: scope: Namespaced group: traefik.containo.us version: v1alpha1 names: kind: IngressRouteUDP plural: ingressrouteudps singular: ingressrouteudp 部署 CRD 资源\n1 kubectl apply -f traefik-crd.yaml 创建 RBAC 权限\nKubernetes 在 1.6 以后的版本中引入了基于角色的访问控制（RBAC）策略，方便对 Kubernetes 资源和 API 进行细粒度控制。Traefik 需要一定的权限，所以这里提前创建好 Traefik ServiceAccount 并分配一定的权限。\ntraefik-rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 apiVersion: v1 kind: ServiceAccount metadata: namespace: ingress-traefik name: traefik-ingress-controller --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;,\u0026#34;endpoints\u0026#34;,\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;ingresses\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;ingresses/status\u0026#34;] verbs: [\u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;traefik.containo.us\u0026#34;] resources: [\u0026#34;middlewares\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;traefik.containo.us\u0026#34;] resources: [\u0026#34;ingressroutes\u0026#34;,\u0026#34;traefikservices\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;traefik.containo.us\u0026#34;] resources: [\u0026#34;ingressroutetcps\u0026#34;,\u0026#34;ingressrouteudps\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;traefik.containo.us\u0026#34;] resources: [\u0026#34;tlsoptions\u0026#34;,\u0026#34;tlsstores\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: ingress-traefik 部署 Traefik RBAC 资源\n1 kubectl apply -f traefik-rbac.yaml 创建 Traefik 配置文件\n由于 Traefik 配置很多，使用 CLI 定义操作过于繁琐，尽量使用将其配置选项放到配置文件中，然后存入 ConfigMap，将其挂入 traefik 中。\ntraefik-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 kind: ConfigMap apiVersion: v1 metadata: name: traefik-config namespace: ingress-traefik data: traefik.yaml: |- serversTransport: insecureSkipVerify: true api: insecure: true dashboard: true debug: true metrics: prometheus: \u0026#34;\u0026#34; entryPoints: web: address: \u0026#34;:80\u0026#34; websecure: address: \u0026#34;:443\u0026#34; providers: kubernetesCRD: \u0026#34;\u0026#34; kubernetesingress: \u0026#34;\u0026#34; log: filePath: \u0026#34;\u0026#34; level: error format: json accessLog: filePath: \u0026#34;\u0026#34; format: json bufferingSize: 0 filters: retryAttempts: true minDuration: 20 fields: defaultMode: keep names: ClientUsername: drop headers: defaultMode: keep names: User-Agent: redact Authorization: drop Content-Type: keep 部署 Traefik ConfigMap 资源\n1 kubectl apply -f traefik-config.yaml -n kube-system 设置Label标签\n由于使用的Kubernetes DeamonSet方式部署Traefik，所以需要提前给节点设置Label，当程序部署Pod会自动调度到设置 Label的node节点上。\n节点设置 Label 标签 1 kubectl label nodes k8s-node-1 IngressProxy=true 验证是否成功\n1 2 3 4 5 [root@k8s-master1 traefik]# kubectl get node --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master Ready \u0026lt;none\u0026gt; 6d1h v1.18.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux k8s-node1 Ready \u0026lt;none\u0026gt; 6d1h v1.18.3 IngressProxy=true,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linux k8s-node2 Ready \u0026lt;none\u0026gt; 6d v1.18.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node2,kubernetes.io/os=linux 节点删除Label标签\n1 kubectl label nodes k8s-node-1 IngressProxy- Kubernetes 部署 Traefik\n按照以前Traefik1.7部署方式，使用DaemonSet类型部署，以便于在多服务器间扩展，使用 hostport 方式占用服务器 80、443 端口，方便流量进入。\ntraefik-deploy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 apiVersion: v1 kind: Service metadata: name: traefik namespace: ingress-traefik spec: ports: - name: web port: 80 - name: websecure port: 443 - name: admin port: 8080 selector: app: traefik --- apiVersion: apps/v1 kind: DaemonSet metadata: name: traefik-ingress-controller namespace: ingress-traefik labels: app: traefik spec: selector: matchLabels: app: traefik template: metadata: name: traefik labels: app: traefik spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 1 containers: - image: traefik:2.2.0 name: traefik-ingress-lb ports: - name: web containerPort: 80 hostPort: 80 #hostPort方式，将端口暴露到集群节点 - name: websecure containerPort: 443 hostPort: 443 #hostPort方式，将端口暴露到集群节点 - name: admin containerPort: 8080 resources: limits: cpu: 2000m memory: 1024Mi requests: cpu: 1000m memory: 1024Mi securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --configfile=/config/traefik.yaml volumeMounts: - mountPath: \u0026#34;/config\u0026#34; name: \u0026#34;config\u0026#34; volumes: - name: config configMap: name: traefik-config tolerations: #设置容忍所有污点，防止节点被设置污点 - operator: \u0026#34;Exists\u0026#34; nodeSelector: #设置node筛选器，在特定label的节点上启动 IngressProxy: \u0026#34;true\u0026#34; 部署 Traefik\n1 kubectl apply -f traefik-deploy.yaml 3.Traefik 路由规则基础配置 配置 HTTP 路由规则 （Traefik Dashboard 为例）\nTraefik 应用已经部署完成，但是想让外部访问 Kubernetes 内部服务，还需要配置路由规则，这里开启了 Traefik Dashboard 配置，所以首先配置 Traefik Dashboard 看板的路由规则，使外部能够访问 Traefik Dashboard。\ntraefik-dashboard-route.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-dashboard-route namespace: ingress-traefik spec: entryPoints: - web routes: - match: Host(`traefik.example.cn`) kind: Rule services: - name: traefik port: 8080 部署Traefik Dashboard 路由规则对象\n1 kubectl apply -f traefik-dashboard-route.yaml 1 2 接下来配置dnsmasq，客户端想通过域名访问服务，必须要进行DNS解析，我使用的本地 DNS 服务器进行域名解析，将 Traefik 指定节点的 IP 和自定义 域名 绑定，重启dnsmasq服务即可。 打开任意浏览器输入地址：http://traefik.example.cn进行访问，此处没有配置验证登录，如果想配置验证登录，使用middleware即可。 4. 配置 HTTPS 路由规则（Kubernetes Dashboard） 这里我们创建 Kubernetes 的 Dashboard，它是 基于 Https 协议方式访问，由于它是需要使用 Https 请求，所以我们需要配置 Https 的路由规则并指定证书。\n测试域名：kuboard.heian.com （内网建立的）\n创建证书文件\n1 2 3 4 5 # 创建自签名证书 openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=kuboard.heian.com\u0026#34; # 将证书存储到Kubernetes Secret中，新建的k8dash-sa-tls必须与k8dash-route中的tls: secretName一致。 kubectl create secret tls k8dash-sa-tls --key=tls.key --cert=tls.crt -n kube-system k8dash-route.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: k8dash-sa-route namespace: kube-system spec: entryPoints: - websecure tls: secretName: k8dash-sa-tls routes: - match: Host(`kuboard.heian.com`) kind: Rule services: # 此处的services是Kubernetes中的svc name 与 端口 可以使用kubectl get svc --namespace=kube-system获取 - name: kuboard port: 80 打开任意浏览器输入地址：https://kuboard.heian.com进行访问 ","date":"2021-03-29T17:15:18Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/202103291715/","title":"Kubernetes 1.18.3 部署 Traefik2.2"},{"content":"ubernetes部署metrics-server后执行kubectl top pod或kubectl top node报错\nError from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)\n一、问题检查步骤：\n1.1、查看metrics-server服务日志\n1 Cluster doesn\u0026#39;t provide requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won\u0026#39;t work. 检查发现是由于调用metrics-server无权限，返回了http 403错误\n1.2、检查是否配置了以下参数\n1 2 3 4 5 args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-insecure-tls=true - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,externalDNS 1.3、问题总结：\nmetrics-server服务配置是没有问题的，但服务依然报错 Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)，有两种方法可以解决问题\n1、授权集群角色给用户system:anonymous\n1 kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous 2、创建system:metrics-server角色并授权\n二、问题解决(创建system:metrics-server角色并授权)\n配置metrics-server证书\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # vim metrics-server-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;system:metrics-server\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;system\u0026#34; } ] } 1 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes metrics-server-csr.json | cfssljson -bare metrics-server 配置metrics-server RBAC授权\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat \u0026gt; auth-metrics-server.yaml \u0026lt;\u0026lt; EOF --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:auth-metrics-server-reader labels: rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-admin: \u0026#34;true\u0026#34; rules: - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metrics-server:system:auth-metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-metrics-server-reader subjects: - kind: User name: system:metrics-server namespace: kube-system EOF kube-apiserver添加metrics-server需要的配置\n1 2 3 4 5 6 7 --requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \\ --requestheader-allowed-names=aggregator,metrics-server \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --proxy-client-cert-file=/opt/kubernetes/ssl/metrics-server.pem \\ --proxy-client-key-file=/opt/kubernetes/ssl/metrics-server-key.pem 检查是否能够正常获取到监控信息\n","date":"2021-03-23T19:16:02Z","image":"https://www.ownit.top/title_pic/04.jpg","permalink":"https://www.ownit.top/p/202103231916/","title":"二进制部署Kubernetes安装metrics-server遇到的问题"},{"content":"1、什么是ingress ngress（在kubernetes v1.1时添加）暴露从集群外到集群内服务的HTTP或HTTPS路由。定义在ingress资源上的规则控制流量的路由。\n1 2 3 4 5 internet | [ Ingress ] --|-----|-- [ Services ] 一个ingress可以配置用于提供外部可访问的服务url、负载均衡流量、SSL终端和提供虚拟主机名配置。ingress controller负责实现（通常使用负载均衡器(loadbalancer)）入口（ingress）。但是它也可以配置你的边缘路由器或额外的前端来帮助处理流量。\ningress不暴露任何端口或协议。将HTTP和HTTPS之外的服务公开到因特网通常使用类型是NodePort或loadbalance的service。\n2、Ingress区别 差别：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md\nIngress-nginx Ingress-nginx：kubernetes官方维护的ingress\nIngress-nginx的官方文档：https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#rewrite\nIngress-nginx Github：https://github.com/kubernetes/ingress-nginx\nNginx-ingress Nginx-ingress：nginx官方维护的ingress\nNginx-ingress的官方文档：https://docs.nginx.com/nginx-ingress-controller/configuration/ingress-resources/advanced-configuration-with-annotations/\nNginx-ingress Github：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/nginx-ingress-controllers.md\n类似的还有：Traefik、HAProxy、Istio\n3、Ingress-nginx部署 （1）yaml部署Ingress-nginx\n（2）helm部署Ingress-nginx\nIngress-nginx的高级介绍，我这边以Kubernetes的那个插件为主。\n4、Ingress-nginx创建流程 （1）首先创建pod\n（2）创建service\n（3）创建ingress-nginx\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 kubectl create ns heian #创建命名空间后，运行下面yaml，就可以实现上面三个步骤的工作 --- apiVersion: apps/v1 kind: Deployment metadata: namespace: heian name: ingress-heian annotations: k8s.kuboard.cn/workload: ingress-heian deployment.kubernetes.io/revision: \u0026#39;1\u0026#39; k8s.kuboard.cn/service: ClusterIP k8s.kuboard.cn/ingress: \u0026#39;true\u0026#39; labels: app: ingress-heian spec: selector: matchLabels: app: ingress-heian revisionHistoryLimit: 10 template: metadata: labels: app: ingress-heian spec: affinity: {} securityContext: seLinuxOptions: {} imagePullSecrets: [] restartPolicy: Always initContainers: [] containers: - image: \u0026#39;wangyanglinux/myapp:v1\u0026#39; imagePullPolicy: IfNotPresent name: ingress-heian volumeMounts: - name: tz-config mountPath: /usr/share/zoneinfo/Asia/Shanghai - name: tz-config mountPath: /etc/localtime - name: timezone mountPath: /etc/timezone resources: limits: cpu: 100m memory: 100Mi requests: cpu: 10m memory: 10Mi env: - name: TZ value: Asia/Shanghai - name: LANG value: C.UTF-8 lifecycle: {} ports: - name: web containerPort: 80 protocol: TCP terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumes: - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Shanghai type: \u0026#39;\u0026#39; - name: timezone hostPath: path: /etc/timezone type: \u0026#39;\u0026#39; dnsPolicy: ClusterFirst dnsConfig: options: [] schedulerName: default-scheduler terminationGracePeriodSeconds: 30 progressDeadlineSeconds: 600 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 0 maxSurge: 1 replicas: 1 --- apiVersion: v1 kind: Service metadata: namespace: heian name: ingress-heian annotations: k8s.kuboard.cn/workload: ingress-heian labels: app: ingress-heian spec: selector: app: ingress-heian type: ClusterIP ports: - port: 80 targetPort: 80 protocol: TCP name: ingress-web-1 nodePort: 0 sessionAffinity: None --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: namespace: heian name: ingress-heian annotations: k8s.kuboard.cn/workload: ingress-heian labels: app: ingress-heian spec: rules: - host: ingress.heian.com http: paths: - path: / backend: serviceName: ingress-heian servicePort: 80 效果截图：\n5、Ingress-nginx的域名重定向（Redirect） annotations声明\n1 2 3 #重定向，就是这一句 ，现在访问这个域名，会重定向到我的博客地址 annotations: nginx.ingress.kubernetes.io/permanent-redirect: \u0026#39;https://blog.csdn.net/heian_99\u0026#39; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/permanent-redirect: \u0026#39;https://blog.csdn.net/heian_99\u0026#39; name: ingress-heian namespace: heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: / pathType: ImplementationSpecific 这个是ingress-nginx里面nginx的配置\n6、Ingress-nginx的前后端分离（Rewrite） 1 2 annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 name: ingress-heian namespace: heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: /something(/|$)(.*) pathType: ImplementationSpecific 7、Ingress-nginx的SSL配置 官网：https://kubernetes.github.io/ingress-nginx/user-guide/tls/\n创建自建证书（主：浏览器不认可的）\nIngress-nginx配置了SSL，会自动跳转到https的网页的\n1 2 3 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.cert -subj \u0026#34;/CN=ingress.heian.com/O=ingress.heian.com\u0026#34; kubectl create secret tls ca-ceart --key tls.key --cert tls.cert -n heian 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 name: ingress-heian namespace: heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: /something(/|$)(.*) pathType: ImplementationSpecific tls: - hosts: - ingress.heian.com secretName: ca-ceart 1 2 3 4 tls: - hosts: - ingress.heian.com secretName: ca-ceart 禁用https强制跳转\n1 2 annotations: nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; 设置默认证书：--default-ssl-certificate=default/foo-tls\n更改的ingress-controller的启动参数\n8、Ingress-nginx的黑白名单 Annotations：只对指定的ingress生效\nConfigMap：全局生效\n黑名单可以使用ConfigMap去配置，白名单建议使用Annotations去配置。\n（1）、白名单 添加白名单的方式可以直接写annotation，也可以配置在ConfigMap中。 写在annotation中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/whitelist-source-range: 192.168.0.100 name: ingress-heian namespace: heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: / pathType: ImplementationSpecific 也可以写固定IP，也可以写网段。 配置到ConfigMap中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-2.1.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.32.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: whitelist-source-range: 10.1.10.0/24 （2）、黑名单 黑名单就只能通过ConfigMap来配置 ConfigMap配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-2.1.0 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.32.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: whitelist-source-range: 10.1.10.0/24 block-cidrs: 10.1.10.100 annotation配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/server-snippet: |- deny 192.168.0.1; deny 192.168.0.100; allow all; creationTimestamp: null name: ingress-heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: / pathType: ImplementationSpecific status: loadBalancer: {} 9、Ingress-nginx的匹配请求头 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/server-snippet: |- set $agentflag 0; if ($http_user_agent ~* \u0026#34;(iPhone)\u0026#34; ){ set $agentflag 1; } if ( $agentflag = 1 ) { return 301 https://m.heian.com; } creationTimestamp: null name: ingress-heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: / pathType: ImplementationSpecific status: loadBalancer: {} 10、Ingress-nginx的速率限制 1 2 3 4 5 6 7 8 9 10 限速¶ 这些注释定义了对连接和传输速率的限制。这些可以用来减轻DDoS攻击。 nginx.ingress.kubernetes.io/limit-connections：单个IP地址允许的并发连接数。超出此限制时，将返回503错误。 nginx.ingress.kubernetes.io/limit-rps：每秒从给定IP接受的请求数。突发限制设置为此限制乘以突发乘数，默认乘数为5。当客户端超过此限制时，将 返回limit-req-status-code默认值： 503。 nginx.ingress.kubernetes.io/limit-rpm：每分钟从给定IP接受的请求数。突发限制设置为此限制乘以突发乘数，默认乘数为5。当客户端超过此限制时，将 返回limit-req-status-code默认值： 503。 nginx.ingress.kubernetes.io/limit-burst-multiplier：突发大小限制速率的倍数。默认的脉冲串乘数为5，此注释将覆盖默认的乘数。当客户端超过此限制时，将 返回limit-req-status-code默认值： 503。 nginx.ingress.kubernetes.io/limit-rate-after：最初的千字节数，在此之后，对给定连接的响应的进一步传输将受到速率的限制。必须在启用代理缓冲的情况下使用此功能。 nginx.ingress.kubernetes.io/limit-rate：每秒允许发送到给定连接的千字节数。零值禁用速率限制。必须在启用代理缓冲的情况下使用此功能。 nginx.ingress.kubernetes.io/limit-whitelist：客户端IP源范围要从速率限制中排除。该值是逗号分隔的CIDR列表。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-nginx annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/limit-rate: 100K nginx.ingress.kubernetes.io/limit-whitelist: 10.1.10.100 nginx.ingress.kubernetes.io/limit-rps: 1 nginx.ingress.kubernetes.io/limit-rpm: 30 spec: rules: - host: iphone.coolops.cn http: paths: - path: backend: serviceName: ng-svc servicePort: 80 nginx.ingress.kubernetes.io/limit-rate：限制客户端每秒传输的字节数 nginx.ingress.kubernetes.io/limit-whitelist：白名单中的IP不限速 nginx.ingress.kubernetes.io/limit-rps：单个IP每秒的连接数 nginx.ingress.kubernetes.io/limit-rpm：单个IP每分钟的连接数 11、Ingress-nginx的基本认证 有些访问是需要认证访问的，比如dubbo-admin，我们在访问的时候会先叫你输入用户名和密码。ingress nginx也可以实现这种。\n（1）、创建密码，我这里用http的命令工具来生成 1 2 3 4 5 6 7 8 9 [root@k8s-master01 ingress]# htpasswd -c auth heian New password: Re-type new password: Adding password for user heian [root@k8s-master01 ingress]# ls auth tls.cert tls.key [root@k8s-master01 ingress]# cat auth heian:$apr1$8LffOJL7$ZIGV4XRNSuginqO5GMxAZ. [root@k8s-master01 ingress]# （2）、创建secret 1 2 [root@k8s-master01 ingress]# kubectl create secret generic basic-auth --from-file=auth -n heian secret/basic-auth created （3）、配置Ingress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/auth-realm: Need to longin nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-type: basic creationTimestamp: null name: ingress-heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian servicePort: 80 path: / pathType: ImplementationSpecific status: loadBalancer: {} 12、Ingress-nginx实现灰度金丝雀发布 Nginx Annotations 支持以下 4 种 Canary 规则：\nnginx.ingress.kubernetes.io/canary-by-header：基于 Request Header 的流量切分，适用于灰度发布以及 A/B 测试。当 Request Header 设置为 always时，请求将会被一直发送到 Canary 版本；当 Request Header 设置为 never时，请求不会被发送到 Canary 入口；对于任何其他 Header 值，将忽略 Header，并通过优先级将请求与其他金丝雀规则进行优先级的比较。 nginx.ingress.kubernetes.io/canary-by-header-value：要匹配的 Request Header 的值，用于通知 Ingress 将请求路由到 Canary Ingress 中指定的服务。当 Request Header 设置为此值时，它将被路由到 Canary 入口。该规则允许用户自定义 Request Header 的值，必须与上一个 annotation (即：canary-by-header）一起使用。 nginx.ingress.kubernetes.io/canary-weight：基于服务权重的流量切分，适用于蓝绿部署，权重范围 0 - 100 按百分比将请求路由到 Canary Ingress 中指定的服务。权重为 0 意味着该金丝雀规则不会向 Canary 入口的服务发送任何请求。权重为 100 意味着所有请求都将被发送到 Canary 入口。 nginx.ingress.kubernetes.io/canary-by-cookie：基于 Cookie 的流量切分，适用于灰度发布与 A/B 测试。用于通知 Ingress 将请求路由到 Canary Ingress 中指定的服务的cookie。当 cookie 值设置为 always时，它将被路由到 Canary 入口；当 cookie 值设置为 never时，请求不会被发送到 Canary 入口；对于任何其他值，将忽略 cookie 并将请求与其他金丝雀规则进行优先级的比较。 定义两个版本的代码。 创建两个同域名的ingress\nv2版\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: heian nginx.ingress.kubernetes.io/canary-by-header-value: v2 nginx.ingress.kubernetes.io/canary-weight: \u0026#34;50\u0026#34; name: ingress-heian2 namespace: heian spec: rules: - host: ingress.heian.com http: paths: - backend: serviceName: ingress-heian2 servicePort: 80 path: / pathType: ImplementationSpecific 13、Ingress-nginx自定义错误页面 github地址：https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/customization/custom-errors/custom-default-backend.yaml\n1 kubectl apply -f error.yaml -n heian 修改ds配置文件，添加这个\n1 - --default-backend-service=heian/nginx-errors 验证\n执行 kubectl get pod \\-n ingress-nginx 查看pod是否已经重启过， 如果没有自动重启，需要杀掉pod。\n打开一个不存在的链接， 查看是否显示的是定义的错误页面。\n部分浏览器不支持页面显示，如谷歌浏览器会显示“无法显示此网站”\n","date":"2021-03-20T17:38:51Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/202103201738/","title":"Kubernetes Ingress-nginx高级用法"},{"content":"prometheus黑盒测试 这次使用基于黑盒测试上面。用来收集监控blackbox的数据\nhttps://github.com/prometheus/blackbox_exporter\nhttps://github.com/prometheus/blackbox_exporter/blob/master/blackbox.yml\nhttps://grafana.com/grafana/dashboards/5345\n1、创建secrets 官网地址：https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/additional-scrape-config.md\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat prometheus-additional.yaml - job_name: \u0026#39;blackbox\u0026#39; metrics_path: /probe params: module: [http_2xx] # Look for a HTTP 200 response. static_configs: - targets: - http://www.baidu.com\t# 这里我们监控百度网站测试 relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: blackbox-exporter:9115 # The blackbox exporter\u0026#39;s real hostname:port. 1 2 3 kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml --dry-run -oyaml \u0026gt; additional-scrape-configs.yaml kubectl apply -f additional-scrape-configs.yaml -n monitoring 2、修改Prometheus的CRD 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [root@k8s-master01 manifests]# vim prometheus-prometheus.yaml apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: labels: prometheus: k8s spec: alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web image: quay.io/prometheus/prometheus:v2.15.2 nodeSelector: kubernetes.io/os: linux podMonitorNamespaceSelector: {} podMonitorSelector: {} replicas: 1 resources: requests: memory: 700Mi ruleSelector: matchLabels: prometheus: k8s role: alert-rules securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 1000 serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: {} serviceMonitorSelector: {} version: v2.15.2 additionalScrapeConfigs: name: additional-scrape-configs key: prometheus-additional.yaml [root@k8s-master01 manifests]# kubectl replace -f prometheus-prometheus.yaml prometheus.monitoring.coreos.com/k8s replaced 3、Prometheus测试验证 ","date":"2021-03-18T22:21:16Z","image":"https://www.ownit.top/title_pic/43.jpg","permalink":"https://www.ownit.top/p/202103182221/","title":"Prometheus_additional传统配置"},{"content":"白盒监控：监控一些内部的数据，topic的监控数据，Redis key的大小。内部暴露的指标被称为白盒监控。比较关注的是原因。\n黑盒监控：站在用户的角度看到的东西。网站不能打开，网站打开的比较慢。比较关注现象，表示正在发生的问题，正在发生的告警。\ngithub文档：https://github.com/prometheus/blackbox_exporter\n关于部署，已经做成yaml格式，一键部署上去就可以了\n创建comfingmap -\u0026ndash;》 挂载到deployments --》暴露service服务使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 [root@k8s-master01 config]# cat exe.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: blackbox-exporter name: blackbox-exporter namespace: monitoring spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: blackbox-exporter strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: blackbox-exporter spec: containers: - args: - --config.file=/mnt/blackbox.yml env: - name: TZ value: Asia/Shanghai - name: LANG value: C.UTF-8 image: prom/blackbox-exporter:master imagePullPolicy: IfNotPresent lifecycle: {} name: blackbox-exporter ports: - containerPort: 9115 name: web protocol: TCP resources: limits: cpu: 324m memory: 443Mi requests: cpu: 10m memory: 10Mi securityContext: allowPrivilegeEscalation: false privileged: false readOnlyRootFilesystem: false runAsNonRoot: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/zoneinfo/Asia/Shanghai name: tz-config - mountPath: /etc/localtime name: tz-config - mountPath: /etc/timezone name: timezone - mountPath: /mnt name: config dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - hostPath: path: /usr/share/zoneinfo/Asia/Shanghai type: \u0026#34;\u0026#34; name: tz-config - hostPath: path: /etc/timezone type: \u0026#34;\u0026#34; name: timezone - configMap: defaultMode: 420 name: blackbox-conf name: config --- apiVersion: v1 data: blackbox.yml: |- modules: http_2xx: prober: http http_post_2xx: prober: http http: method: POST tcp_connect: prober: tcp pop3s_banner: prober: tcp tcp: query_response: - expect: \u0026#34;^+OK\u0026#34; tls: true tls_config: insecure_skip_verify: false ssh_banner: prober: tcp tcp: query_response: - expect: \u0026#34;^SSH-2.0-\u0026#34; irc_banner: prober: tcp tcp: query_response: - send: \u0026#34;NICK prober\u0026#34; - send: \u0026#34;USER prober prober prober :prober\u0026#34; - expect: \u0026#34;PING :([^ ]+)\u0026#34; send: \u0026#34;PONG ${1}\u0026#34; - expect: \u0026#34;^:[^ ]+ 001\u0026#34; icmp: prober: icmp kind: ConfigMap metadata: name: blackbox-conf namespace: monitoring --- apiVersion: v1 kind: Service metadata: labels: app: blackbox-exporter name: blackbox-exporter namespace: monitoring spec: ports: - name: container-1-web-1 port: 9115 protocol: TCP targetPort: 9115 selector: app: blackbox-exporter sessionAffinity: None type: ClusterIP 测试验证\n1 curl http://10.96.203.216:9115/probe?target=www.baidu.com\u0026amp;module=http_2xx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 [root@k8s-master01 config]# curl http://10.96.203.216:9115/probe?target=www.baidu.com\u0026amp;module=http_2xx [1] 4221 #获取baidu.com的相关信息 [root@k8s-master01 config]# # HELP probe_dns_lookup_time_seconds Returns the time taken for probe dns lookup in seconds # TYPE probe_dns_lookup_time_seconds gauge probe_dns_lookup_time_seconds 0.338946599 # HELP probe_duration_seconds Returns how long the probe took to complete in seconds # TYPE probe_duration_seconds gauge probe_duration_seconds 0.682111152 # HELP probe_failed_due_to_regex Indicates if probe failed due to regex # TYPE probe_failed_due_to_regex gauge probe_failed_due_to_regex 0 # HELP probe_http_content_length Length of http content response # TYPE probe_http_content_length gauge probe_http_content_length -1 # HELP probe_http_duration_seconds Duration of http request by phase, summed over all redirects # TYPE probe_http_duration_seconds gauge probe_http_duration_seconds{phase=\u0026#34;connect\u0026#34;} 0.00913168 probe_http_duration_seconds{phase=\u0026#34;processing\u0026#34;} 0.008949937 probe_http_duration_seconds{phase=\u0026#34;resolve\u0026#34;} 0.338946599 probe_http_duration_seconds{phase=\u0026#34;tls\u0026#34;} 0 probe_http_duration_seconds{phase=\u0026#34;transfer\u0026#34;} 0.324605594 # HELP probe_http_redirects The number of redirects # TYPE probe_http_redirects gauge probe_http_redirects 0 # HELP probe_http_ssl Indicates if SSL was used for the final redirect # TYPE probe_http_ssl gauge probe_http_ssl 0 # HELP probe_http_status_code Response HTTP status code # TYPE probe_http_status_code gauge probe_http_status_code 200 # HELP probe_http_uncompressed_body_length Length of uncompressed response body # TYPE probe_http_uncompressed_body_length gauge probe_http_uncompressed_body_length 297140 # HELP probe_http_version Returns the version of HTTP of the probe response # TYPE probe_http_version gauge probe_http_version 1.1 # HELP probe_ip_addr_hash Specifies the hash of IP address. It\u0026#39;s useful to detect if the IP address changes. # TYPE probe_ip_addr_hash gauge probe_ip_addr_hash 2.882632397e+09 # HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6 # TYPE probe_ip_protocol gauge probe_ip_protocol 4 # HELP probe_success Displays whether or not the probe was a success # TYPE probe_success gauge probe_success 1 ","date":"2021-03-18T18:01:08Z","image":"https://www.ownit.top/title_pic/45.jpg","permalink":"https://www.ownit.top/p/202103181801/","title":"prometheus黑盒测试"},{"content":"我前面使用Kubernetes构建prometheus监控，没有什么大问题，但是monitoring/kube-controller-manager/0 (0/0 up)，这个没有数值。\n一般出现这个问题，都是Kubernetes部署时，没有对应的标签，导致无法找到资源\n问题 问题一：ip地址 正常开启的，但是这个监听端口是127.0.0.1的，普罗米修斯无法直接访问\n1 2 3 [root@k8s-master01 jiankong]# netstat -lntp | grep control tcp 0 0 127.0.0.1:10257 0.0.0.0:* LISTEN 112736/kube-control tcp6 0 0 :::10252 :::* LISTEN 112736/kube-control 问题二：SVC没有标签资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [root@k8s-master01 jiankong]# kubectl get servicemonitors.monitoring.coreos.com -n monitoring kube-controller-manager -oyaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: annotations: .............................. sourceLabels: - __name__ - action: drop regex: etcd_(debugging|disk|request|server).* sourceLabels: - __name__ port: http-metrics jobLabel: k8s-app namespaceSelector: matchNames: - kube-system selector: matchLabels: k8s-app: kube-controller-manager #通过这个标签匹配kube-controller-manager [root@k8s-master01 jiankong]# kubectl get svc -n kube-system -l k8s-app= kube-controller-manager error: name cannot be provided when a selector is specified #这边在kube-system没有这个标签的svc，所以无法匹配到 解决： 1、修改监听地址 1 2 3 vim /etc/kubernetes/manifests/kube-controller-manager.yaml - --bind-address=0.0.0.0 开0.0.0.0，是可以的，因为Kubernetes通信需要证书，是双向的。\n2、添加svc ep标签 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: v1 kind: Service metadata: labels: k8s-app: kube-controller-manager name: kube-controller-manage-monitor namespace: kube-system spec: ports: - name: http-metrics port: 10252 protocol: TCP targetPort: 10252 sessionAffinity: None type: ClusterIP apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: kube-controller-manager name: kube-controller-manage-monitor namespace: kube-system subsets: - addresses: - ip: 192.168.0.100 ports: - name: http-metrics port: 10252 protocol: TCP 3、测试 ","date":"2021-03-18T12:32:12Z","image":"https://www.ownit.top/title_pic/16.jpg","permalink":"https://www.ownit.top/p/202103181232/","title":"解决prometheus监控monitoring/kube-controller-manager/0 (0/0 up)的问题"},{"content":"Kubernetes用operator部署prometheus 上面采用Kubernetes部署prometheus\n我们可以使用prometheus来监控自带metrics接口的应用。\netcd是Kubernetes的数据库，自带接口，我们可以用etcd作为实例来看看怎么操作。\n一、监控etcd集群 1.1、查看接口信息 二进制和kubeadm安装方式不同，他们etcd的存放证书位置也不同\n二进制\n1 2 3 [root@k8s-master01 ~]# curl --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem https://192.168.1.201:2379/metrics -k # 这样也行 curl -L http://localhost:2379/metrics kubeadm\n1 2 3 4 5 [root@k8s-master01 ~]# find / -name \u0026#34;etcd\u0026#34; /etc/kubernetes/pki/etcd curl --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key https://localhost:2379/metrics -k 1.2、创建service和Endpoints 创建ep和svc代理外部的etcd服务，其他自带metrics接口的服务也是如此！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: v1 kind: Endpoints metadata: labels: app: etcd-k8s name: etcd-k8s namespace: kube-system #注意命名空间 subsets: - addresses: # etcd节点对应的主机ip，有几台就写几台 - ip: 192.168.0.100 ports: - name: etcd-port port: 2379 # etcd端口 protocol: TCP --- apiVersion: v1 kind: Service metadata: labels: app: etcd-k8s name: etcd-k8s namespace: kube-system spec: ports: - name: etcd-port port: 2379 protocol: TCP targetPort: 2379 type: ClusterIP 1.3、测试是否代理成功 1 2 3 4 5 6 7 #再次curl，把IP换成svc的IP测试，输出相同内容即创建成功 [root@k8s-master01 ~]# kubectl get svc -n kube-system etcd-k8s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE etcd-ep ClusterIP 10.103.53.103 \u0026lt;none\u0026gt; 2379/TCP 8m54s # 再次请求接口 [root@k8s-master01 ~]#curl --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key https://10.96.156.166:2379/metrics -k 有上面测试数值，代表接口已经暴露出来，现在包证书挂载上去。\n1.4、创建secret 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 1、这里我们k8s-master01节点进行创建,ca为k8sca证书，剩下2个为etcd证书，这是我证书所在位置 cert-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd.pem\u0026#39; key-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-key.pem\u0026#39; trusted-ca-file: \u0026#39;/etc/kubernetes/pki/etcd/etcd-ca.pem\u0026#39; # 2、接下来我们需要创建一个secret，让prometheus pod节点挂载 kubectl create secret generic etcd-ssl --from-file=/etc/kubernetes/pki/etcd/etcd-ca.pem --from-file=/etc/kubernetes/pki/etcd/etcd.pem --from-file=/etc/kubernetes/pki/etcd/etcd-key.pem -n monitoring # 3、创建完成后可以检查一下 [root@k8s-master01 prometheus-down]# kubectl describe secrets -n monitoring etcd-ssl Name: etcd-ssl Namespace: monitoring Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== etcd-ca.pem: 1367 bytes etcd-key.pem: 1679 bytes etcd.pem: 1509 bytes 1.5、编辑prometheus，把证书挂载进去 1 2 3 4 5 6 7 8 9 10 11 12 13 # 1、通过edit直接编辑prometheus 或者修改yaml文件 [root@k8s-master01 ~]# kubectl edit prometheus k8s -n monitoring # 在replicas底下加上secret名称 replicas:2 secrets: - etcd-ssl #添加secret名称 # 进入容器查看，就可以看到证书挂载进去了 [root@k8s-master01 prometheus-down]# kubectl exec -it -n monitoring prometheus-k8s-0 /bin/sh # 查看文件是否存在 /prometheus $ ls /etc/prometheus/secrets/etcd-ssl/ etcd-ca.pem etcd-key.pem etcd.pem 1.6、创建ServiceMonitor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [root@k8s-master01 ~]# cat etcd-servicemonitor.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: etcd-k8s namespace: monitoring labels: app: etcd-k8s spec: jobLabel: app endpoints: - interval: 30s port: etcd-port # 这个port对应 Service.spec.ports.name scheme: https tlsConfig: caFile: /etc/prometheus/secrets/etcd-ssl/etcd-ca.pem #证书路径 (在prometheus pod里路径) certFile: /etc/prometheus/secrets/etcd-ssl/etcd.pem keyFile: /etc/prometheus/secrets/etcd-ssl/etcd-key.pem insecureSkipVerify: true # 关闭证书校验 selector: matchLabels: app: etcd-k8s # 跟scv的lables保持一致 namespaceSelector: matchNames: - kube-system # 跟svc所在namespace保持一致 # 匹配Kube-system这个命名空间下面具有app=etcd-k8s这个label标签的Serve，job label用于检索job任务名称的标签。由于证书serverName和etcd中签发的证书可能不匹配，所以添加了insecureSkipVerify=true将不再对服务端的证书进行校验 1.7、页面查看etcd节点都获取到数据 此处数据获取有点慢，需要等待一下\n1.8、grafana模板导入 数据采集完成后，接下来可以在grafana中导入dashboard\n# 打开官网来的如下图所示，点击下载JSO文件\ngrafana官网：https://grafana.com/grafana/dashboards/3070\n中文版ETCD集群插件：https://grafana.com/grafana/dashboards/9733\n已经成功\n","date":"2021-03-18T11:37:38Z","image":"https://www.ownit.top/title_pic/14.jpg","permalink":"https://www.ownit.top/p/202103181137/","title":"Kubernetes监控etcd集群（自带metrics接口）"},{"content":"Operator 模式 Operator 是 Kubernetes 的扩展软件，它利用 定制资源 管理应用及其组件。 Operator 遵循 Kubernetes 的理念，特别是在控制器 方面。\nOperator的场景就是专门给有状态应用而设计的。\n为什么只给有状态应用？\n因为无状态应用简单啊，没有服务间的交互，要再开一家火锅店，跟k8s说一声，开一家一样的就可以了。\n有状态不一样，你开了一家火锅店以后，客户的信息怎么同步，就涉及到与别的火锅店交涉的问题，当然你也可以写个别的程序做这个数据同步的操作。\n但是operator做的事情就是能自动识别到火锅店客户信息的不对称，主动同步，你只用告诉operator我要再开一家连锁火锅店就好了。\nKubernetes 上的 Operator Kubernetes 为自动化而生。无需任何修改，你即可以从 Kubernetes 核心中获得许多内置的自动化功能。 你可以使用 Kubernetes 自动化部署和运行工作负载， 甚至 可以自动化 Kubernetes 自身。\nKubernetes 控制器 使你无需修改 Kubernetes 自身的代码，即可以扩展集群的行为。 Operator 是 Kubernetes API 的客户端，充当 定制资源 的控制器\n官网文档：https://kubernetes.io/zh/docs/concepts/extend-kubernetes/operator/\n部署prometheus 1.1、下载 1 git clone -b release-0.7 --single-branch https://github.com/coreos/kube-prometheus.git 1.2、安装operator 1 2 3 4 5 6 7 [root@k8s-master01 ~]# cd /root/kube-prometheus/manifests/setup [root@k8s-master01 setup]# kubectl create -f . # 查看是否Running [root@k8s-master01 ~]# kubectl get pod -n monitoring NAME READY STATUS RESTARTS AGE prometheus-operator-848d669f6d-bz2tc 2/2 Running 0 4m16s 1.3、安装Prometheus 1 2 [root@k8s-master01 ~]# cd /root/kube-prometheus/manifests [root@k8s-master01 manifests]# kubectl create -f . 1.4、创建ingress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 [root@k8s-master01 manifests]# cat svc-ingress.yal apiVersion: extensions/v1beta1 kind: Ingress metadata: name: prom-ingresses namespace: monitoring spec: rules: - host: alert.test.com http: paths: - backend: serviceName: alertmanager-main servicePort: 9093 path: / - host: grafana.test.com http: paths: - backend: serviceName: grafana servicePort: 3000 path: / - host: prom.test.com http: paths: - backend: serviceName: prometheus-k8s servicePort: 9090 path: / [root@k8s-master01 manifests]# kubectl get ingress -n monitoring NAME CLASS HOSTS ADDRESS PORTS AGE prom-ingresses \u0026lt;none\u0026gt; alert.test.com,grafana.test.com,prom.test.com 10.96.107.62 80 23h alert.test.com（报警） prom.test.com（普罗米修斯） grafana.test.com（图形展示） ","date":"2021-03-17T22:51:35Z","image":"https://www.ownit.top/title_pic/50.jpg","permalink":"https://www.ownit.top/p/202103172251/","title":"Kubernetes用operator部署prometheus"},{"content":"日常工作中，我们有时会对一些比较低的组件做升级。\n版本的升级，可以解决一下bug和漏洞，稳定系统的性能。\n这次我升级对coredns升级。\ncoredns主要是Kubernetes中对域名和ip解析，可以作为内网的dns解析服务器。\n一、查看当前coredns版本 1 2 3 4 5 6 7 [root@k8s-master01 ~]# kubectl get pod -n kube-system coredns-7ff77c879f-h2jw9 -oyaml | grep image f:image: {} f:imagePullPolicy: {} image: registry.aliyuncs.com/google_containers/coredns:1.6.7 imagePullPolicy: IfNotPresent image: registry.aliyuncs.com/google_containers/coredns:1.6.7 imageID: docker-pullable://registry.aliyuncs.com/google_containers/coredns@sha256:695a5e109604331f843d2c435f488bf3f239a88aec49112d452c1cbf87e88405 二、升级 2.1、查询最新版coredns的版本 1 2 3 4 5 6 # coredns官网：https://github.com/coredns/coredns # 老版本用：kube-dns # 新版的都用：coredns # 部署文档：https://github.com/coredns/deployment/tree/master/kubernetes 最新版地址：https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed\n现在最新版是1.8.3\n2.2、备份原来的cm、deploy、clusterrole、clusterrolebinding 切记：日常我们做任何操作（升级，删除，修改），一定要备份，防止错误操作，导致数据丢失，或者功能无法使用\n备份好处，可以确保数据安全性，如果出现功能不能使用，可以立刻回滚，不长时间影响业务\n1 2 3 4 5 mkdir coredns \u0026amp;\u0026amp; cd coredns kubectl get cm -n kube-system coredns -oyaml \u0026gt; coredns-config.yaml kubectl get deploy -n kube-system coredns -oyaml \u0026gt; coredns-controllers.yaml kubectl get clusterrole system:coredns -oyaml \u0026gt; coredns-clusterrole.yaml kubectl get clusterrolebinding system:coredns -oyaml \u0026gt; coredns-clusterrolebinding.yaml 2.3、升级coredns 下载地址：git clone https://github.com/coredns/deployment.git\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 1、下载文件 git clone https://github.com/coredns/deployment.git # 2、升级 cd deployment/kubernetes/ ./deploy.sh -s | kubectl apply -f - [root@k8s-master01 ~]# cd deployment/kubernetes/ [root@k8s-master01 kubernetes]# ./deploy.sh -s | kubectl apply -f - Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply serviceaccount/coredns configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply clusterrole.rbac.authorization.k8s.io/system:coredns configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply clusterrolebinding.rbac.authorization.k8s.io/system:coredns configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply configmap/coredns configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply deployment.apps/coredns configured Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply service/kube-dns configured 已经升级成功\n","date":"2021-03-17T21:39:04Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202103172139/","title":"Kubernetes升级coredns1.8.3"},{"content":"问题：Kubernetes安装普罗米修斯，其中kube-state-metrics 容器一直报错\n环境：Kubernetes 1.18\n1 2 3 4 5 6 7 [root@k8s-master01 manifests]# kubectl logs -f kube-state-metrics-bdb8874fd-tnrrg -n monitoring -c kube-state-metrics I0316 13:12:52.295699 1 main.go:86] Using default collectors I0316 13:12:52.295788 1 main.go:98] Using all namespace I0316 13:12:52.295798 1 main.go:139] metric white-blacklisting: blacklisting the following items: W0316 13:12:52.295807 1 client_config.go:543] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work. I0316 13:12:52.297186 1 main.go:184] Testing communication with server F0316 13:13:22.298801 1 main.go:147] Failed to create client: error while trying to communicate with apiserver: Get https://10.96.0.1:443/version?timeout=32s: dial tcp 10.96.0.1:443: i/o timeout 分析：\n首先，这个kube-state-metrics-bdb8874fd-tnrrg 中有三个容器。问题出现在kube-state-metrics。导致容器不断的重启\n看问题，是无法连接到10.96.0.1:443这个ip和端口上。\n1 2 3 4 5 [root@k8s-master01 ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-metrics-cephfsplugin ClusterIP 10.96.218.238 \u0026lt;none\u0026gt; 8080/TCP 21h kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 83d nginx ClusterIP 10.96.215.251 \u0026lt;none\u0026gt; 80/TCP 16d 由svc可见，这个是Kubernetes的核心ip和端口。但是他那边显示无法连接。\n测试一下\n这个端口是通的，但是无法连接。报错显示io线程延时。\n解决：\nkube-state-metrics原本安装在k8s-node02上。这边删除，还是从重建k8s-node02.\n分析一下node02的cpu有点高，我直接指定到k8s-node01上。\n然后测试，没有显示io延时报错\n","date":"2021-03-16T21:36:32Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/202103162136/","title":"Failed to create client- error while trying to communicate with apiserver- 报错解决"},{"content":"首先介绍一下bitnami\n_BitNami_是一个开源项目，该项目产生的开源软件包括安装 Web应用程序和解决方案堆栈，以及虚拟设备（通俗易懂的说：就是封装好各种应用包，提供人们使用。）\n我们平时要部署一套高可用集群，大部分都是找到模板，没必要重复造轮子。BitNami就是提供轮子的。\nbitnami官方地址： https://bitnami.com/\n这次我们创建部署zookeeper和kafka集群，采取bitnami提供helm仓库，进行安装和部署。\n不懂Helm的可以看看 Helm部署RabbitMQ集群，提供参考\n部署安装文档：https://docs.bitnami.com/tutorials/deploy-scalable-kafka-zookeeper-cluster-kubernetes\nhelm添加_BitNami仓库_ 1 helm repo add bitnami https://charts.bitnami.com/bitnami 部署zookeeper集群 1 2 3 4 helm install zookeeper bitnami/zookeeper --set replicaCount=3 --set auth.enabled=false --set allowAnonymousLogin=true 部署kafka集群 1 2 3 4 helm install kafka bitnami/kafka --set zookeeper.enabled=false --set replicaCount=3 --set externalZookeeper.servers=ZOOKEEPER-SERVICE-NAME 查看kafka集群，连接zookeeper\n扩容和缩容 修改下面这个参数，重新运行。\n1 --set replicaCount=7 如果遇到拉去镜像失败，这可以使用以下的镜像源\nhttps://www.daocloud.io/mirror\n1 curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io 1 2 3 4 [root@k8s-master01 kafka]# cat /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;] } ","date":"2021-03-15T16:57:54Z","image":"https://www.ownit.top/title_pic/75.jpg","permalink":"https://www.ownit.top/p/202103151657/","title":"Helm（bitnami）部署zookeeper和kafka集群"},{"content":"Helm 帮助您管理 Kubernetes 应用程序——Helm Charts 帮助您定义、安装和升级最复杂的 Kubernetes 应用程序。\nHelm 可以使用 Charts 启动 Kubernetes 集群，提供可用的工作流：\n一个 Redis 集群\n一个 Postgres 数据库\n一个 HAProxy 边界负载均衡\n特性：\n查找并使用流行的软件，将其打包为 Helm Charts，以便在 Kubernetes 中运行 以 Helm Charts 的形式共享您自己的应用程序 为您的 Kubernetes 应用程序创建可复制的构建 智能地管理您的 Kubernetes 清单文件 管理 Helm 包的发行版 helm的安装 官方文档：https://helm.sh/docs/intro/install/\n版本下载地址：https://github.com/helm/helm/releases\n1 2 3 4 5 6 7 tar -zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm 已经安装完成 [root@k8s-master01 ~]# helm version version.BuildInfo{Version:\u0026#34;v3.5.3\u0026#34;, GitCommit:\u0026#34;041ce5a2c17a58be0fcd5f5e16fb3e7e95fea622\u0026#34;, GitTreeState:\u0026#34;dirty\u0026#34;, GoVersion:\u0026#34;go1.15.8\u0026#34;} 至于怎么添加仓库，可以百度\n1 2 3 4 5 [root@k8s-master01 ~]# helm repo list NAME URL aliyuncs https://apphub.aliyuncs.com stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts ingress-nginx\thttps://kubernetes.github.io/ingress-nginx 查看可使用tabbitmq-ha的版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [root@k8s-master01 ~]# helm search repo rabbitmq-ha NAME CHART VERSION\tAPP VERSION\tDESCRIPTION aliyuncs/rabbitmq-ha\t1.39.0 3.8.0 Highly available RabbitMQ cluster, the open sou... stable/rabbitmq-ha 1.0.0 3.7.3 Highly available RabbitMQ cluster, the open sou... [root@k8s-master01 ~]# helm search repo rabbitmq-ha --versions NAME CHART VERSION\tAPP VERSION\tDESCRIPTION aliyuncs/rabbitmq-ha\t1.39.0 3.8.0 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.38.2 3.8.0 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.38.1 3.8.0 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.36.4 3.8.0 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.36.3 3.8.0 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.36.0 3.8.0 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.34.1 3.7.19 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.34.0 3.7.19 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.33.0 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.32.4 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.32.3 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.32.2 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.32.0 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.31.0 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.30.0 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.29.1 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.29.0 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.28.0 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.27.2 3.7.15 Highly available RabbitMQ cluster, the open sou... aliyuncs/rabbitmq-ha\t1.27.1 3.7.12 Highly available RabbitMQ cluster, the open sou... stable/rabbitmq-ha 1.0.0 3.7.3 Highly available RabbitMQ cluster, the open sou... stable/rabbitmq-ha 0.1.1 3.7.0 Highly available RabbitMQ cluster, the open sou... 拉去指定版本的配置 1 2 3 [root@k8s-master01 ~]# helm pull aliyuncs/rabbitmq-ha --version=1.33.0 [root@k8s-master01 ~]# ls helm old rabbitmq-cluster rabbitmq-ha-1.33.0.tgz #这个就只拉去的文件，可以解压 查询配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [root@k8s-master01 ~]# tar -xf rabbitmq-ha-1.33.0.tgz [root@k8s-master01 ~]# ls helm old rabbitmq-cluster rabbitmq-ha rabbitmq-ha-1.33.0.tgz [root@k8s-master01 ~]# tree rabbitmq-ha rabbitmq-ha ├── Chart.yaml ├── OWNERS ├── README.md ├── templates │ ├── alerts.yaml │ ├── configmap.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ ├── pdb.yaml │ ├── rolebinding.yaml │ ├── role.yaml │ ├── secret.yaml │ ├── serviceaccount.yaml │ ├── service-discovery.yaml │ ├── servicemonitor.yaml │ ├── service.yaml │ └── statefulset.yaml └── values.yaml 1 directory, 18 files 参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ├── charts # 依赖文件 ├── Chart.yaml # 这个chart的版本信息 ├── templates #模板 │ ├── deployment.yaml │ ├── _helpers.tpl # 自定义的模板或者函数 │ ├── ingress.yaml │ ├── NOTES.txt #这个chart的信息 │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml #配置全局变量或者一些参数 创建namespaces helm指定namespaces，如果没有namespaces空间，就会报错。需要提前创建\n1 2 [root@k8s-master01 ~]# helm create rabbitmq-cluster Creating rabbitmq-cluster 运行rabbitmq集群 镜像配置方面，可以修改values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 [root@k8s-master01 rabbitmq-ha]# pwd /root/rabbitmq-ha [root@k8s-master01 rabbitmq-ha]# ls Chart.yaml OWNERS README.md templates values.yaml [root@k8s-master01 rabbitmq-ha]# helm install rabbitmq --namespace rabbitmq-cluster --set ingress.enabled=true,ingress.hostName=rabbitmq.akiraka.net --set rabbitmqUsername=aka,rabbitmqPassword=rabbitmq,managementPassword=rabbitmq,rabbitmqErlangCookie=secretcookie . NAME: rabbitmq LAST DEPLOYED: Sun Mar 14 17:25:11 2021 NAMESPACE: rabbitmq-cluster STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** Credentials: Username : aka Password : $(kubectl get secret --namespace rabbitmq-cluster rabbitmq-rabbitmq-ha -o jsonpath=\u0026#34;{.data.rabbitmq-password}\u0026#34; | base64 --decode) Management username : management Management password : $(kubectl get secret --namespace rabbitmq-cluster rabbitmq-rabbitmq-ha -o jsonpath=\u0026#34;{.data.rabbitmq-management-password}\u0026#34; | base64 --decode) ErLang Cookie : $(kubectl get secret --namespace rabbitmq-cluster rabbitmq-rabbitmq-ha -o jsonpath=\u0026#34;{.data.rabbitmq-erlang-cookie}\u0026#34; | base64 --decode) RabbitMQ can be accessed within the cluster on port 5672 at rabbitmq-rabbitmq-ha.rabbitmq-cluster.svc.cluster.local To access the cluster externally execute the following commands: export POD_NAME=$(kubectl get pods --namespace rabbitmq-cluster -l \u0026#34;app=rabbitmq-ha\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl port-forward $POD_NAME --namespace rabbitmq-cluster 5672:5672 15672:15672 To Access the RabbitMQ AMQP port: amqp://127.0.0.1:5672/ To Access the RabbitMQ Management interface: URL : http://127.0.0.1:15672 [root@k8s-master01 rabbitmq-ha]# 查看集群 已经正常运行起来了\n1 2 3 4 5 6 7 8 9 10 11 12 [root@k8s-master01 rabbitmq-ha]# kubectl get svc,pod,ingress -n rabbitmq-cluster NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/rabbitmq-rabbitmq-ha ClusterIP None \u0026lt;none\u0026gt; 15672/TCP,5672/TCP,4369/TCP 7m23s service/rabbitmq-rabbitmq-ha-discovery ClusterIP None \u0026lt;none\u0026gt; 15672/TCP,5672/TCP,4369/TCP 7m23s NAME READY STATUS RESTARTS AGE pod/rabbitmq-rabbitmq-ha-0 1/1 Running 0 7m23s pod/rabbitmq-rabbitmq-ha-1 1/1 Running 0 6m56s pod/rabbitmq-rabbitmq-ha-2 1/1 Running 0 6m34s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.extensions/rabbitmq-rabbitmq-ha \u0026lt;none\u0026gt; rabbitmq.akiraka.net 10.96.107.62 80 7m23s 很方便的，几个命令就完成集群的部署\n补充 1 2 helm install helm-test2 --set fullnameOverride=aaaaaaa --dry-run . 这个是模拟运行，懂的人都懂（带值） 1 2 3 删除helm uninstall rabbitmq-cluster -n public-service # helm v3 --keep-history helm delete/del NAME --purge 升级helm upgrade rabbitmq-cluster -n public-service . 卸载保留历史记录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [root@k8s-master01 rabbitmq-ha]# helm uninstall rabbitmq -n rabbitmq-cluster --keep-history release \u0026#34;rabbitmq\u0026#34; uninstalled [root@k8s-master01 rabbitmq-ha]# helm list -n rabbitmq-cluster NAME\tNAMESPACE\tREVISION\tUPDATED\tSTATUS\tCHART\tAPP VERSION [root@k8s-master01 rabbitmq-ha]# helm list NAME\tNAMESPACE\tREVISION\tUPDATED\tSTATUS\tCHART\tAPP VERSION [root@k8s-master01 rabbitmq-ha]# helm ls NAME\tNAMESPACE\tREVISION\tUPDATED\tSTATUS\tCHART\tAPP VERSION [root@k8s-master01 rabbitmq-ha]# helm status rabbitmq -n rabbitmq-cluster NAME: rabbitmq LAST DEPLOYED: Sun Mar 14 17:25:11 2021 NAMESPACE: rabbitmq-cluster STATUS: uninstalled REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** Credentials: Username : aka Password : $(kubectl get secret --namespace rabbitmq-cluster rabbitmq-rabbitmq-ha -o jsonpath=\u0026#34;{.data.rabbitmq-password}\u0026#34; | base64 --decode) Management username : management Management password : $(kubectl get secret --namespace rabbitmq-cluster rabbitmq-rabbitmq-ha -o jsonpath=\u0026#34;{.data.rabbitmq-management-password}\u0026#34; | base64 --decode) ErLang Cookie : $(kubectl get secret --namespace rabbitmq-cluster rabbitmq-rabbitmq-ha -o jsonpath=\u0026#34;{.data.rabbitmq-erlang-cookie}\u0026#34; | base64 --decode) RabbitMQ can be accessed within the cluster on port 5672 at rabbitmq-rabbitmq-ha.rabbitmq-cluster.svc.cluster.local To access the cluster externally execute the following commands: export POD_NAME=$(kubectl get pods --namespace rabbitmq-cluster -l \u0026#34;app=rabbitmq-ha\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl port-forward $POD_NAME --namespace rabbitmq-cluster 5672:5672 15672:15672 To Access the RabbitMQ AMQP port: amqp://127.0.0.1:5672/ To Access the RabbitMQ Management interface: URL : http://127.0.0.1:15672 ","date":"2021-03-14T17:37:25Z","image":"https://www.ownit.top/title_pic/71.jpg","permalink":"https://www.ownit.top/p/202103141737/","title":"Helm部署RabbitMQ集群"},{"content":"RabbitMQ是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）\n可伸缩性：集群服务\n消息持久化：从内存持久化消息到硬盘，再从硬盘加载到内存 工作中，我们经常使用到Rabbitmq，有单节点，有集群的。\n通过该Kubernetes来安装RabbitMQ，通过插【rabbitmq_management,rabbitmq_peer_discovery_k8s】来自动发现注册集群：\nRabbitMQ通过StatefulSet来部署，可以通过域名来访问，方便处理\n（1）环境 需要Kubernetes集群\n下载部署文件\n1 下载地址：https://github.com/dotbalo/k8s/tree/master/k8s-rabbitmq-cluster （2）创建命名空间 1 kubectl create ns public-service 如果不使用public-service，需要更改所有yaml文件的public-service为你namespace。\n1 sed -i \u0026#34;s#public-service#YOUR_NAMESPACE#g\u0026#34; *.yaml （3）修改配置文件 1 2 3 在rabbitmq-configmap.yaml配置，添加这两个字段，解决密码不生效的问题 default_pass = RABBITMQ_PASS default_user = RABBITMQ_USER 这边PV是使用nfs的。测试环境，这边我是手动创建的，也可以使用插件，自动创建，并回收。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 [root@k8s-master01 k8s-rabbitmq-cluster]# cat rabbitmq-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-rmq-1 spec: capacity: storage: 1Gi accessModes: - ReadWriteMany volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle storageClassName: \u0026#34;rmq-storage-class\u0026#34; nfs: # real share directory path: /ifs/kubernetts/rabbitmq-cluster-0 # nfs real ip server: 192.168.0.109 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-rmq-2 spec: capacity: storage: 1Gi accessModes: - ReadWriteMany volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle storageClassName: \u0026#34;rmq-storage-class\u0026#34; nfs: # real share directory path: /ifs/kubernetts/rabbitmq-cluster-1 # nfs real ip server: 192.168.0.109 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv-rmq-3 spec: capacity: storage: 1Gi accessModes: - ReadWriteMany volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle storageClassName: \u0026#34;rmq-storage-class\u0026#34; nfs: # real share directory path: /ifs/kubernetts/rabbitmq-cluster-2 # nfs real ip server: 192.168.0.109 （4）创建集群 1 2 3 4 5 6 7 8 9 10 11 12 [root@k8s-master01 k8s-rabbitmq-cluster]# kubectl apply -f . statefulset.apps/rmq-cluster created configmap/rmq-cluster-config created persistentvolume/pv-rmq-1 created persistentvolume/pv-rmq-2 created persistentvolume/pv-rmq-3 created serviceaccount/rmq-cluster created role.rbac.authorization.k8s.io/rmq-cluster created rolebinding.rbac.authorization.k8s.io/rmq-cluster created secret/rmq-cluster-secret created service/rmq-cluster created service/rmq-cluster-balancer created 查看容器日志，显示这个就是正常的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 kubectl logs -f rmq-cluster-0 -n public-service \u0026#39;/etc/rabbitmq/rabbitmq.conf\u0026#39; -\u0026gt; \u0026#39;/var/lib/rabbitmq/rabbitmq.conf\u0026#39; 2021-03-11 02:24:03.469 [info] \u0026lt;0.9.0\u0026gt; Feature flags: list of feature flags found: 2021-03-11 02:24:03.469 [info] \u0026lt;0.9.0\u0026gt; Feature flags: feature flag states written to 。。。。。。。。。。。。。。。 。。。。。。。。。。。。。。。 2021-03-11 02:24:04.273 [info] \u0026lt;0.692.0\u0026gt; Statistics database started. 2021-03-11 02:24:04.274 [info] \u0026lt;0.691.0\u0026gt; Starting worker pool \u0026#39;management_worker_pool\u0026#39; with 3 processes in it completed with 5 plugins. 2021-03-11 02:24:04.411 [info] \u0026lt;0.9.0\u0026gt; Server startup complete; 5 plugins started. * rabbitmq_management * rabbitmq_management_agent * rabbitmq_web_dispatch * rabbitmq_peer_discovery_k8s * rabbitmq_peer_discovery_common 2021-03-11 02:24:23.135 [info] \u0026lt;0.434.0\u0026gt; node \u0026#39;rabbit@rmq-cluster-1.rmq-cluster.public-service.svc.cluster.local\u0026#39; up 2021-03-11 02:24:23.904 [info] \u0026lt;0.434.0\u0026gt; rabbit on node \u0026#39;rabbit@rmq-cluster-1.rmq-cluster.public-service.svc.cluster.local\u0026#39; up 2021-03-11 02:24:44.524 [info] \u0026lt;0.434.0\u0026gt; node \u0026#39;rabbit@rmq-cluster-2.rmq-cluster.public-service.svc.cluster.local\u0026#39; up 2021-03-11 02:24:45.477 [info] \u0026lt;0.434.0\u0026gt; rabbit on node \u0026#39;rabbit@rmq-cluster-2.rmq-cluster.public-service.svc.cluster.local\u0026#39; up 创建资源如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [root@k8s-master01 ~]# kubectl get pod,sts,svc,ep,pv,pvc -n public-service NAME READY STATUS RESTARTS AGE pod/rmq-cluster-0 1/1 Running 0 8m10s pod/rmq-cluster-1 1/1 Running 0 7m34s pod/rmq-cluster-2 1/1 Running 0 7m18s NAME READY AGE statefulset.apps/rmq-cluster 3/3 8m11s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/rmq-cluster ClusterIP None \u0026lt;none\u0026gt; 5672/TCP 8m11s service/rmq-cluster-balancer NodePort 10.96.58.102 \u0026lt;none\u0026gt; 15672:31824/TCP,5672:31993/TCP 8m11s NAME ENDPOINTS AGE endpoints/rmq-cluster 10.244.32.129:5672,10.244.58.225:5672,10.244.85.246:5672 8m11s endpoints/rmq-cluster-balancer 10.244.32.129:5672,10.244.58.225:5672,10.244.85.246:5672 + 3 more... 8m11s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pv-rmq-1 1Gi RWX Recycle Bound public-service/rabbitmq-storage-rmq-cluster-2 rmq-storage-class 8m11s persistentvolume/pv-rmq-2 1Gi RWX Recycle Bound public-service/rabbitmq-storage-rmq-cluster-1 rmq-storage-class 8m11s persistentvolume/pv-rmq-3 1Gi RWX Recycle Bound public-service/rabbitmq-storage-rmq-cluster-0 rmq-storage-class 8m11s persistentvolume/pv0001 2Gi RWO Recycle Bound default/test-pvc2 slow 9d NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/rabbitmq-storage-rmq-cluster-0 Bound pv-rmq-3 1Gi RWX rmq-storage-class 8m11s persistentvolumeclaim/rabbitmq-storage-rmq-cluster-1 Bound pv-rmq-2 1Gi RWX rmq-storage-class 7m34s persistentvolumeclaim/rabbitmq-storage-rmq-cluster-2 Bound pv-rmq-1 1Gi RWX rmq-storage-class 7m18s [root@k8s-master01 ~]# （5）验证集群 可以通过svc的nodeport端口暴露，或者ingress来访问rabbitmq可视化界面\n1 2 3 default_user = RABBITMQ_USER #账号 default_pass = RABBITMQ_PASS #密码 （6）集群扩容，缩容 扩容，如果使用的pv，建议查看pv是否够，不然状态一直是pending\n1 2 3 kubectl scale statefulset -n public-service --replicas=4 rmq-cluster --replicas=4 改这个值，就可以用来扩容和缩容 https://github.com/dotbalo/k8s/tree/master/k8s-rabbitmq-cluster\n","date":"2021-03-11T10:58:13Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202103111058/","title":"Kubernetes通过插件，自动发现注册Rabbitmq集群"},{"content":"通过operator部署redis集群 operator部署有状态的应用会简单很多\ngithub文档：https://github.com/ucloud/redis-cluster-operator#deploy-redis-cluster-operator\nRedis Cluster Operator在Kubernetes上管理Redis-Cluster集群\n每个主节点及其从节点都由statefulSet管理，为每个statefulSet创建无头svc，并为所有节点创建clusterIP服务。\n每个有状态集都使用PodAntiAffinity来确保主节点和从节点分散在不同的节点上。同时，当操作员在每个有状态集中选择主节点时，它会优先选择具有不同k8s节点的容器作为主节点。\n（1）下载redis-cluster-operator 1 git clone https://github.com/ucloud/redis-cluster-operator.git 在名称空间：redis-cluster下部署Redis集群，注意修改yaml文件的namespace参数\n创建名称空间Namespace：redis-cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@k8s-master01 redis-cluster-operator-master]# kubectl create ns redis-cluster namespace/redis-node created [root@k8s-master01 redis-cluster-operator-master]# kubectl get ns NAME STATUS AGE default Active 76d ingress-nginx Active 9d kube-node-lease Active 76d kube-public Active 76d kube-system Active 76d kube-users Active 2d21h kubernetes-dashboard Active 45h redis-cluster Active 11s rook-ceph Active 33h [root@k8s-master01 redis-cluster-op （2）创建自定义资源（CRD） 1 2 3 [root@k8s-master01 redis]# kubectl apply -f deploy/crds/ customresourcedefinition.apiextensions.k8s.io/distributedredisclusters.redis.kun created customresourcedefinition.apiextensions.k8s.io/redisclusterbackups.redis.kun created （3）创建operator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@k8s-master01 redis]# kubectl create -f deploy/service_account.yaml serviceaccount/redis-cluster-operator created [root@k8s-master01 redis]# kubectl create -f deploy/cluster/cluster_role.yaml clusterrole.rbac.authorization.k8s.io/redis-cluster-operator created [root@k8s-master01 redis]# kubectl create -f deploy/cluster/cluster_role_binding.yaml clusterrolebinding.rbac.authorization.k8s.io/redis-cluster-operator created [root@k8s-master01 redis]# kubectl create -f deploy/cluster/operator.yaml deployment.apps/redis-cluster-operator created configmap/redis-admin created [root@k8s-master01 redis]# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE metrics-metrics-server 1/1 1 1 10d redis-cluster-operator 1/1 1 1 12s // cluster-scoped 命令 $ kubectl create -f deploy/service_account.yaml $ kubectl create -f deploy/cluster/cluster_role.yaml $ kubectl create -f deploy/cluster/cluster_role_binding.yaml $ kubectl create -f deploy/cluster/operator.yaml （4）部署样本Redis集群 注意：只有使用持久性存储（pvc）的redis集群在意外删除或滚动更新后才能恢复。即使您不使用持久性（如rdb或aof），也需要将pvc设置为redis。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: redis.kun/v1alpha1 kind: DistributedRedisCluster metadata: annotations: # if your operator run as cluster-scoped, add this annotations redis.kun/scope: cluster-scoped name: example-distributedrediscluster spec: image: redis:5.0.4-alpine masterSize: 3 clusterReplicas: 1 resources: limits: cpu: 200m memory: 200Mi requests: cpu: 200m memory: 100Mi 因为使用样本，没有资源限制，会因为内存不足导致初始化失败，限制使用这个测试\n1 kubectl create -f deploy/example/custom-resources.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@k8s-master01 redis]# kubectl get pod,svc NAME READY STATUS RESTARTS AGE pod/drc-example-distributedrediscluster-0-0 1/1 Running 0 6m39s pod/drc-example-distributedrediscluster-0-1 1/1 Running 0 6m2s pod/drc-example-distributedrediscluster-1-0 1/1 Running 0 6m39s pod/drc-example-distributedrediscluster-1-1 1/1 Running 0 6m7s pod/drc-example-distributedrediscluster-2-0 1/1 Running 0 6m39s pod/drc-example-distributedrediscluster-2-1 1/1 Running 0 6m6s pod/metrics-metrics-server-6c7745d876-cw72h 1/1 Running 0 8h pod/redis-cluster-operator-7f6cf86475-dhttx 1/1 Running 0 11m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/example-distributedrediscluster ClusterIP 10.96.104.199 \u0026lt;none\u0026gt; 6379/TCP,16379/TCP 6m38s service/example-distributedrediscluster-0 ClusterIP None \u0026lt;none\u0026gt; 6379/TCP,16379/TCP 6m38s service/example-distributedrediscluster-1 ClusterIP None \u0026lt;none\u0026gt; 6379/TCP,16379/TCP 6m38s service/example-distributedrediscluster-2 ClusterIP None \u0026lt;none\u0026gt; 6379/TCP,16379/TCP 6m38s service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 76d service/metrics-metrics-server ClusterIP 10.96.86.164 \u0026lt;none\u0026gt; 443/TCP 10d service/nginx ClusterIP 10.96.215.251 \u0026lt;none\u0026gt; 80/TCP 9d service/redis-cluster-operator-metrics ClusterIP 10.96.58.127 \u0026lt;none\u0026gt; 8383/TCP,8686/TCP 11m [root@k8s-master01 redis]# 创建指定命名空间和动态存储的，我这个没有构建动态存储\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 cat redis-cluster.yaml apiVersion: redis.kun/v1alpha1 kind: DistributedRedisCluster metadata: annotations: # if your operator run as cluster-scoped, add this annotations redis.kun/scope: cluster-scoped name: example-distributedrediscluster namespace: redis-cluster spec: image: redis:5.0.4-alpine imagePullPolicy: IfNotPresent masterSize: 3\t#master节点数量 clusterReplicas: 1\t#每个master节点的从节点数量 serviceName: redis-svc # resources config resources: limits: cpu: 300m memory: 200Mi requests: cpu: 200m memory: 150Mi # pv storage storage: type: persistent-claim size: 2Gi class: nfs-storage deleteClaim: true ]# kubectl apply -f redis-cluster.yaml （5）验证集群 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@k8s-master01 redis]# kubectl exec -it drc-example-distributedrediscluster-0-0 -- sh /data # redis-cli -c -h redis-svc Could not connect to Redis at redis-svc:6379: Name does not resolve not connected\u0026gt; /data # redis-cli -c -h example-distributedrediscluster example-distributedrediscluster:6379\u0026gt; cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:5 cluster_my_epoch:0 cluster_stats_messages_ping_sent:511 cluster_stats_messages_pong_sent:485 cluster_stats_messages_meet_sent:1 cluster_stats_messages_sent:997 cluster_stats_messages_ping_received:481 cluster_stats_messages_pong_received:512 cluster_stats_messages_meet_received:4 cluster_stats_messages_received:997 example-distributedrediscluster:6379\u0026gt; set a b -\u0026gt; Redirected to slot [15495] located at 10.244.58.209:6379 OK 10.244.58.209:6379\u0026gt; （6）扩展Redis集群 增加masterSize触发放大。（注意：这个也直接可以使用edit修改。）\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: redis.kun/v1alpha1 kind: DistributedRedisCluster metadata: annotations: # if your operator run as cluster-scoped, add this annotations redis.kun/scope: cluster-scoped name: example-distributedrediscluster spec: # Increase the masterSize to trigger the scaling. masterSize: 4 ClusterReplicas: 1 image: redis:5.0.4-alpine （7）缩减Redis集群 减小masterSize触发缩小。\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: redis.kun/v1alpha1 kind: DistributedRedisCluster metadata: annotations: # if your operator run as cluster-scoped, add this annotations redis.kun/scope: cluster-scoped name: example-distributedrediscluster spec: # Increase the masterSize to trigger the scaling. masterSize: 3 ClusterReplicas: 1 image: redis:5.0.4-alpine （8）删除redis集群 1 2 3 4 5 6 7 8 9 ]# cd redis-cluster-operator/ ]# kubectl delete -f redis-cluster.yaml ]# cd cluster/ ]# kubectl delete -f operator.yaml ]# kubectl delete -f cluster_role_binding.yaml ]# kubectl delete -f cluster_role.yaml ]# kubectl delete -f service_account.yaml ]# kubectl delete -f deploy/crds/ ]# kubectl delete -f ns-redis-cluster.yaml github文档：https://github.com/ucloud/redis-cluster-operator#deploy-redis-cluster-operator\n","date":"2021-03-09T21:43:40Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202103092143/","title":"Kubernetes使用operator安装Redis集群"},{"content":"Kubernetes集群中，我们时常要部署中间件，nginx，redis等等\n首先，这些中间件要容器化，才可以部署到Kubernetes集群中。\n（1）一般我们到dockerhub官方，寻找镜像 https://hub.docker.com/search?q=redis\u0026type=image\n（2）我们可以自己定制，，公司需要的docker 镜像\n部署单个Redis\n（1）首先下载redis的镜像（版本：） 1 docker pull redis:5.0.4-alpine （2）编写configmap 因为需要指定命名空间 redis-node\n1 kubectl create cm redis-node 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 apiVersion: v1 data: redis.conf: |- #heian bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize no supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile /var/log/redis.log databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb dir /data slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly no appendfilename \u0026#34;appendonly.aof\u0026#34; appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \u0026#34;\u0026#34; hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes kind: ConfigMap metadata: name: redis-conf namespace: redis-node (3)编写单个redis的yaml文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 apiVersion: apps/v1 kind: Deployment metadata: labels: app: redis-single-node name: redis-single-node namespace: redis-node spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: redis-single-node strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: redis-single-node spec: containers: - command: - sh - -c - redis-server \u0026#34;/mnt/redis.conf\u0026#34; env: - name: TZ value: Asia/Shanghai - name: LANG value: C.UTF-8 image: redis:5.0.4-alpine imagePullPolicy: IfNotPresent lifecycle: {} livenessProbe: failureThreshold: 2 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 6379 timeoutSeconds: 2 name: redis-single-node ports: - containerPort: 6379 name: web protocol: TCP readinessProbe: failureThreshold: 2 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 6379 timeoutSeconds: 2 resources: limits: cpu: 100m memory: 339Mi requests: cpu: 10m memory: 10Mi securityContext: privileged: false runAsNonRoot: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/zoneinfo/Asia/Shanghai name: tz-config - mountPath: /etc/localtime name: tz-config - mountPath: /etc/timezone name: timezone - mountPath: /mnt name: redis-conf readOnly: true dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 30 - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 30 volumes: - hostPath: path: /usr/share/zoneinfo/Asia/Shanghai type: \u0026#34;\u0026#34; name: tz-config - hostPath: path: /etc/timezone type: \u0026#34;\u0026#34; name: timezone - configMap: defaultMode: 420 name: redis-conf name: redis-conf 因为修改configmap，不会自动更新，需要删除容器才会重新加载。我这边尝试挂载configmap为文件，指定启动配置文件\n(4)暴露端口，service 这边我做了service暴露。用的ClusterIP，接下来可以通过ingress包这个ip通过域名映射出去\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Service metadata: labels: app: redis-single-node spec: ports: - name: redis-port port: 6379 protocol: TCP targetPort: 6379 selector: app: redis-single-node sessionAffinity: None type: ClusterIP ","date":"2021-03-09T10:36:44Z","image":"https://www.ownit.top/title_pic/07.jpg","permalink":"https://www.ownit.top/p/202103091036/","title":"Kubernetes部署单Redis"},{"content":"Rook：\n一个自我管理的分布式存储编排系统，它本身并不是存储系统，在存储和k8s之前搭建了一个桥梁，存储系统的搭建或者维护变得特别简单，Rook支持CSI，CSI做一些PVC的快照、PVC扩容等操作。\nRook是专用于Cloud-Native环境的文件、块、对象存储服务。它实现了一个自我管理的、自我扩容的、自我修复的分布式存储服务。\nRook支持自动部署、启动、配置、分配（provisioning）、扩容/缩容、升级、迁移、灾难恢复、监控，以及资源管理。 为了实现所有这些功能，Rook依赖底层的容器编排平台。\n目前Rook仍然处于Alpha版本，初期专注于Kubernetes+Ceph。Ceph是一个分布式存储系统，支持文件、块、对象存储，在生产环境中被广泛应用。\nOperator：主要用于有状态的服务，或者用于比较复杂应用的管理。\nHelm：主要用于无状态的服务，配置分离。 Rook：\nAgent：在每个存储节点上运行，用于配置一个FlexVolume插件，和k8s的存储卷进行集成。挂载网络存储、加载存储卷、格式化文件系统。\nDiscover：主要用于检测链接到存储节点上的存储设备。\nCeph：\nOSD：直接连接每一个集群节点的物理磁盘或者是目录。集群的副本数、高可用性和容错性。\nMON：集群监控，所有集群的节点都会向Mon汇报。他记录了集群的拓扑以及数据存储位置的信息。\nMDS：元数据服务器，负责跟踪文件层次结构并存储ceph元数据。\nRGW：restful API接口。\nMGR：提供额外的监控和界面。\nRook 官方文档:https://rook.io/docs/rook/v1.5/ceph-quickstart.html\n环境部署\n1 2 3 4 5 6 git clone --single-branch --branch v1.5.8 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml -f common.yaml -f operator.yaml 修改cluster.yaml文件 kubectl create -f cluster.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 vim cluster.yaml storage: # cluster level storage configuration and selection useAllNodes: false #所有结节为存储节点，改为false useAllDevices: false #使用所有的磁盘 改为false nodes: - name: \u0026#34;k8s-node02\u0026#34; devices: # specific devices to use for storage can be specified for each node - name: \u0026#34;sdb\u0026#34; #k8s-node02新加的裸盘 - name: \u0026#34;k8s-node01\u0026#34; directories: - path: \u0026#34;/data/ceph\u0026#34; rook的dashboard： https://rook.io/docs/rook/v1.5/ceph-dashboard.html\n1 2 3 4 kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rook-ceph-mgr ClusterIP 10.108.111.192 \u0026lt;none\u0026gt; 9283/TCP 3h rook-ceph-mgr-dashboard ClusterIP 10.110.113.240 \u0026lt;none\u0026gt; 8443/TCP 3h 第一项服务用于报告Prometheus指标，而后一项服务用于仪表板。如果您在集群中的节点上，则可以通过使用服务的DNS名称https://rook-ceph-mgr-dashboard-https:8443或通过连接到集群IP（在本示例中为）来连接到仪表板https://10.110.113.240:8443。或者使用NodePort暴露端口使用\n查询密码\n1 kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\u0026#34;{[\u0026#39;data\u0026#39;][\u0026#39;password\u0026#39;]}\u0026#34; | base64 --decode \u0026amp;\u0026amp; echo ","date":"2021-03-08T22:05:09Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202103082205/","title":"Kubernetes搭建RooK+Ceph"},{"content":"一、什么是Qos 1 2 QoS类是Kubernetes用来决定Pod的调度和驱逐的策略 本文介绍怎样配置Pod让其获得特定的服务质量（QoS）类 二、QoS级别 2.1、QoS级别\nGuaranteed：POD中所有容器都必须统一设置了limits，并且设置参数都一致，如果有一个容器要设置requests，那么所有容器都要设置，并设置参数同limits一致 Burstable：POD中只要有一个容器，这个容器requests和limits的设置同其他容器设置的不一致 BestEffort：POD中的所有容器都没有指定CPU和内存的requests和limits 删除策略：先删除服务质量为BestEffort，然后在删除Burstable，Guaranteed最后被删除\n三、举例说明 3.0、创建命名空间\n创建一个命名空间，以便将本练习所创建的资源与集群的其余资源相隔离\n1 kubectl create namespace qos-example 创建一个 QoS 类为 Guaranteed 的 Pod 对于 QoS 类为 Guaranteed 的 Pod：\nPod 中的每个容器，包含初始化容器，必须指定内存请求和内存限制，并且两者要相等。 Pod 中的每个容器，包含初始化容器，必须指定 CPU 请求和 CPU 限制，并且两者要相等。 下面是包含一个容器的 Pod 配置文件。 容器设置了内存请求和内存限制，值都是 200 MiB。 容器设置了 CPU 请求和 CPU 限制，值都是 700 milliCPU：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: qos-demo namespace: qos-example spec: containers: - name: qos-demo-ctr image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;700m\u0026#34; requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;700m\u0026#34; 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed。 结果也确认了 Pod 容器设置了与内存限制匹配的内存请求，设置了与 CPU 限制匹配的 CPU 请求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 root@k8s-master01 ~]# kubectl get pod qos-demo --namespace=qos-example --output=yaml spec: containers: - image: nginx:1.15.2 imagePullPolicy: IfNotPresent name: qos-demo-ctr resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi ............ status: qosClass: Guaranteed # 4、删除Pod [root@k8s-master01 ~]# kubectl delete pod qos-demo --namespace=qos-example pod \u0026#34;qos-demo\u0026#34; deleted 说明： 如果容器指定了自己的内存限制，但没有指定内存请求，Kubernetes 会自动为它指定与内存限制匹配的内存请求。 同样，如果容器指定了自己的 CPU 限制，但没有指定 CPU 请求，Kubernetes 会自动为它指定与 CPU 限制匹配的 CPU 请求\n3.2、创建一个 QoS 类为 Burstable 的 Pod\n如果满足下面条件，将会指定 Pod 的 QoS 类为 Burstable：\nPod 不符合 Guaranteed QoS 类的标准。 Pod 中至少一个容器具有内存或 CPU 请求。 下面是包含一个容器的 Pod 配置文件。 容器设置了内存限制 200 MiB 和内存请求 100 MiB。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: v1 kind: Pod metadata: name: qos-demo-2 namespace: qos-example spec: containers: - name: qos-demo-2-ctr image: nginx resources: limits: memory: \u0026#34;200Mi\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; # 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed。 结果也确认了 Pod 容器设置了与内存限制匹配的内存请求，设置了与 CPU 限制匹配的 CPU 请求。 spec: containers: ... resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi ...... status: qosClass: Guaranteed 3.3、创建一个 QoS 类为 BestEffort 的 Pod\n对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求 下面是包含一个容器的 Pod 配置文件。 容器没有设置内存和 CPU 限制或请求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: name: qos-demo-3 namespace: qos-example spec: containers: - name: qos-demo-3-ctr image: nginx # 结果表明 Kubernetes 为 Pod 配置的 QoS 类为 BestEffort。 spec: containers: ... resources: {} ... status: qosClass: BestEffort 3.4、创建包含两个容器的 Pod\n下面是包含两个容器的 Pod 配置文件。 一个容器指定了内存请求 200 MiB。 另外一个容器没有指定任何请求和限制。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: Pod metadata: name: qos-demo-4 namespace: qos-example spec: containers: - name: qos-demo-4-ctr-1 image: nginx resources: requests: memory: \u0026#34;200Mi\u0026#34; - name: qos-demo-4-ctr-2 image: redis # 结果表明 spec: containers: ... name: qos-demo-4-ctr-1 resources: requests: memory: 200Mi ... name: qos-demo-4-ctr-2 resources: {} ... status: qosClass: Burstable 注意此 Pod 满足 Burstable QoS 类的标准。 也就是说它不满足 Guaranteed QoS 类标准，因为它的一个容器设有内存请求。\n原文地址：https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/\n","date":"2021-03-07T17:40:02Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/202103071740/","title":"Kubernetes的服务质量（QoS）"},{"content":"一、什么是K8S之准入控制 1 就是在创建资源经过身份验证之后，kube-apiserver在数据写入etcd之前做一次拦截，然后对资源进行更改、判断正确性等操作。 一个 LimitRange（限制范围） 对象提供的限制能够做到：\n在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。 在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。 在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。 设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。 二、启用 LimitRang 对 LimitRange 的支持自 Kubernetes 1.10 版本默认启用。 LimitRange 支持在很多 Kubernetes 发行版本中也是默认启用的。 LimitRange 的名称必须是合法的 DNS 子域名。 三、限制范围总览 管理员在一个命名空间内创建一个 LimitRange 对象。 用户在命名空间内创建 Pod ，Container 和 PersistentVolumeClaim 等资源。 LimitRanger 准入控制器对所有没有设置计算资源需求的 Pod 和 Container 设置默认值与限制值， 并跟踪其使用量以保证没有超出命名空间中存在的任意 LimitRange 对象中的最小、最大资源使用量以及使用量比值。 若创建或更新资源（Pod、 Container、PersistentVolumeClaim）违反了 LimitRange 的约束， 向 API 服务器的请求会失败，并返回 HTTP 状态码 403 FORBIDDEN 与描述哪一项约束被违反的消息。 若命名空间中的 LimitRange 启用了对 cpu 和 memory 的限制， 用户必须指定这些值的需求使用量与限制使用量。否则，系统将会拒绝创建 Pod。 LimitRange 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证。 能够使用限制范围创建的策略示例有：\n在一个有两个节点，8 GiB 内存与16个核的集群中，限制一个命名空间的 Pod 申请 100m 单位，最大 500m 单位的 CPU，以及申请 200Mi，最大 600Mi 的内存。 为 spec 中没有 cpu 和内存需求值的 Container 定义默认 CPU 限制值与需求值 150m，内存默认需求值 300Mi。 在命名空间的总限制值小于 Pod 或 Container 的限制值的总和的情况下，可能会产生资源竞争。 在这种情况下，将不会创建 Container 或 Pod。\n竞争和对 LimitRange 的改变都不会影响任何已经创建了的资源\n四、配置LimitRange（对Pod进行配置、限额） 4.1、配置 CPU 最小和最大约束\n1 2 3 # 创建一个命名空间，以便本练习中创建的资源和集群的其余资源相隔离 [root@k8s-master01 ~]# kubectl create namespace constraints-cpu-example namespace/constraints-cpu-example created 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 1、LimitRange 的配置文件 [root@k8s-master01 ~]# cat cpu-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits: - max: cpu: \u0026#34;800m\u0026#34; min: cpu: \u0026#34;200m\u0026#34; type: Container # 2、创建 LimitRange: [root@k8s-master01 ~]# kubectl apply -f cpu-constraints.yaml -n constraints-cpu-example limitrange/cpu-min-max-demo-lr created # 3、查看结果 # 输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。 [root@k8s-master01 ~]# kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example # 只列出关键的 spec: limits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container 现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：\n如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。 核查容器声明的 CPU 请求确保其大于或者等于 200 millicpu。 核查容器声明的 CPU 限制确保其小于或者等于 800 millicpu。 流程就是说，我们创建了一个namespace=constraints-cpu-example的名称空间，然后在这个名称空间创建了一个LimitRange。然后现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行上面那些步骤！我们创建的LimitRange对这个namespace=constraints-cpu-example的名称空间里面起低调Pod都起了限制的作用\n4.2、配置命名空间的最小和最大内存约束\nLimitRange 的配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: LimitRange metadata: name: mem-min-max-demo-lr spec: limits: - max: memory: 1Gi min: memory: 500Mi type: Container # 输出显示预期的最小和最大内存约束。 但请注意，即使你没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。 limits: - default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container 现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤：\n如果 Container 未指定自己的内存请求和限制，将为它指定默认的内存请求和限制。 验证 Container 的内存请求是否大于或等于500 MiB。 验证 Container 的内存限制是否小于或等于1 GiB。 4.3、配置 CPU 最小和最大约束\nLimitRange 的配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits: - max: cpu: \u0026#34;800m\u0026#34; min: cpu: \u0026#34;200m\u0026#34; type: Container # 输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。 limits: - default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container 在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：\n如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。 核查容器声明的 CPU 请求确保其大于或者等于 200 millicpu。 核查容器声明的 CPU 限制确保其小于或者等于 800 millicpu。 说明： 当创建 LimitRange 对象时，你也可以声明大页面和 GPU 的限制。 当这些资源同时声明了 \u0026lsquo;default\u0026rsquo; 和 \u0026lsquo;defaultRequest\u0026rsquo; 参数时，两个参数值必须相同。\n五、ResourceQuot（对名称空间进行配置、限额） 5.1、配置内存和 CPU 配额\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 1、创建命名空间，以便本练习中创建的资源和集群的其余部分相隔离。 [root@k8s-master01 ~]# kubectl create namespace quota-mem-cpu-example namespace/quota-mem-cpu-example created # 2、创建 ResourceQuota [root@k8s-master01 ~]# vim quota-mem-cpu.yaml apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo spec: hard: requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi # 3、创建 ResourceQuota [root@k8s-master01 ~]# kubectl apply -f quota-mem-cpu.yaml -n quota-mem-cpu-example resourcequota/mem-cpu-demo created # 4、查看 ResourceQuota 详情： [root@k8s-master01 ~]# kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml # 跟我们配置的一样 spec: hard: limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求：\n每个容器必须有内存请求和限制，以及 CPU 请求和限制。 所有容器的内存请求总和不能超过1 GiB。 所有容器的内存限制总和不能超过2 GiB。 所有容器的 CPU 请求总和不能超过1 cpu。 所有容器的 CPU 限制总和不能超过2 cpu。 也就是在名称空间 quota-mem-cpu-example种创建Pod，必须遵守我们在上面定义的要求\n5.1.1、创建 Pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 cat \u0026gt; quota-mem-cpu-pod.yaml \u0026lt;\u0026lt; EFO apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo spec: containers: - name: quota-mem-cpu-demo-ctr image: nginx resources: limits: memory: \u0026#34;800Mi\u0026#34; cpu: \u0026#34;800m\u0026#34; requests: memory: \u0026#34;600Mi\u0026#34; cpu: \u0026#34;400m\u0026#34; EFO # create Pod kubectl apply -f quota-mem-cpu-pod.yaml --namespace=quota-mem-cpu-example # 查看配额，能看到用了多少 [root@k8s-master01 ~]# kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml spec: hard: limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi status: hard: limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi used: limits.cpu: 800m limits.memory: 800Mi requests.cpu: 400m requests.memory: 600Mi 5.1.2、尝试创建第二个 Pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [root@k8s-master01 ~]# cat quota-mem-cpu-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo-2 spec: containers: - name: quota-mem-cpu-demo-2-ctr image: redis resources: limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;800m\u0026#34; requests: memory: \u0026#34;700Mi\u0026#34; cpu: \u0026#34;400m\u0026#34; # 尝试创建 [root@k8s-master01 ~]# kubectl apply -f quota-mem-cpu-pod-2.yaml --namespace=quota-mem-cpu-example Error from server (Forbidden): error when creating \u0026#34;quota-mem-cpu-pod-2.yaml\u0026#34;: pods \u0026#34;quota-mem-cpu-demo-2\u0026#34; is forbidden: exceeded quota: mem-cpu-demo, requested: requests.memory=700Mi, used: requests.memory=600Mi, limited: requests.memory=1Gi # 第二个 Pod 不能被创建成功。输出结果显示创建第二个 Pod 会导致内存请求总量超过内存请求配额。 # 删除你的命名空间： kubectl delete namespace quota-mem-cpu-example 5.2、配置命名空间下 Pod 配额\n如何配置一个命名空间下可运行的 Pod 个数配额？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 1、创建一个命名空间 kubectl create namespace quota-pod-example # 2、创建 ResourceQuota，指定改ns只可以创建2个pod apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo spec: hard: pods: \u0026#34;2\u0026#34; # 3、apply ResourceQuota kubectl apply -f quota-pod.yaml --namespace=quota-pod-example # 4、查看资源配额的详细信息： kubectl get resourcequota pod-demo --namespace=quota-pod-example --output=yaml # 5、创建Deployment，且replicas是3，那么肯定只有2个Pod能正常运行！自己去试试吧 apiVersion: apps/v1 kind: Deployment metadata: name: pod-quota-demo spec: selector: matchLabels: purpose: quota-demo replicas: 3 template: metadata: labels: purpose: quota-demo spec: containers: - name: pod-quota-demo image: nginx 中文官网文档：https://kubernetes.io/zh/docs/concepts/policy/limit-range/\n","date":"2021-03-07T13:37:09Z","image":"https://www.ownit.top/title_pic/52.jpg","permalink":"https://www.ownit.top/p/202103071337/","title":"Kubernetes准入控制"},{"content":"Ratel是有杜宽开发一个类似Kubernetes-**Dashboard，**功能正在慢慢完善\ndotbalo (dotbalo)杜宽github\nratel地址：https://github.com/dotbalo/ratel-doc\n1 2 3 4 5 6 7 Ratel是一个Kubernetes资源平台，基于管理Kubernetes的资源开发， 可以管理Kubernetes的Deployment、DaemonSet、StatefulSet、Service、Ingress、Pods、Nodes。 也可以管理Kubernetes的Role、ClusterRole、Rolebinding、ClusterRoleBinding、Secret、ConfigMap、PV、PVC等。 立志于基于图形界面管理所有的Kubernetes的资源。 一、安装Ratel 1.1、安装说明 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 集群安装配置需要两类文件: servers.yaml和集群管理的kubeconfig文件 servers.yaml是ratel的配置文件, 格式如下: - serverName: \u0026#39;xiqu\u0026#39; serverAddress: \u0026#39;https://1.1.1.1:8443\u0026#39; #serverAdminUser: \u0026#39;xxx\u0026#39; #serverAdminPassword: \u0026#39;xxx#\u0026#39; serverAdminToken: \u0026#39;null\u0026#39; serverDashboardUrl: \u0026#34;https://k8s.xxx.com.cn/#\u0026#34; production: \u0026#39;false\u0026#39; kubeConfigPath: \u0026#34;/mnt/xxx.config\u0026#34; harborConfig: \u0026#34;HarborUrl, HarborUsername, HarborPassword, HarborEmail\u0026#34; 其中管理的方式有两种(Token暂不支持): 账号密码和kubeconfig形式, 只需配置一种即可, kubeconfig优先级高 参数解析: serverName: 集群别名 serverAddress: Kubernetes APIServer地址 serverAdminUser: Kubernetes管理员账号(需要配置basic auth) serverAdminPassword: Kubernetes管理员密码 serverAdminToken: Kubernetes管理员Token // 暂不支持 serverDashboardUrl: Kubernetes官方dashboard地址，1.x版本需要添加/#!，2.x需要添加/# kubeConfigPath: Kubernetes kube.config路径(绝对路径) harborConfig: 对于多集群管理的情况下，可能会存在不同的harbor仓库，配置此参数可以在拷贝资源的时候自动替换harbor配置 kubeConfigPath 通过secret挂载到容器的/mnt目录或者其他目录 本文档是将Ratel安装在Kubernetes集群，如果没有Kubernetes集群，可以参考本人写的另一篇文章，CentOS 8二进制高可用安装Kubernetes集群: https://www.cnblogs.com/dukuan/p/11780729.html 1.2 创建Secret 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 假设配置两个集群，对应的kubeconfig是test1.config和test2.config ratel配置文件servers.yaml内容如下: - serverName: \u0026#39;test1\u0026#39; serverAddress: \u0026#39;https://1.1.1.1:8443\u0026#39; #serverAdminUser: \u0026#39;xxx\u0026#39; #serverAdminPassword: \u0026#39;xxx#\u0026#39; serverAdminToken: \u0026#39;null\u0026#39; serverDashboardUrl: \u0026#34;https://k8s.test1.com.cn/#\u0026#34; production: \u0026#39;false\u0026#39; kubeConfigPath: \u0026#34;/mnt/test1.config\u0026#34; harborConfig: \u0026#34;HarborUrl, HarborUsername, HarborPassword, HarborEmail\u0026#34; - serverName: \u0026#39;test2\u0026#39; serverAddress: \u0026#39;https://1.1.1.2:8443\u0026#39; #serverAdminUser: \u0026#39;xxx\u0026#39; #serverAdminPassword: \u0026#39;xxx#\u0026#39; serverAdminToken: \u0026#39;null\u0026#39; serverDashboardUrl: \u0026#34;https://k8s.test2.com.cn/#!\u0026#34; production: \u0026#39;false\u0026#39; kubeConfigPath: \u0026#34;/mnt/test2.config\u0026#34; harborConfig: \u0026#34;HarborUrl, HarborUsername, HarborPassword, HarborEmail\u0026#34; 创建Secret: kubectl create secret generic ratel-config --from-file=test1.config --from-file=test2.config --from-file=servers.yaml -n kube-system #test1.config是master的权限配置 cp /root/.kube/config test1.config 我的配置 - serverName: \u0026#39;test1\u0026#39; serverAddress: \u0026#39;https://192.168.0.100:6443\u0026#39; #serverAdminUser: \u0026#39;xxx\u0026#39; #serverAdminPassword: \u0026#39;xxx#\u0026#39; serverAdminToken: \u0026#39;null\u0026#39; serverDashboardUrl: \u0026#34;http://krm.test.com/#\u0026#34; production: \u0026#39;false\u0026#39; kubeConfigPath: \u0026#34;/mnt/test1.config\u0026#34; kubectl create secret generic ratel-config --from-file=test1.config --from-file=servers.yaml -n kube-system 1.3 创建RBAC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 创建权限管理namespace kubectl create ns kube-users 然后添加如下的ClusterroleBinding vim ratel-rbac.yaml apiVersion: v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; name: ratel-namespace-readonly rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get - list - watch - apiGroups: - metrics.k8s.io resources: - pods verbs: - get - list - watch - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ratel-pod-delete rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - list - delete - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ratel-pod-exec rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods - pods/log verbs: - get - list - apiGroups: - \u0026#34;\u0026#34; resources: - pods/exec verbs: - create - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; name: ratel-resource-edit rules: - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - persistentvolumeclaims - services - services/proxy verbs: - patch - update - apiGroups: - apps resources: - daemonsets - deployments - deployments/rollback - deployments/scale - statefulsets - statefulsets/scale verbs: - patch - update - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - patch - update - apiGroups: - batch resources: - cronjobs - jobs verbs: - patch - update - apiGroups: - extensions resources: - daemonsets - deployments - deployments/rollback - deployments/scale - ingresses verbs: - patch - update - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ratel-resource-readonly rules: - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - endpoints - persistentvolumeclaims - pods - replicationcontrollers - replicationcontrollers/scale - serviceaccounts - services verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - bindings - events - limitranges - namespaces/status - pods/log - pods/status - replicationcontrollers/status - resourcequotas - resourcequotas/status verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get - list - watch - apiGroups: - apps resources: - controllerrevisions - daemonsets - deployments - deployments/scale - replicasets - replicasets/scale - statefulsets - statefulsets/scale verbs: - get - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - get - list - watch - apiGroups: - batch resources: - cronjobs - jobs verbs: - get - list - watch - apiGroups: - extensions resources: - daemonsets - deployments - deployments/scale - ingresses - networkpolicies - replicasets - replicasets/scale - replicationcontrollers/scale verbs: - get - list - watch - apiGroups: - policy resources: - poddisruptionbudgets verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - networkpolicies verbs: - get - list - watch - apiGroups: - metrics.k8s.io resources: - pods verbs: - get - list - watch kind: List metadata: resourceVersion: \u0026#34;\u0026#34; selfLink: \u0026#34;\u0026#34; 1 kubectl create -f ratel-rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 vim ratel-rbac-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ratel-namespace-readonly-sa roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ratel-namespace-readonly subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:serviceaccounts:kube-users kubectl create -f ratel-rbac-binding.yaml 1.4 部署ratel 1 ratel的部署文件内容如下: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 apiVersion: apps/v1 kind: Deployment metadata: labels: app: ratel name: ratel namespace: kube-system spec: replicas: 1 selector: matchLabels: app: ratel strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: ratel spec: containers: - command: - sh - -c - ./ratel -c /mnt/servers.yaml env: - name: TZ value: Asia/Shanghai - name: LANG value: C.UTF-8 - name: ProRunMode value: prod - name: ADMIN_USERNAME value: admin - name: ADMIN_PASSWORD value: password image: registry.cn-beijing.aliyuncs.com/dotbalo/ratel:latest imagePullPolicy: Always livenessProbe: failureThreshold: 2 initialDelaySeconds: 10 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8888 timeoutSeconds: 2 name: ratel ports: - containerPort: 8888 name: web protocol: TCP readinessProbe: failureThreshold: 2 initialDelaySeconds: 10 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8888 timeoutSeconds: 2 resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi volumeMounts: - mountPath: /mnt name: ratel-config dnsPolicy: ClusterFirst # imagePullSecrets: # - name: myregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: ratel-config secret: defaultMode: 420 secretName: ratel-config 需要更改的内容如下: ProRunMode: 区别在于dev模式打印的是debug日志, 其他模式是info级别的日志, 实际使用时应该配置为非dev ADMIN_USERNAME: ratel自己的管理员账号 ADMIN_PASSWORD: ratel自己的管理员密码 实际使用时账号密码应满足复杂性要求,因为ratel可以直接操作所有配置的资源。 其他无需配置, 端口配置暂不支持。 1.5 Service和Ingress配置 注意：如果没有安装ingress controller，需要把type: ClusterIP改成type: NodePort，然后通过主机IP+Port进行访问\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 创建ratel Service的文件如下: apiVersion: v1 kind: Service metadata: labels: app: ratel name: ratel namespace: kube-system spec: ports: - name: container-1-web-1 port: 8888 protocol: TCP targetPort: 8888 selector: app: ratel type: ClusterIP 创建ratel Ingress: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ratel namespace: kube-system labels: app: ratel spec: rules: - host: krm.test.com http: paths: - backend: serviceName: ratel servicePort: 8888 path: / 1.6 访问ratel 注意：如果没有安装ingress controller，需要把type: ClusterIP改成type: NodePort，然后通过主机IP+Port进行访问\n1 通过Ingress配置的krm.test.com/ratel访问，ratel登录页如下: ","date":"2021-03-07T10:18:07Z","image":"https://www.ownit.top/title_pic/10.jpg","permalink":"https://www.ownit.top/p/202103071018/","title":"Kubernetes安装Ratel"},{"content":"由于一些原因,在国内无法访问gcr.io上的镜像,在安装kubernetes时经常访问阿里云的地址。面结合实际经验,列举出几种常用的办法来解决这个问题！\n一、使用阿里云镜像地址 1 2 地址一：registry.aliyuncs.com/google_containers 地址二：registry.cn-hangzhou.aliyuncs.com/google_containers 二、使用dockerhub下的mirrorgooglecontainers ​ 这个域名下同步了很多谷歌镜像,比如说要下载gcr.io/google_containers/coredns:1.7.0,\n可以使用docker pull mirrorgooglecontainers/coredns:1.7.0来进行下载,下载以后对镜像重新打标签:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1、先pull下来 [root@k8s-master01 Ratel]# docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.3 v1.11.3: Pulling from mirrorgooglecontainers/kube-proxy-amd64 06545d1c6152: Pull complete d5f5a75f5817: Pull complete c21dcda023ab: Pull complete Digest: sha256:cd0c257e3f4a79a0ae7964b3429c491e9d43bf1bb015618a4c311165d3915b7b Status: Downloaded newer image for mirrorgooglecontainers/kube-proxy-amd64:v1.11.3 docker.io/mirrorgooglecontainers/kube-proxy-amd64:v1.11.3 # 2、重新打标签 [root@k8s-master01 Ratel]# docker tag docker.io/mirrorgooglecontainers/kube-proxy-amd64:v1.11.3 k8s.gcr.io/kube-proxy-amd64:v1.11.3 # 3、查看镜像，然后就可以直接使用这个镜像了 [root@k8s-master01 Ratel]# docker images | grep k8s.gcr.io/kube-proxy-amd64 k8s.gcr.io/kube-proxy-amd64 v1.11.3 be5a6e1ecfa6 2 years ago 97.8MB 三、使用国内作者制作的gcr.io镜像安装工具 1 项目地址: https://github.com/zhangguanzhang/gcr.io 3.0、使用search命令的时候,如果没有安装jq则会提示安装jq.jq在centos下安装方法:\n安装EPEL源： 1 [root@k8s-master01 ~]# yum install epel-release 安装完EPEL源后，可以查看下jq包是否存在： 1 [root@k8s-master01 ~]# yum list jq 安装jq： 1 [root@k8s-master01 ~]# yum install jq -y 3.1、查询namespace\n1 2 3 4 5 6 7 8 9 10 [root@k8s-master01 ~]# curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s search gcr.io cloud-builders cloud-datalab cloudsql-docker distroless google-appengine google-samples google_containers google_samples heptio-images 3.2、查询某一名称空间下镜像列表\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@k8s-master01 ~]# curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s search gcr.io/google_containers # gcr.io/google_containers ---\u0026gt; namespace ——————\u0026gt; 根据上面查询出来的namespace查 addon-builder addon-resizer-amd64 addon-resizer-arm addon-resizer-arm64 addon-resizer-ppc64le addon-resizer-s390x addon-resizer aggregator alpine-iptables-amd64 alpine-iptables-arm alpine-iptables-arm64 3.3、查询某一镜像的版本所有版本tag\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@k8s-master01 ~]# curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s search gcr.io/google_containers/coredns # 在namespace后面搜索image的版本tag 1.0.1 1.0.1__amd64_linux 1.0.1__arm64_linux 1.0.1__arm_linux 1.0.1__ppc64le_linux 1.0.1__s390x_linux 1.0.6 1.0.6__amd64_linux 1.0.6__arm64_linux 1.0.6__arm_linux 1.0.6__ppc64le_linux 1.0.6__s390x_linux 1.1.3 1.1.3__amd64_linux 3.4 下载镜像\n1 curl -s https://zhangguanzhang.github.io/bash/pull.sh | bash -s gcr.io/google_containers/coredns:1.7.0 原文地址：https://www.cnblogs.com/tylerzhou/p/10971341.html\n","date":"2021-03-06T22:46:41Z","image":"https://www.ownit.top/title_pic/13.jpg","permalink":"https://www.ownit.top/p/202103062246/","title":"Docker无法访问gcr.io的几种解决办法"},{"content":"一、什么是RBAC？ Role-based access control(RBAC)基于企业内个人用户属于角色来访问计算和网络的常规访问控制方法。\n简单理解为权限与角色关联，用户通过成为角色的成员来得到角色的权限。K8S的RBAC使用rbac.authorization.k8s.io/v1 API组驱动认证决策，准许管理员通过API动态配置策略。为了启用RBAC，需要在apiserver启动参数添加\u0026ndash;authorization-mode=RBAC。目前支持RBAC,ABAC(基于属性的访问控制)，Node（默认node和apiserver就是采用这种模式）,Webhook。\n二、RBAC分类介绍（API 对象） 2.1、RBAC有4种顶级资源\nRole ClusterRole RoleBinding ClusterRoleBinding 2.2、资源介绍\nRole：角色，包含一组权限的规则。没有拒绝规则，只是附加允许。Namespace隔离，只作用于命名空间内！\nClusterRole：集群角色，和Role一样。和Role的区别，Role是只作用于命名空间内，ClusterRole作用于整个集群！\nRoleBinding：作用于命令空间内，将ClusterRole或者Role绑定到User、Group、ServiceAccount！\nClusterRoleBinding：作用于整个集群。\n中文官方文档：https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/\n三、示例 3.1、Role 示例\nRole 总是用来在某个名字空间内设置访问权限；在你创建 Role 时，你必须指定该 Role 所属的名字空间\n1 2 3 4 5 6 7 8 9 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 标明 core API 组 resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 3.2、ClusterRole 示例\nClusterRole 有若干用法。你可以用它来：\n定义对某名字空间域对象的访问权限，并将在各个名字空间内完成授权； 为名字空间作用域的对象设置访问权限，并跨所有名字空间执行授权； 为集群作用域的资源定义访问权限。 1 2 3 4 5 6 7 8 9 10 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # \u0026#34;namespace\u0026#34; 被忽略，因为 ClusterRoles 不受名字空间限制 name: secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # 在 HTTP 层面，用来访问 Secret 对象的资源的名称为 \u0026#34;secrets\u0026#34; resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 3.3、RoleBinding 示例\n下面的例子中的 RoleBinding 将 \u0026ldquo;pod-reader\u0026rdquo; Role 授予在 \u0026ldquo;default\u0026rdquo; 名字空间中的用户 \u0026ldquo;jane\u0026rdquo;。 这样，用户 \u0026ldquo;jane\u0026rdquo; 就具有了读取 \u0026ldquo;default\u0026rdquo; 名字空间中 pods 的权限。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: rbac.authorization.k8s.io/v1 # 此角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;default\u0026#34; 名字空间中的 Pods kind: RoleBinding metadata: name: read-pods namespace: default subjects: # 你可以指定不止一个“subject（主体）” - kind: User # 这里可以是User,Group,ServiceAccount name: jane # \u0026#34;name\u0026#34; 是不区分大小写的 apiGroup: rbac.authorization.k8s.io roleRef: # \u0026#34;roleRef\u0026#34; 指定与某 Role 或 ClusterRole 的绑定关系 kind: Role # 此字段必须是 Role 或 ClusterRole name: pod-reader # 此字段必须与你要绑定的 Role 或 ClusterRole 的名称匹配 apiGroup: rbac.authorization.k8s.io 3.3.1、RoleBinding 引用 ClusterRole\nRoleBinding 也可以引用 ClusterRole，以将对应 ClusterRole 中定义的访问权限授予 RoleBinding 所在名字空间的资源。这种引用使得你可以跨整个集群定义一组通用的角色， 之后在多个名字空间中复用！\n下面的例子中的 RoleBinding，将\u0026quot;secret-reader\u0026quot; ClusterRole授予在\u0026quot;development\u0026quot; namespace中的用户\u0026quot;dave\u0026quot;。这样，用户 \u0026ldquo;dave\u0026rdquo; 就具有了读取 \u0026ldquo;development\u0026rdquo; 名字空间中 pods 的权限。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: rbac.authorization.k8s.io/v1 # 此角色绑定使得用户 \u0026#34;dave\u0026#34; 能够读取 \u0026#34;default\u0026#34; 名字空间中的 Secrets # 你需要一个名为 \u0026#34;secret-reader\u0026#34; 的 ClusterRole kind: RoleBinding metadata: name: read-secrets # RoleBinding 的名字空间决定了访问权限的授予范围。 # 这里仅授权在 \u0026#34;development\u0026#34; 名字空间内的访问权限。 namespace: development subjects: - kind: User name: dave # \u0026#39;name\u0026#39; 是不区分大小写的 apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 3.4、ClusterRoleBinding 示例\n要跨整个集群完成访问权限的授予，你可以使用一个 ClusterRoleBinding。 下面的 ClusterRoleBinding 允许 \u0026ldquo;manager\u0026rdquo; 组内的所有用户访问任何名字空间中的 Secrets。\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: rbac.authorization.k8s.io/v1 # 此集群角色绑定允许 “manager” 组中的任何人访问任何名字空间中的 secrets kind: ClusterRoleBinding metadata: name: read-secrets-global subjects: - kind: Group name: manager # \u0026#39;name\u0026#39; 是不区分大小写的 apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 注意：\n​ 创建了绑定之后，你不能再修改绑定对象所引用的 Role 或 ClusterRole。 试图改变绑定对象的 roleRef 将导致合法性检查错误。 如果你想要改变现有绑定对象中 roleRef 字段的内容，必须删除重新创建绑定对象。\n这种限制有两个主要原因：\n针对不同角色的绑定是完全不一样的绑定。要求通过删除/重建绑定来更改 roleRef, 这样可以确保要赋予绑定的所有主体会被授予新的角色（而不是在允许修改 roleRef 的情况下导致所有现有主体胃镜验证即被授予新角色对应的权限）。 将 roleRef 设置为不可以改变，这使得可以为用户授予对现有绑定对象的 update 权限， 这样可以让他们管理主体列表，同时不能更改被授予这些主体的角色。 命令 kubectl auth reconcile 可以创建或者更新包含 RBAC 对象的清单文件， 并且在必要的情况下删除和重新创建绑定对象，以改变所引用的角色\n3.5、聚合的 ClusterRole\n下面是一个聚合 ClusterRole 的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring aggregationRule: clusterRoleSelectors: - matchLabels: rbac.example.com/aggregate-to-monitoring: \u0026#34;true\u0026#34; rules: [] # 控制面自动填充这里的规则 ------- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring-endpoints labels: rbac.example.com/aggregate-to-monitoring: \u0026#34;true\u0026#34; # 当你创建 \u0026#34;monitoring-endpoints\u0026#34; ClusterRole 时， # 下面的规则会被添加到 \u0026#34;monitoring\u0026#34; ClusterRole 中 rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 下面的 ClusterRoles 让默认角色 \u0026ldquo;admin\u0026rdquo; 和 \u0026ldquo;edit\u0026rdquo; 拥有管理自定义资源 \u0026ldquo;CronTabs\u0026rdquo; 的权限， \u0026ldquo;view\u0026rdquo; 角色对 CronTab 资源拥有读操作权限。 你可以假定 CronTab 对象在 API 服务器所看到的 URL 中被命名为 \u0026quot;crontabs\u0026quot;。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: aggregate-cron-tabs-edit labels: # 添加以下权限到默认角色 \u0026#34;admin\u0026#34; 和 \u0026#34;edit\u0026#34; 中 rbac.authorization.k8s.io/aggregate-to-admin: \u0026#34;true\u0026#34; rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; rules: - apiGroups: [\u0026#34;stable.example.com\u0026#34;] resources: [\u0026#34;crontabs\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: aggregate-cron-tabs-view labels: # 添加以下权限到 \u0026#34;view\u0026#34; 默认角色中 rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; rules: - apiGroups: [\u0026#34;stable.example.com\u0026#34;] resources: [\u0026#34;crontabs\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] ","date":"2021-03-06T22:16:21Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/202103062216/","title":"Kubernetes认证之RBAC"},{"content":"无聊，近期痴迷编程，没事就写写shell和Python代码练练手。\n这次写了个自动安装saltstack的shell代码菜单\n功能比较low，相对比较省事。多台主机安装方便。仅供参考，大佬勿喷\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 #!/bin/bash #颜色控制 red=\u0026#39;\\033[1;31m\u0026#39; black=\u0026#39;\\033[0m\u0026#39; Orange=\u0026#39;\\033[35m\u0026#39; background=\u0026#39;\\033[0m\u0026#39; minion_file=\u0026#39;/etc/salt/minion\u0026#39; datetime=`date +\u0026#34;%F %T\u0026#34;` #判断软件是否安装 function if_installed(){ salt_name=$1 salt_status=`rpm -qa | grep $salt_name` if [ ! -n \u0026#34;$salt_status\u0026#34; ];then echo \u0026#34;$salt_name没有安装\u0026#34; return 2 else echo \u0026#34;$salt_name已经安装\u0026#34; echo $salt_status fi } #安装master组件 function install_master(){ salt_name=$1 if_installed $salt_name if [ $? -eq 2 ];then yum install -y https://repo.saltstack.com/yum/redhat/salt-repo-latest-2.el7.noarch.rpm sed -i \u0026#34;s/repo.saltstack.com/mirrors.aliyun.com\\/saltstack/g\u0026#34; /etc/yum.repos.d/salt-latest.repo yum install -y $salt_name systemctl enable $salt_name systemctl start $salt_name\techo \u0026#34;s$salt_name已经安装\u0026#34; fi } #4)salt-minion配置 function salt-minion(){ read -p \u0026#34;请输salt-master的IP:\u0026#34; master_ip if [ ! -n \u0026#34;$master_ip\u0026#34; ];then echo \u0026#34;未获取salt-master的IP，无法配置salt-minion\u0026#34; else if [ -f \u0026#34;$minion_file\u0026#34; ]; then num1=`grep -vE \u0026#39;^#|^$\u0026#39; $minion_file | grep master |wc -l ` old_ip=`grep -vE \u0026#39;^#|^$\u0026#39; $minion_file | grep master | awk -F \u0026#34;:\u0026#34; \u0026#39;{print $2}\u0026#39;` if [ $num1 -eq 0 ];then sed -i \u0026#34;17i master: $master_ip\u0026#34; $minion_file echo \u0026#34;salt-minion配置salt-master的IP为：$master_ip\u0026#34; else sed -i \u0026#34;s/$old_ip/$master_ip/\u0026#34; $minion_file echo \u0026#34;salt-minion配置salt-master的IP为：$master_ip\u0026#34; fi else echo \u0026#34;$minion_file文件不存在\u0026#34; fi fi } #删除软件 function salt_remove(){ salt_name=$1 salt_status=`rpm -qa | grep $salt_name` if [ ! -n \u0026#34;$salt_status\u0026#34; ];then echo \u0026#34;$salt_name没有安装\u0026#34; else rpm -qa | grep $salt_name | xargs rpm -e if [ $? -eq 0 ];then echo \u0026#34;$salt_name已经卸载\u0026#34; fi fi } #重启服务 function salt_restart(){ salt_name=$1 if_installed $salt_name if [ $? -eq 2 ];then echo \u0026#34;\u0026#34; else systemctl restart $salt_name echo \u0026#34;$salt_name已经重启\u0026#34; fi } #菜单 function menu() { echo -e \u0026#34; $datetime\u0026#34; cat \u0026lt;\u0026lt;EOF -------------------------------------------- `echo -e \u0026#34; $black SaltStack菜单主页$background\u0026#34;` `echo -e \u0026#34;$Orange 1)安装slat-master $background\u0026#34;` `echo -e \u0026#34;$Orange 2)安装salt-minion$background\u0026#34;` `echo -e \u0026#34;$Orange 3)是否软件安装查询$background\u0026#34;` `echo -e \u0026#34;$Orange 4)salt-minion配置$background\u0026#34;` `echo -e \u0026#34;$Orange 5)卸载slat-master$background\u0026#34;` `echo -e \u0026#34;$Orange 6)卸载slat-minion$background\u0026#34;` `echo -e \u0026#34;$Orange 7)重启slat-master$background\u0026#34;` `echo -e \u0026#34;$Orange 8)重启slat-minion$background\u0026#34;` `echo -e \u0026#34;$Orange Q)退出$background\u0026#34;` -------------------------------------------- EOF read -p \u0026#34;请输入对应序列号：\u0026#34; num1 case $num1 in 1) install_master salt-master menu ;; 2) install_master salt-minion menu ;; 3) if_installed salt-master if_installed salt-minion menu ;; 4) salt-minion menu ;; 5) salt_remove salt-master menu ;; 6) salt_remove salt-minion menu ;; 7) salt_restart salt-master menu ;; 8) salt_restart salt-minion menu ;; Q|q) exit 0 ;; *) echo -e \u0026#34;\\033[31m err：请输入正确的编号\\033[0m\u0026#34; menu esac } menu ","date":"2021-02-27T18:39:16Z","image":"https://www.ownit.top/title_pic/30.jpg","permalink":"https://www.ownit.top/p/202102271839/","title":"SaltStack脚本安装"},{"content":"此脚本适合Redhat系列\ncentos系列，内存缓存计数位置不同，可能不准确\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #!/bin/bash ##系统信息## sys_check(){ os_type=`uname` echo \u0026#34;操作系统类型是:$os_type\u0026#34; os_banben=`cat /etc/redhat-release` echo \u0026#34;操作系统版本号是:$os_banben\u0026#34; os_neihe=`uname -r` echo \u0026#34;操作系统的内核是:$os_neihe\u0026#34; os_time=`date +%F_%T` echo \u0026#34;操作系统当前时间是:$os_time\u0026#34; os_uptime=`uptime | awk \u0026#39;{print $3}\u0026#39;|awk -F , \u0026#39;{print $1}\u0026#39;` echo \u0026#34;操作系统最后重启时间为:$os_uptime\u0026#34; os_hostname=`hostname` echo \u0026#34;操作系统主机名称为:$os_hostname\u0026#34; } ##网络信息## net_check(){ net_ip=`/sbin/ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk \u0026#39;{print $2}\u0026#39;|tr -d \u0026#34;addr:\u0026#34;` echo \u0026#34;操作系统的ip是:$net_ip\u0026#34; ping -c1 www.baidu.com \u0026gt;/dev/null if [ $? -eq 0 ];then echo \u0026#34;外网可以连通\u0026#34; else echo \u0026#34;外网连不通，请检查\u0026#34; fi } cpu_check(){ physical_id=`cat /proc/cpuinfo | grep \u0026#34;physical id\u0026#34;|sort|uniq|wc -l` echo \u0026#34;操作系统cpu物理个数是:$physical_id\u0026#34; cpu_core=`cat /proc/cpuinfo | grep \u0026#34;cpu cores\u0026#34;|sort|uniq|awk -F \u0026#39;:\u0026#39; \u0026#39;{print $2}\u0026#39;` echo \u0026#34;操作系统的cpu核心数是:$cpu_core\u0026#34; cpu_type=`cat /proc/cpuinfo | grep \u0026#34;model name\u0026#34;|sort|uniq|awk -F \u0026#39;:\u0026#39; \u0026#39;{print $2}\u0026#39;` echo \u0026#34;操作系统的cpu型号是:$cpu_type\u0026#34; free_total=`free -m | grep Mem|awk \u0026#39;{printf $2}\u0026#39;` echo \u0026#34;操作系统的内存总大小为:$free_total M\u0026#34; free_used=`free -m | grep Mem|awk \u0026#39;{printf $3}\u0026#39;` echo \u0026#34;操作系统已使用内存为:$free_used M\u0026#34; free_shengyu=`free -m | grep Mem|awk \u0026#39;{printf $4}\u0026#39;` echo \u0026#34;操作系统剩余内存为:$free_shengyu M\u0026#34; used_baifen=`echo \u0026#34;scale=2;$free_used/$free_total*100\u0026#34;|bc` echo \u0026#34;已使用内存百分比是:$used_baifen\u0026#34;% shengyu_baifen=`echo \u0026#34;scale=2;$free_shengyu/$free_total*100\u0026#34;|bc` echo \u0026#34;未使用内存百分比是:$shengyu_baifen\u0026#34;% } disk_check(){ disk_size=`lsblk | grep -w sda |awk \u0026#39;{print $4}\u0026#39;` echo \u0026#34;磁盘总量为:$disk_size\u0026#34; a=($(df -m | grep -v \u0026#34;tmpfs\u0026#34; | egrep -A 1 \u0026#34;mapper|sd\u0026#34; | awk \u0026#39;NF\u0026gt;1{print $(NF-2)}\u0026#39;)) sum=0 for i in ${a[*]} do let sum=sum+$i done shengfree=$[$sum/1024] echo \u0026#34;剩余磁盘总量为:$shengfree\u0026#34; G } sys_check net_check cpu_check disk_check ","date":"2021-02-25T14:11:07Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/202102251411/","title":"Redhat机器巡检脚本"},{"content":"数据库备份脚本\n按照时间来创建目录备份数据，需要配合crontab\n1 00 1 * * * root /etc/mysqldumpjumpser.sh 每天早上凌晨1点备份数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/bin/bash USER=jumpserver PASS=jumpserver DBDIR=/databak/Data_Backup #DAY=`date +%Y%m%d` #年月 MONTH=`date +%Y%m` #日期时间 DT=`date \u0026#39;+%Y%m%d%H%M\u0026#39;` #主机ip DBIP=`cat /etc/sysconfig/network-scripts/ifcfg-eth0 | grep IPADDR | awk -F \u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39;` #创建备份目录 mkdir -p $DBDIR/$DBIP/$MONTH 备份数据库 for dbname in jumpserver do mysqldump -u$USER -p$PASS -R --single-transaction $dbname 2\u0026gt;\u0026gt;$DBDIR/$DBIP/$MONTH/error-$DT.log |gzip \u0026gt; $DBDIR/$DBIP/$MONTH/$dbname-$DBIP-$DT.sql.gz # 生成md5sum文件 md5sum $DBDIR/$DBIP/$MONTH/$dbname-$DBIP-$DT.sql.gz \u0026gt; $DBDIR/$DBIP/$MONTH/$dbname-$DBIP-$DT.sql.gz.MD5 done # check and delete old datafile. #删除超过30天的文件，并删除目录 del_backup_dir=/databak/Data_Backup/$DBIP cd $del_backup_dir if [ $? = 0 ]; then find ./ -type f -mtime +30 -exec rm -rf {} \\; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 find -depth -type d -empty -exec rmdir {} \\; fi #压缩备份 #mysqldump -uroot -proot --databases abc 2\u0026gt;/dev/null |gzip \u0026gt;/abc.sql.gz #还原 #gunzip -c abc.sql.gz |mysql -uroot -proot abc 每月备份的数据会放到一个目录，十分乱，不容易看，这边改进一下\n没有多大的改变，就是加了DAY=`date -d \u0026lsquo;-1 days\u0026rsquo; +%d` ：显示上一天的日期，我们crontab是凌晨1点备份，也就是备份上一天的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash USER=jumpserver PASS=jumpserver DBDIR=/databak/Data_Backup DAY=`date -d \u0026#39;-1 days\u0026#39; +%d` MONTH=`date +%Y%m` DT=`date \u0026#39;+%Y%m%d%H%M\u0026#39;` DBIP=`cat /etc/sysconfig/network-scripts/ifcfg-eth0 | grep IP | awk -F \u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39;` mkdir -p $DBDIR/$DBIP/$MONTH/$DAY for dbname in jumpserver do mysqldump -u$USER -p$PASS -R --single-transaction $dbname 2\u0026gt;\u0026gt;$DBDIR/$DBIP/$MONTH/$DAY/error-$DT.log |gzip \u0026gt; $DBDIR/$DBIP/$MONTH/$DAY/$dbname-$DBIP-$DT.sql.gz # 生成md5sum文件 md5sum $DBDIR/$DBIP/$MONTH/$DAY/$dbname-$DBIP-$DT.sql.gz \u0026gt; $DBDIR/$DBIP/$MONTH/$DAY/$dbname-$DBIP-$DT.sql.gz.MD5 done # check and delete old datafile. del_backup_dir=/databak/Data_Backup/$DBIP cd $del_backup_dir if [ $? = 0 ]; then find ./ -type f -mtime +30 -exec rm -rf {} \\; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 find -depth -type d -empty -exec rmdir {} \\; fi #压缩备份 #mysqldump -uroot -proot --databases abc 2\u0026gt;/dev/null |gzip \u0026gt;/abc.sql.gz #还原 #gunzip -c abc.sql.gz |mysql -uroot -proot abc 以日期目录分类，可以更方便的清楚\n","date":"2021-02-25T14:07:58Z","image":"https://www.ownit.top/title_pic/13.jpg","permalink":"https://www.ownit.top/p/202102251407/","title":"Mysql数据库备份脚本"},{"content":"rsync 1.1 rsync是什么\nrsync是一款开源的，快速的、多功能的、可实现全量及增量的本地或远程数据同步备份的优秀工具同步备份的优秀工具\n1.2 rsync的特性如下：\n支持拷贝特殊文件如链接文件，设备等 可以有排除指定文件或目录同步的功能，相当于打包命令tar的排除功能。 可以做到保持原文件或目录等权限，时间，软硬链接，属主，属组等所有属性均不改变 –p 可实现增量同步，即只同步发生变化的数据，因此数据传输效率很高 可以使用rcp,rsh,ssh等方式来配置传输文件（rsync本身不对数据加密） 可以通过socket（守护进程方式）传输文件和数据（服务端和客户端） 支持匿名或认证（无需系统用户）的进程模式传输，可实现方便安全的进行数据备份及镜像 rsync也相当于ls命令 1.3 rsync的企业工作场景说明\n两台服务器之间数据同步（定时任务+rsync） 实时同步（解决存储服务器的单点问题） Rsync+Inotify-tools与Rsync+sersync这两种架构有什么区别 1．Rsync+Inotify-tools （1）：Inotify-tools只能记录下被监听的目录发生了变化（包括增加、删除、修改），并没有把具体是哪个文件或者哪个目录发生了变化记录下来；\n（2）：rsync在同步的时候，并不知道具体是哪个文件或者哪个目录发生了变化，每次都是对整个目录进行同步，当数据量很大时，整个目录同步非常耗时（rsync要对整个目录遍历查找对比文件），因此，效率很低。\n2．Rsync+sersync （1）sersync可以记录下被监听目录中发生变化的（包括增加、删除、修改）具体某一个文件或某一个目录的名字；\n（2）rsync在同步的时候，只同步发生变化的这个文件或者这个目录（每次发生变化的数据相对整个同步目录数据来说是很小的，rsync在遍历查找比对文件时，速度很快），因此，效率很高。\n同步过程： 1. 在同步服务器上开启sersync服务，sersync负责监控配置路径中的文件系统事件变化；\n2. 调用rsync命令把更新的文件同步到目标服务器；\n3. 需要在主服务器配置sersync，在同步目标服务器配置rsync server（注意：是rsync服务）\n同步过程和原理： 1. 用户实时的往sersync服务器上写入更新文件数据；\n2. 此时需要在同步主服务器上配置sersync服务；\n3. 在另一台服务器开启rsync守护进程服务，以同步拉取来自sersync服务器上的数据；\n通过rsync的守护进程服务后可以发现，实际上sersync就是监控本地的数据写入或更新事件；然后，在调用rsync客户端的命令，将写入或更新事件对应的文件通过rsync推送到目标服务器\n小结：当同步的目录数据量不大时，建议使用Rsync+Inotify-tools；当数据量很大（几百G甚至1T以上）、文件很多时，建议使用Rsync+sersync。\n三、配置操作 **一台装rsync服务 **192.168.0.10\n**一台装sersync **192.168.0.20\n对20网站根目录的/backup目录备份到10的/backup\n**Rsync服务器（备份端磁盘大专门用来存储数据 ,目标机器）：**192.168.0.10\n**Sersync服务器（数据源【部署的项目，代码等】,源机器 ）： **192.168.0.20\n（1）、使用rsync备份数据 系统用户 两台服务器都需要安装rsync\n1 yum -y install xinetd rsync 20需要安装sersync\n原理：10 上 使用系统配置文件 vim /etc/rsyncd.conf 来备份数据，创建备份账户，最后把rsync以deamon方式运行\n** vim /etc/rsyncd.conf rsyncd.conf配置文件**\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash uid = root gid = root use chroot = no disable = no fake super = yes max connections = 200 timeout = 300 pid file=/var/run/rsyncd.pid lock file=/var/run/rsync.lock log file = /var/log/rsyncd.log [backup] path = /backup ignore errors read only = false list = false hosts allow = 192.168.0.0/24 host deny = 0.0.0.0/0 auth users = rsync_backup secrets file = /etc/rsync.password [root@backup ~]# useradd -M -s /sbin/nologin rsync #可以使用rsync，我使用的是root [root@backup backup]# mkdir /backup/ 创建备份的目录 创建密码文件\n1 2 3 [root@backup ~]# echo \u0026#39;rsync_backup:123456\u0026#39; \u0026gt;/etc/rsync.password [root@backup ~]# cat /etc/rsync.password rsync_backup:123456 修改权限\n1 2 3 [root@backup ~]# chmod 600 /etc/rsync.password [root@backup ~]# ll /etc/rsync.password -rw-------. 1 root root 20 Jul 24 04:27 /etc/rsync.password 启动rsync服务\n1 [root@backup ~]# rsync -–daemon 加入开机自启动\n1 2 3 4 5 6 7 8 [root@backup ~]# vim /etc/rc.local # rsync server progress /usr/bin/rsync --daemon 1.5.7 第六个里程碑-检查 rsync的端口号为873 [root@backup ~]# ss -tlunp|grep rsync tcp LISTEN 0 5 :::873 :::* users:((\u0026#34;rsync\u0026#34;,2452,5)) tcp LISTEN 0 5 *:873 *:* users:((\u0026#34;rsync\u0026#34;,2452,4)) 重启rsync\n1 2 3 4 5 6 7 8 9 10 11 12 [root@rsync-client-sersync ~]# ps -ef | grep rsync root 1287 1 0 22:52 ? 00:00:00 rsync --daemon root 1294 1248 0 22:52 pts/0 00:00:00 grep --color=auto rsync [root@rsync-client-sersync ~]# kill -9 1287 [root@rsync-client-sersync ~]# ps -ef | grep rsync root 1298 1248 0 22:53 pts/0 00:00:00 grep --color=auto rsync [root@rsync-client-sersync ~]# rsync --daemon [root@rsync-client-sersync ~]# ps -ef | grep rsync root 1300 1 0 22:53 ? 00:00:00 rsync --daemon root 1302 1248 0 22:53 pts/0 00:00:00 grep --color=auto rsync 重启，就是杀掉rsync的进程，重新rsync --daemon 在客户端进行测试（20）\n1 2 3 4 5 6 7 8 [root@rsync-client-sersync ~]# rsync -avz /etc/hosts rsync_backup@192.168.0.10::backup Password: sending incremental file list hosts sent 140 bytes received 43 bytes 33.27 bytes/sec total size is 158 speedup is 0.86 成功的将hosts文件推向服务器端的/backup目录中 客户端添加密码\n1 2 3 4 [root@nfs01 tmp]# echo \u0026#39;123456\u0026#39; \u0026gt;/etc/rsync.password 将密码重定向到/etc/rsync.password文件 [root@nfs01 tmp]# cat /etc/rsync.password 查看 123456 [root@nfs01 tmp]# chmod 600 /etc/rsync.password 给/etc/rsync.password文件600的权限 1 rsync -avz /etc/hosts rsync_backup@172.16.1.41::backup --password-file=/etc/rsync.password 指定密码文件 实例：排除文件\n1 2 3 4 5 [root@backup tmp]# rsync -a –-exclude=/etc/hosts /etc/services 172.16.1.31:/tmp/ –-exclude=/etc/hosts 排除/etc/services中的/etc/hosts文件 --bwlimit=RATE limt socket I/O bandwidth 传输的时候限速 --delete 让源目录和目标目录一模一样（即：我有什么你就有什么，我没什么你就没有什么） 实例：无差异同步 1 2 3 [root@backup tmp]# rsync -a --delete /tmp/ 172.16.1.31:/tmp/ root@172.16.1.31\u0026#39;s password: --delete 本地有什么，远端就有什么 本地没有什么，远端就没有什么 本地有远端没有，就会删除远 （2）、使用sersync实时监控推送 1**、下载sersync**\n在google code下载sersync的可执行文件版本，里面有配置文件与可执行文件，这用\n1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir -p /applition/tools cd /applition/tools wgethttps://sersync.googlecode.com/files/sersync2.5.4_64bit_binary_stable_final.tar.gz 【有时下载失败，所有要本地留存才行】 [root@web ~]# tar fxzsersync2.5.4_64bit_binary_stable_final.tar.gz -C /usr/local/ [root@web ~]# cd /usr/local/ [root@cache local]# mv GNU-Linux-x86 sersync [root@cache local]# treesersync/ sersync/ ├── confxml.xml # 配置文件 └── sersync2 # 二进制文件【启动sersync使用】 0 directories, 2 files 2**、配置sersync**\n1 2 3 4 5 6 [root@cache local]# cp sersync/confxml.xmlsersync/confxml.xml.$(date +%F) [root@cache local]# ll sersync/confxml.xml -rwxr-xr-x. 1 root root 2214Oct 26 2011 sersync/confxml.xml [root@cache local]# llsersync/confxml.xml* -rwxr-xr-x. 1 root root 2214Oct 26 2011 sersync/confxml.xml -rwxr-xr-x. 1 root root 2214Jun 5 06:38sersync/confxml.xml.2015-06-05 更改优化sersync配置文件：\n1 2 3 4 5 \u0026lt;localpathwatch=\u0026#34;/opt/tongbu\u0026#34;\u0026gt; # 定义本地要同步的目录 \u0026lt;remote ip=\u0026#34;127.0.0.1\u0026#34;name=\u0026#34;tongbu1\u0026#34;/\u0026gt; \u0026lt;!--\u0026lt;remoteip=\u0026#34;192.168.8.39\u0026#34; name=\u0026#34;tongbu\u0026#34;/\u0026gt;--\u0026gt; # 同步到哪台机器上 tongbu模块rsync端模块名字 \u0026lt;!--\u0026lt;remoteip=\u0026#34;192.168.8.40\u0026#34; name=\u0026#34;tongbu\u0026#34;/\u0026gt;--\u0026gt; # 同步到哪台机器上 tongbu模块 \u0026lt;/localpath\u0026gt; b**）修改31\u0026ndash;34行，认证部分【rsync密码认证】**\n1 2 3 4 5 6 7 8 9 10 \u0026lt;rsync\u0026gt; \u0026lt;commonParamsparams=\u0026#34;-artuz\u0026#34;/\u0026gt; \u0026lt;auth start=\u0026#34;false\u0026#34;users=\u0026#34;root\u0026#34; passwordfile=\u0026#34;/etc/rsync.pas\u0026#34;/\u0026gt; \u0026lt;userDefinedPortstart=\u0026#34;false\u0026#34; port=\u0026#34;874\u0026#34;/\u0026gt;\u0026lt;!-- port=874 --\u0026gt; \u0026lt;timeoutstart=\u0026#34;false\u0026#34; time=\u0026#34;100\u0026#34;/\u0026gt;\u0026lt;!-- timeout=100 --\u0026gt; \u0026lt;sshstart=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/rsync\u0026gt; # ***修改内容为 rsync的密码文件以及 同步所使用的账号类似： rsync -avzP /data/www/rsync_backup@172.16.1.25::www/ --password-file=/etc/rsync.password 1 2 \u0026lt;failLog path=\u0026#34;/usr/local/sersync/logs/rsync_fail_log.sh\u0026#34;timeToExecute=\u0026#34;60\u0026#34;/\u0026gt;\u0026lt;!--default every 60mins execute once--\u0026gt; # 当同步失败后，日志记录到/usr/local/sersync/logs/rsync_fail_log.sh文件中，并且每60分钟对失败的log进行重新同步 修改后的完整配置文件为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;ISO-8859-1\u0026#34;?\u0026gt; \u0026lt;head version=\u0026#34;2.5\u0026#34;\u0026gt; \u0026lt;host hostip=\u0026#34;localhost\u0026#34; port=\u0026#34;8008\u0026#34;\u0026gt;\u0026lt;/host\u0026gt; \u0026lt;debug start=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;fileSystem xfs=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;filter start=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;exclude expression=\u0026#34;(.*)\\.svn\u0026#34;\u0026gt;\u0026lt;/exclude\u0026gt; \u0026lt;exclude expression=\u0026#34;(.*)\\.gz\u0026#34;\u0026gt;\u0026lt;/exclude\u0026gt; \u0026lt;exclude expression=\u0026#34;^info/*\u0026#34;\u0026gt;\u0026lt;/exclude\u0026gt; \u0026lt;exclude expression=\u0026#34;^static/*\u0026#34;\u0026gt;\u0026lt;/exclude\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;inotify\u0026gt; \u0026lt;delete start=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;createFolder start=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;createFile start=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;closeWrite start=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;moveFrom start=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;moveTo start=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;attrib start=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;modify start=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/inotify\u0026gt; \u0026lt;sersync\u0026gt; \u0026lt;localpath watch=\u0026#34;/etc/openvpn\u0026#34;\u0026gt; \u0026lt;remote ip=\u0026#34;192.168.0.10\u0026#34; name=\u0026#34;openvpn\u0026#34;/\u0026gt; \u0026lt;!--\u0026lt;remote ip=\u0026#34;192.168.8.39\u0026#34; name=\u0026#34;tongbu\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;remote ip=\u0026#34;192.168.8.40\u0026#34; name=\u0026#34;tongbu\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;/localpath\u0026gt; \u0026lt;rsync\u0026gt; \u0026lt;commonParams params=\u0026#34;-artuz\u0026#34;/\u0026gt; \u0026lt;auth start=\u0026#34;true\u0026#34; users=\u0026#34;rsync_backup\u0026#34; passwordfile=\u0026#34;/etc/rsync.password\u0026#34;/\u0026gt; \u0026lt;userDefinedPort start=\u0026#34;false\u0026#34; port=\u0026#34;874\u0026#34;/\u0026gt;\u0026lt;!-- port=874 --\u0026gt; \u0026lt;timeout start=\u0026#34;start\u0026#34; time=\u0026#34;100\u0026#34;/\u0026gt;\u0026lt;!-- timeout=100 --\u0026gt; \u0026lt;ssh start=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/rsync\u0026gt; \u0026lt;failLog path=\u0026#34;/tmp/rsync_fail_log.sh\u0026#34; timeToExecute=\u0026#34;60\u0026#34;/\u0026gt;\u0026lt;!--default every 60mins execute once--\u0026gt; \u0026lt;crontab start=\u0026#34;false\u0026#34; schedule=\u0026#34;600\u0026#34;\u0026gt;\u0026lt;!--600mins--\u0026gt; \u0026lt;crontabfilter start=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;exclude expression=\u0026#34;*.php\u0026#34;\u0026gt;\u0026lt;/exclude\u0026gt; \u0026lt;exclude expression=\u0026#34;info/*\u0026#34;\u0026gt;\u0026lt;/exclude\u0026gt; \u0026lt;/crontabfilter\u0026gt; \u0026lt;/crontab\u0026gt; \u0026lt;plugin start=\u0026#34;false\u0026#34; name=\u0026#34;command\u0026#34;/\u0026gt; \u0026lt;/sersync\u0026gt; \u0026lt;plugin name=\u0026#34;command\u0026#34;\u0026gt; \u0026lt;param prefix=\u0026#34;/bin/sh\u0026#34; suffix=\u0026#34;\u0026#34; ignoreError=\u0026#34;true\u0026#34;/\u0026gt;\t\u0026lt;!--prefix /opt/tongbu/mmm.sh suffix--\u0026gt; \u0026lt;filter start=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;include expression=\u0026#34;(.*)\\.php\u0026#34;/\u0026gt; \u0026lt;include expression=\u0026#34;(.*)\\.sh\u0026#34;/\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin name=\u0026#34;socket\u0026#34;\u0026gt; \u0026lt;localpath watch=\u0026#34;/opt/tongbu\u0026#34;\u0026gt; \u0026lt;deshost ip=\u0026#34;192.168.138.20\u0026#34; port=\u0026#34;8009\u0026#34;/\u0026gt; \u0026lt;/localpath\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin name=\u0026#34;refreshCDN\u0026#34;\u0026gt; \u0026lt;localpath watch=\u0026#34;/data0/htdocs/cms.xoyo.com/site/\u0026#34;\u0026gt; \u0026lt;cdninfo domainname=\u0026#34;ccms.chinacache.com\u0026#34; port=\u0026#34;80\u0026#34; username=\u0026#34;xxxx\u0026#34; passwd=\u0026#34;xxxx\u0026#34;/\u0026gt; \u0026lt;sendurl base=\u0026#34;http://pic.xoyo.com/cms\u0026#34;/\u0026gt; \u0026lt;regexurl regex=\u0026#34;false\u0026#34; match=\u0026#34;cms.xoyo.com/site([/a-zA-Z0-9]*).xoyo.com/images\u0026#34;/\u0026gt; \u0026lt;/localpath\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/head\u0026gt; 3**、开启sersync守护进程同步数据**\n1 2 3 4 5 [root@web ~]# /usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml 配置sersync环境变量 [root@web ~]# echo\u0026#34;PATH=$PATH:/usr/local/sersync/\u0026#34;\u0026gt;\u0026gt;/etc/profile [root@web ~]# source /etc/profile [root@web ~]# sersync2 启动命令后返回结果如下为正常：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 set the system param execute：echo50000000 \u0026gt; /proc/sys/fs/inotify/max_user_watches execute：echo 327679\u0026gt; /proc/sys/fs/inotify/max_queued_events parse the command param option: -d run as a daemon option: -r rsync all the local files to the remoteservers before the sersync work option: -o config xml name： /usr/local/sersync/confxml.xml daemon thread num: 10 parse xml config file host ip : localhost host port: 8008 daemon start，sersync runbehind the console use rsync password-file : user is rsync_backup passwordfile is /etc/rsync.password config xml parse success please set /etc/rsyncd.confmax connections=0 Manually sersync working thread 12 = 1(primary thread) + 1(fail retry thread) + 10(daemon sub threads) Max threads numbers is: 32 = 12(Thread pool nums) +20(Sub threads) please according your cpu ，use -n paramto adjust the cpu rate chmod: cannot access`/usr/local/sersync/logs/rsync_fail_log.sh\u0026#39;: No such file or directory ------------------------------------------ rsync the directory recursivlyto the remote servers once working please wait... execute command: cd /backup\u0026amp;\u0026amp; rsync -artuz -R --delete ./ --timeout=100 rsync_backup@192.168.0.10::www--password-file=/etc/rsync.password \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 run the sersync: watch path is: /data/www 如果你上面的配置和验证都没问题，可以下面的配置了。设置开机自启动\n1 2 3 4 5 #vi /etc/rc.d/rc.local /usr/local/sersync/sersync2 -d -r -o /usr/local/sersync/confxml.xml ＃设置开机自动运行脚本 # chmod +x /etc/rc.d/rc.local 添加脚本监控sersync是否正常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cd /root touch check_sersync.sh chmod 755 check.sersync.sh vim check_sersync.sh #!/bin/sh sersync=\u0026#34;/usr/local/sersync/sersync2\u0026#34; confxml=\u0026#34;/usr/local/sersync/confxml.xml\u0026#34; status=$(ps aux |grep \u0026#39;sersync2\u0026#39;|grep -v \u0026#39;grep\u0026#39;|wc -l) if [ $status -eq 0 ]; then $sersync -d -r -o $confxml \u0026amp; else exit 0; fi check_sersync.sh 1 2 3 #vi /etc/crontab */5 * * * * root /root/check_sersync.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 #每隔5分钟执行一次脚本 注意：\n1、手动执行check_sersync.sh检测sersync是否运行正常。\n2、如果着急测试，不用等重启服务器后执行，先执行　1\n/usr/local/sersync/sersync2\u0026nbsp;-d\u0026nbsp;-r\u0026nbsp;-o\u0026nbsp;\u0026nbsp;/usr/local/sersync/confxml.xml\n3、在源服务器指定目录下对文件文件夹做出更改，查看目标服务器同步目录下是否同步成功。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 1 -v, --verbose 详细模式输出 2 3 -q, --quiet 精简输出模式 4 5 -c, --checksum 打开校验开关，强制对文件传输进行校验 6 7 -a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD 8 9 -r, --recursive 对子目录以递归模式处理 10 11 -R, --relative 使用相对路径信息 12 13 -b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。 14 15 --backup-dir 将备份文件(如~filename)存放在在目录下。 16 17 -suffix=SUFFIX 定义备份文件前缀 18 19 -u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件) 20 21 -l, --links 保留软链结 22 23 -L, --copy-links 想对待常规文件一样处理软链结 24 25 --copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结 26 27 --safe-links 忽略指向SRC路径目录树以外的链结 28 29 -H, --hard-links 保留硬链结 30 31 -p, --perms 保持文件权限 32 33 -o, --owner 保持文件属主信息 34 35 -g, --group 保持文件属组信息 36 37 -D, --devices 保持设备文件信息 38 39 -t, --times 保持文件时间信息 40 41 -S, --sparse 对稀疏文件进行特殊处理以节省DST的空间 42 43 -n, --dry-run现实哪些文件将被传输 44 45 -W, --whole-file 拷贝文件，不进行增量检测 46 47 -x, --one-file-system 不要跨越文件系统边界 48 49 -B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节 50 51 -e, --rsh=COMMAND 指定使用rsh、ssh方式进行数据同步 52 53 --rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息 54 55 -C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件 56 57 --existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件 58 59 --delete 删除那些DST中SRC没有的文件 60 61 --delete-excluded 同样删除接收端那些被该选项指定排除的文件 62 63 --delete-after 传输结束以后再删除 64 65 --ignore-errors 及时出现IO错误也进行删除 66 67 --max-delete=NUM 最多删除NUM个文件 68 69 --partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输 70 71 --force 强制删除目录，即使不为空 72 73 --numeric-ids 不将数字的用户和组ID匹配为用户名和组名 74 75 --timeout=TIME IP超时时间，单位为秒 76 77 -I, --ignore-times 不跳过那些有同样的时间和长度的文件 78 79 --size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间 80 81 --modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0 82 83 -T --temp-dir=DIR 在DIR中创建临时文件 84 85 --compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份 86 87 -P 等同于 --partial 88 89 --progress 显示备份过程 90 91 -z, --compress 对备份的文件在传输时进行压缩处理 92 93 --exclude=PATTERN 指定排除不需要传输的文件模式 94 95 --include=PATTERN 指定不排除而需要传输的文件模式 96 97 --exclude-from=FILE 排除FILE中指定模式的文件 98 99 --include-from=FILE 不排除FILE指定模式匹配的文件 100 101 --version 打印版本信息 102 103 --address 绑定到特定的地址 104 105 --config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件 106 107 --port=PORT 指定其他的rsync服务端口 108 109 --blocking-io 对远程shell使用阻塞IO 110 111 -stats 给出某些文件的传输状态 112 113 --progress 在传输时现实传输过程 114 115 --log-format=formAT 指定日志文件格式 116 117 --password-file=FILE 从FILE中得到密码 118 119 --bwlimit=KBPS 限制I/O带宽，KBytes per second 120 121 -h, --help 显示帮助信息 rsync详细参数 参考地址：\nhttps://blog.51cto.com/liubao0312/1677586\nhttps://blog.csdn.net/ljx1528/article/details/105348259\nhttps://www.cnblogs.com/lei0213/p/8598072.html\n","date":"2021-01-29T23:09:59Z","image":"https://www.ownit.top/title_pic/51.jpg","permalink":"https://www.ownit.top/p/202101292309/","title":"sersync+rsync原理及部署"},{"content":"Linux系统下可通过history命令查看用户所有的历史操作记录，在安全应急响应中起着非常重要的作用，但在未进行附加配置情况下，history命令只能查看用户历史操作记录，并不能区分用户以及操作时间，不便于审计分析。\n当然，一些不好的操作习惯也可能通过命令历史泄露敏感信息。下面我们来介绍如何让history日志记录更细化，更便于我们审计分析。\nHistory 通过设置export HISTTIMEFORMAT=\u0026rsquo;%F %T \u0026lsquo;，让历史记录中带上命令执行时间。\n注意”%T”和后面的”’”之间有空格，不然查看历史记录的时候，时间和命令之间没有分割。\n要一劳永逸，这个配置可以写在/etc/profile中，当然如果要对指定用户做配置，这个配置可以写在/home/$USER/.bash_profile中。\n本文将以/etc/profile为例进行演示。\n1 2 3 4 5 6 vim /etc/profile export HISTTIMEFORMAT=\u0026#39;%F %T \u0026#39; source /etc/profile 要使配置立即生效请执行source /etc/profile，我们再查看history记录，可以看到记录中带上了命令执行时间。 如果想要实现更细化的记录，比如登陆过系统的用户、IP地址、操作命令以及操作时间一一对应，可以通过在/etc/profile里面加入以下代码实现\n1 2 3 4 USER_IP=`who -u am i 2\u0026gt;/dev/null| awk \u0026#39;{print $NF}\u0026#39;|sed -e \u0026#39;s/[()]//g\u0026#39;` export HISTTIMEFORMAT=\u0026#34;[%F %T][`whoami`][${USER_IP}] \u0026#34; 修改/etc/profile并加载后，history记录如下，时间、IP、用户及执行的命令都一一对应。 通过以上配置，我们基本上可以满足日常的审计工作了，但了解系统的朋友应该很容易看出来，这种方法只是设置了环境变量，攻击者unset掉这个环境变量，或者直接删除命令历史，对于安全应急来说，这无疑是一个灾难。\n针对这样的问题，我们应该如何应对，通过修改bash源码，让history记录通过syslog发送到远程logserver中，大大增加了攻击者对history记录完整性破坏的难度。\nhistory高级用法\n上面是记录History的方法。我们也可以通过写入文件，更加方便的记录和审计命令\n1 2 3 4 5 #histroy HISTSIZE=409600 export HISTTIMEFORMAT=\u0026#34;[%F %T]\u0026#34; export HISTORY_FILE=/var/log/.audit.log export PROMPT_COMMAND=\u0026#39;{ thisHistID=`history 1|awk \u0026#34;{print \\\\$1}\u0026#34;`;lastCommand=`history 1| awk \u0026#34;{\\\\$1=\\\u0026#34;\\\u0026#34; ;print}\u0026#34;`;user=`id -un`;whoStr=(`who -u am i`);realUser=${whoStr[0]};logMonth=${whoStr[2]};logDay=${whoStr[3]};logTime=${whoStr[4]};pid=${whoStr[6]};ip=${whoStr[7]};if [ ${thisHistID}x != ${lastHistID}x ];then echo -E `date \u0026#34;+%Y/%m/%d %H:%M:%S\u0026#34;` $user\\($realUser\\)@$ip[PID:$pid][LOGIN:$logMonth $logDay $logTime] --- $lastCommand ;lastHistID=$thisHistID;fi; } \u0026gt;\u0026gt; $HISTORY_FILE\u0026#39; 这个命令会在生成**/var/log/.audit.log 文件 **这里面记录的是命令\n这样更为直观的分析和审计\n","date":"2021-01-22T10:31:21Z","image":"https://www.ownit.top/title_pic/19.jpg","permalink":"https://www.ownit.top/p/202101221031/","title":"谁动了我的主机?（ History）"},{"content":"目录\n1. 安装docker与docker-compose\n2. 环境条件\n3. 创建主从配置文件\n4. 创建docker-compose.yml文件\n5. docker-compose启动mysql容器\n6. 销毁两个容器\n7. 重建MySQL容器\n经常拉去数据库做测试，还需要主从，每次环境还要还原，经常重复比较麻烦。\n现在采用docker和docker-compose一键构建集成环境，方便测试。\n1. 安装docker与docker-compose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 卸载老版本docker [root@docker ~]# yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine [root@docker ~]# yum -y install epel-release wget [root@docker ~]# wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo [root@docker ~]# sed -i \u0026#39;s#download.docker.com#mirrors.tuna.tsinghua.edu.cn/docker-ce#g\u0026#39; /etc/yum.repos.d/docker-ce.repo [root@docker ~]# yum -y install docker-ce # 启动并配置镜像加速 [root@docker ~]# systemctl start docker.service \u0026amp;\u0026amp; systemctl enable docker.service [root@docker ~]# cat /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://99dxqyb6.mirror.aliyuncs.com\u0026#34;] } [root@docker ~]# systemctl restart docker.service 安装docker-compose 1 2 3 [root@docker ~]# curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.25.3/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose [root@docker ~]# chmod +x /usr/local/bin/docker-compose [root@docker ~]# ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 2. 环境条件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 [root@docker ~]# docker pull mysql:5.7 [root@Master new_date]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/mysql 5.7 cc8775c0fe94 6 days ago 449 MB [root@docker ~]# mkdir -p /data/mysql/master/{conf,data} [root@docker ~]# mkdir -p /data/mysql/slave/conf [root@docker ~]# mkdir -p /data/mysql/{init-db-m,init-db-s} [root@docker ~]# chown -R mysql.mysql /data/mysql # 主库的创建用户SQL脚本 [root@docker ~]# cat /data/mysql/init-db-m/create_user_1.sql grant all on *.* to \u0026#39;dumpuser\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; grant replication slave on *.* to \u0026#39;repl\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; # 从库远程备份主库及创建主从通道Bash脚本 [root@docker ~]# cat /data/mysql/init-db-s/dump-repl_1.sh #!/bin/bash while ! mysql -uping -p123456 -hmysql_m_compose -P3306 -e \u0026#34;select 1\u0026#34; do sleep 1 done sleep 3 mysqldump -udumpuser -p123456 -hmysql_m_compose -P3306 --single-transaction --default-character-set=utf8mb4 --set-gtid-purged=on --master-data=2 --flush-logs --hex-blob --triggers --routines --events --all-databases \u0026gt; /tmp/full.sql mysql -uroot -p123456 -e \u0026#34;reset master;\u0026#34; mysql -uroot -p123456 -e \u0026#34;source /tmp/full.sql;\u0026#34; mysql -uroot -p123456 -e \u0026#34;CHANGE MASTER TO MASTER_HOST=\u0026#39;mysql_m_compose\u0026#39;,MASTER_USER=\u0026#39;repl\u0026#39;,MASTER_PASSWORD=\u0026#39;123456\u0026#39;,MASTER_PORT=3306,MASTER_CONNECT_RETRY=10,MASTER_AUTO_POSITION=1;\u0026#34; mysql -uroot -p123456 -e \u0026#34;start slave;\u0026#34; 3. 创建主从配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # mysql_m_compose节点 [root@docker ~]# cat /data/mysql/master/conf/my.cnf [mysqld] server_id = 33060000 port = 3306 log_timestamps=SYSTEM max_allowed_packet = 16M read_only = 0 character_set_server = utf8mb4 secure_file_priv = \u0026#34;\u0026#34; max_connect_errors = 100000 interactive_timeout = 1800 wait_timeout = 1800 # BINLOG log_bin = mysql-bin binlog_format = row log_slave_updates = 1 max_binlog_size = 200M relay_log = relay-bin sync_binlog = 1 # GTID gtid_mode = ON enforce_gtid_consistency = 1 binlog_gtid_simple_recovery = 1 # ENGINE default_storage_engine = InnoDB innodb_flush_log_at_trx_commit=1 # ---------------------------------------------------------------------------------------------------------------------- # mysql_s_compose节点 [root@docker ~]# cat /data/mysql/slave/conf/my.cnf [mysqld] server_id = 33060001 port = 3306 log_timestamps=SYSTEM max_allowed_packet = 16M read_only = 0 character_set_server = utf8mb4 secure_file_priv = \u0026#34;\u0026#34; max_connect_errors = 100000 interactive_timeout = 1800 wait_timeout = 1800 # BINLOG log_bin = mysql-bin binlog_format = row log_slave_updates = 1 max_binlog_size = 200M relay_log = relay-bin sync_binlog = 1 # GTID gtid_mode = ON enforce_gtid_consistency = 1 binlog_gtid_simple_recovery = 1 # ENGINE default_storage_engine = InnoDB innodb_flush_log_at_trx_commit=1 4. 创建docker-compose.yml文件 将init_sql下的文件映射到/docker-entrypoint-initdb.d目录下(注：/docker-entrypoint-initdb.d下以sql或sh结尾的文件会在数据库初始化完成后自动执行) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [root@docker ~]# cat /data/mysql/docker-compose.yml version: \u0026#39;3\u0026#39; services: mysql_m_compose: image: mysql:5.7.30 container_name: mysql_m restart: always ports: - 33061:3306 environment: - MYSQL_USER=ping - MYSQL_PASSWORD=123456 - MYSQL_ROOT_PASSWORD=123456 volumes: - ./master/conf/my.cnf:/etc/my.cnf - ./master/data:/var/lib/mysql - ./init-db-m:/docker-entrypoint-initdb.d mysql_s_compose: image: mysql:5.7.30 container_name: mysql_s restart: always ports: - 33062:3306 depends_on: - mysql_m_compose environment: - MYSQL_ROOT_PASSWORD=123456 volumes: - ./slave/conf/my.cnf:/etc/my.cnf - ./init-db-s:/docker-entrypoint-initdb.d 5. docker-compose启动mysql容器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 [root@docker mysql]# docker-compose up -d Creating network \u0026#34;mysql_default\u0026#34; with the default driver Creating mysql_m ... done Creating mysql_s ... done [root@docker ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 35cc9c6fbf52 mysql:5.7.30 \u0026#34;docker-entrypoint.s…\u0026#34; 6 minutes ago Up 6 minutes 33060/tcp, 0.0.0.0:33062-\u0026gt;3306/tcp mysql_s 53a04e176ecc mysql:5.7.30 \u0026#34;docker-entrypoint.s…\u0026#34; 6 minutes ago Up 6 minutes 33060/tcp, 0.0.0.0:33061-\u0026gt;3306/tcp mysql_m # 检查主从复制关系 # 主库 mysql\u0026gt; CREATE DATABASE`db` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; mysql_m\u0026gt; use db Database changed mysql_m\u0026gt; create table t1(id int,name varchar(32)); Query OK, 0 rows affected (0.01 sec) mysql_m\u0026gt; insert into t1 values(1,\u0026#39;aa\u0026#39;),(2,\u0026#39;bb\u0026#39;),(3,\u0026#39;cc\u0026#39;); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql_m\u0026gt; select * from db.t1; +------+------+ | id | name | +------+------+ | 1 | aa | | 2 | bb | | 3 | cc | +------+------+ 3 rows in set (0.00 sec) # 从库 mysql_s\u0026gt; select * from db.t1; +------+------+ | id | name | +------+------+ | 1 | aa | | 2 | bb | | 3 | cc | +------+------+ 3 rows in set (0.00 sec) # 查看主从复制状态信息 mysql\u0026gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: mysql_m_compose Master_User: repl Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysql-bin.000004 Read_Master_Log_Pos: 845 Relay_Log_File: relay-bin.000004 Relay_Log_Pos: 1018 Relay_Master_Log_File: mysql-bin.000004 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 845 Relay_Log_Space: 1219 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 33060000 Master_UUID: f28ef661-597b-11eb-beb8-0242ac120002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: f28ef661-597b-11eb-beb8-0242ac120002:10-12 Executed_Gtid_Set: f28ef661-597b-11eb-beb8-0242ac120002:1-12 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 6. 销毁两个容器 1 2 3 4 5 6 7 8 9 [root@Master mysql]# ls docker-compose.yml init-db-m init-db-s master slave [root@Master mysql]# docker-compose down Stopping mysql_s ... done Stopping mysql_m ... Stopping mysql_m ... done Removing mysql_s ... done Removing mysql_m ... done Removing network mysql_default 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@Master mysql]# ll -sh /data/mysql/master/data/ 总用量 188M 4.0K -rw-r----- 1 polkitd ssh_keys 56 1月 18 18:57 auto.cnf 4.0K -rw------- 1 polkitd ssh_keys 1.7K 1月 18 18:57 ca-key.pem 4.0K -rw-r--r-- 1 polkitd ssh_keys 1.1K 1月 18 18:57 ca.pem 4.0K -rw-r--r-- 1 polkitd ssh_keys 1.1K 1月 18 18:57 client-cert.pem 4.0K -rw------- 1 polkitd ssh_keys 1.7K 1月 18 18:57 client-key.pem 0 drwxr-x--- 2 polkitd ssh_keys 48 1月 18 19:09 db 4.0K -rw-r----- 1 polkitd ssh_keys 1.4K 1月 18 18:57 ib_buffer_pool 76M -rw-r----- 1 polkitd ssh_keys 76M 1月 18 19:34 ibdata1 48M -rw-r----- 1 polkitd ssh_keys 48M 1月 18 19:34 ib_logfile0 48M -rw-r----- 1 polkitd ssh_keys 48M 1月 18 18:57 ib_logfile1 12M -rw-r----- 1 polkitd ssh_keys 12M 1月 18 19:34 ibtmp1 4.0K drwxr-x--- 2 polkitd ssh_keys 4.0K 1月 18 18:57 mysql 4.0K -rw-r----- 1 polkitd ssh_keys 177 1月 18 18:57 mysql-bin.000001 3.0M -rw-r----- 1 polkitd ssh_keys 3.0M 1月 18 18:57 mysql-bin.000002 4.0K -rw-r----- 1 polkitd ssh_keys 241 1月 18 18:57 mysql-bin.000003 4.0K -rw-r----- 1 polkitd ssh_keys 845 1月 18 19:34 mysql-bin.000004 4.0K -rw-r----- 1 polkitd ssh_keys 241 1月 18 19:34 mysql-bin.000005 4.0K -rw-r----- 1 polkitd ssh_keys 194 1月 18 19:34 mysql-bin.000006 4.0K -rw-r----- 1 polkitd ssh_keys 114 1月 18 19:34 mysql-bin.index 12K drwxr-x--- 2 polkitd ssh_keys 8.0K 1月 18 18:57 performance_schema 4.0K -rw------- 1 polkitd ssh_keys 1.7K 1月 18 18:57 private_key.pem 4.0K -rw-r--r-- 1 polkitd ssh_keys 452 1月 18 18:57 public_key.pem 4.0K -rw-r--r-- 1 polkitd ssh_keys 1.1K 1月 18 18:57 server-cert.pem 4.0K -rw------- 1 polkitd ssh_keys 1.7K 1月 18 18:57 server-key.pem 12K drwxr-x--- 2 polkitd ssh_keys 8.0K 1月 18 18:57 sys 7. 重建MySQL容器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 [root@docker mysql]# docker-compose up -d Creating network \u0026#34;mysql_default\u0026#34; with the default driver Creating mysql_m ... done Creating mysql_s ... done [root@docker mysql]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7cd912fa9721 mysql:5.7.30 \u0026#34;docker-entrypoint.s…\u0026#34; 8 seconds ago Up 8 seconds 33060/tcp, 0.0.0.0:33062-\u0026gt;3306/tcp mysql_s d9e3514a1e19 mysql:5.7.30 \u0026#34;docker-entrypoint.s…\u0026#34; 9 seconds ago Up 8 seconds 33060/tcp, 0.0.0.0:33061-\u0026gt;3306/tcp mysql_m # 主库 mysql_m\u0026gt; select * from db.t1; +------+------+ | id | name | +------+------+ | 1 | aa | | 2 | bb | | 3 | cc | +------+------+ 3 rows in set (0.00 sec) # 从库 mysql_s\u0026gt; select * from db.t1; +------+------+ | id | name | +------+------+ | 1 | aa | | 2 | bb | | 3 | cc | +------+------+ 3 rows in set (0.01 sec) mysql_s\u0026gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: mysql_m_compose Master_User: repl Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysql-bin.000006 Read_Master_Log_Pos: 194 Relay_Log_File: relay-bin.000004 Relay_Log_Pos: 367 Relay_Master_Log_File: mysql-bin.000006 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 194 Relay_Log_Space: 568 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 33060000 Master_UUID: f93b936b-9b6b-11ea-9bda-0242c0a82002 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: f93b936b-9b6b-11ea-9bda-0242c0a82002:1-12 Auto_Position: 1 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) ","date":"2021-01-18T19:52:48Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/202101181952/","title":"MySQL容器部署及数据持久化（主从复制）"},{"content":"当MySQL主从数据不一致，怎么解决？？？ 上面是采用mysqldbcompare工具，对比数据库的信息是否一致。\npercona-toolkit percona-toolkit官网：https://www.percona.com/doc/percona-toolkit/LATEST/installation.html\n我们可以使用percona-toolkit工具做校验，而该工具包含\n1. pt-table-checksum 负责检测MySQL主从数据一致性\n2. pt-table-sync负责挡住从数据不一致时修复数据，让他们保存数据的一致性\n3. pt-heartbeat 负责监控MySQL主从同步延迟\n一致性校验思路：\n1、确定校验的主库，根据需要校验的表，分段获取数据\n2、与从库进行校验（根据id）\n3、校验的过程发现数据不一致的时候\n4、在主库创建一个表 记录校验不一致的数据\n5、恢复只需要读取这个表\n工具下载：https://downloads.percona.com/downloads/percona-toolkit/percona-toolkit-3.3.0/binary/redhat/7/x86_64/percona-toolkit-3.3.0-1.el7.x86_64.rpm\n安装percona-toolkit 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 非docker容器安装 yum install perl-IO-Socket-SSL perl-DBD-MySQL perl-Time-HiRes perl perl-DBI -y wget https://downloads.percona.com/downloads/percona-toolkit/percona-toolkit-3.3.0/binary/redhat/7/x86_64/percona-toolkit-3.3.0-1.el7.x86_64.rpm yum install percona-toolkit-3.3.0-1.el7.x86_64.rpm yum list | grep percona-toolkit -y [root@Master new_date]# yum list | grep percona percona-toolkit.x86_64 3.3.0-1.el7 installed docker容器中安装 apt-get update apt-get install percona-toolkit pt-table-checksum --help pt-table-checksum使用 pt-table-checksum [options] [dsn] pt-table-checksum：在主（master）上通过执行校验的查询对复制的一致性进行检查，对比主从的校验值，从而产生结果。DSN指向的是主的地址，该工具的退出状态不为零，如果发现有任何差别，或者如果出现任何警告或错误，更多信息请查看官方资料。 1、准备实战模拟一下数据的不一致，首先在主库中创建一个数据库，创建数据表，然后添加一些数据 环境已经准备好\n有需要的可以参考\n这边我才用docker来模拟，环境很快构建完成来演示。\n下面是链接，可以参考\nDocker安装MySQL集群【读写分离】 2、主库中进行检测数据是否一致 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 create database mytest; use mytest; create table user( id int auto_increment not null primary key, name varchar(20) )engine=InnoDB charset=utf8; show tables; insert into user values (1,\u0026#39;aa\u0026#39;); insert into user values (2,\u0026#39;bb\u0026#39;); insert into user values (3,\u0026#39;cc\u0026#39;); select * from user; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 注意常用的参数解释： --nocheck-replication-filters ：不检查复制过滤器，建议启用。后面可以用--databases来指定需要检查的数据库。 --no-check-binlog-format : 不检查复制的binlog模式，要是binlog模式是ROW，则会报错。 --replicate-check-only :只显示不同步的信息。 --replicate= ：把checksum的信息写入到指定表中，建议直接写到被检查的数据库当中。 --databases= ：指定需要被检查的数据库，多个则用逗号隔开。 --tables= ：指定需要被检查的表，多个用逗号隔开 --host | h= ：Master的地址 --user | u= ：用户名 --passwork | p=：密码 --Port | P= ：端口 检测 root@71399784f284:/# pt-table-checksum --nocheck-replication-filters --replicate=check_data.checksums --databases=mytest --tables=user --user=root --password=123456 Checking if all tables can be checksummed ... Starting checksum ... Replica 00847056d2fa has binlog_format ROW which could cause pt-table-checksum to break replication. Please read \u0026#34;Replicas using row-based replication\u0026#34; in the LIMITATIONS section of the tool\u0026#39;s documentation. If you understand the risks, specify --no-check-binlog-format to disable this check. 1)上面的错误信息主要是因为，检测主库与从库的binlog日志的模式 - 通常来说可以不用改binlog添加 --no-check-binlog-format 跳过检测,但是可能也会出现如下的问题 root@71399784f284:/# pt-table-checksum --nocheck-replication-filters --replicate=check_data.checksums --databases=mytest --no-check-binlog-format --tables=user --user=root --password=123456 Diffs cannot be detected because no slaves were found. Please read the —recursion-method documentation for information. 2)问题原因是没有找到从库的地址，MySQL在做主从的时候可能会因为环境配置等因素，让pt-table-checksum没有很好地找到从库的地址 检测的方式： 1. 是否是指定在主库运行进行校验 2. 就是配置--recursion-method参数，然后在从库中指定好对应的地址 正确情况下： root@71399784f284:/# pt-table-checksum --nocheck-replication-filters --replicate=check_data.checksums --databases=mytest --no-check-binlog-format --tables=user --user=root --password=123456 Checking if all tables can be checksummed ... Starting checksum ... TS ERRORS DIFFS ROWS DIFF_ROWS CHUNKS SKIPPED TIME TABLE 05-14T09:48:45 0 0 3 0 1 0 0.030 mytest.user 补充（pt-mysql-summary） pt-summary #显示和系统相关的基本信息：\n1 [root@master ~]# pt-summary pt-mysql-summary #查看mysql的各个统计信息：\n1 pt-mysql-summary --host=192.168.0.10 --port=3307 --user=root --password=root --all-databases pt-slave-find #查找和显示指定的Master 有多少个Slave：\n1 2 3 4 5 6 7 8 9 10 11 12 [root@Master ~]# pt-slave-find --host=192.168.0.10 --port=3307 --user=root --password=root 192.168.0.10:3307 Version 5.7.32-log Server ID 1 Uptime 36:46 (started 2021-01-18T11:10:35) Replication Is not a slave, has 1 slaves connected, is not read_only Filters binlog_do_db=CMS,CRM Binary logging ROW Slave status Slave mode STRICT Auto-increment increment 1, offset 1 InnoDB version 5.7.32 pt-table-sync工具恢复数据 手册地址：https://www.percona.com/doc/percona-toolkit/LATEST/pt-table-sync.html\n1、在从数据库中人为添加两条数据，从而让主从数据不一致\n2、主库中进行检测数据一致性问题\n3、主库打印恢复数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pt-table-sync --replicate=rep_test.checksums h=172.17.0.2,u=root,p=123456 h=172.17.0.3,u=slave_check,p=123456 --print pt-table-sync --replicate=rep_test.checksums h=172.17.0.2,u=root,p=123456 h=172.17.0.3,u=slave_check,p=123456 --execute 或者 pt-table-sync --sync-to-master h=172.17.0.3,u=slave_check,p=123456,P=3306 --databases=mytest --print pt-table-sync --sync-to-master h=172.17.0.3,u=slave_check,p=123456,P=3306 --databases=mytest --execute --replicate= ：指定通过pt-table-checksum得到的表，这2个工具差不多都会一直用。 --databases= : 指定执行同步的数据库，多个用逗号隔开。 --tables= ：指定执行同步的表，多个用逗号隔开。 --sync-to-master ：指定一个DSN，即从的IP，他会通过show processlist或show slave status 去自动的找主。 h=127.0.0.1 ：服务器地址，命令里有2个ip，第一次出现的是Master的地址，第2次是Slave的地址。 u=root ：帐号。 p=123456 ：密码。 --print ：打印，但不执行命令。 --execute ：执行命令。 4、主库执行数据恢复 ，从库中查看数据发现已经恢复。\n上面操作是手动执行的数据检测与数据恢复，但实际工作中是不可能每天手动这样进行操作的，那么怎么做呢？\n接下来我们可以写一个shell脚本来定时执行检测与恢复操作，这样就可以省掉不少麻烦。 1 2 3 4 5 6 7 8 9 10 11 12 13 vi /home/pt-check-sync.sh #!/bin/bash NUM=`pt-table-checksum --tables=user --databases=mytest --user=root --password=\u0026#39;123456\u0026#39; --replicate=check_data.checksums --no-check-binlog-format --recursion-method dsn=t=mytest.dsns,h=172.17.0.3,u=slave_check,p=123456,P=3306 | awk \u0026#39;NR\u0026gt;1{sum+=$3}END{print sum}\u0026#39;` if [ $NUM -eq 0 ] ; then echo \u0026#34;Data is ok!\u0026#34; else echo \u0026#34;Data is error!\u0026#34; pt-table-sync --sync-to-master h=172.17.0.3,u=slave_check,p=123456,P=3306 --databases=mytest --print pt-table-sync --sync-to-master h=172.17.0.3,u=slave_check,p=123456,P=3306 --databases=mytest --execute fi 执行sh pt-check-sync.sh文件，也可以写定时器执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apt-get update apt-get install -y --no-install-recommends cron chmod +x ./docker-entrypoint.sh # 保存环境变量，开启crontab服务 env \u0026gt;\u0026gt; /etc/default/locale /etc/init.d/cron start crontab -e 20 23 * * * /home/pt-check-sync.sh 表示每天晚上23:20运行这个脚本 ","date":"2021-01-18T12:33:38Z","image":"https://www.ownit.top/title_pic/19.jpg","permalink":"https://www.ownit.top/p/202101181233/","title":"当MySQL主从数据不一致，怎么解决？？？（2）"},{"content":"当MySQL主从数据不一致，怎么解决？？？ 在使用mysql replication时，有时候会担心，如果主库和备库的数据不一致，怎么办？以前是重新停掉从库，重新做主从，但是耗费时间太多\n最近找到了一个mysql的工具，mysqldbcompare，可以实现对多库数据的对比。\n下面MySQL主从原理 一、主从复制\nMySQL数据库复制操作大致可以分成三个步骤：\n1. 主服务器将数据的改变记录到二进制日志（binary log）中。\n2. 从服务器将主服务器的binary log events 复制到它的中继日志（relay log）中。\n3. 从服务器重做中继日志中的事件，将数据的改变与从服务器保持同步。\n首先，主服务器会记录二进制日志，每个事务更新数据完成之前，主服务器将这些操作的信息记录在二进制日志里面在事件写入二进制日志完成后主服务器通知 存储引擎提交事务。\n准备： 了解binlog日志，MySQL用户-权限 mysql服务器配置复制不难，但是因为场景不同可能会存在一定的差异化，总的来说分为一下几步：\n1. 在服务器上创建复制账号。\n2. 通知备库连接到主库并从主库复制数据。\n二、主从一致性问题校验\n在理想情况下，备库和主库的数据应该是完全一样的。但事实上备库可能发生错误并导致数据不一致。即使没有明显的错误，备库同样可能因为MySQL自身的特性导致数据不一致，例如MySQL的Bug感、网络中断、服务器崩溃，非正常关闭或者其他一些错误。 按照我们的经验来看，主备一致应该是一种规范，而不是例外，也就是说，检查你的主备库一致性应该是一个日常工作，特别是当使用备库来做备份时尤为重要，因为肯定不希望从一个已经损坏的备库里获得备份数据。\n产生原因：\n1、网络中断\n2、服务器产生了问题\n3、mysql自带bug\n4、从库进行了非正当的操作（比如从库进行了添加，删除，修改，主库就会断开）\n环境构建 这边我才用docker来模拟，环境很快构建完成来演示。\n下面是链接，可以参考\nDocker安装MySQL集群【读写分离】 MySQL：5.7\n环境以及构建完成，下面开始试验吧。\n安装工具： 1 2 3 4 5 [root@db03 ~]# wget https://downloads.mysql.com/archives/get/p/30/file/mysql-utilities-1.6.5-1.el7.noarch.rpm [root@db03 ~]# wget https://downloads.mysql.com/archives/get/p/29/file/mysql-connector-python-2.1.7-1.el7.x86_64.rpm [root@db03 ~]# yum -y localinstall mysql-connector-python-2.1.7-1.el7.x86_64.rpm [root@db03 ~]# yum -y localinstall mysql-utilities-1.6.5-1.el7.noarch.rpm 检查两个实例的数据：\n可以看到mster和salver都有CMS库，可以同步数据。\n现在数据都是一致的。现在对比看一下。\nmysqldbcompare的语法如下：\n1 $ mysqldbcompare --server1=user:pass@host:port:socket --server2=user:pass@host:port:socket db1:db2 以上参数中：\n--server1：MySQL服务器1配置。 --server2：MySQL服务器2配置。如果是同一服务器，--server2可以省略。 db1:db2：要比较的两个数据库。如果比较不同服务器上的同名数据库，可以省略:db2。 --all：比较所有两服务器上所有的同名数据库。--exclude排除无需比较的数据库。 --run-all-tests：运行完整比较，遇到第一次差异时不停止。 --changes-for=：修改对象。例如--changes-for=server2，那么对比以sever1为主，生成的差异的修改也是针对server2的对象的修改。 -d DIFFTYPE,--difftype=DIFFTYPE：差异的信息显示的方式，有[unified|context|differ|sql]，默认是unified。如果使用sql，那么就直接生成差异的SQL，这样非常方便。 --show-reverse：在生成的差异修改里面，同时会包含server2和server1的修改。 --skip-table-options：保持表的选项不变，即对比的差异里面不包括表名、AUTO_INCREMENT、ENGINE、CHARSET等差异。 --skip-diff：跳过对象定义比较检查。所谓对象定义，就是CREATE语句()里面的部分，--skip-table-options是()外面的部分。 --skip-object-compare：默认情况下，先检查两个数据库中相互缺失的对象，再对都存在对象间的差异。这个参数的作用就是，跳过第一步，不检查相互缺失的对象。 --skip-checksum-table：数据一致性验证时跳过CHECKSUM TABLE。 --skip-data-check：跳过数据一致性验证。 --skip-row-count：跳过字段数量检查。 1 mysqldbcompare --server1=root:root@127.0.0.1:3307 --server2=root:root@127.0.0.1:3316 --changes-for=server2 --difftype=sql --run-all-tests --all \u0026gt; mysqldbcompare_diff_2.sql 检查没有错误\n停掉主从同步 从库执行\n1 stop slave; 修改主库数据 看这边已经对比出修改的字段了\n1 UPDATE `CMS`.`CI_CHANNEL` SET `SHORT_NAME` = \u0026#39;welcome heian\u0026#39;, `UPDATE_TIME` = \u0026#39;2021-01-14 15:36:59\u0026#39; WHERE `ID` = \u0026#39;1\u0026#39; 我已经在从库执行这条语句\n删除大量的数据做对比\n数据就可以恢复了\n","date":"2021-01-17T23:46:12Z","image":"https://www.ownit.top/title_pic/53.jpg","permalink":"https://www.ownit.top/p/202101172346/","title":"当MySQL主从数据不一致，怎么解决？？？"},{"content":"目录\n1、项目迁移到k8s平台的怎样的流程\n2、Kubernetes基本概念\n3、构建项目镜像\n4、部署项目镜像到Kubernetes平台\n1、项目迁移到k8s平台的怎样的流程 2、Kubernetes基本概念  Pod • 最小部署单元 • 一组容器的集合 • 一个Pod中的容器共享网络命名空间 • Pod是短暂的\n Controllers • Deployment ： 无状态应用部署 • StatefulSet ： 有状态应用部署 • DaemonSet ： 确保所有Node运行同一个Pod • Job ： 一次性任务 • Cronjob ： 定时任务\n更高级层次对象，部署和管理Pod\n Service • 防止Pod失联 • 定义一组Pod的访问策略\n Label ： 标签，附加到某个资源上，用于关联对象、查询和筛选\n Namespaces ： 命名空间，将对象逻辑上隔离\n3、构建项目镜像 1、准备Jar包 2、制作镜像 解压文件\n1 yum install -y unzip \u0026amp;\u0026amp; unzip tomcat-java-demo-master.zip 把这个sql文件导入mysql。\n在Node1上下载mysql:5.6的镜像。\n1 2 3 4 5 6 docker run -p 3306:3306 --name mysql-master \\ -v /mydata/mysql/master/log:/var/log/mysql \\ -v /mydata/mysql/master/data:/var/lib/mysql \\ -v /mydata/mysql/master/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=root \\ -d mysql:5.6 接下导入sql数据。\n可以使用工具导入方便。\n修改代码里的配置文件\n安装JDk和Maven环境\n1 yum install -y java-1.8.0-openjdk maven 修改maven源\n1 2 3 4 5 6 7 8 9 vim /etc/maven/settings.xml \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; 编译源码 【漫长的等待】\n1 mvn clean package -D maven.test.skip=true 准备使用DockerFile文件来构建镜像。\n1 2 3 4 FROM lizhenliang/tomcat LABEL maintainer www.ctnrs.com RUN rm -rf /usr/local/tomcat/webapps/* ADD target/*.war /usr/local/tomcat/webapps/ROOT.war 构建镜像\n1 docker build -t lizhenliang/java-demo -f Dockerfile . 已经成功\n写ymal文件【生成模板，在修改】\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 kubectl create deployment java-demo --image=lizhenliang/java-demo --dry-run -o yaml #内容 apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: java-demo name: java-demo spec: replicas: 1 selector: matchLabels: app: java-demo strategy: {} template: metadata: creationTimestamp: null labels: app: java-demo spec: containers: - image: lizhenliang/java-demo name: java-demo resources: {} status: {} 重定向，生成本地的yaml\n1 kubectl create deployment java-demo --image=lizhenliang/java-demo --dry-run -o yaml \u0026gt; deploy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@master ~]# cat deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: java-demo name: java-demo spec: replicas: 2 selector: matchLabels: app: java-demo template: metadata: creationTimestamp: null labels: app: java-demo spec: containers: - image: lizhenliang/java-demo name: java-demo 运行\n1 kubectl apply -f deploy.yaml 1 kubectl get pods 4、部署项目镜像到Kubernetes平台 ","date":"2020-12-25T10:13:24Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/202012251013/","title":"在Kubernetes（k8s）中部署Java应用"},{"content":" 南宫乘风：我在学Python哦\nPython：你不写Python项目，是不会有进步的，年轻人，耗子尾汁吧\n南宫乘风：我要进步，要学习，开始项目\nPython：来骗，来偷袭。很快啊！年轻人，不讲码德\n学了一点时间Python，为了巩固基础，拿一个简单的项目练手。\nATM+购物车功能（ps：很简单，大佬不要嘲笑哈） 项目需求： 1 2 3 4 5 6 7 8 9 10 1.额度15000或自定义 --\u0026gt; 注册功能 2.实现购物商城，买东西加入购物车，调用信用卡接口结账 --\u0026gt; 购物功能、支付功能 3.可以提现，手续费5% --\u0026gt; 提现功能 4.支持多账户登录 --\u0026gt; 登录功能 5.支持账户间转账 --\u0026gt; 转账功能 6.记录日常消费 --\u0026gt; 记录流水功能 7.提供还款接口 --\u0026gt; 还款功能 8.ATM记录操作日志 --\u0026gt; 记录日志功能 9.提供管理接口，包括添加账户、用户额度，冻结账户等。。。 ---\u0026gt; 管理员功能 10.用户认证用装饰器 --\u0026gt; 登录认证装饰器 \u0026ldquo;用户视图层\u0026rdquo; 展示给用户选择的功能 1 2 3 4 5 6 7 8 9 10 11 12 user 1、注册功能 2、登录功能 3、查看余额 4、提现功能 5、还款功能 6、转账功能 7、查看流水 8、购物功能 9、查看购物车 10、管理员功能 admin\n1 2 3 4 5 1、添加账号 2、修改额度 3、冻结账号 4、解冻账号 5、返回上一层 架构流程图 采取MVC的架构方式\n各层分开，编写接口\n三层架构：\n1 2 3 1、把每个功能都分层三部分，逻辑清晰 2、如果用户更换不同的用户界面或不同的数据库存储机制，不会影响接口层的核心逻辑代码，扩展性强。 3）可以在接口层，准确的记录日志与流水。 三层架构构建 项目代码：https://wwx.lanzoux.com/i1vhGixijzi 启动界面 start.py（程序的入口） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026#39;\u0026#39;\u0026#39; 程序入口 \u0026#39;\u0026#39;\u0026#39; import os import sys from core import src # 添加解释器的环境变量 sys.path.append( os.path.dirname(__file__) ) # 开始执行项目main if __name__ == \u0026#34;__main__\u0026#34;: src.run() conf setting.py（log日志的配置文件，项目的环境配置） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 \u0026#39;\u0026#39;\u0026#39; 配置文件设置 \u0026#39;\u0026#39;\u0026#39; import os BASE=os.path.dirname(os.path.dirname(__file__)) # print(BASE) #获取user_date文件夹路径 USER_DATE=os.path.join(BASE,\u0026#39;db\u0026#39;,\u0026#39;user_date\u0026#39;) # print(USER_DATE) \u0026#34;\u0026#34;\u0026#34; 日志配置字典LOGGING_DIC \u0026#34;\u0026#34;\u0026#34; # 1、定义三种日志输出格式，日志中可能用到的格式化串如下 # %(name)s Logger的名字 # %(levelno)s 数字形式的日志级别 # %(levelname)s 文本形式的日志级别 # %(pathname)s 调用日志输出函数的模块的完整路径名，可能没有 # %(filename)s 调用日志输出函数的模块的文件名 # %(module)s 调用日志输出函数的模块名 # %(funcName)s 调用日志输出函数的函数名 # %(lineno)d 调用日志输出函数的语句所在的代码行 # %(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示 # %(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数 # %(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒 # %(thread)d 线程ID。可能没有 # %(threadName)s 线程名。可能没有 # %(process)d 进程ID。可能没有 # %(message)s用户输出的消息 import os BASE_PATH=os.path.dirname(os.path.dirname(__file__)) a1_path=os.path.join(BASE_PATH,\u0026#39;log\u0026#39;,\u0026#39;a1.log\u0026#39;) a2_path=os.path.join(BASE_PATH,\u0026#39;log\u0026#39;,\u0026#39;a2.log\u0026#39;) # print(a1_path+\u0026#39;\\n\u0026#39;+a2_path) # 2、强调：其中的%(name)s为getlogger时指定的名字 standard_format = \u0026#39;%(asctime)s - %(threadName)s:%(thread)d - 日志名字:%(name)s - %(filename)s:%(lineno)d -\u0026#39; \\ \u0026#39;%(levelname)s - %(message)s\u0026#39; simple_format = \u0026#39;[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s\u0026#39; test_format = \u0026#39;%(asctime)s] %(message)s\u0026#39; # 3、日志配置字典 LOGGING_DIC = { \u0026#39;version\u0026#39;: 1, \u0026#39;disable_existing_loggers\u0026#39;: False, \u0026#39;formatters\u0026#39;: { \u0026#39;standard\u0026#39;: { \u0026#39;format\u0026#39;: standard_format }, \u0026#39;simple\u0026#39;: { \u0026#39;format\u0026#39;: simple_format }, \u0026#39;test\u0026#39;: { \u0026#39;format\u0026#39;: test_format }, }, \u0026#39;filters\u0026#39;: {}, # handlers是日志的接收者，不同的handler会将日志输出到不同的位置 \u0026#39;handlers\u0026#39;: { #打印到终端的日志 \u0026#39;console\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.StreamHandler\u0026#39;, # 打印到屏幕 \u0026#39;formatter\u0026#39;: \u0026#39;simple\u0026#39; }, \u0026#39;default\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.handlers.RotatingFileHandler\u0026#39;, # 保存到文件 # \u0026#39;maxBytes\u0026#39;: 1024*1024*5, # 日志大小 5M \u0026#39;maxBytes\u0026#39;: 1024*1024*5, \u0026#39;backupCount\u0026#39;: 5, \u0026#39;filename\u0026#39;: a1_path, # os.path.join(os.path.dirname(os.path.dirname(__file__)),\u0026#39;log\u0026#39;,\u0026#39;a2.log\u0026#39;) \u0026#39;encoding\u0026#39;: \u0026#39;utf-8\u0026#39;, \u0026#39;formatter\u0026#39;: \u0026#39;standard\u0026#39;, }, #打印到文件的日志,收集info及以上的日志 \u0026#39;other\u0026#39;: { \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, \u0026#39;class\u0026#39;: \u0026#39;logging.FileHandler\u0026#39;, # 保存到文件 \u0026#39;filename\u0026#39;: a1_path, # os.path.join(os.path.dirname(os.path.dirname(__file__)),\u0026#39;log\u0026#39;,\u0026#39;a2.log\u0026#39;) \u0026#39;encoding\u0026#39;: \u0026#39;utf-8\u0026#39;, \u0026#39;formatter\u0026#39;: \u0026#39;standard\u0026#39;, }, }, # loggers是日志的产生者，产生的日志会传递给handler然后控制输出 \u0026#39;loggers\u0026#39;: { #logging.getLogger(__name__)拿到的logger配置 \u0026#39;kkk\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;,\u0026#39;other\u0026#39;], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, # loggers(第一层日志级别关限制)---\u0026gt;handlers(第二层日志级别关卡限制) \u0026#39;propagate\u0026#39;: False, # 默认为True，向上（更高level的logger）传递，通常设置为False即可，否则会一份日志向上层层传递 }, \u0026#39;终端提示\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;console\u0026#39;,], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, # loggers(第一层日志级别关限制)---\u0026gt;handlers(第二层日志级别关卡限制) \u0026#39;propagate\u0026#39;: False, # 默认为True，向上（更高level的logger）传递，通常设置为False即可，否则会一份日志向上层层传递 }, \u0026#39;\u0026#39;: { \u0026#39;handlers\u0026#39;: [\u0026#39;default\u0026#39;, ], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 \u0026#39;level\u0026#39;: \u0026#39;DEBUG\u0026#39;, # loggers(第一层日志级别关限制)---\u0026gt;handlers(第二层日志级别关卡限制) \u0026#39;propagate\u0026#39;: False, # 默认为True，向上（更高level的logger）传递，通常设置为False即可，否则会一份日志向上层层传递 }, }, } core admin.py（admin用户视图） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 from core import src from interface import admin_interface def add_user(): src.register() def change_balance(): while True: # 输入修改修改用户名 change_user = input(\u0026#39;请输入需要修改额度的用户：\u0026#39;).strip() change_money = input(\u0026#39;请输入需要修改的用户额度：\u0026#39;).strip() if not change_money.isdigit(): continue if int(change_money) \u0026gt;= 0: flag, msg = admin_interface.change_balance_interafce(change_user, change_money) if flag: print(msg) else: print(msg) else: print(\u0026#39;输入的额度不正确\u0026#39;) def disable_user(): # 输入要冻结的用户 while True: dis_user = input(\u0026#39;请输入要冻结的用户：\u0026#39;).strip() flag, msg = admin_interface.disable_user_interface(dis_user) if flag: print(msg) break else: print(msg) def enable_user(): # 输入要解冻冻结的用户 while True: dis_user = input(\u0026#39;请输入要解冻冻结的用户：\u0026#39;).strip() flag, msg = admin_interface.enable_user_interface(dis_user) if flag: print(msg) break else: print(msg) def lsat_men(): from core import src src.run() func_dic = { \u0026#39;1\u0026#39;: add_user, \u0026#39;2\u0026#39;: change_balance, \u0026#39;3\u0026#39;: disable_user, \u0026#39;4\u0026#39;: enable_user, \u0026#39;5\u0026#39;: lsat_men, } def admin_run(): while True: print(\u0026#39;\u0026#39;\u0026#39; 1、添加账号 2、修改额度 3、冻结账号 4、解冻账号 5、返回上一层 \u0026#39;\u0026#39;\u0026#39;) choice = input(\u0026#39;请输入管理员功能编号：\u0026#39;).strip() if choice not in func_dic: print(\u0026#34;请输入正确的功能编号\u0026#34;) continue func_dic.get(choice)() # if __name__ == \u0026#39;__main__\u0026#39;: # admin() src.py（用户的视图） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 \u0026#39;\u0026#39;\u0026#39; 用户视图层 \u0026#39;\u0026#39;\u0026#39; from interface import user_interface, bank_interface, shop_interface from lib import common from core import admin # 1、注册功能 \u0026#39;\u0026#39;\u0026#39;def register(): while True: # 1 让用户输入用户名称和密码进行校验 username = input(\u0026#39;请输入用户：\u0026#39;).strip() if username.strip()==\u0026#39;\u0026#39;: print(\u0026#34;请输入正确的用户名\u0026#34;) continue user_path = os.path.join(setting.USER_DATE, f\u0026#34;{username}.json\u0026#34;) if os.path.exists(user_path): print(f\u0026#34;您输入的用户名：{username}已经存在\u0026#34;) continue password = input(\u0026#34;请输入密码：\u0026#34;).strip() re_password = input(\u0026#34;请确认密码：\u0026#34;).strip() # 判断密码是否一致 if password == re_password: # 2 查看用户是否存在 # 3 如用户存在，则让用户重新输入 # 4 如用户不存在，则保存用户数据 # 4.1 组织用户的数据的字典信息 user_dic = { \u0026#39;username\u0026#39;: username, \u0026#39;password\u0026#39;: password, \u0026#39;balance\u0026#39;: 15000, # 用于记录用户的流水类表 \u0026#39;flow\u0026#39;: [], # 用于记录用户购物车 \u0026#39;shop_car\u0026#39;: [], # locaked :用于记录用户是否冻结 # false：未冻结 True：已经被冻结 \u0026#39;locaked\u0026#39;: False } # 用户数据tank.json user_path = os.path.join(setting.USER_DATE, f\u0026#34;{username}.json\u0026#34;) with open(user_path, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(user_dic, f) else: print(\u0026#39;您的两次密码不一致,请重新输入\u0026#39;)\u0026#39;\u0026#39;\u0026#39; login_user = None # 分层版 def register(): while True: # 1 让用户输入用户名称和密码进行校验 username = input(\u0026#39;请输入用户：\u0026#39;).strip() if username.strip() == \u0026#39;\u0026#39;: print(\u0026#34;请输入正确的用户名\u0026#34;) continue password = input(\u0026#34;请输入密码：\u0026#34;).strip() re_password = input(\u0026#34;请确认密码：\u0026#34;).strip() # 判断密码是否一致 if password == re_password: # 2 调用接口层的注册结果。将用户名和密码传到接口层 # (True, 用户注册成功)， (False, 注册失败) flag, msg = user_interface.register_interface( username, password ) # 3) 根据flag判断用户注册是否成功，flag控制break的结束 if flag: print(msg) break else: print(msg) else: print(\u0026#39;您的两次密码不一致,请重新输入\u0026#39;) # 2、登录功能 def login(): # 登录视图层 while True: username = input(\u0026#39;请输入用户名：\u0026#39;).strip() password = input(\u0026#39;请输入用户名密码：\u0026#39;).strip() flag, msg = user_interface.longin_interface(username, password) if flag: global login_user login_user = username print(msg) break else: print(msg) # 3、查看余额 @common.login_auth def check_balance(): # 直接调用查看用户接口，获取用余额 balance = user_interface.check_balance_interface(login_user) print(f\u0026#39;用户：{login_user}账号余额为：{balance}\u0026#39;) # 4、提现功能 @common.login_auth def withdraw(): while True: # 让用户输入体现金额 input_money = input(\u0026#39;请输入提现金额：\u0026#39;).strip() # 判断用户输入的金额是否是数字 if not input_money.isdigit(): print(\u0026#39;请重新输入：\u0026#39;) continue # 用户提现，将金额提交接口处理 flag, msg = bank_interface.withdraw_interface(login_user, int(input_money)) if flag: print(msg) break else: print(msg) # 5、还款功能 @common.login_auth def repay(): while True: # 让用户输入体现金额 repay_money = input(\u0026#39;请输入还款金额：\u0026#39;).strip() # 判断用户输入的金额是否是数字 if not repay_money.isdigit(): print(\u0026#39;请重新输入\u0026#39;) continue # 用户提现，将金额提交接口处理 if int(repay_money) \u0026gt; 0: flag, msg = bank_interface.repay_interface(login_user, int(repay_money)) if flag: print(msg) break else: print(\u0026#39;不能输入小于0的数字\u0026#39;) # 6、转账功能 @common.login_auth def transfer(): \u0026#39;\u0026#39;\u0026#39; 接收用户的转账金额 接收用户输入的转账目标 :return: \u0026#39;\u0026#39;\u0026#39; while True: user_money = input(\u0026#39;请输入转账目标用户：\u0026#39;).strip() money = input(\u0026#39;请输入转账金额：\u0026#39;).strip() if not money.isdigit(): print(\u0026#39;请输入正确的金额！\u0026#39;) continue money = int(money) if money \u0026gt; 0: # 转账接口 flag, msg = bank_interface.transfer_interface(login_user, user_money, money) if flag: print(msg) break else: print(msg) else: print(\u0026#39;请输入正确的金额！\u0026#39;) # 7、查看流水 @common.login_auth def check_flow(): # 直接调用查看流水的列表 flow_list = bank_interface.check_flow_interface(login_user) if flow_list: for flow in flow_list: print(flow) else: print(\u0026#39;当前用户没有流水记录\u0026#39;) # 8、购物功能 @common.login_auth def shopping(): \u0026#39;\u0026#39;\u0026#39; { \u0026#39;0\u0026#39;:{\u0026#39;name\u0026#39;:\u0026#39;包子\u0026#39;,\u0026#39;price\u0026#39;:30}, \u0026#39;0\u0026#39;: {\u0026#39;name\u0026#39;: \u0026#39;包子\u0026#39;, \u0026#39;price\u0026#39;: 30}, \u0026#39;0\u0026#39;: {\u0026#39;name\u0026#39;: \u0026#39;包子\u0026#39;, \u0026#39;price\u0026#39;: 30}, } :return: \u0026#39;\u0026#39;\u0026#39; shop_list = [ [\u0026#39;包子\u0026#39;, 30], [\u0026#39;衣服\u0026#39;, 150], [\u0026#39;安全套\u0026#39;, 25], [\u0026#39;情趣内衣\u0026#39;, 520], [\u0026#39;cosplay\u0026#39;, 250], ] shopping_car = {} while True: # 先打印商品的信息，让用户选择 print(\u0026#39;===================欢迎来到情趣商城================\u0026#39;) for index, shop in enumerate(shop_list): shop_name, shop_price = shop print(f\u0026#39;商品编号：{index}, 商品名称：{shop_name},商品单价：{shop_price}\u0026#39;) print(\u0026#39;========================end======================\u0026#39;) shop_choice = input(\u0026#34;请输入你要选购的商品编号(是否结账y or n)：\u0026#34;).strip() if shop_choice.upper() == \u0026#39;Y\u0026#39;: if not shopping_car: print(\u0026#39;购物车是空的，不能支付，请重新输入\u0026#39;) continue # 调用支付接口进行支付 flag,msg=shop_interface.shopping_interface(login_user,shopping_car) if flag: print(msg) break else: print(msg) elif shop_choice.upper() == \u0026#39;N\u0026#39;: # 判断当前用户是否添加过购物车 if not shopping_car: print(\u0026#39;购物车是空的，不能添加，请重新输入\u0026#39;) continue flag,msg=shop_interface.add_shop_car_interface(login_user,shopping_car) if flag: print(msg) break if not shop_choice.isdigit(): print(\u0026#39;请输入正确的编号！\u0026#39;) continue shop_choice = int(shop_choice) if shop_choice not in range(len(shop_list)): print(\u0026#39;请输入正确的编号！\u0026#39;) continue shop_name, shop_price = shop_list[shop_choice] if shop_name in shopping_car: shopping_car[shop_name][1] += 1 else: shopping_car[shop_name] = [shop_price, 1] print(\u0026#34;当前购物车：\u0026#34;,shopping_car) # 9、查看购物车 @common.login_auth def check_shop_car(): shop_list = shop_interface.check_shop_car_interface(login_user) if shop_list: for shop_name,print_number in shop_list.items(): print(f\u0026#39;商品：[{shop_name}],数量：[{print_number[1]}]\u0026#39;) else: print(\u0026#39;当前用户没有流水记录\u0026#39;) @common.login_auth def admin_spuer(): admin.admin_run() # 创建函数功能字典 func_dic = { \u0026#39;1\u0026#39;: register, \u0026#39;2\u0026#39;: login, \u0026#39;3\u0026#39;: check_balance, \u0026#39;4\u0026#39;: withdraw, \u0026#39;5\u0026#39;: repay, \u0026#39;6\u0026#39;: transfer, \u0026#39;7\u0026#39;: check_flow, \u0026#39;8\u0026#39;: shopping, \u0026#39;9\u0026#39;: check_shop_car, \u0026#39;10\u0026#39;: admin_spuer, } def run(): while True: print(\u0026#39;\u0026#39;\u0026#39; ==========ATM+购物车========== 1、注册功能 2、登录功能 3、查看余额 4、提现功能 5、还款功能 6、转账功能 7、查看流水 8、购物功能 9、查看购物车 10、管理员功能 ========== end ========== \u0026#39;\u0026#39;\u0026#39;) choice = input(\u0026#34;请输入功能编号：\u0026#34;).strip() if choice not in func_dic: print(\u0026#34;请输入正确的功能编号\u0026#34;) continue func_dic.get(choice)() db user_date (用户存放json的目录) q.json （用户数据的存放文件，字典形式） 1 {\u0026#34;username\u0026#34;: \u0026#34;q\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;099b3b060154898840f0ebdfb46ec78f\u0026#34;, \u0026#34;balance\u0026#34;: 14375.0, \u0026#34;flow\u0026#34;: [\u0026#34;用户[q] 提现金额 [500],手续费为：[25.0]\u0026#34;, \u0026#34;用户：[q]给用户[w] 转账：[100] 元 成功\u0026#34;], \u0026#34;shop_car\u0026#34;: {}, \u0026#34;locked\u0026#34;: false} json格式\n1 2 3 4 5 6 7 8 { \u0026#34;username\u0026#34;: \u0026#34;q\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;099b3b060154898840f0ebdfb46ec78f\u0026#34;, \u0026#34;balance\u0026#34;: 14375.0, \u0026#34;flow\u0026#34;: [\u0026#34;用户[q] 提现金额 [500],手续费为：[25.0]\u0026#34;, \u0026#34;用户：[q]给用户[w] 转账：[100] 元 成功\u0026#34;], \u0026#34;shop_car\u0026#34;: {}, \u0026#34;locked\u0026#34;: false } db_hander.py（对数据处理，相当于mysql接口处理） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026#39;\u0026#39;\u0026#39; 数据处理层 专门用户处理数据的 \u0026#39;\u0026#39;\u0026#39; import json import os from conf import setting # 判断用户是否已经注册 def user_select(username): # 1) 接收接口层传过来的username用户名，拼接用户json文件路径 user_path = os.path.join( setting.USER_DATE, f\u0026#39;{username}.json\u0026#39; ) # 2）校验用户json文件是否存在 if os.path.exists(user_path): # 3) 打开数据，并返回给接口层 with open(user_path, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: user_dic = json.load(f) return user_dic # 3) 不return，默认return None # 保存数据处理 def user_save(user_dic): user_path = os.path.join(setting.USER_DATE, f\u0026#34;{user_dic[\u0026#39;username\u0026#39;]}.json\u0026#34;) with open(user_path, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(user_dic, f, ensure_ascii=False) interface admin_interface.py（admin的业务接口处理） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from db import db_hander from lib import common user_logger=common.get_log(\u0026#39;admin\u0026#39;) # 修改额度接口 def change_balance_interafce(change_user, change_money): user_dic = db_hander.user_select(change_user) if user_dic: user_dic[\u0026#39;balance\u0026#39;] = int(change_money) db_hander.user_save(user_dic) msg=f\u0026#39;[{change_user}]的额度修改{change_money}成功\u0026#39; user_logger.info(msg) return True, msg return False, \u0026#39;修改额度用户不存在\u0026#39; def disable_user_interface(dis_user): user_dic = db_hander.user_select(dis_user) if user_dic: user_dic[\u0026#39;locked\u0026#39;] = True db_hander.user_save(user_dic) msg = f\u0026#39;[{dis_user}]的冻结成功\u0026#39; user_logger.info(msg) return True, msg else: return False, \u0026#39;冻结用户不存在\u0026#39; # 解冻用户的接口 def enable_user_interface(dis_user): user_dic = db_hander.user_select(dis_user) if user_dic: if user_dic[\u0026#39;locked\u0026#39;]: user_dic[\u0026#39;locked\u0026#39;] = False db_hander.user_save(user_dic) msg = f\u0026#39;[{dis_user}]的冻结成功\u0026#39; user_logger.info(msg) return True, msg return False,f\u0026#39;用户：{dis_user}处于正常状态\u0026#39; else: return False, \u0026#39;冻结用户不存在\u0026#39; bank_interface.py（银行业务处理接口） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 \u0026#39;\u0026#39;\u0026#39; 银行相关的业务 \u0026#39;\u0026#39;\u0026#39; from db import db_hander from lib import common user_logger=common.get_log(\u0026#39;bank\u0026#39;) # 用户的提现金额接口 def withdraw_interface(login_user, input_money): # 先获取用户字典 user_dic = db_hander.user_select(login_user) # 获取银行的金额 balance = int(user_dic.get(\u0026#39;balance\u0026#39;)) # 本金+ 手续费 money = int(input_money) * 1.05 repair = round(money - input_money, 2) # 判断用户金额是否足够 if balance \u0026gt;= money: # 修改用户字典的金额 balance -= money user_dic[\u0026#39;balance\u0026#39;] = balance flow = f\u0026#39;用户[{login_user}] 提现金额 [{input_money}],手续费为：[{repair}]\u0026#39; user_dic[\u0026#39;flow\u0026#39;].append(flow) # 在保存数据 db_hander.user_save(user_dic) msg=f\u0026#39;用户[{login_user}] 提现金额 [{input_money}],手续费为：[{repair}]\u0026#39; user_logger.info(msg) return True, msg return False, \u0026#39;提现金额不足，请重新输入\u0026#39; # 还款接口 def repay_interface(login_user, repay_money): # 先获取用户字典 user_dic = db_hander.user_select(login_user) # 获取银行的金额 balance = int(user_dic.get(\u0026#39;balance\u0026#39;)) # 本金+ 手续费 money = int(repay_money) # repair = round(money - repay_money, 2) # 修改用户字典的金额 balance += money user_dic[\u0026#39;balance\u0026#39;] = balance # 在保存数据 flow = f\u0026#39;用户：[{login_user}] 还款金额 ：[{repay_money}]成功]\u0026#39; user_dic[\u0026#39;flow\u0026#39;].append(flow) db_hander.user_save(user_dic) msg = f\u0026#39;用户：[{login_user}] 还款金额 ：[{repay_money}]成功]\u0026#39; user_logger.info(msg) return True, msg # 转账接口 def transfer_interface(login_user, user_money, money): # 先获取用户字典 login_user_dic = db_hander.user_select(login_user) user_money_dic = db_hander.user_select(user_money) # 判断目标用户是否存在 if not user_money_dic: return False, \u0026#39;目标用户不存在\u0026#39; if login_user == user_money: return False, \u0026#39;不能给自己转账\u0026#39; if login_user_dic[\u0026#39;balance\u0026#39;] \u0026gt;= money: login_user_dic[\u0026#39;balance\u0026#39;] -= money user_money_dic[\u0026#39;balance\u0026#39;] += money flow1 = f\u0026#39;用户：[{login_user}]给用户[{user_money}] 转账：[{money}] 元 成功\u0026#39; login_user_dic[\u0026#39;flow\u0026#39;].append(flow1) flow2 = f\u0026#39;用户：[{user_money}]接受用户[{login_user}] 转账：[{money}] 元 成功\u0026#39; user_money_dic[\u0026#39;flow\u0026#39;].append(flow2) db_hander.user_save(login_user_dic) db_hander.user_save(user_money_dic) msg = f\u0026#39;用户：[{login_user}]给用户[{user_money}] 转账：[{money}] 元 成功\u0026#39; user_logger.info(msg) return True, msg msg = f\u0026#39;用户：[{login_user}]银行余额不足\u0026#39; user_logger.info(msg) return False, msg # 流水接口列表查询 def check_flow_interface(login_user): # 先获取用户字典 login_user_dic = db_hander.user_select(login_user) flow_list = login_user_dic[\u0026#39;flow\u0026#39;] return flow_list # 支付接口 def pay_interface(login_user, cost): user_dic = db_hander.user_select(login_user) if user_dic.get(\u0026#39;balance\u0026#39;) \u0026gt;= cost: user_dic[\u0026#39;balance\u0026#39;] -= cost flow = f\u0026#39;用户：{login_user}消费金额：{cost}\u0026#39; user_dic[\u0026#39;flow\u0026#39;].append(flow) db_hander.user_save(user_dic) user_logger.info(flow) return True return False shop_interface.py（购物业务接口处理） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \u0026#39;\u0026#39;\u0026#39; 购物商城接口 \u0026#39;\u0026#39;\u0026#39; from interface import bank_interface from db import db_hander from lib import common shop_logger = common.get_log(log_type=\u0026#39;shop\u0026#39;) # 商品准备结算接口 def shopping_interface(login_user, shopping_car): # 计算商品总价 cost = 0 for shop_price in shopping_car.values(): price, number = shop_price cost += (price * number) # 逻辑校验成功，调用银行的支付接口 flag = bank_interface.pay_interface(login_user, cost) if flag: msg = f\u0026#39;用户:[{login_user}]支付 [{cost}$] 成功, 准备发货!\u0026#39; shop_logger.info(msg) return True, msg return False, \u0026#39;支付失败，金额不足\u0026#39; # 商品添加购物车功能 def add_shop_car_interface(login_user, shopping_car): # 获取当前用户的购物车 user_dic = db_hander.user_select(login_user) #原来用户的字典的商城列表 shop_car = user_dic.get(\u0026#39;shop_car\u0026#39;) for shop_name, price_number in shopping_car.items(): number = price_number[1] if shop_name in shop_car: user_dic[\u0026#39;shop_car\u0026#39;][shop_name][1] += number else: user_dic[\u0026#39;shop_car\u0026#39;].update( {shop_name: price_number} ) db_hander.user_save(user_dic) return True, \u0026#39;添加购物车成功\u0026#39; # 查看购物车接口 def check_shop_car_interface(login_user): # 获取当前用户的购物车 user_dic = db_hander.user_select(login_user) shop_list = user_dic[\u0026#39;shop_car\u0026#39;] return shop_list user_interface.py（用户业务接口处理） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 \u0026#39;\u0026#39;\u0026#39; 用户相关的接口 \u0026#39;\u0026#39;\u0026#39; import os import json from db import db_hander from lib import common user_logger=common.get_log(\u0026#39;user\u0026#39;) # 用户注册 def register_interface(username, password, balance=15000): # 2）查看用户是否存在 # 2.1) 调用 数据处理层 中的 select函数，会返回 用户字典 或 None user_dic = db_hander.user_select(username) # {user: user, pwd: pwd...} or None # 若用户存在，则return，告诉用户重新输入 if user_dic: # return (False, \u0026#39;用户名已存在!\u0026#39;) return False, \u0026#39;用户名已存在!\u0026#39; # 3）若用户不存在，则保存用户数据 # 做密码加密 password = common.salt(username, password) # 3.1) 组织用户的数据的字典信息 user_dic = { \u0026#39;username\u0026#39;: username, \u0026#39;password\u0026#39;: password, \u0026#39;balance\u0026#39;: balance, # 用于记录用户流水的列表 \u0026#39;flow\u0026#39;: [], # 用于记录用户购物车 \u0026#39;shop_car\u0026#39;: {}, # locked：用于记录用户是否被冻结 # False: 未冻结 True: 已被冻结 \u0026#39;locked\u0026#39;: False } # 3.2）保存数据 db_hander.user_save(user_dic) msg=f\u0026#39;{username} 注册成功!\u0026#39; user_logger.info(msg) return True, msg # 登录接口 def longin_interface(username, password): # 先查看用户是否存在 user_dic = db_hander.user_select(username) password_md5 = common.salt(username, password) if user_dic: print(user_dic.get(\u0026#39;locked\u0026#39;)) if user_dic.get(\u0026#39;locked\u0026#39;): return False, f\u0026#39;用户：[{username}]已经被冻结\u0026#39; if password_md5 == user_dic[\u0026#39;password\u0026#39;]: msg=f\u0026#39;用户：[{username}] 登录成功 \u0026#39; user_logger.info(msg) return True,msg else: msg = f\u0026#39;用户：[{username}]密码错误\u0026#39; user_logger.warning(msg) return False, msg msg = f\u0026#39;用户[{username}]不存在，请重新输入\u0026#39; user_logger.info(msg) return False, msg # 查看用户余额接口 def check_balance_interface(login_user): user_dic = db_hander.user_select(login_user) return user_dic.get(\u0026#39;balance\u0026#39;) lib common.py（公用类，盐MD5加密，登录装饰器） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import hashlib from core import src from conf import setting import logging.config # 盐加密，通名字加密 密码 def salt(username, password): password_ms5= hashlib.md5() password_ms5.update((username+password).encode(\u0026#39;utf-8\u0026#39;)) res = password_ms5.hexdigest() return res # 登录认证装饰器 def login_auth(func): def inner(*args, **kwargs): if src.login_user: res = func(*args, **kwargs) return res else: print(\u0026#39;未出示证明，无法享受服务\u0026#39;) src.login() return inner # 添加日志功能(日志功能在接口层使用) def get_log(log_type): \u0026#39;\u0026#39;\u0026#39; :param log_type: user日志 bank日志 购物商城日志 :return: \u0026#39;\u0026#39;\u0026#39; # 1、加载日志配置信息 logging.config.dictConfig( setting.LOGGING_DIC ) # 获取日志对象 logger = logging.getLogger(log_type) return logger log（日志文件记录） a1.log 1 2 3 2020-11-29 23:03:52,782 - MainThread:13396 - 日志名字:user - user_interface.py:58 -INFO - 用户：[q]登录成功！ 2020-11-29 23:04:11,089 - MainThread:13396 - 日志名字:user - user_interface.py:58 -INFO - 用户：[w]登录成功！ 2020-11-29 23:04:15,584 - MainThread:13396 - 日志名字:user - user_interface.py:69 -INFO - 用户不存在，请重新输入 整体项目架构不错，可以很清楚了解到三层架构的构建\n用户点击：用户视图层————————》业务接口处理————————》数据库数据处理\n数据返回用户：数据处理完数据————————》业务接口处理————————》用户视图层\n","date":"2020-11-30T21:07:54Z","image":"https://www.ownit.top/title_pic/76.jpg","permalink":"https://www.ownit.top/p/202011302107/","title":"Python构建简单的ATM+购物功能项目"},{"content":"1、 基本环境配置 1、Kubectl debug 设置一个临时容器\n2、Sidecar\n3、Volume：更改目录权限，fsGroup\n4、ConfigMap和Secret\nK8S官网：https://kubernetes.io/docs/setup/\n最新版高可用安装：https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/\n主机名\nIP地址\n说明\nk8s-master01 ~ 03\n192.168.0.106 ~ 20\nmaster节点 * 3\nk8s-master-lb\n192.168.0.200\nkeepalived虚拟IP\nk8s-node01 ~ 02\n192.168.0.108 ~ 22\nworker节点 * 2\nVIP（虚拟IP）不要和公司内网IP重复，首先去ping一下，不通才可用。VIP需要和主机在同一个局域网内！\n所有节点配置hosts，修改/etc/hosts如下：\n1 2 3 4 5 6 7 [root@k8s-master01 ~]# cat /etc/hosts 192.168.0.100 k8s-master01 192.168.0.106 k8s-master02 192.168.0.107 k8s-master03 192.168.0.200 k8s-master-lb 192.168.0.108 k8s-node01 192.168.0.109 k8s-node02 所有节点关闭防火墙、selinux、dnsmasq、swap。服务器配置如下\n1 2 3 4 5 systemctl disable --now firewalld systemctl disable --now dnsmasq #systemctl disable --now NetworkManager #CentOS8无需关闭 setenforce 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [root@k8s-master01 ~]# cat !$ cat /etc/sysconfig/selinux SELINUX=disabled swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 [root@k8s-master01 ~]# vi /etc/fstab [root@k8s-master01 ~]# cat /etc/fstab # # /etc/fstab # Created by anaconda on Fri Nov 1 23:02:53 2019 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk/\u0026#39;. # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info. # # After editing this file, run \u0026#39;systemctl daemon-reload\u0026#39; to update systemd # units generated from this file. # /dev/mapper/cl-root / xfs defaults 0 0 UUID=6897cd7b-9b3a-42b0-a827-57991141b297 /boot ext4 defaults 1 2 #/dev/mapper/cl-swap swap swap defaults 0 0 安装ntpdate（CentOS 7 无需安装，自带ntpdate命令）\n所有节点同步时间。时间同步配置如下：\n1 2 3 4 5 6 7 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo \u0026#39;Asia/Shanghai\u0026#39; \u0026gt;/etc/timezone ntpdate time2.aliyun.com # 加入到crontab */5 * * * * ntpdate time2.aliyun.com # 加入到开机自动同步，/etc/rc.local ntpdate time2.aliyun.com 所有节点配置limit：\n1 ulimit -SHn 65535 Master01节点免密钥登录其他节点，安装过程中生成配置文件和证书均在Master01上操作，集群管理也在Master01上操作，阿里云或者AWS上需要单独一台kubectl服务器。密钥配置如下：\n1 2 3 ssh-keygen -t rsa for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done 在源码中的repo目录配置使用的是国内仓库源，将其复制到所有节点：\n1 2 git clone https://github.com/dotbalo/k8s-ha-install.git 2、CentOS 7安装yum源如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF sed -i -e \u0026#39;/mirrors.cloud.aliyuncs.com/d\u0026#39; -e \u0026#39;/mirrors.aliyuncs.com/d\u0026#39; /etc/yum.repos.d/CentOS-Base.repo 所有节点升级系统并重启，此处升级没有升级内核，下节会单独升级内核：\n1 2 3 yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 -y yum update -y --exclude=kernel* \u0026amp;\u0026amp; reboot #CentOS7需要升级，8不需要 3、内核配置\n（如果内核达到，可以不用跳过此步骤）\nCentOS7 需要升级内核至4.18+\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 使用如下方式安装最新版内核 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 查看最新版内核yum --disablerepo=\u0026#34;*\u0026#34; --enablerepo=\u0026#34;elrepo-kernel\u0026#34; list available [root@k8s-node01 ~]# yum --disablerepo=\u0026#34;*\u0026#34; --enablerepo=\u0026#34;elrepo-kernel\u0026#34; list available 安装最新版： yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel –y 安装完成后reboot 更改内核顺序： grub2-set-default 0 \u0026amp;\u0026amp; grub2-mkconfig -o /etc/grub2.cfg \u0026amp;\u0026amp; grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; \u0026amp;\u0026amp; reboot 开机后查看内核 [appadmin@k8s-node01 ~]$ uname -a Linux k8s-node01 5.7.7-1.el7.elrepo.x86_64 #1 SMP Wed Jul 1 11:53:16 EDT 2020 x86_64 x86_64 x86_64 GNU/Linux 本所有节点安装ipvsadm：\n1 yum install ipvsadm ipset sysstat conntrack libseccomp -y 所有节点配置ipvs模块，在内核4.19+版本nf_conntrack_ipv4已经改为nf_conntrack，本例安装的内核为4.18，使用nf_conntrack_ipv4即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 cat /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip 然后执行systemctl enable --now systemd-modules-load.service即可 检查是否加载：\n1 2 3 4 5 [root@k8s-master01 ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4 nf_conntrack_ipv4 16384 23 nf_defrag_ipv4 16384 1 nf_conntrack_ipv4 nf_conntrack 135168 10 xt_conntrack,nf_conntrack_ipv6,nf_conntrack_ipv4,nf_nat,nf_nat_ipv6,ipt_MASQUERADE,nf_nat_ipv4,xt_nat,nf_conntrack_netlink,ip_vs 开启一些k8s集群中必须的内核参数，所有节点配置k8s内核：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 fs.may_detach_mounts = 1 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_watches=89100 fs.file-max=52706963 fs.nr_open=52706963 net.netfilter.nf_conntrack_max=2310720 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_keepalive_intvl =15 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_max_orphans = 327680 net.ipv4.tcp_orphan_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.ip_conntrack_max = 65536 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_timestamps = 0 net.core.somaxconn = 16384 EOF sysctl --system 所有节点配置完内核后，重启服务器，保证重启后内核依旧加载\n1 2 reboot lsmod | grep --color=auto -e ip_vs -e nf_conntrack 4、基本组件安装 本节主要安装的是集群中用到的各种组件，比如Docker-ce、Kubernetes各组件等。\n4.1安装Docker组件 查看可用docker-ce版本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 yum list docker-ce.x86_64 --showduplicates | sort -r [root@k8s-master01 k8s-ha-install]# wget https://download.docker.com/linux/centos/7/x86_64/edge/Packages/containerd.io-1.2.13-3.2.el7.x86_64.rpm [root@k8s-master01 k8s-ha-install]# yum install containerd.io-1.2.13-3.2.el7.x86_64.rpm -y 安装指定版本的Docker： yum -y install docker-ce-17.09.1.ce-1.el7.centos 安装最新版本的Docker yum install docker-ce –y [root@k8s-master01 ~]# docker -v Docker version 19.03.13, build 4484c46d9d 温馨提示：\n由于新版kubelet建议使用systemd，所以可以把docker的CgroupDriver改成systemd\n1 2 3 4 5 6 cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF 4.2安装k8s组件： 1 2 3 4 5 6 7 yum list kubeadm.x86_64 --showduplicates | sort -r 所有节点安装最新版本kubeadm： yum install kubeadm -y 所有节点安装指定版本k8s组件： yum install -y kubeadm-1.18.5-0.x86_64 kubelet-1.18.5-0.x86_64 kubectl-1.18.5-0.x86_64 4.3启动Docker和Kubernetes 所有节点设置开机自启动Docker：\n1 2 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now docker 默认配置的pause镜像使用gcr.io仓库，国内可能无法访问，所以这里配置Kubelet使用阿里云的pause镜像：\n1 2 3 4 cat \u0026gt;/etc/sysconfig/kubelet\u0026lt;\u0026lt;EOF KUBELET_EXTRA_ARGS=\u0026#34;--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1\u0026#34; EOF 设置Kubelet开机自启动：\n1 2 systemctl daemon-reload systemctl enable --now kubelet 5、高可用组件安装 5.1所有Master节点通过yum安装HAProxy和KeepAlived 1 yum install keepalived haproxy -y 所有Master节点配置HAProxy（详细配置参考HAProxy文档，所有Master节点的HAProxy配置相同）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [root@k8s-master01 etc]# mkdir /etc/haproxy [root@k8s-master01 etc]# vim /etc/haproxy/haproxy.cfg global maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30s defaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15s frontend monitor-in bind *:33305 mode http option httplog monitor-uri /monitor frontend k8s-master bind 0.0.0.0:16443 bind 127.0.0.1:16443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-master backend k8s-master mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master01\t192.168.0.100:6443 check server k8s-master02\t192.168.0.106:6443 check server k8s-master03\t192.168.0.107:6443 check 5.2Master01节点的配置： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [root@k8s-master01 etc]# mkdir /etc/keepalived [root@k8s-master01 ~]# vim /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 2 weight -5 fall 3 rise 2 } vrrp_instance VI_1 { state MASTER interface ens33 mcast_src_ip 192.168.0.100 virtual_router_id 51 priority 100 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.0.200 } # track_script { # chk_apiserver # } } 5.3Master02节点的配置： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [root@k8s-master02 etc]# mkdir /etc/keepalived [root@k8s-master02 ~]# vim /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 2 weight -5 fall 3 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.0.106 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.0.200 } # track_script { # chk_apiserver # } } 5.4Master03节点的配置： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [root@k8s-master03 etc]# mkdir /etc/keepalived [root@k8s-master03 ~]# vim /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script chk_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 2 weight -5 fall 3 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens33 mcast_src_ip 192.168.0.1067 virtual_router_id 51 priority 101 advert_int 2 authentication { auth_type PASS auth_pass K8SHA_KA_AUTH } virtual_ipaddress { 192.168.0.200 } # track_script { # chk_apiserver # } } 5.5注意上述的健康检查是关闭的，集群建立完成后再开启： 1 2 3 # track_script { # chk_apiserver # } 5.6配置KeepAlived健康检查文件（所有Master节点） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 [root@k8s-master01 keepalived]# cat /etc/keepalived/check_apiserver.sh #!/bin/bash err=0 for k in $(seq 1 5) do check_code=$(pgrep kube-apiserver) if [[ $check_code == \u0026#34;\u0026#34; ]]; then err=$(expr $err + 1) sleep 5 continue else err=0 break fi done if [[ $err != \u0026#34;0\u0026#34; ]]; then echo \u0026#34;systemctl stop keepalived\u0026#34; /usr/bin/systemctl stop keepalived exit 1 else exit 0 fi 5.7启动haproxy和keepalived（所有Master节点） 1 2 3 [root@k8s-master01 keepalived]# systemctl enable --now haproxy [root@k8s-master01 keepalived]# systemctl enable --now keepalived 5.8配置K8S组件 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/\n各Master节点的kubeadm-config.yaml配置文件如下：\nMaster01：\ndaocloud.io/daocloud\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: 7t2weq.bjbawausm0jaxury ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.0.100 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master01 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: certSANs: - 192.168.0.200 timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: 192.168.0.200:16443 controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.18.5 networking: dnsDomain: cluster.local podSubnet: 172.168.0.0/16 serviceSubnet: 10.96.0.0/12 scheduler: {} ============================================== #如果这个镜像地址无法下载，可以替换：daocloud.io/daocloud 下载 imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers kubernetesVersion: v1.18.5 #注意安装的Kubernetes的版本 更新kubeadm文件，如果你使用高版本的，可以使用这条命令生成高版本的yaml文件\n1 2 kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml 所有Master节点提前下载镜像，可以节省初始化时间：\n1 2 3 4 kubeadmconfig images pull --config /root/kubeadm-config.yaml - 所有节点设置开机自启动kubelet systemctl enable --now kubelet Master01节点初始化，初始化以后会在/etc/kubernetes目录下生成对应的证书和配置文件，之后其他Master节点加入Master01即可：\n1 kubeadminit --config /root/kubeadm-config.yaml --upload-certs 1 2 3 不用配置文件初始化： kubeadm init --control-plane-endpoint \u0026#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT\u0026#34; --upload-certs 如果初始化失败，重置后再次初始化，命令如下：\n1 kubeadm reset 初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录下初始化成功生成的token值（令牌值）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.0.200:16443 --token 5joxsb.zo1vh747wljgzrlt \\ --discovery-token-ca-cert-hash sha256:86ee9b6a65c6d8641507e9e56e66dad47cfa15b41b52a11e175c5f9588a485b8 \\ --control-plane --certificate-key bc4726d06255be0cd54592e29068e32c5a49eb8fd30a691342412cf79b3d47c7 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.200:16443 --token 5joxsb.zo1vh747wljgzrlt \\ --discovery-token-ca-cert-hash sha256:86ee9b6a65c6d8641507e9e56e66dad47cfa15b41b52a11e175c5f9588a485b8 5.9所有Master节点配置环境变量，用于访问Kubernetes集群： 1 2 3 4 cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /root/.bashrc export KUBECONFIG=/etc/kubernetes/admin.conf EOF source /root/.bashrc 查看节点状态：\n1 2 3 4 5 [root@k8s-master01 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 NotReady master 14m v1.12.3 采用初始化安装方式，所有的系统组件均以容器的方式运行并且在kube-system命名空间内，此时可以查看Pod状态：\n1 2 3 4 5 6 7 8 9 10 11 [root@k8s-master01 ~]# kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE coredns-777d78ff6f-kstsz 0/1 Pending 0 14m \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; coredns-777d78ff6f-rlfr5 0/1 Pending 0 14m \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; etcd-k8s-master01 1/1 Running 0 14m 192.168.0.100 k8s-master01 kube-apiserver-k8s-master01 1/1 Running 0 13m 192.168.0.100 k8s-master01 kube-controller-manager-k8s-master01 1/1 Running 0 13m 192.168.0.100 k8s-master01 kube-proxy-8d4qc 1/1 Running 0 14m 192.168.0.100 k8s-master01 kube-scheduler-k8s-master01 1/1 Running 0 13m 192.168.0.100 k8s-master01 6、Calico组件的安装 注意：如果国内用户下载Calico较慢，所有节点可以配置加速器(如果该文件有其他配置，别忘了加上去)\n1 2 3 4 5 6 7 8 9 10 11 vim /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34; ] } systemctl daemon-reload systemctl restart docker 1 2 3 4 5 6 7 8 9 Calico：https://www.projectcalico.org/ https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises curl https://docs.projectcalico.org/manifests/calico.yaml -O - name: CALICO_IPV4POOL_CIDR value: \u0026#34;172.168.0.0/16\u0026#34; kubectl apply -f calico.yaml 7、高可用Master 1 2 3 4 5 6 7 8 9 10 11 [root@k8s-master01 ~]# kubectl get secret -n kube-system [root@k8s-master01 ~]# kubectl get secret -n kube-system bootstrap-token-7t2weq -oyaml Token过期后生成新的token： Node节点生成 kubeadm token create --print-join-command Master需要生成--certificate-key kubeadm init phase upload-certs --upload-certs 1 2 3 kubeadm join 192.168.0.200:16443 --token 9zp1xe.h5kpi1b9kd5blk76 --discovery-token-ca-cert-hash sha256:6ba6e5205ac27e39e03d3b89a639ef70f6503fb877b1cf8a332b399549471740 \\ --control-plane --certificate-key 309f945f612dd7f0d830b11868edd5135e6cf358ed503107eb645dc8d7c84405 8、Node节点的配置 Node节点上主要部署公司的一些业务应用，生产环境中不建议Master节点部署系统组件之外的其他Pod，测试环境可以允许Master节点部署Pod以节省系统资源。\n1 2 kubeadm join 192.168.0.200:16443 --token 9zp1xe.h5kpi1b9kd5blk76 --discovery-token-ca-cert-hash sha256:6ba6e5205ac27e39e03d3b89a639ef70f6503fb877b1cf8a332b399549471740 9、 Metrics部署 在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。\nHeapster更改metrics的部署文件证书，将metrics-server-3.6.1/metrics-server-deployment.yaml的front-proxy-ca.pem改为front-proxy-ca.crt\n1 2 3 4 5 将Master01节点的front-proxy-ca.crt复制到所有Node节点 scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt 安装metrics server kubectl create -f metrics-server-3.6.1/ 10、Dashboard部署 官方GitHub：https://github.com/kubernetes/dashboard\nDashboard用于展示集群中的各类资源，同时也可以通过Dashboard实时查看Pod的日志和在容器中执行一些命令等。\n可以在官方dashboard查看到最新版dashboard\n1 2 3 4 5 6 kubectl apply –f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml [root@k8s-master01 ]# kubectl get svc -n kubernetes-dashboard [root@k8s-master01]# kubectl edit svc kubernetes-dashboard -n !$ 在谷歌浏览器（Chrome）启动文件中加入启动参数，用于解决无法访问Dashboard的问题\n--test-type--ignore-certificate-errors\n访问Dashboard：https://192.168.0.200:30000，选择登录方式为令牌（即token方式）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 vim admin.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system kubectl apply -f admin.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) 将token值输入到令牌后，单击登录即可访问Dashboard\n1 2 3 4 5 6 7 8 将Kube-proxy改为ipvs模式，因为在初始化集群的时候注释了ipvs配置，所以需要自行修改一下： kubectl edit cm kube-proxy -n kube-system mode: “ipvs” 更新Kube-Proxy的Pod： kubectl patch daemonset kube-proxy -p \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;template\\\u0026#34;:{\\\u0026#34;metadata\\\u0026#34;:{\\\u0026#34;annotations\\\u0026#34;:{\\\u0026#34;date\\\u0026#34;:\\\u0026#34;`date +\u0026#39;%s\u0026#39;`\\\u0026#34;}}}}}\u0026#34; -n kube-system 验证Kube-Proxy模式 [root@k8s-master01 1.1.1]# curl 127.0.0.1:10249/proxyMode ipvs 1 2 3 4 5 6 7 [root@k8s-master01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready master 5h50m v1.18.5 k8s-master02 Ready master 5h31m v1.18.5 k8s-master03 Ready master 5h30m v1.18.5 k8s-node01 Ready \u0026lt;none\u0026gt; 5h26m v1.18.5 k8s-node02 Ready \u0026lt;none\u0026gt; 5h26m v1.18.5 ","date":"2020-11-27T19:10:39Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/202011271910/","title":"kubeadm高可用安装k8s集群1.18.5"},{"content":"1、可迭代对象 1)、可迭代对象定义 对于迭代器来说，我们更熟悉的应该是可迭代对象，之前无论是源码还是讲课中或多或少我们提到过可迭代对象这个词。之前为了便于大家理解可迭代对象，可能解释的不是很正确，所以今天我们正式的聊一聊什么是可迭代对象。从字面意思来说，我们先对其进行拆解：什么是对象？Python中一切皆对象，之前我们讲过的一个变量，一个列表，一个字符串，文件句柄，函数名等等都可称作一个对象，其实一个对象就是一个实例，就是一个实实在在的东西。那么什么叫迭代？其实我们在日常生活中经常遇到迭代这个词儿，更新迭代等等，迭代就是一个重复的过程，但是不能是单纯的重复（如果只是单纯的重复那么他与循环没有什么区别）每次重复都是基于上一次的结果而来。比如你爹生你，你生你爹，哦不对，你生你儿子，你儿子生你孙子等等，每一代都是不一样的；还有你使用过得app，微信，抖音等，隔一段时间就会基于上一次做一些更新，那么这就是迭代。可迭代对象从字面意思来说就是一个可以重复取值的实实在在的东西。\nstr list tuple dic set range 文件句柄等，那么int，bool这些为什么不能称为可迭代对象呢？虽然在字面意思这些看着不符合，但是我们要有一定的判断标准或者规则去判断该对象是不是可迭代对象。\n在python中，但凡内部含有__iter__方法的对象，都是可迭代对象\n2、 查看对象内部方法 该对象内部含有什么方法除了看源码还有什么其他的解决方式么？当然有了， 可以通过dir() 去判断一个对象具有什么方法\n1 2 s1 = \u0026#39;heian\u0026#39; print(dir(s1)) dir()会返回一个列表，这个列表中含有该对象的以字符串的形式所有方法名。这样我们就可以判断python中的一个对象是不是可迭代对象了：\n1 2 3 4 s1 = \u0026#39;heian\u0026#39; i = 100 print(\u0026#39;__iter__\u0026#39; in dir(i)) # False print(\u0026#39;__iter__\u0026#39; in dir(s1)) # True 3、判断一个对象是否可以迭代 1 2 3 name = [1,2,3,4] print(dir(name)) print(\u0026#39;__iter__\u0026#39; in dir(name)) 4、小结： 字面意思：循环更新的一个实实在在的值\n内部意思：内部含有“__iter__()”方法的对象，可迭代对象\n判断方法：\u0026rsquo;__iter__\u0026rsquo; in dir(对象)\nstr list tuple dict set range\n5、迭代对象优缺点 优点：\n1.存储的数据直接能显示，比较直观\n2.拥有的方法比较多，操作方便\n缺点：\n1.占用内存\n2.不能直接通过for循环，不能直接取值（索引和key除外）\nfor循环在底层做了一个小小的转化，就是先将可迭代对象转化成迭代器，然后在进行取值的。\n** 2、迭代器** 1、迭代器的定义 从字面意思来说迭代器，是一个可以迭代取值的工具，器：在这里当做工具比较合适。\n从专业角度来说：迭代器是这样的对象：实现了无参数的__next__方法，返回序列中的下一个元素，如果没有元素了，那么抛出StopIteration异常.python中的迭代器还实现了__iter__方法，因此迭代器也可以迭代。 出自《流畅的python》\n那么对于上面的解释有一些超前，和难以理解，不用过于纠结，我们简单来说：在python中，内部含有\u0026rsquo;__Iter__\u0026lsquo;方法并且含有\u0026rsquo;__next__\u0026lsquo;方法的对象就是迭代器。\n2、如何判断该对象是否是迭代器 1 2 with open(\u0026#39;heian\u0026#39;,mode=\u0026#39;w\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) as f: print(\u0026#39;__iter__\u0026#39; in dir(f) and \u0026#39;__next__\u0026#39; in dir(f)) 3、可迭代对象如何转化成迭代器 1 2 3 4 l1 = [1, 2, 3, 4, 5, 6] obj = l1.__iter__() # 或者 iter(l1)print(obj) # \u0026lt;list_iterator object at 0x000002057FE1A3C8\u0026gt; **　4、 迭代器取值：** 可迭代对象是不可以一直迭代取值的（除去用索引，切片以及Key），但是转化成迭代器就可以了，迭代器是利用__next__()进行取值：\n1 2 3 4 5 6 7 8 9 10 11 12 13 l1 = [1, 2, 3,] obj = l1.__iter__() # 或者 iter(l1) # print(obj) # \u0026lt;list_iterator object at 0x000002057FE1A3C8\u0026gt; ret = obj.__next__() print(ret) ret = obj.__next__() print(ret) ret = obj.__next__() print(ret) ret = obj.__next__() # StopIteration print(ret) # 迭代器利用next取值：一个next取对应的一个值，如果迭代器里面的值取完了，还要next， # 那么就报StopIteration的错误。 5、while模拟for的内部循环机制 1 2 3 4 5 6 7 8 9 10 l1 = [1, 2, 3, 4, 5, 6] # 1 将可迭代对象转化成迭代器 obj = iter(l1) # 2,利用while循环，next进行取值 while 1: # 3,利用异常处理终止循环 try: print(next(obj)) except StopIteration: break 6、迭代器的优缺点 优点：\n1.节省内存\n2.惰性机制。 next一次，取一个值，绝不过多取值。​\n缺点：\n1.速度慢\n2.不走回头路\n1 2 3 4 5 6 7 8 l1 = [1, 2, 3, 4, 5, 6] obj = iter(l1) for i in range(2): print(next(obj)) for i in range(2): print(next(obj)) 3、可迭代对象与迭代器对比 可迭代对象：\n是一个私有的方法比较多，操作灵活（比如列表，字典的增删改查，字符串的常用操作方法等）,比较直观，但是占用内存，而且不能直接通过循环迭代取值的这么一个数据集。\n** 应用**：当你侧重于对于数据可以灵活处理，并且内存空间足够，将数据集设置为可迭代对象是明确的选择。\n** 迭代器：**\n是一个非常节省内存，可以记录取值位置，可以直接通过循环+next方法取值，但是不直观，操作方法比较单一的数据集。\n应用：当你的数据量过大，大到足以撑爆你的内存或者你以节省内存为首选因素时，将数据集设置为迭代器是一个不错的选择。（可参考为什么python把文件句柄设置成迭代器）。\n","date":"2020-11-15T23:53:21Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/202011152353/","title":"Python迭代对象和迭代器"},{"content":"企业微信机器人告警 企业微信群聊里面增加机器人，机器人会提供发送信息的URL\npython 脚本进行实现的\n1、创建企业微信机器人 没有企业微信的可以自己在企业微信官网申请注册个企业，创建企业微信群至少 3 个人以上\n这个 webhook 后面需要使用到\n2、配置 zabbix server 2.1：配置脚本执行目录\n定义脚本目录，我这里就选择了默认的目录\n1 2 [root@zabbix-master ~]# grep -Ev \u0026#39;^$|#\u0026#39; /etc/zabbix/zabbix_server.conf | grep ^A AlertScriptsPath=/usr/lib/zabbix/alertscripts 2.2：创建脚本\n进入该定义的脚本存放路径下创建用来推送告警消息的脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@zabbix-master ~]# [root@zabbix-master ~]# cd /usr/lib/zabbix/alertscripts [root@zabbix-master alertscripts]# vim wechat.py #!/usr/bin/python # -*- coding: utf-8 -*- import requests import json import sys import os headers = {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json;charset=utf-8\u0026#39;} api_url = \u0026#34;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=36d51b45-162f6c9d13909\u0026#34; #这就是先前的webhook地址 def msg(text): json_text= { \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: text }, } print requests.post(api_url,json.dumps(json_text),headers=headers).content if __name__ == \u0026#39;__main__\u0026#39;: text = sys.argv[1] msg(text) ~ 2.3：赋予脚本执行权限\n1 2 3 4 [root@zabbix-master alertscripts]# chmod +x wechat.py [root@zabbix-master alertscripts]# python wechat.py 你好 {\u0026#34;errcode\u0026#34;:0,\u0026#34;errmsg\u0026#34;:\u0026#34;ok\u0026#34;} [root@zabbix-master alertscripts]# 3、zabbix Web 页面配置 3.1：创建报警媒介\n管理\u0026ndash;\u0026gt; 报警媒介类型\u0026ndash;\u0026gt; 创建媒介类型\n新建一个企业微信的报警，脚本名称就是我们脚本名 wechat.py\n3.2：创建动作\n默认标题：\n1 2 3 4 5 6 7 8 9 10 11 12 故障{TRIGGER.STATUS},服务器:{HOSTNAME1}发生: {TRIGGER.NAME}故障! 故障{TRIGGER.STATUS},服务器:{HOSTNAME1}发生: {TRIGGER.NAME}故障! 告警主机:{HOSTNAME1} 告警地址：{HOST.IP} 告警时间:{EVENT.DATE} {EVENT.TIME} 告警等级:{TRIGGER.SEVERITY} 告警信息: {TRIGGER.NAME} 告警项目:{TRIGGER.KEY1} 问题详情:{ITEM.NAME}:{ITEM.VALUE} 当前状态:{TRIGGER.STATUS}:{ITEM.VALUE1} 事件ID:{EVENT.ID} 恢复操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 恢复{TRIGGER.STATUS}, 服务器:{HOSTNAME1}: {TRIGGER.NAME}已恢复! 恢复{TRIGGER.STATUS}, 服务器:{HOSTNAME1}: {TRIGGER.NAME}已恢复! 告警主机:{HOSTNAME1} 告警地址：{HOST.IP} 告警时间:{EVENT.DATE} {EVENT.TIME} 告警等级:{TRIGGER.SEVERITY} 告警信息: {TRIGGER.NAME} 告警项目:{TRIGGER.KEY1} 问题详情:{ITEM.NAME}:{ITEM.VALUE} 当前状态:{TRIGGER.STATUS}:{ITEM.VALUE1} 事件ID:{EVENT.ID} 4、测试发送告警 ","date":"2020-10-30T18:22:15Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/202010301822/","title":"Zabbix配置企业微信群(机器人)警告"},{"content":"目录\n1.配置 yum 源\n2.设定 runlevel 3\n3.精简开机启动服务\n4.配置 sudo 授权管理\n5.ssh 服务\n6.修改 linux 默认字符集\n7.服务器时间同步\n8.加大服务器文件描述符\n9.清理 clientmqueue 垃圾文件防止 inode 被占满\n10.调整内核优化\n11.grep 设置高亮显示\nUlimit管理系统资源\nlinux 上传下载小工具\ntcp/ip 调优\n优化总结\n1.配置 yum 源 1、添加普通用户，使用普通用户 su - root 登陆到 root\n2、设置更新源\nLinux 下方便安装软件的优秀工具叫做 yum 工具，linux 的二进制软件包一般是 rpm 包，类似windows 下的 exe 程序。\n通过 yum 工具安装软件，默认获取 rpm 包的软件配置是从国外 centos 官方源下载。\n因此，我们 yum 安装软件速度会比较慢，因此需要把默认获取 rpm 包的配置从国外官方源改为国内。\n1 2 3 4 5 6 7 8 centos 5.8 64 位 yum 源 http://mirrors.sohu.com/help/CentOS-Base-sohu.repo centos 6.4 64 位 yum 源 http://mirrors.163.com/.help/CentOS6-Base-163.repo cd /etc/yum.repos.d/ //进入 yum 源7 /bin/mv CentOS-Base.repo CentOS-Base.repo.ori //备份 yum 源 wget http://mirrors.sohu.com/help/CentOS-Base-sohu.repo //下载 soho 源 /bin/mv CentOS-Base-sohu.repo CentOS-Base.repo 说明：我们现在使用的是互联网上的门户网站提供的 yum 源，将来我们也可以把 iso 镜像或光盘配\n置成 yum 源，你还可以自己配置一个像门户网站提供的这种 yum 源\n配置公网 yum 源及制作 rpm 包。\n3、使用 yum upgrade 相当于 windows 下的打补丁，这个功能就用到了 yum 源，速度会比较快\n4、安装必要的软件包\n1 yum -y install lrzsz 一、关闭 selinux\n由于安装服务、软件中，经常和 selinux 冲突，在国内生产环境中，都是关掉 selinux。\n1 2 3 4 5 6 vi /etc/selinux/config //配置文件 默认是 enforcing 启用状态 disabled 是完全关闭状态 permissive 是打印警告，selinu 不生效 这种修改只能重启生效 gentenforce 0 临时关闭 selinux，使用 gentenforce 查看当前 selinux 状态 2.设定 runlevel 3\n1 2 runlevel 查看当前系统运行级别 vi /etc/inittab //运行级别配置文件 3.精简开机启动服务\n1、可以使用 setup-system services 里面调整，这样调整起来效率低\n2、或者 ntsysv 调出来\n3、使用脚本一件关闭\n1 2 3 4 5 6 7 8 9 10 #LANG=en #显示出所有服务的所有运行级别的启动状态 8 chkconfig --list #停止所有在运行级别 3 上开机启动的服务 for oldboy in `chkconfig --list|grep3:on |awk \u0026#39;{print $1}\u0026#39;`;do chkconfig --level 3 $oldboyoff;done #在开启常用的服务，crond,network,rsyslog,sshd for oldboy in crond network rsyslogsshd;do chkconfig --level 3 $oldboy on;done #显示出所有 3 运行级别下的所有服务(根据需求决定哪个服务启动) chkconfig --list |grep3:on 刚装完的操作系统，只需要开启几个服务，剩下的以后用到再开，这样安全，遵循最小化原则，没\n用的不启动。\ncrond 定时任务 network 网络服务 sshd 远程服务 syslog 日志服务 4.配置 sudo 授权管理 为什么使用 sudo，如果普通用户使用 su - root 切换到管理员。进行非法操作，比如 passwd root\n修改 root 密码。那么系统其他用户将无法访问系统。这个普通管理员说白了，已经”功高盖主“\n1、在 root 权限下执行 visudo 在 98 行修改。命令使用逗号隔开，使用全路径。\n1 2 3 4 5 # user MACHINE= COMMANDS root ALL =（ALL） ALL 用户 机器=（授权哪个角色的权限） /usr/sbin/useradd,/usr/sbin/passwd 如果不授权，默认是不能执行 useradd 命令的 #su - liuyalei //切换到普通用户 liuyalei$sudo /usr/sbin/useradd test //创建 test 用户 注意，\n前面要加上 sudo，打个比方就是一把钥匙开一把锁。第一次执行 sudo 需要输入普通用户密码，防\n止被非法利用。下次 5 分钟内不需要输入密码\n如果你是运维经理，带一个小弟，不会吧整个 root 的权限都给他，只给他一些普通权限。目的：既\n能让菜的运维干活，又不能威胁系统安全\n如果你是小弟，你的运维经历要把自己的账户提升成 root，那么直接复制，把 root 改成经理账号\n即可。下次经理直接使用 sudo su - root 就能切到 root 权限下\nboss ALL =（ALL） ALL\n这样做，boss 既有了 root 权限还不知道 root 的密码，但是每次切到 root，都要输入 boss 普通用\n户密码，太繁琐了，可以在 visudo 中改成如下，这样就不用输入密码了，但是不安全。\n1 boss ALL =（ALL） NOPASSWD： ALL su 命令总结\n普通用户切换到 root，使用 su - 或者 su - root 需要输入 root 密码\n超级用户切换到普通用户不需要密码。但是 centos5.8 会有环境变量问题，需要给普通用户修改环\n境变量\nsu 优点是给管理服务器带来方便，但是他的缺点就是大家都知道 root 的密码，而且还能改掉 root\n的密码。在一定程度上，对服务器带来了很大的安全隐患。在工作用几乎有一半的问题来自于内部\n5.ssh 服务 linux 默认管理员 root，port 端口号是 22，为了安全，我们要改掉默认的管理员和端口\n配置文件/etc/ssh/sshd_config\n1 2 3 4 5 6 7 8 9 10 [root@oldboy ~]# vi /etc/ssh/sshd_config 添加如下内容保存。 52113#→ssh 连接默认的端口，谁都知道，必须要改。 PermitRootLogin no#→root 用户黑客都知道的，禁止它远程登陆。 PermitEmptyPasswords no #→禁止空密码登陆 UseDNSno#→不使用 DNS GSSAPIAuthentication no 重启 sshd 服务 # /etc/init.d/sshd restart 注意：yum，rpm 安装的软件，启动程序一般都在/etc/init.d\n6.修改 linux 默认字符集 1 2 3 4 [root@eric6 ~]# cat /etc/sysconfig/i18n //查看 linux 默认的字符集，默认是 UTF-8 LANG=\u0026#34;zh_CN.UTF-8\u0026#34; cp /etc/sysconfig/i18n /etc/sysconfig/i18n.ori //备份默认字符集 echo \u0026#39;LANG=\u0026#34;ZH_CN.GB18030\u0026#34;\u0026#39; \u0026gt;/etc/sysconfig/i18n //修改字符集为 GB18030 什么是字符集？\n简单的说就是一套文字符号及其编码。常用的字符集有：\nGBK 定长 双字节 不是国际标准，支持的系统不少\nUTF-8 非定长 1-4 字节 广泛支持，MYSQL 也使用 UTF-8\n这个不一定要修改，有的公司使用的就是 UTF-8，因为 linux 对中文支持不好\n7.服务器时间同步 如果时间不同步，经常带来业务不正常。如果公司规模小，可以使用联网手动同步，如果服务器多，\n可以在公司内部搭建 ntp server。还有一种可能是公司服务器不出外网，比如内部数据库服务器。\n可以在内网搭建两台时间服务器，因为一台可能 down 掉，搭建两台可以做到冗余的目的（50 台-100\n台以上在用）。\n手动同步方法：\n1 2 3 4 5 /sbin/ntpdate time.nist.gov //必须要联网 /usr/sbin/ntpdate time.nist.gov //6.4 在/usr/sbin 下 自动同步方法（每五分钟同步一次）： echo \u0026#39;#time sync by oldboy at 2010-2-1\u0026#39; \u0026gt;\u0026gt;/var/spool/cron/root echo \u0026#39;*/5 * * * * /usr/sbin/ntpdate time.nist.gov \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\u0026#39; \u0026gt;\u0026gt;/var/spool/cron/root 8.加大服务器文件描述符 最简单的说，在 unix/liux 里面，你的服务只要开启一个进程，就要占用文件描述符的。liunx 默认是 1024，如果描述符少了，你的访问量多了，你的服务器支撑不了，所以要把描述符加大。\n1 2 #echo \u0026#39;* - nofile 65535 \u0026#39; \u0026gt;\u0026gt;/etc/security/limits.conf #ulimit -n //查看当前文件描述符数量 有的时候，你的服务器硬盘/内存没那么大，如果文件描述符过大，访问量过来，有可能把服务器搞垮。但是网友们通常都改成 65535\n9.清理 clientmqueue 垃圾文件防止 inode 被占满 1 #find /var/spool/clientmqueue/ -type -f |xargs rm -f 10.调整内核优化 所谓内核优化，主要是在 linux 中针对业务服务应用而进行的系统内核参数优化，优化并无特殊的标准，下面以常见生产环境 linux 的内核优化为例讲解，仅供大家参考：\n内核调优\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #vi /etc/sysctl.cof net.ipv4.tcp_fin_timeout = 2 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_keepalive_time = 600 net.ipv4.ip_local_port_range = 4000 65000 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.route.gc_timeout = 100 net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_synack_retries = 1 net.core.somaxconn = 16384 12 net.core.netdev_max_backlog = 16384 net.ipv4.tcp_max_orphans = 16384 #以下参数是对 iptables 防火墙的优化，防火墙不开会提示，可以忽略不理\n1 2 3 4 5 6 net.ipv4.ip_conntrack_max = 25000000 net.ipv4.netfilter.ip_conntrack_max=25000000 net.ipv4.netfilter.ip_conntrack_tcp_timeout_established=180 net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait=120 net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait=60 net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait=120 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [root@eric6 ~]# sysctl -p //使配置文件生效 net.ipv4.ip_forward = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.default.accept_source_route = 0 kernel.sysrq = 0 kernel.core_uses_pid = 1 net.ipv4.tcp_syncookies = 1 error: \u0026#34;net.bridge.bridge-nf-call-ip6tables\u0026#34; is an unknown key error: \u0026#34;net.bridge.bridge-nf-call-iptables\u0026#34; is an unknown key error: \u0026#34;net.bridge.bridge-nf-call-arptables\u0026#34; is an unknown key kernel.msgmnb = 65536 kernel.msgmax = 65536 kernel.shmmax = 68719476736 kernel.shmall = 4294967296 net.ipv4.tcp_fin_timeout = 2 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 13 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_keepalive_time = 600 net.ipv4.ip_local_port_range = 4000 65000 net.ipv4.tcp_max_syn_backlog = 16384 net.ipv4.tcp_max_tw_buckets = 36000 net.ipv4.route.gc_timeout = 100 net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_synack_retries = 1 net.core.somaxconn = 16384 net.core.netdev_max_backlog = 16384 net.ipv4.tcp_max_orphans = 16384 error: \u0026#34;net.ipv4.ip_conntrack_max\u0026#34; is an unknown key error: \u0026#34;net.ipv4.netfilter.ip_conntrack_max\u0026#34; is an unknown key error: \u0026#34;net.ipv4.netfilter.ip_conntrack_tcp_timeout_established\u0026#34; is an unknown key error: \u0026#34;net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait\u0026#34; is an unknown key error: \u0026#34;net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait\u0026#34; is an unknown key error: \u0026#34;net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait\u0026#34; is an unknown key 防火墙未开启报错，不用管,5.8 的话，不会报错\n11.grep 设置高亮显示 1 2 3 [root@eric ~]# vi /etc/profile alias grep=\u0026#39;grep --color=auto\u0026#39; [root@eric ~]# source /etc/profile Ulimit管理系统资源 具体的 options 含义以及简单示例可以参考以下表格。\nlinux 上传下载小工具 1 yum install lrzsz -y tcp/ip 调优 sysctl 变量修改方法：sysctl –a\n使用 sysctl 命令修改系统变量，和通过编辑 sysctl.conf 文件来修改系统变量两种。但并不是所有的\n变量都可以在这个模式下设定。\n注：sysctl 变量的设置通常是字符串、数字或者布尔型。 (布尔型用 1 来表示\u0026rsquo;yes\u0026rsquo;，用 0 来表示\u0026rsquo;no\u0026rsquo;)。\n1 2 3 4 5 6 7 8 9 10 [root@localhost ~]#sysctl -w net.ipv4.tcp_keepalive_time=30 [root@localhost ~]#sysctl -w net.ipv4.tcp_keepalive_probes=2 [root@localhost ~]#sysctl -w net.ipv4.tcp_keepalive_intvl=2 [root@localhost~]# sysctl -a | grep keepalive net.ipv4.tcp_keepalive_time= 30 net.ipv4.tcp_keepalive_probes= 2 net.ipv4.tcp_keepalive_intvl= 2 执行以下命令使变动立即生效： [root@localhost ~]# sysctl centos 6.4 安装软件包：\n软件自定义-基本\n性能工具\n调试工具兼容程序库 开发工具\n服务器\u0026mdash;-系统管理工具\n系统管理\u0026mdash;snmp/系统管理\n优化总结\n01）添加普通用户，通过 sudo 授权管理 02）定时自动更新服务器时间 03）配置 yum 更新源 04）关闭 selinux 和 iptables 05）调整文件描述符数量 06）定时自动清理/var/spool/clientmquene/目录垃圾文件，防止 inodes 节点被占满 07）精简开机自动启动服务（sshd，crond，network，syslog） 08）内核参数优化/etc/sysctl.config，sysctl -p 生效 09）更改默认 ssh 服务端口，及禁止 root 用户远程登陆 10）更改字符集，支持中文 11）锁定关键系统文件 1 2 3 4 5 chattr +i /etc/passwd chattr +i /etc/inittab chattr +i /etc/group chattr +i /etc/shadow chattr +i /etc/gshadow 12）清空/etc/issue，去除系统及内核版本登陆前的屏幕显示 13）更改系统登录后的信息 /etc/motd ","date":"2020-09-19T21:55:05Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202009192155/","title":"Linux 安装后基本优化操作"},{"content":"上一章节，讲了自己悲剧的删库事件。\n原因总结：\n（1）手贱\n（2）还是手贱\n（3）不过大脑\n（4）Linux没有回收站功能\n俗话说，吃一堑长一智。接下来就是我的解决方案。\n给Linux添加一个回收站的功能，这是不是很高大上啊\n这下就不怕手贱删库了\n手误【删库】 == 跑路，不存在的 ——删瓦辛格 删除是危险系数很高的操作，一旦误删可能会造成难以估计的损失。\n在 Linux 系统中这种危险尤为明显，一条简单的语句：rm –rf /* 就会把整个系统全部删除，而 Linux 并不会因为这条语句的不合理而拒绝执行。\n这时，有个像Windows的那样的回收站是多么的重要啊。下面可以使用代码实现\nrm命令修改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim /etc/bashrc alias rm=delete #命令别名，通过delete来实现rm改为mv alias r=delete alias rl=\u0026#39;ls /trash\u0026#39; #rl 命令显示回收站中的文件 alias ur=undelfile #ur 命令找回回收站的文件 undelfile() { mv /trash/$@ ./ } delete() { if [ ! -d \u0026#34;/trash/\u0026#34; ];then mkdir /trash fi \\mv --backup=numbered $@ /trash/ } cleartrash() { read -p \u0026#34;clear sure?[n]\u0026#34; confirm [ $confirm == \u0026#39;y\u0026#39; ] || [ $confirm == \u0026#39;Y\u0026#39; ] \u0026amp;\u0026amp; /bin/rm -rf /trash/* } 添加完毕后保存，执行source命令生效\n1 source /etc/bashrc 使用方法 以使用rm（删除），ur（撤销），rl（列出回收站），cleartrash（清空回收站）命令了。\n删除一个文件夹，hellworld下面的文件均被移到回收站中\n1 rm helloworld/ 或者 r helloworld/ 或者 delete helloworld/ 删除一个文件\n1 rm 123.txt 或者 r 123.txt 或者 delete 123.txt 列出回收站信息\n要查看回收站内容详细信息，只要加个参数就好\n撤销123.txt\n1 ur 123.txt 或者 undelfile 123.txt 撤销helloworld文件夹\n1 ur helloworld 或者 undelfile helloworld 清空回收站\n1 2 3 [root@Master ~]# cleartrash #会弹出是否清空 clear sure?[n]y [root@Master ~]# 删库，我害怕删库吗 ","date":"2020-07-27T23:14:37Z","image":"https://www.ownit.top/title_pic/55.jpg","permalink":"https://www.ownit.top/p/202007272314/","title":"手误【删库】 == 跑路，不存在的    Linux回收站"},{"content":"数据备份是容灾的基础，是指为防止系统出现操作失误或系统故障导致数据丢失，而将全部或部分数据集合从应用主机的硬盘或阵列复制到其它的存储介质的过程。传统的数据备份主要是采用内置或外置的磁带机进行冷备份。但是这种方式只能防止操作失误等人为故障，而且其恢复时间也很长。随着技术的不断发展，数据的海量增加，不少的企业开始采用网络备份。网络备份一般通过专业的数据存储管理软件结合相应的硬件和存储设备来实现。\n实现基础架构图\n1、基本备份要求 已知3台服务器主机名分别为web01，backup、nfs01，主机信息见下表：\n要求：\n每天晚上00点整在Web服务器上打包备份系统配置文件、网站程序目录及访问日志，\n通过rsync命令推送备份服务器backup上备份保留\n（备份思路可以是先在本地按日期打包，然后再推到备份服务器backup上）\n具体要求如下：\n1）Web服务器和备份服务器的备份目录必须都为/backup\n2）要备份的系统配置文件包括但不限于：\na.定时任务服务的配置文件（var/spool/cron/root）。.\nb.开机自启动的配置文件（/etc/rc.local）\nc.日常脚本的目录（/server/scripts）\nd.防火墙iptables的配置文件（/etc/sysconfig/iptables）。.\ne.自己思考下还有什么需要备份呢？\n3）Web服务器站点目录假定为（var/htm/www）.\n4）Web服务器A访问日志路径假定为（/app/logs）.\n5）Web服务器保留打包后的7天的备份数据即可（本地留存不能多于7天，因为太多硬盘会满）\n6）备份服务器上，保留最近7天的备份数据，同时保留6个月内每周一的所有数据副本。\n7）备份服务器上要按照备份数据服务器的内网IP为目录保存备份，备份的文件按照时间名字保存。\n8）需要确保备份的数据尽量完整性，在备份服务器上对备份的数据进行检查，把备份的成功及失败结果信息发给系统管理员邮箱中\n数据备份项目实战：\n解题思路\n1、搭建backup服务器\na、resync服务器\n2、搭建web01服务器\na、验证rsync服务器能否推送成功\nb、开发脚本实现打包，备份，推送，校检，删除\nc、配置定时任务每天0点定时推送\n3、backup服务器\na、开发脚本实现校检，删除、报警\nb、配置定时任务每天6点定时执行\n4、同理搭建存储nfs01服务器\n2、项目实施 配置所有hosts服务器 1 2 3 4 5 cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt;EOF 192.168.116.129 web 192.168.116.130 backup 192.168.116.131 nfs EOF backup服务器安装rsync 1 yum install -y rsync 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@backup ~]# useradd -s /sbin/nologin -M rsync [root@backup ~]# id rsync uid=1001(rsync) gid=1001(rsync) 组=1001(rsync) [root@backup ~]# vim /etc/rsyncd.conf port=873 uid = rsync gid = rsync use chroot = no max connections = 200 timeout = 300 motd file = /var/rsyncd/rsync.motd pid file = /var/run/rsyncd.pid lock file = /var/run/rsync.lock log file = /var/log/rsyncd.log dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 [backup] path = /backup/ ignore errorsread only = false write only = false list = falsehosts allow = 192.168.116.0/24 hosts deny = 0.0.0.0/32 auth users = rsync_backup secrets file = /etc/rsyncd.passwd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ######### 全局配置参数 ########## port=888 # 指定rsync端口。默认873 uid = rsync # rsync服务的运行用户，默认是nobody，文件传输成功后属主将是这个uid gid = rsync # rsync服务的运行组，默认是nobody，文件传输成功后属组将是这个gid use chroot = no # rsync daemon在传输前是否切换到指定的path目录下，并将其监禁在内 max connections = 200 # 指定最大连接数量，0表示没有限制 timeout = 300 # 确保rsync服务器不会永远等待一个崩溃的客户端，0表示永远等待 motd file = /var/rsyncd/rsync.motd # 客户端连接过来显示的消息 pid file = /var/run/rsyncd.pid # 指定rsync daemon的pid文件 lock file = /var/run/rsync.lock # 指定锁文件 log file = /var/log/rsyncd.log # 指定rsync的日志文件，而不把日志发送给syslog dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 # 指定哪些文件不用进行压缩传输 ###########下面指定模块，并设定模块配置参数，可以创建多个模块########### [longshuai] # 模块ID path = /longshuai/ # 指定该模块的路径，该参数必须指定。启动rsync服务前该目录必须存在。rsync请求访问模块本质就是访问该路径。 ignore errors # 忽略某些IO错误信息 read only = false # 指定该模块是否可读写，即能否上传文件，false表示可读写，true表示可读不可写。所有模块默认不可上传 write only = false # 指定该模式是否支持下载，设置为true表示客户端不能下载。所有模块默认可下载 list = false # 客户端请求显示模块列表时，该模块是否显示出来，设置为false则该模块为隐藏模块。默认true hosts allow = 10.0.0.0/24 # 指定允许连接到该模块的机器，多个ip用空格隔开或者设置区间 hosts deny = 0.0.0.0/32 # 指定不允许连接到该模块的机器 auth users = rsync_backup # 指定连接到该模块的用户列表，只有列表里的用户才能连接到模块，用户名和对应密码保存在secrts file中， # 这里使用的不是系统用户，而是虚拟用户。不设置时，默认所有用户都能连接，但使用的是匿名连接 secrets file = /etc/rsyncd.passwd # 保存auth users用户列表的用户名和密码，每行包含一个username:passwd。由于\u0026#34;strict modes\u0026#34; # 默认为true，所以此文件要求非rsync daemon用户不可读写。只有启用了auth users该选项才有效。 [xiaofang] # 以下定义的是第二个模块 path=/xiaofang/ read only = false ignore errors comment = anyone can access 授权\n1 2 3 4 [root@backup ~]# mkdir -p /backup [root@backup ~]# chown -R rsync.rsync /backup/ [root@backup ~]# ls -ld /backup/ drwxr-xr-x 2 rsync rsync 6 6月 17 10:39 /backup/ 配置密码\n1 2 3 4 5 6 7 8 ​ [root@backup ~]# touch /etc/rsyncd.passwd [root@backup ~]# echo \u0026#34;rsync_backup:root\u0026#34; \u0026gt; /etc/rsyncd.passwd [root@backup ~]# chmod 600 /etc/rsyncd.passwd [root@backup ~]# ls -ld /etc/rsyncd.passwd -rw------- 1 root root 18 6月 17 10:42 /etc/rsyncd.passwd ​ 重启看是否成功\n1 2 3 4 [root@backup ~]# rsync --daemon [root@backup ~]# ss -lntup | grep rsync tcp LISTEN 0 5 *:873 *:* users:((\u0026#34;rsync\u0026#34;,pid=2280,fd=4)) tcp LISTEN 0 5 :::873 :::* users:((\u0026#34;rsync\u0026#34;,pid=2280,fd=5)) 开机自启动\n1 2 3 4 5 [root@backup ~]# which rsync /usr/bin/rsync [root@backup ~]# echo \u0026#34;/usr/bin/rsync --daemon\u0026#34; \u0026gt;\u0026gt; /etc/rc.local [root@backup ~]# tail -1 /etc/rc.local /usr/bin/rsync --daemon web服务器测试 rsync服务器接收客户端的请求\n下面测试，显示成功\n1 2 3 4 5 6 7 8 9 [root@web backup]# echo \u0026#34;root\u0026#34; \u0026gt;\u0026gt; /etc/rsync.password [root@web backup]# chmod 600 /etc/rsync.password [root@web backup]# rsync -avz /backup/ rsync://rsync_backup@192.168.116.130/backup --password-file=/etc/rsy nc.password sending incremental file list rsync: chgrp \u0026#34;.\u0026#34; (in backup) failed: Operation not permitted (1) ./ mysql-5.7.28-linux-glibc2.12-x86_64.tar.gz 另一种传输方法 1 rsync -avz /backup/ rsync_backup@192.168.116.130::backup --password-file=/etc/rsync.password web服务器，创建模拟数据\n1 2 3 4 5 6 [root@web backup]# [root@web backup]# mkdir /var/html/www -p [root@web backup]# cd /var/html/www/ [root@web www]# touch {1..10} [root@web www]# mkdir -p /app/logs [root@web www]# touch /app/logs/{a..g} 备份脚本存放目录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [root@web www]# mkdir -p /server/scripts vim /server/scripts/bak.sh #!/bin/sh export PATH=/sbin:/bin:/usr/sbin:/usr/bin IP=$(ifconfig ens33|awk -F \u0026#34;[ :]+\u0026#34; \u0026#39;NR==2{print $3}\u0026#39;) BakPath=/backup mkdir $BakPath/$IP -p if [ $(date +%w) -eq 2 ];then date=\u0026#34;$(date +%F -d \u0026#34;-1day\u0026#34;)_week1\u0026#34; else date=\u0026#34;$(date +%F -d \u0026#34;-1day\u0026#34;)\u0026#34; fi cd / \u0026amp;\u0026amp;\\ tar zcfh $BakPath/$IP/sys_config_${date}.tar.gz var/spool/cron etc/rc.local server/scripts \u0026amp;\u0026amp;\\ tar zcfh $BakPath/$IP/webdata_${date}.tar.gz var/html/www/ \u0026amp;\u0026amp;\\ tar zcf $BakPath/$IP/access_log_${date}.tar.gz app/logs \u0026amp;\u0026amp;\\ find $BakPath -type f -name \u0026#34;*.tar.gz\u0026#34;|xargs md5sum \u0026gt;$BakPath/$IP/flag_${date} ###bak data rsync -az $BakPath/ rsync_backup@192.168.116.130::backup --password-file=/etc/rsync.password ###del data 7 days ago. find $BakPath -type f -mtime +7|xargs rm -f 数据校检MD5sum\n定时任务\n1 2 #backup 00 00 * * * /bin/sh /server/scripts/bak.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 Backup服务器校检，删除，报警 backup备份推送类容\n邮件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 yum install mailx -y vim /etc/mail.rc #设置发件人名称 set from=32915245@qq.com #设置邮件服务器 set smtp=smtp.qq.com #填写自己邮箱地址 set smtp-auth-user=3291245@qq.com #输入邮箱验证码 set smtp-auth-password=wrgcmeapjw #smtp的认证方式，默认是login set smtp-auth=login 测试【已经完成】\n1 echo \u0026#34;admin ,文件内容\u0026#34; | mail -s \u0026#34;标题\u0026#34; 你的qq@qq.com 检验文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 vim /server/scripts/checkbak.sh #!/bin/sh export LANG=en find /backup/ -name \u0026#34;flag_$(date +%F -d \u0026#34;-1day\u0026#34;)*\u0026#34;|xargs md5sum -c \u0026amp;\u0026gt;\u0026gt;/tmp/mail_$(date +%F).log if [ $(date +%w) -eq 2 ];then date=\u0026#34;$(date +%F -d \u0026#34;-1day\u0026#34;)_week1\u0026#34; else date=\u0026#34;$(date +%F -d \u0026#34;-1day\u0026#34;)\u0026#34; fi find /backup/ -type f -name \u0026#34;*.tar.gz\u0026#34; -a ! -name \u0026#34;*week1*\u0026#34; -mtime +1|xargs rm -f mail -s \u0026#34;backup `date`\u0026#34; 1794748404@qq.com \u0026lt;/tmp/mail_$(date +%F).log \\cp /tmp/mail_$(date +%F).log /tmp/mail_$(date +%F).log.ori \u0026gt;/tmp/mail_$(date +%F).log 定时任务\n1 2 #backup 00 06 * * * /bin/sh /server/scripts/checkbak.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 正确\n错误\n然后可以把脚本使用ansible批量发布各个主机，然后就可以大规模集群备份\n","date":"2020-07-25T17:59:29Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/202007251759/","title":"大规模集群全网数据备份解决方案"},{"content":"Nginx配置防盗链 1.什么是资源盗链 简单地说，就是某些不法网站未经许可，通过在其自身网站程序里非法调用其他网站的资源，然后在自己的网站上显示这些调用的资源，达到填充自身网站的效果。这一举动不仅浪费了调用资源网站的网络流量，还造成其他网站的带宽及服务压力吃紧，甚至宕机。\n** 2.网站资源被盗链带来的问题** 若网站图片及相关资源被盗链，最直接的影响就是网络带宽占用加大了，带宽费用多了，网络流量也可能忽高忽低，Nagios/Zabbix等报警服务频繁报警\n最严重的情况就是网站的资源被非法使用，使网站带宽成本加大和服务器压力加大，这有可能导致数万元的损失，且网站的正常用户访问也会受到影响\n3.企业真实案例：网站资源被盗链，出现严重问题 某日，接到从事运维工作的朋友的紧急求助，其公司的CDN源站，源站的流量没有变动，但CDN加速那边的流量无故超了好几个GB，不知道怎么处理。\n该故障的影响：由于是购买的CDN网站加速服务，因此虽然流量多了几个GB，但是业务未受影响。只是，这么大的异常流量，持续下去可直接导致公司无故损失数万元。解决这个问题可体现运维的价值。\n4.常见防盗链解决方案的基本原理 （1）根据HTTP referer实现防盗链 在HTTP协议中，有一个表头字段叫referer，使用URL格式来表示是哪里的链接用了当前网页的资源。通过referer可以检测访问的来源网页，如果是资源文件，可以跟踪到显示它的网页地址，一旦检测出来源不是本站，马上进行阻止或返回指定的页面。\nHTTP referer是header的一部分，当浏览器向Web服务器发送请求时，一般会带上referer，告诉服务器我是从哪个页面链接过来的，服务器借此获得一些信息用于处理。Apache、Nginx、Lighttpd三者都支持根据HTTP referer实现防盗链，referer是目前网站图片、附件、html等最常用的防盗链手段\n（2）根据cookie防盗链 对于一些特殊的业务数据，例如流媒体应用通过ActiveX显示的内容（例如，Flash、Windows Media视频、流媒体的RTSP协议等），因为它们不向服务器提供referer header，所以若采用上述的referer的防盗链手段，就达不到想要的效果。\n对于Flash、Windows Media视频这种占用流量较大的业务数据，防盗链是比较困难的，此时可以采用Cookie技术，解决Flash、Windows Media视频等的防盗链问题。\n例如：ActiveX插件不传递referer，但会传递Cookie，可以在显示ActiveX的页面的\u0026lt;head\u0026gt;\u0026lt;/head\u0026gt;标签内嵌入一段JavaScript代码，设置“Cookie：Cache=av”如下：\n1 2 3 4 5 6 7 \u0026lt;script\u0026gt; document.cookie=\u0026#34;Cache=av； domain=domain.com； path=/\u0026#34;； \u0026lt;/script\u0026gt; 然后就可以通过各种手段来判断这个Cookie的存在，以及验证其值的操作了。\n根据Cookie来防盗链的技术非本书的内容，读者了解即可，如果企业确实有需要，可以阅读其他书籍或进入交流群获取这部分的知识。\n（3）通过加密变换访问路径实现防盗链 此种方法比较适合视频及下载类业务数据的网站。例如：Lighttpd有类似的插件mod_secdownload来实现此功能。先在服务器端配置此模块，设置一个固定用于加密的字符串，比如oldboy，然后设置一个url前缀，比如/mp4/，再设置一个过期时间，比如1小时，然后写一段PHP代码，利用加密字符串和系统时间等通过md5算法生成一个加密字符串。最终获取到的文件的URL链接中会带有一个时间戳和一个加密字符的md5数值，在访问时系统会对这两个数据进行验证。如果时间不在预期的时间段内（如1小时内）则失效；如果时间戳符合条件，但是加密的字符串不符合条件也会失效，从而达到防盗链的效果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;php $secret = \u0026#34;oldboy\u0026#34;； // 加密字符串，必须和 lighttpd.conf里的保持一致 $uri_prefix = \u0026#34;/mp4/\u0026#34;； // 虚拟的路径、前缀，必须和 lighttpd.conf里的保持一致 $file = \u0026#34;/test.mp4\u0026#34;； // 实际文件名，必须加 \u0026#34;/\u0026#34;斜杠 $timestamp = time（）； // current timestamp $t_hex = sprintf（ \u0026#34;%08x\u0026#34;， $timestamp）； $m = md5（ $secret.$file.$t_hex）； printf（ \u0026#39;%s\u0026#39;， $uri_prefix， $m， $t_hex， $file， $file）； //生成 url地址串 \u0026gt; Nginx Web服务实现防盗链实战 在默认情况下，只需要进行简单的配置，即可实现防盗链处理\n利用referer，并且针对扩展名rewrite重定向\n第一步\n第二步\n第三步\n相关配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 配置前 location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$ { expires 30d; error_log off; access_log /dev/null; } 配置后 location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$ { valid_referers *.heian99.top heian99.top; if ($invalid_referer) { rewrite ^/ https://ftp.bmp.ovh/imgs/2020/06/75d45131a596abbd.jpg; #return 404; } expires 30d; } 效果图\n打开这个图片，放到别的浏览器查看\n就会变成下面这个图片了\n","date":"2020-07-13T10:34:13Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/202007131034/","title":"宝塔Nginx配置防盗链"},{"content":"目录\n1、环境要求\n2、架构工作原理\n2.1架构介绍:\n2.2 MHA软件构成\n3、Mysql环境搭建\n3.1环境准备（主从都需要下面步骤）\n3.2用户的创建处理原始环境\n3.3解压文件，更改文件目录\n3.4设置环境变量\n3.5环境目录规划\n3.6my.cnf配置文件\n3.7mysql数据库初始化\n3.8启动数据库2种方式\n1. sys-v\n2. systemd\n3.9修改数据库的密码\n4、mysql主从配置\n1、主库创建用户（db01）\n2、从库开启连接（db02）\n5、MHA环境搭建\n5.1配置关键程序软连接\n5.2配置各个节点互信\n5.3安装MHA软件\n1、所有节点安装Node软件依赖包\n2、在db01主库中创建mha需要的用户\n3、Manager软件安装（db02）\n5.4配置文件准备（db02）\n5.5状态检查\n互信检查\n主从状态检查\n5.6开启HMA（db02）\n开启\n检测MHA状态\n6、MHA 的vip功能\n参数\n修改脚本内容\n更改manager配置文件：\n主库上，手工生成第一个vip地址\n重启mha\n7、 binlog server（db02）\n参数：\n创建必要目录\n拉取主库binlog日志\n重启MHA\n8、邮件提醒\n1. 参数\n2. 准备邮件脚本\n3. 修改manager配置文件，调用邮件脚本\n重启MHA\n9、测试MHA\n关闭主库,看警告邮件 1、环境要求 MHA实施文档： MHA实施文档.pdf-群集服务文档类资源-CSDN下载\nMHA实施软件集合： MHA实施文档.zip-群集服务文档类资源-CSDN下载（包含所用到的软件）\n****系统：****CentOS Linux release 7.4.1708 (Core)\nMyssql：5****.7.20****\n****MHA：****mha4mysql-manager-0.56-0.el6.noarch\n**** mha4mysql-node-0.56-0.el6.noarch****\n主机\nIP\n端口\n主库（db01）\n172.17.1.145\n3306\n从库（db02）\n172.17.1.146\n3306\n虚拟ip（vrrp漂移）\n172.17.1.100\n2、架构工作原理 主库宕机处理过程\n1. 监控节点 (通过配置文件获取所有节点信息)\n系统,网络,SSH连接性\n主从状态,重点是主库\n2. 选主\n(1) 如果判断从库(position或者GTID),数据有差异,最接近于Master的slave,成为备选主\n(2) 如果判断从库(position或者GTID),数据一致,按照配置文件顺序,选主.\n(3) 如果设定有权重(candidate_master=1),按照权重强制指定备选主.\n1. 默认情况下如果一个slave落后master 100M的relay logs的话，即使有权重,也会失效.\n2. 如果check_repl_delay=0的化,即使落后很多日志,也强制选择其为备选主\n3. 数据补偿\n(1) 当SSH能连接,从库对比主库GTID 或者position号,立即将二进制日志保存至各个从节点并且应用(save_binary_logs )\n(2) 当SSH不能连接, 对比从库之间的relaylog的差异(apply_diff_relay_logs)\n4. Failover\n将备选主进行身份切换,对外提供服务\n其余从库和新主库确认新的主从关系\n5. 应用透明(VIP)\n6. 故障切换通知(send_reprt)\n7. 二次数据补偿(binlog_server)\n2**.1架构介绍:****** 1主1从，master：db01 slave：db02 ）：\nMHA 高可用方案软件构成\nManager软件：选择一个从节点安装\nNode软件：所有节点都要安装\n2**.2 MHA软件构成****** anager工具包主要包括以下几个工具：\nmasterha_manger 启动MHA\nmasterha_check_ssh 检查MHA的SSH配置状况\nmasterha_check_repl 检查MySQL复制状况\nmasterha_master_monitor 检测master是否宕机\nmasterha_check_status 检测当前MHA运行状态\nmasterha_master_switch 控制故障转移（自动或者手动）\nmasterha_conf_host 添加或删除配置的server信息\nNode工具包主要包括以下几个工具：\n这些工具通常由MHA Manager的脚本触发，无需人为操作\nsave_binary_logs 保存和复制master的二进制日志\napply_diff_relay_logs 识别差异的中继日志事件并将其差异的事件应用于其他的\npurge_relay_logs 清除中继日志（不会阻塞SQL线程）\n3、Mysql环境搭建 3**.**1环境准备（主从都需要下面步骤） 创建目录，上传所需要的文件（主从都需要上传）\n1 2 3 [root@db01 ~]# mkdir -p /tools/mysql [root@db01 ~]# cd /tools/mysql [root@db01 mysql]# scp * root@172.17.1.146:/tools/mysql/ 3**.2用户的创建处理原始环境****** 1 2 3 4 [root@db01 ~]# rpm -qa |grep mariadb [root@db01 ~]# yum remove mariadb-libs-5.5.56-2.el7.x86_64 -y 添加mysql用户 [root@db01 ~]# useradd -s /sbin/nologin mysql 3.3解压文件，更改文件目录**** 1 2 3 4 5 6 存放mysql程序目录 [root@db01 mysql]# mkdir -p /app 解压 [root@db01 mysql]# tar xf mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 移动 [root@db01 mysql]# mv mysql-5.7.20-linux-glibc2.12-x86_64 /app/mysql 3**.4设置环境变量****** 1 2 3 4 5 6 7 8 9 10 vim /etc/profile export PATH=/app/mysql/bin:$PATH [root@db01 ~]# source /etc/profile [root@db01 ~]# mysql -V mysql Ver 14.14 Distrib 5.7.20, for linux-glibc2.12 (x86_64) using EditLine wrapper 3**.5**环境目录规划 创建文件，并授权\n1 2 3 4 5 [root@db01 mysql]# [root@db01 mysql]# mkdir -p /data/{mysql,binlog} [root@db01 mysql]# mkdir -p /data/mysql/data [root@db01 mysql]# chown -R mysql.mysql /app/mysql/* [root@db01 mysql]# chown -R mysql.mysql /data/* 错误日志存放\n1 2 3 4 [root@db01 ~]# touch /var/log/mysql.log [root@db01 ~]# chown mysql.mysql /var/log/mysql.log [root@db01 ~]# ll /var/log/mysql.log -rw-r--r-- 1 mysql mysql 0 6月 21 20:38 /var/log/mysql.log Sock环境配置\n1 2 [root@db01 data]# touch /tmp/mysql.sock [root@db01 data]# chown mysql.mysql /tmp/mysql.sock 慢日志（有需要可以加下面参数）\n1 2 3 4 5 6 7 8 开关: slow_query_log=1 文件位置及名字 slow_query_log_file=/data/mysql/slow.log 设定慢查询时间: long_query_time=0.1 没走索引的语句也记录: log_queries_not_using_indexes 3**.6**my.cnf配置文件 主库server_id=145\n从库server_id=146\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [mysqld] basedir=/data/mysql datadir=/data/mysql/data socket=/tmp/mysql.sock #错误日志 log_error=/var/log/mysql.log log_timestamps=system #server_id server_id=145 port=3306 secure-file-priv=/tmp autocommit=0 log_bin=/data/binlog/mysql-bin binlog_format=row #GTID gtid-mode=on enforce-gtid-consistency=true log-slave-updates=1 # 允许最大连接数 max_connections=200 # 服务端使用的字符集默认为8比特编码的latin1字符集 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB [mysql] socket=/tmp/mysql.sock prompt=db01 [\\d]\u0026gt; 3**.7**mysql数据库初始化 1 [root@db01 ~]# mysqld --initialize-insecure --user=mysql --basedir=/app/mysql --datadir=/data/mysql/data 3.8启动数据库2种方式**** 1. sys-v 1 2 3 4 5 6 7 [root@db01 data]# cp /app/mysql/support-files/mysql.server /etc/init.d/mysqld [root@db01 data]# vim /etc/init.d/mysqld [root@db01 data]# grep -Ev \u0026#34;^(#|$)\u0026#34; /etc/init.d/mysqld basedir=/app/mysql datadir=/data/mysql/data ……………………………………….. 1 2 3 [root@db02 mysql]# service mysqld restart [root@db02 mysql]# service mysqld stop [root@db02 mysql]# service mysqld start 1 2 3 [root@db02 mysql]# /etc/init.d/mysqld restart [root@db02 mysql]# /etc/init.d/mysqld stop [root@db02 mysql]# /etc/init.d/mysqld start 2. systemd 注意： sysv方式启动过的话，需要先提前关闭，才能以下方式登录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026gt;/etc/systemd/system/mysqld.service \u0026lt;\u0026lt;EOF [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/app/mysql/bin/mysqld --defaults-file=/etc/my.cnf LimitNOFILE = 5000 EOF 1 2 3 [root@db02 mysql]# systemctl restart mysqld [root@db02 mysql]# systemctl stop mysqld [root@db02 mysql]# systemctl start mysqld 3**.9**修改数据库的密码 ****注意：****5.8以上数据库，需要先创用户，授权\n5.7的数据库，你授权时，就行给你创建用户\n（坑：在更改密码时，要注意空格，防止密码里有空格，而自己没注意\n1 2 3 4 5 6 7 use mysql; 本地连接密码 grant all on *.* to root@\u0026#39;localhost\u0026#39; identified by \u0026#39;xdzh@2020\u0026#39;; 同一网段可连接的root权限 grant all on *.* to root@\u0026#39;172.17.1.%\u0026#39; identified by \u0026#39;xdzh@2020\u0026#39;; 刷新权限表 flush privileges; 本地登录测试\n4、mysql主从配置 1、主库创建用户（db01**）****** 账号密码可以自己定义\n1 2 grant replication slave on *.* to repl@\u0026#39;172.17.1.%\u0026#39; identified by \u0026#39;123\u0026#39;; flush privileges; 2、从库开启连接（db02******）****** 执行语句，连接主库，同步数据\n1 2 3 4 5 change master to master_host=\u0026#39;172.17.1.145\u0026#39;, master_user=\u0026#39;repl\u0026#39;, master_password=\u0026#39;123\u0026#39; , MASTER_AUTO_POSITION=1; 开启从库\n1 start slave; 查看用户\n******5、**MHA环境搭建 规划\n主机\nMHA软件\n主库（db01）\nNode\n从库（db02）\nNode，Master\n5**.1**配置关键程序软连接 注意：一定要配置，不然后面数据库切换会出现问题（主从都配置）\n1 2 ln -s /app/mysql/bin/mysqlbinlog /usr/bin/mysqlbinlog ln -s /app/mysql/bin/mysql /usr/bin/mysql 5**.2**配置各个节点互信 配置SSH\n1 2 3 4 5 6 db01： rm -rf /root/.ssh ssh-keygen cd /root/.ssh mv id_rsa.pub authorized_keys scp -r /root/.ssh 172.17.1.146:/root 各节点验证：\n1 2 3 4 5 6 7 db01: ssh 172.17.1.145 date ssh 172.17.1.146 date db02: ssh 172.17.1.145 date ssh 172.17.1.146 date 主库\n从库\n5**.3**安装MHA软件 mha官网：https://code.google.com/archive/p/mysql-master-ha/\ngithub下载地址：https://github.com/yoshinorim/mha4mysql-manager/wiki/Downloads\n******1、**所有节点安装Node软件依赖包 1 2 yum install perl-DBD-MySQL -y rpm -ivh mha4mysql-node-0.56-0.el6.noarch.rpm ******2、**在db01主库中创建mha需要的用户 账号密码可以自己定义\n1 grant all privileges on *.* to mha@\u0026#39;172.17.1.%\u0026#39; identified by \u0026#39;mha\u0026#39;; 3、Manager软件安装（db02） 注意：这边如果yum安装缺少依赖，换成阿里云的源和epel\n1 2 yum install -y perl-Config-Tiny epel-release perl-Log-Dispatch perl-Parallel-ForkManager perl-Time-HiRes rpm -ivh mha4mysql-manager-0.56-0.el6.noarch.rpm 5**.4配置文件准备（db02********）****** 1 2 3 4 创建配置文件目录 mkdir -p /etc/mha 创建日志目录 mkdir -p /var/log/mha/app1 编辑mha配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 vim /etc/mha/app1.cnf [server default] manager_log=/var/log/mha/app1/manager manager_workdir=/var/log/mha/app1 master_binlog_dir=/data/binlog user=mha password=mha ping_interval=2 repl_password=123 repl_user=repl ssh_user=root [server1] hostname=172.17.1.145 port=3306 [server2] hostname=172.17.1.146 port=3306 5**.5状态检查****** 1 2 3 4 5 6 7 检测repl状态 masterha_check_repl --conf=/etc/mha/app1.cnf 检测ssh状态 masterha_check_ssh --conf=/etc/mha/app1.cnf 检测运行状态 masterha_check_status --conf=/etc/mha/app1.cnf 互信检查 masterha_check_ssh \u0026nbsp;--conf=/etc/mha/app1.cnf\n主从状态检查 masterha_check_repl \u0026nbsp;--conf=/etc/mha/app1.cnf\n5**.6开启HMA（db02********）****** 开启 nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover \u0026nbsp;\u0026lt; /dev/null\u0026gt; /var/log/mha/app1/manager.log 2\u0026gt;\u0026amp;1 \u0026amp;\n检测MHA状态 masterha_check_status --conf=/etc/mha/app1.cnf\n[root@db02 mysql]# masterha_check_status --conf=/etc/mha/app1.cnf\napp1 (pid:17248) is running(0:PING_OK), master:172.17.1.145\n[root@db02 mysql]# mysql -umha -pmha -h172.17.1.145 -e \"show variables like 'server_id'\"\nmysql: [Warning] Using a password on the command line interface can be insecure.\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| server_id \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;| 145 \u0026nbsp;\u0026nbsp;|\n+---------------+-------+\n[root@db02 mysql]# mysql -umha -pmha -h172.17.1.146 -e \"show variables like 'server_id'\"\nmysql: [Warning] Using a password on the command line interface can be insecure.\n+---------------+-------+\n| Variable_name | Value |\n+---------------+-------+\n| server_id \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;| 146 \u0026nbsp;\u0026nbsp;|\n+---------------+-------+\n6**、**MHA 的vip功能 参数 注意：/usr/local/bin/master_ip_failover，必须事先准备好\nmaster_ip_failover_script=/usr/local/bin/master_ip_failover\n修改脚本内容 1 2 3 4 5 vi /usr/local/bin/master_ip_failover my $vip = \u0026#39;172.17.1.100/24\u0026#39;; my $key = \u0026#39;1\u0026#39;; my $ssh_start_vip = \u0026#34;/sbin/ifconfig eth0:$key $vip\u0026#34;; my $ssh_stop_vip = \u0026#34;/sbin/ifconfig eth0:$key down\u0026#34;; 更改manager配置文件： 1 2 3 4 5 6 7 vi /etc/mha/app1.cnf 添加： master_ip_failover_script=/usr/local/bin/master_ip_failover 注意： [root@db03 ~]# dos2unix /usr/local/bin/master_ip_failover dos2unix: converting file /usr/local/bin/master_ip_failover to Unix format ... [root@db03 ~]# chmod +x /usr/local/bin/master_ip_failover 主库上，手工生成第一个vip地址 手工在主库上绑定vip，注意一定要和配置文件中的ethN一致，我的是eth0:1(1是key指定的值)\nifconfig ens33:1 172.17.1.100/24\n重启mha masterha_stop --conf=/etc/mha/app1.cnf\nnohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover \u0026lt; /dev/null \u0026gt; /var/log/mha/app1/manager.log 2\u0026gt;\u0026amp;1 \u0026amp;\n7、** binlog server（db02）****** 参数： binlogserver配置：\n找一台额外的机器，必须要有5.6以上的版本，支持gtid并开启，我们直接用slave（db02）\nvim /etc/mha/app1.cnf\n1 2 3 4 [binlog1] no_master=1 hostname= 172.17.1.146 master_binlog_dir=/data/mysql/binlog 创建必要目录 mkdir -p /data/mysql/binlog\nchown -R mysql.mysql /data/*\n修改完成后，将主库binlog拉过来（从000001开始拉，之后的binlog会自动按顺序过来）\n拉取主库binlog日志 cd /data/mysql/binlog \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;-----》必须进入到自己创建好的目录\nmysqlbinlog \u0026nbsp;-R --host=172.17.1.145 --user=mha --password=mha --raw \u0026nbsp;--stop-never mysql-bin.000001 \u0026amp;\n注意：\n拉取日志的起点,需要按照目前从库的已经获取到的二进制日志点为起点\n重启MHA masterha_stop --conf=/etc/mha/app1.cnf\nnohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover \u0026lt; /dev/null \u0026gt; /var/log/mha/app1/manager.log 2\u0026gt;\u0026amp;1 \u0026amp;\n8、邮件提醒 1. 参数 report_script=/usr/local/bin/send\n2. 准备邮件脚本 send_report\n(1)准备发邮件的脚本(上传 email_2019-最新.zip中的脚本，到/usr/local/bin/中)\n(2)将准备好的脚本添加到mha配置文件中,让其调用\n3. 修改manager配置文件，调用邮件脚本 1 2 vi /etc/mha/app1.cnf report_script=/usr/local/bin/send 重启MHA masterha_stop --conf=/etc/mha/app1.cnf\nnohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover \u0026lt; /dev/null \u0026gt; /var/log/mha/app1/manager.log 2\u0026gt;\u0026amp;1 \u0026amp;\n9**、测试MHA****** 关闭主库**,看警告邮件 ****** 切换完后，HMA会退出，还有binlogserver\n** 145数据库挂掉后，MHA自动切换IP到146上，无需人为修改。**\n","date":"2020-06-24T11:43:46Z","image":"https://www.ownit.top/title_pic/37.jpg","permalink":"https://www.ownit.top/p/202006241143/","title":"MHA架构实施（一主一从）学不会，你来打我？加油！奥利给"},{"content":"目录\n1、为什么需要主从复制？\n2、什么是mysql主从复制\n3、Mysql复制原理\n4、主从复制简介\n5、 延时从库\n6、半同步复制\n7、过滤复制\n8、GTID复制\n9. 主从故障监控\\分析\\处理\n主从复制故障分析\n1、为什么需要主从复制？ 1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。\n2、做数据的热备\n3、架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。\n2、什么是mysql主从复制 Mysql主从复制是指数据可以从一个mysql数据库服务器节点复制到另一个或者多个节点。\nMysql默认采用异步复制方式，这样从节点不用一直访问主服务器来跟新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。\n3、Mysql复制原理 原理：\n（1）master服务器将数据的改变记录二进制binlog日志，当master的数据发生改变是，这将改变写入二进制日志中；\n（2）slave服务器会在一定时间间隔内对master二进制日志进行探测是否发生改变，如果发生改变，则开始一个IO线程请求master二进制时间\n（3）同时主节点为每个IO线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动sql线程从中继日志读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后IO线程和sql线程进入睡眠状态，等待下一次唤醒\n从库会生成两个线程,一个I/O线程,一个SQL线程; I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中; 主库会生成一个log dump线程,用来给从库I/O线程传binlog; SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行; 注意：\n1、master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。\n2、slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。\n3、Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。\n4、Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本）\n5、master和slave两节点间时间需同步\n4、主从复制简介 1.1. 基于二进制日志复制的\n1.2. 主库的修改操作会记录二进制日志\n1.3. 从库会请求新的二进制日志并回放,最终达到主从数据同步\n1.4. 主从复制核心功能:\n辅助备份,处理物理损坏 扩展新型的架构:高可用,高性能,分布式架构等\n前提\n2.1 两台以上mysql实例 ,server_id,server_uuid不同 2.2 主库开启二进制日志 2.3 专用的复制用户 2.4 保证主从开启之前的某个时间点,从库数据是和主库一致(补课) 2.5 告知从库,复制user,passwd,IP port,以及复制起点(change master to) 2.6 线程(三个):Dump thread IO thread SQL thread 开启(start slave) 主从复制搭建过程\n清理主库数据\n1 rm -rf /data/3307/data/* 重新初始化3307\n1 mysqld --initialize-insecure --user=mysql --basedir=/app/mysql --datadir=/data/3307/data 修改my.cnf ,开启二进制日志功能\n1 2 [root@db01 3307]# vim /data/3307/my.cnf log_bin=/data/3307/data/mysql-bin 启动所有节点\n1 2 3 4 5 6 7 8 9 10 [root@db01 3307]# systemctl start mysqld3307 [root@db01 3307]# systemctl start mysqld3308 [root@db01 3307]# systemctl start mysqld3309 [root@db01 3307]# ps -ef |grep mysqld mysql 3684 1 4 09:59 ? 00:00:00 /app/mysql/bin/mysqld --defaults-file=/data/3307/my.cnf mysql 3719 1 7 09:59 ? 00:00:00 /app/mysql/bin/mysqld --defaults-file=/data/3308/my.cnf mysql 3754 1 8 09:59 ? 00:00:00 /app/mysql/bin/mysqld --defaults-file=/data/3309/my.cnf [root@db01 3307]# mysql -S /data/3307/mysql.sock -e \u0026#34;select @@server_id\u0026#34; [root@db01 3307]# mysql -S /data/3308/mysql.sock -e \u0026#34;select @@server_id\u0026#34; [root@db01 3307]# mysql -S /data/3309/mysql.sock -e \u0026#34;select @@server_id\u0026#34; 主库中创建复制用户\n1 2 3 [root@db01 3307]# mysql -S /data/3307/mysql.sock db01 [(none)]\u0026gt;grant replication slave on *.* to repl@\u0026#39;10.0.0.%\u0026#39; identified by \u0026#39;123\u0026#39;; db01 [(none)]\u0026gt;select user,host from mysql.user; 备份主库并恢复到从库\n1 2 3 4 [root@db01 3307]# mysqldump -S /data/3307/mysql.sock -A --master-data=2 --single-transaction -R --triggers \u0026gt;/backup/full.sql -- CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;mysql-bin.000001\u0026#39;, MASTER_LOG_POS=653; [root@db01 3307]# mysql -S /data/3308/mysql.sock db01 [(none)]\u0026gt;source /backup/full.sql 告知从库关键复制信息\n1 2 3 4 5 6 7 8 9 10 11 12 ip port user password binlog position [root@db01 3307]# mysql -S /data/3308/mysql.sock db01 [mysql]\u0026gt;help change master to CHANGE MASTER TO MASTER_HOST=\u0026#39;10.0.0.51\u0026#39;, #主库ip地址 MASTER_USER=\u0026#39;repl\u0026#39;, #复制用户 MASTER_PASSWORD=\u0026#39;123\u0026#39;, #密码 MASTER_PORT=3307, #端口 MASTER_LOG_FILE=\u0026#39;mysql-bin.000001\u0026#39;, #复制binlog的起始位置 MASTER_LOG_POS=653, #起始位置的pos好 MASTER_CONNECT_RETRY=10; #尝试连接次数 开启主从专用线程\n1 start slave ; 检查复制状态\n1 2 3 db01 [mysql]\u0026gt;show slave status \\G Slave_IO_Running: Yes Slave_SQL_Running: Yes 主从复制原理描述：\n1.change master to 时，ip pot user password binlog position写入到master.info进行记录 2. start slave 时，从库会启动IO线程和SQL线程 3.IO_T，读取master.info信息，获取主库信息连接主库 4. 主库会生成一个准备binlog DUMP线程，来响应从库 5. IO_T根据master.info记录的binlog文件名和position号，请求主库DUMP最新日志 6. DUMP线程检查主库的binlog日志，如果有新的，TP(传送)给从从库的IO_T 7. IO_T将收到的日志存储到了TCP/IP 缓存，立即返回ACK给主库 ，主库工作完成 8.IO_T将缓存中的数据，存储到relay-log日志文件,更新master.info文件binlog 文件名和postion，IO_T工作完成 9.SQL_T读取relay-log.info文件，获取到上次执行到的relay-log的位置，作为起点，回放relay-log 10.SQL_T回放完成之后，会更新relay-log.info文件。 11. relay-log会有自动清理的功能。 细节： 1.主库一旦有新的日志生成，会发送“信号”给binlog dump ，IO线程再请求 5、 延时从库 是我们认为配置的一种特殊从库.人为配置从库和主库延时N小时.\n为什么要有延时从？\n1 2 3 4 5 数据库故障? 物理损坏 主从复制非常擅长解决物理损坏. 逻辑损坏 普通主从复制没办法解决逻辑损坏 配置延时从库\n1 2 3 4 5 6 7 8 9 SQL线程延时:数据已经写入relaylog中了,SQL线程\u0026#34;慢点\u0026#34;运行 一般企业建议3-6小时,具体看公司运维人员对于故障的反应时间 mysql\u0026gt;stop slave; mysql\u0026gt;CHANGE MASTER TO MASTER_DELAY = 300; mysql\u0026gt;start slave; mysql\u0026gt; show slave status \\G SQL_Delay: 300 SQL_Remaining_Delay: NULL 延时从库应用\n1主1从,从库延时5分钟,主库误删除1个库\n1. 5分钟之内 侦测到误删除操作\n2. 停从库SQL线程\n3. 截取relaylog\n起点 :停止SQL线程时,relay最后应用位置\n终点:误删除之前的position(GTID)\n4. 恢复截取的日志到从库\n5. 从库身份解除,替代主库工作\n故障模拟及恢复\n1 2 3 4 5 6 1.主库数据操作 db01 [(none)]\u0026gt;create database relay charset utf8; db01 [(none)]\u0026gt;use relay db01 [relay]\u0026gt;create table t1 (id int); db01 [relay]\u0026gt;insert into t1 values(1); db01 [relay]\u0026gt;drop database relay; 1 2 2. 停止从库SQL线程 stop slave sql_thread; 1 2 3 4 5 6 7 8 9 3. 找relaylog的截取起点和终点 起点: Relay_Log_File: db01-relay-bin.000002 Relay_Log_Pos: 482 终点: show relaylog events in \u0026#39;db01-relay-bin.000002\u0026#39; | db01-relay-bin.000002 | 1046 | Xid | 7 | 2489 | COMMIT /* xid=144 */ | | db01-relay-bin.000002 | 1077 | Anonymous_Gtid | 7 | 2554 | SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; | mysqlbinlog --start-position=482 --stop-position=1077 /data/3308/data/db01-relay-bin.000002\u0026gt;/tmp/relay.sql 1 2 4.从库恢复relaylog source /tmp/relay.sql 1 2 3 5.从库身份解除 db01 [relay]\u0026gt;stop slave; db01 [relay]\u0026gt;reset slave all 6、半同步复制 解决主从数据一致性问题\n** 半同步复制工作原理的变化**\n1. 主库执行新的事务,commit时,更新 show master status\\G ,触发一个信号给 2. binlog dump 接收到主库的 show master status\\G信息,通知从库日志更新了 3. 从库IO线程请求新的二进制日志事件 4. 主库会通过dump线程传送新的日志事件,给从库IO线程 5. 从库IO线程接收到binlog日志,当日志写入到磁盘上的relaylog文件时,给主库ACK_receiver线程 6. ACK_receiver线程触发一个事件,告诉主库commit可以成功了 7. 如果ACK达到了我们预设值的超时时间,半同步复制会切换为原始的异步复制. 配置半同步复制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 加载插件 主: INSTALL PLUGIN rpl_semi_sync_master SONAME \u0026#39;semisync_master.so\u0026#39;; 从: INSTALL PLUGIN rpl_semi_sync_slave SONAME \u0026#39;semisync_slave.so\u0026#39;; 查看是否加载成功: show plugins; 启动: 主: SET GLOBAL rpl_semi_sync_master_enabled = 1; 从: SET GLOBAL rpl_semi_sync_slave_enabled = 1; 重启从库上的IO线程 STOP SLAVE IO_THREAD; START SLAVE IO_THREAD; 查看是否在运行 主: show status like \u0026#39;Rpl_semi_sync_master_status\u0026#39;; 从: show status like \u0026#39;Rpl_semi_sync_slave_status\u0026#39;; 7、过滤复制 主库：\n1 2 3 show master status; Binlog_Do_DB Binlog_Ignore_DB 从库：\n1 2 3 show slave status\\G Replicate_Do_DB: Replicate_Ignore_DB: 实现过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 mysqldump -S /data/3307/mysql.sock -A --master-data=2 --single-transaction -R --triggers \u0026gt;/backup/full.sql vim /backup/full.sql -- CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;mysql-bin.000002\u0026#39;, MASTER_LOG_POS=154; [root@db01 ~]# mysql -S /data/3309/mysql.sock source /backup/full.sql CHANGE MASTER TO MASTER_HOST=\u0026#39;10.0.0.51\u0026#39;, MASTER_USER=\u0026#39;repl\u0026#39;, MASTER_PASSWORD=\u0026#39;123\u0026#39;, MASTER_PORT=3307, MASTER_LOG_FILE=\u0026#39;mysql-bin.000002\u0026#39;, MASTER_LOG_POS=154, MASTER_CONNECT_RETRY=10; start slave; [root@db01 ~]# vim /data/3309/my.cnf replicate_do_db=ppt replicate_do_db=word [root@db01 ~]# systemctl restart mysqld3309 主库： Master [(none)]\u0026gt;create database word; Query OK, 1 row affected (0.00 sec) Master [(none)]\u0026gt;create database ppt; Query OK, 1 row affected (0.00 sec) Master [(none)]\u0026gt;create database excel; Query OK, 1 row affected (0.01 sec) 8、GTID复制 GTID介绍\n1 2 3 4 5 6 GTID(Global Transaction ID)是对于一个已提交事务的唯一编号，并且是一个全局(主从复制)唯一的编号。 它的官方定义如下： GTID = source_id ：transaction_id 7E11FA47-31CA-19E1-9E56-C43AA21293967:29 什么是sever_uuid，和Server-id 区别？ 核心特性: 全局唯一,具备幂等性 GTID核心参数\n1 2 3 4 5 6 7 gtid-mode=on enforce-gtid-consistency=true log-slave-updates=1 gtid-mode=on --启用gtid类型，否则就是普通的复制架构 enforce-gtid-consistency=true --强制GTID的一致性 log-slave-updates=1 --slave更新是否记入日志 GTID复制配置过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 主库db01： cat \u0026gt; /etc/my.cnf \u0026lt;\u0026lt;EOF [mysqld] basedir=/data/mysql/ datadir=/data/mysql/data socket=/tmp/mysql.sock server_id=51 port=3306 secure-file-priv=/tmp autocommit=0 log_bin=/data/binlog/mysql-bin binlog_format=row gtid-mode=on enforce-gtid-consistency=true log-slave-updates=1 [mysql] prompt=db01 [\\\\d]\u0026gt; EOF slave1(db02)： cat \u0026gt; /etc/my.cnf \u0026lt;\u0026lt;EOF [mysqld] basedir=/data/mysql datadir=/data/mysql/data socket=/tmp/mysql.sock server_id=52 port=3306 secure-file-priv=/tmp autocommit=0 log_bin=/data/binlog/mysql-bin binlog_format=row gtid-mode=on enforce-gtid-consistency=true log-slave-updates=1 [mysql] prompt=db02 [\\\\d]\u0026gt; EOF slave2(db03)： cat \u0026gt; /etc/my.cnf \u0026lt;\u0026lt;EOF [mysqld] basedir=/data/mysql datadir=/data/mysql/data socket=/tmp/mysql.sock server_id=53 port=3306 secure-file-priv=/tmp autocommit=0 log_bin=/data/binlog/mysql-bin binlog_format=row gtid-mode=on enforce-gtid-consistency=true log-slave-updates=1 [mysql] prompt=db03 [\\\\d]\u0026gt; EOF 初始化数据\n1 mysqld --initialize-insecure --user=mysql --basedir=/data/mysql --datadir=/data/mysql/data 启动数据库\n1 /etc/init.d/mysqld start 构建主从\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 master:51 slave:52,53 51: grant replication slave on *.* to repl@\u0026#39;10.0.0.%\u0026#39; identified by \u0026#39;123\u0026#39;; 52\\53: change master to master_host=\u0026#39;10.0.0.51\u0026#39;, master_user=\u0026#39;repl\u0026#39;, master_password=\u0026#39;123\u0026#39; , MASTER_AUTO_POSITION=1; start slave; GTID 从库误写入操作处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 查看监控信息: Last_SQL_Error: Error \u0026#39;Can\u0026#39;t create database \u0026#39;oldboy\u0026#39;; database exists\u0026#39; on query. Default database: \u0026#39;oldboy\u0026#39;. Query: \u0026#39;create database oldboy\u0026#39; Retrieved_Gtid_Set: 71bfa52e-4aae-11e9-ab8c-000c293b577e:1-3 Executed_Gtid_Set: 71bfa52e-4aae-11e9-ab8c-000c293b577e:1-2, 7ca4a2b7-4aae-11e9-859d-000c298720f6:1 注入空事物的方法： stop slave; set gtid_next=\u0026#39;99279e1e-61b7-11e9-a9fc-000c2928f5dd:3\u0026#39;; begin;commit; set gtid_next=\u0026#39;AUTOMATIC\u0026#39;; 这里的xxxxx:N 也就是你的slave sql thread报错的GTID，或者说是你想要跳过的GTID。 最好的解决方案：重新构建主从环境 GTID 复制和普通复制的区别\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 CHANGE MASTER TO MASTER_HOST=\u0026#39;10.0.0.51\u0026#39;, MASTER_USER=\u0026#39;repl\u0026#39;, MASTER_PASSWORD=\u0026#39;123\u0026#39;, MASTER_PORT=3307, MASTER_LOG_FILE=\u0026#39;mysql-bin.000001\u0026#39;, MASTER_LOG_POS=444, MASTER_CONNECT_RETRY=10; change master to master_host=\u0026#39;10.0.0.51\u0026#39;, master_user=\u0026#39;repl\u0026#39;, master_password=\u0026#39;123\u0026#39; , MASTER_AUTO_POSITION=1; start slave; （0）在主从复制环境中，主库发生过的事务，在全局都是由唯一GTID记录的，更方便Failover （1）额外功能参数（3个） （2）change master to 的时候不再需要binlog 文件名和position号,MASTER_AUTO_POSITION=1; （3）在复制过程中，从库不再依赖master.info文件，而是直接读取最后一个relaylog的 GTID号 （4） mysqldump备份时，默认会将备份中包含的事务操作，以以下方式 SET @@GLOBAL.GTID_PURGED=\u0026#39;8c49d7ec-7e78-11e8-9638-000c29ca725d:1\u0026#39;; 告诉从库，我的备份中已经有以上事务，你就不用运行了，直接从下一个GTID开始请求binlog就行。 9. 主从故障监控\\分析\\处理 主库:\n1 2 3 4 5 6 7 show full processlist; 每个从库都会有一行dump相关的信息 HOSTS: db01:47176 State: Master has sent all binlog to slave; waiting for more updates 如果现实非以上信息,说明主从之间的关系出现了问题 从库:\n1 2 db01 [(none)]\u0026gt;show slave status \\G *************************** 1. row *************************** 主库相关信息监控\n1 2 3 4 5 Master_Host: 10.0.0.51 Master_User: repl Master_Port: 3307 Master_Log_File: mysql-bin.000005 Read_Master_Log_Pos: 444 从库中继日志的应用状态\n1 2 Relay_Log_File: db01-relay-bin.000002 Relay_Log_Pos: 485 从库复制线程有关的状态\n1 2 3 4 5 6 Slave_IO_Running: Yes Slave_SQL_Running: Yes Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: 过滤复制有关的状态\n1 2 3 4 5 6 Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: 主从延时相关状态(非人为)\n1 Seconds_Behind_Master: 0 延时从库有关的状态(人为)\n1 2 SQL_Delay: 0 SQL_Remaining_Delay: NULL GTID 复制有关的状态\n1 2 3 Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 主从复制故障分析 1 2 3 4 5 6 7 8 9 10 11 (1) 用户 密码 IP port Last_IO_Error: error reconnecting to master \u0026#39;repl@10.0.0.51:3307\u0026#39; - retry-time: 10 retries: 7 [root@db01 ~]# mysql -urepl -p123333 -h 10.0.0.51 -P 3307 ERROR 1045 (28000): Access denied for user \u0026#39;repl\u0026#39;@\u0026#39;db01\u0026#39; (using password: YES) 原因: 密码错误 用户错误 skip_name_resolve 地址错误 端口 处理方法\n1 2 3 4 stop slave reset slave all change master to start slave 主库连接数上线,或者是主库太繁忙\n1 2 3 4 5 6 7 8 9 10 11 12 13 show slave staus \\G Last_IO_Errno: 1040 Last_IO_Error: error reconnecting to master \u0026#39;repl@10.0.0.51:3307\u0026#39; - retry-time: 10 retries: 7 处理思路: 拿复制用户,手工连接一下 [root@db01 ~]# mysql -urepl -p123 -h 10.0.0.51 -P 3307 mysql: [Warning] Using a password on the command line interface can be insecure. ERROR 1040 (HY000): Too many connections 处理方法: db01 [(none)]\u0026gt;set global max_connections=300; (3) 防火墙,网络不通 请求二进制日志\n1 2 3 主库缺失日志 从库方面,二进制日志位置点不对 Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: \u0026#39;could not find next log; the first event \u0026#39;mysql-bin.000001\u0026#39; at 154, the last event read from \u0026#39;/data/3307/data/mysql-bin.000002\u0026#39; at 154, the last byte read from \u0026#39;/data/3307/data/mysql-bin.000002\u0026#39; at 154.\u0026#39; 1 2 3 注意: 在主从复制环境中,严令禁止主库中reset master; 可以选择expire 进行定期清理主库二进制日志 解决方案: 重新构建主从 ** SQL 线程故障**\nSQL线程功能：\n1 2 3 (1)读写relay-log.info (2)relay-log损坏,断节,找不到 (3)接收到的SQL无法执行 导致SQL线程故障原因分析：\n1 2 3 4 5 6 1. 版本差异，参数设定不同，比如：数据类型的差异，SQL_MODE影响 2.要创建的数据库对象,已经存在 3.要删除或修改的对象不存在 4.DML语句不符合表定义及约束时. 归根揭底的原因都是由于从库发生了写入操作. Last_SQL_Error: Error \u0026#39;Can\u0026#39;t create database \u0026#39;db\u0026#39;; database exists\u0026#39; on query. Default database: \u0026#39;db\u0026#39;. Query: \u0026#39;create database db\u0026#39; 处理方法(以从库为核心的处理方案)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 方法一： stop slave; set global sql_slave_skip_counter = 1; #将同步指针向下移动一个，如果多次不同步，可以重复操作。 start slave; 方法二： /etc/my.cnf slave-skip-errors = 1032,1062,1007 常见错误代码: 1007:对象已存在 1032:无法执行DML 1062:主键冲突,或约束冲突 但是，以上操作有时是有风险的，最安全的做法就是重新构建主从。把握一个原则,一切以主库为主. 一劳永逸的方法:\n1 2 3 4 5 6 (1) 可以设置从库只读. db01 [(none)]\u0026gt;show variables like \u0026#39;%read_only%\u0026#39;; 注意： 只会影响到普通用户，对管理员用户无效。 (2)加中间件 读写分离。 主从延时监控及原因 1 主库做了修改操作,从库比较长时间才能追上. 外在因素\n1 2 3 4 网络 主从硬件差异较大 版本差异 参数因素 主库\n1 2 3 4 5 6 7 8 9 10 11 (1) 二进制日志写入不及时 [rep]\u0026gt;select @@sync_binlog; (2) CR的主从复制中,binlog_dump线程,事件为单元,串行传送二进制日志(5.6 5.5) 1. 主库并发事务量大,主库可以并行,传送时是串行 2. 主库发生了大事务,由于是串行传送,会产生阻塞后续的事务. 解决方案: 1. 5.6 开始,开启GTID,实现了GC(group commit)机制,可以并行传输日志给从库IO 2. 5.7 开始,不开启GTID,会自动维护匿名的GTID,也能实现GC,我们建议还是认为开启GTID 3. 大事务拆成多个小事务,可以有效的减少主从延时. 从库\n1 2 3 4 5 6 7 8 9 10 11 SQL线程导致的主从延时 在CR复制情况下: 从库默认情况下只有一个SQL,只能串行回放事务SQL 1. 主库如果并发事务量较大,从库只能串行回放 2. 主库发生了大事务,会阻塞后续的所有的事务的运行 解决方案: 1. 5.6 版本开启GTID之后,加入了SQL多线程的特性,但是只能针对不同库(database)下的事务进行并发回放. 2. 5.7 版本开始GTID之后,在SQL方面,提供了基于逻辑时钟(logical_clock),binlog加入了seq_no机制, 真正实现了基于事务级别的并发回放,这种技术我们把它称之为MTS(enhanced multi-threaded slave). 3. 大事务拆成多个小事务,可以有效的减少主从延时. [https://dev.mysql.com/worklog/task/?id=6314] ","date":"2020-06-09T17:35:28Z","image":"https://www.ownit.top/title_pic/24.jpg","permalink":"https://www.ownit.top/p/202006091735/","title":"什么，你还不会Mysql主从复制？？？快来看"},{"content":"目录\n1、用户的创建处理原始环境\n2、下载，上传文件\n3、解压二进制包\n4、添加一个磁盘，挂载/data数据\n5、创建数据路径并授权\n6、设置环境变量\n7、Mysql账号授权相应的文件\n8、数据库初始化 9、配置文件的准备\n10、启动数据库\n11、如何分析处理MySQL数据库无法启动\n12、管理员密码的设定（root@localhost）\n13、管理员用户密码忘记了？\n14、启动数据库到维护模式\nRDBMS : 关系型数据库 管理系统\nNoSQL : 非关系型的\nNewSQL : 新型的分布式解决方案\n1、用户的创建处理原始环境 1 2 3 [root@db01 ~]# yum remove mariadb-libs-5.5.60-1.el7_5.x86_64 -y [root@db01 ~]# rpm -qa |grep mariadb [root@db01 ~]# useradd -s /sbin/nologin mysql MySQL 5.7.26 二进制版本安装\nhttps://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz\n2、下载，上传文件 1 2 3 4 5 [root@db01 ~]# mkdir -p /server/tools [root@db01 ~]# cd /server/tools/ [root@db01 /server/tools]# yum install -y lrzsz [root@db01 /server/tools]# ls mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz 3、解压二进制包 1 2 3 [root@db01 /server/tools]# tar xf mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz [root@db01 ~]# mkdir /application [root@db01 /server/tools]# mv mysql-5.7.26-linux-glibc2.12-x86_64 /application/mysql 基本思路：把软件存放位置和数据存放位置分开，确保数据安全\n4、添加一个磁盘，挂载/data数据 5、创建数据路径并授权 1 2 3 4 5 6 7 [root@db01 ~]# mkfs.xfs /dev/sdc [root@db01 ~]# mkdir /mysql [root@db01 ~]# blkid [root@db01 ~]# vim /etc/fstab [root@db01 ~]# UUID=\u0026#34;7d7814c3-1ad2-4622-a435-7086d05d6c55\u0026#34; /mysql xfs defaults 0 0 [root@db01 ~]# mount -a [root@db01 ~]# df -h 6、设置环境变量 1 2 3 4 5 vim /etc/profile export PATH=/application/mysql/bin:$PATH [root@db01 ~]# source /etc/profile [root@db01 ~]# mysql -V mysql Ver 14.14 Distrib 5.7.26, for linux-glibc2.12 (x86_64) using EditLine wrapper 7、Mysql账号授权相应的文件 1 2 3 授权 chown -R mysql.mysql /application/* chown -R mysql.mysql /mysql 8、数据库初始化 1 2 3 4 5 5.6 版本 初始化命令 /application/mysql/scripts/mysql_install_db # 5.7 版本 [root@db01 ~]# mkdir /mysql/mysql/data -p [root@db01 ~]# chown -R mysql.mysql /mysql [root@db01 ~]# mysqld --initialize --user=mysql --basedir=/application/mysql --datadir=/mysql/mysql/data 说明：\n--initialize 参数：\n1. 对于密码复杂度进行定制：12位，4种 2. 密码过期时间：180 3. 给root@localhost用户设置临时密码 --initialize-insecure 参数：\n无限制，无临时密码\n1 2 [root@db01 /data/mysql/data]# \\rm -rf /data/mysql/data/* [root@db01 ~]# mysqld --initialize-insecure --user=mysql --basedir=/application/mysql --datadir=/mysql/mysql/data 没有密码\n9、配置文件的准备 1 2 3 4 5 6 7 8 9 10 11 cat \u0026gt;/etc/my.cnf \u0026lt;\u0026lt;EOF [mysqld] user=mysql basedir=/application/mysql datadir=/mysql/mysql/data socket=/tmp/mysql.sock server_id=6 port=3306 [mysql] socket=/tmp/mysql.sock EOF 10、启动数据库 1. sys-v\n1 2 [root@db01 /etc/init.d]# cp /application/mysql/support-files/mysql.server /etc/init.d/mysqld [root@db01 /etc/init.d]# service mysqld restart 2. systemd\n注意： sysv方式启动过的话，需要先提前关闭，才能以下方式登录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026gt;/etc/systemd/system/mysqld.service \u0026lt;\u0026lt;EOF [Unit] Description=MySQL Server Documentation=man:mysqld(8) Documentation=http://dev.mysql.com/doc/refman/en/using-systemd.html After=network.target After=syslog.target [Install] WantedBy=multi-user.target [Service] User=mysql Group=mysql ExecStart=/application/mysql/bin/mysqld --defaults-file=/etc/my.cnf LimitNOFILE = 5000 EOF 11、如何分析处理MySQL数据库无法启动 without updating PID 类似错误\n查看日志：\n在哪？\n/data/mysql/data/主机名.err\n[ERROR] 上下文\n可能情况：\n/etc/my.cnf 路径不对等\n/tmp/mysql.sock文件修改过 或 删除过\n数据目录权限不是mysql\n参数改错了\n12、管理员密码的设定（root@localhost） 1 2 [root@db01 ~]# mysqladmin -uroot -p password oldboy123 Enter password: 13、管理员用户密码忘记了？ 1 2 --skip-grant-tables #跳过授权表 --skip-networking #跳过远程登录 14、启动数据库到维护模式 1 [root@db01 ~]# mysqld_safe --skip-grant-tables --skip-networking \u0026amp; 登录并修改密码\n1 2 3 4 5 mysql\u0026gt; alter user root@\u0026#39;localhost\u0026#39; identified by \u0026#39;1\u0026#39;; ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement mysql\u0026gt; flush privileges; mysql\u0026gt; alter user root@\u0026#39;localhost\u0026#39; identified by \u0026#39;1\u0026#39;; Query OK, 0 rows affected (0.01 sec) 关闭数据库，正常启动验证\n","date":"2020-06-09T16:28:58Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/202006091628/","title":"MySQL二进制安装（版本：5.7.26）"},{"content":"1.运维在数据库备份恢复方面的职责 1.设计备份策略\n全备 、增量、时间、自动\n2.日常备份检查\n备份存在性 备份空间够用否 ** 3.定期恢复演练(测试库)**\n一季度 或者 半年\n4.故障恢复\n通过现有备份,能够将数据库恢复到故障之前的时间点. 5.迁移\n1. 停机时间\n2. 回退方案\n2.Mysql数据损坏类型 1.物理损坏\n磁盘损坏：硬件，磁道坏，dd，格式化\n文件损坏：数据文件损坏，redo损坏\n2.逻辑损坏\n1 2 3 4 drop delete truncate update 3. 备份类型 1.热备\n在数据库正常业务时,备份数据,并且能够一致性恢复（只能是innodb）\n对业务影响非常小\n2.温备\n锁表备份,只能查询不能修改（myisam）\n影响到写入操作\n3.冷备\n关闭数据库业务,数据库没有任何变更的情况下,进行备份数据.\n业务停止\n4. 备份方式及工具介绍 1.逻辑备份工具\n1 2 3 基于SQL语句进行备份 mysqldump ***** mysqlbinlog ***** 2. 物理备份工具\n1 2 3 基于磁盘数据文件备份 xtrabackup(XBK) ：percona 第三方 ***** MySQL Enterprise Backup（MEB） 5. 逻辑备份和物理备份的比较 mysqldump (MDP)\n优点：\n1.不需要下载安装 2.备份出来的是SQL，文本格式，可读性高,便于备份处理 3.压缩比较高，节省备份的磁盘空间 缺点：\n依赖于数据库引擎，需要从磁盘把数据读出，然后转换成SQL进行转储，比较耗费资源，数据量大的话效率较低 建议：\n100G以内的数据量级，可以使用mysqldump 超过TB以上，我们也可能选择的是mysqldump，配合分布式的系统 1EB =1024 PB =1000000 TB 6.备份策略 备份方式：\n全备:全库备份，备份所有数据 增量:备份变化的数据 备份工具\n逻辑备份=mysqldump+mysqlbinlog 物理备份=xtrabackup_full+xtrabackup_incr+binlog或者xtrabackup_full+binlog 备份周期:\n根据数据量设计备份周期 比如：周日全备，周1-周6增量 备份监控\n备份空间 备份日志 7.容灾策略 备份\n架构\n高可用 负载均衡 演示从库 主从同步 灾备库 异地备份 定期的故障恢复演练\n8.mysqldump应用 介绍：逻辑备份工具。备份的是sql语句\n备份方式\nInnoDB\n可以采取快照备份的方式 开启一个独立的事务，获取当前最新的一致性快照，将快照数据，放在临时表中，转换成SQL（Create database，create table，insert），保存到sql文件中 非InnoDB\n需要锁表备份。触发FTWRL，全局锁表。 将快照数据，放在临时表中，转换成SQL（Create database，create table，insert），保存到sql文件中 核心参数\n建立备份目录，授权\n1 2 mkdir -p /data/backup chown -R mysql.mysql /data/backup/ 连接参数 -u #用户 -p #密码 -h #主机地址 -P #端口 -S #socket连接 -A #全备 1 mysqldump -uroot -proot -A \u0026gt;/data/backup/full.sql -B #备份一个或者多个 1 mysqldump -uroot -proot -B gtid gtid2 \u0026gt;/data/backup/db.sql 备份库下的多个表（不加参数）\n1 mysqldump -uroot -proot gtid t1 t1 \u0026gt;/data/backup/db.sql 备份高级参数 1 --master-data=2 1 2 3 4 5 6 7 8 9 10 11 12 13 以注释的形式,保存备份开始时间点的binlog的状态信息 mysqldump -uroot -p -A -R --triggers --master-data=2 \u0026gt;/back/world.sql [root@db01 ~]# grep \u0026#39;CHANGE\u0026#39; /backup/world.sql -- CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;mysql-bin.000035\u0026#39;, MASTER_LOG_POS=194; 功能： （1）在备份时，会自动记录，二进制日志文件名和位置号 0 默认值 1 以change master to命令形式，可以用作主从复制 2 以注释的形式记录，备份时刻的文件名+postion号 （2） 自动锁表 （3）如果配合--single-transaction，只对非InnoDB表进行锁表备份，InnoDB表进行“热“”备，实际上是实现快照备份。 1 --single-transaction 1 2 3 4 5 6 innodb 存储引擎开启热备(快照备份)功能 master-data可以自动加锁 （1）在不加--single-transaction ，启动所有表的温备份，所有表都锁定 （1）加上--single-transaction ,对innodb进行快照备份,对非innodb表可以实现自动锁表功能 例子6: 备份必加参数 mysqldump -uroot -p -A -R -E --triggers --master-data=2 --single-transaction --set-gtid-purged=OFF \u0026gt;/data/backup/full.sql 1 --set-gtid-purged=auto 1 2 3 4 5 6 7 auto , on off 使用场景: 1. --set-gtid-purged=OFF,可以使用在日常备份参数中. mysqldump -uroot -p -A -R -E --triggers --master-data=2 --single-transaction --set-gtid-purged=OFF \u0026gt;/data/backup/full.sql 2. auto , on:在构建主从复制环境时需要的参数配置 mysqldump -uroot -p -A -R -E --triggers --master-data=2 --single-transaction --set-gtid-purged=ON \u0026gt;/data/backup/full.sql 1 --max-allowed-packet=# 1 2 3 4 mysqldump -uroot -p -A -R -E --triggers --master-data=2 --single-transaction --set-gtid-purged=OFF --max-allowed-packet=256M \u0026gt;/data/backup/full.sql --max-allowed-packet=# The maximum packet length to send to or receive from server. -R 备份存储过程及函数 --triggers 备份触发器 -E 备份事件 生产备份语句（只是建议，根据自己需求来改） 1 mysqldump -uroot -p -A -R -E --triggers --master-data=2 --single-transaction --set-gtid-purged=OFF --max-allowed-packet=256M \u0026gt;/data/backup/full.sql 练习 实现所有表的单独备份 1 2 3 4 5 提示： information_schema.tables mysqldump -uroot -p123 world city \u0026gt;/backup/world_city.sql select concat(\u0026#34;mysqldump -uroot -p123 \u0026#34;,table_schema,\u0026#34; \u0026#34;,table_name,\u0026#34; --master-data=2 --single-transaction --set-gtid-purged=0 -R -E --triggers\u0026gt;/backup/\u0026#34;,table_schema,\u0026#34;_\u0026#34;,table_name,\u0026#34;.sql\u0026#34;) from information_schema.tables where table_schema not in (\u0026#39;sys\u0026#39;,\u0026#39;information_schema\u0026#39;,\u0026#39;performance_schema\u0026#39;); 9. 备份时优化参数: 1 2 3 4 5 6 7 (1) max_allowed_packet 最大的数据包大小 mysqldump -uroot -p123 -A -R --triggers --set-gtid-purged=OFF --master-data=2 max_allowed_packet=128M --single-transaction|gzip \u0026gt; /backup/full_$(date +%F).sql.gz (2) 增加key_buffer_size (临时表有关) (3) 分库分表并发备份 (作业) (4) 架构分离,分别备份 (架构拆分,分布式备份) 10. MySQL物理备份工具-xtrabackup(XBK、Xbackup) 安装并安装\n1 2 3 4 5 wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.12/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.12-1.el7.x86_64.rpm https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/binary/redhat/6/x86_64/percona-xtrabackup-24-2.4.4-1.el6.x86_64.rpm yum -y install percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 备份命令介绍:\n1 2 xtrabackup innobackupex ****** 备份方式——物理备份\n1 2 （1）对于非Innodb表（比如 myisam）是，锁表cp数据文件，属于一种温备份。 （2）对于Innodb的表（支持事务的），不锁表，拷贝数据页，最终以数据文件的方式保存下来，把一部分redo和undo一并备走，属于热备方式。 xbk 在innodb表备份恢复的流程 1 2 3 4 0、xbk备份执行的瞬间,立即触发ckpt,已提交的数据脏页,从内存刷写到磁盘,并记录此时的LSN号 1、备份时，拷贝磁盘数据页，并且记录备份过程中产生的redo和undo一起拷贝走,也就是checkpoint LSN之后的日志 2、在恢复之前，模拟Innodb“自动故障恢复”的过程，将redo（前滚）与undo（回滚）进行应用 3、恢复过程是cp 备份到原来数据目录下 innobackupex使用 全备\n1 innobackupex --user=root --password=123 /data/backup 自主定制备份路径名\n1 innobackupex --user=root --password=123 --no-timestamp /data/backup/full 备份集中多出来的文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 -rw-r----- 1 root root 24 Jun 29 09:59 xtrabackup_binlog_info -rw-r----- 1 root root 119 Jun 29 09:59 xtrabackup_checkpoints -rw-r----- 1 root root 489 Jun 29 09:59 xtrabackup_info -rw-r----- 1 root root 2560 Jun 29 09:59 xtrabackup_logfile xtrabackup_binlog_info ：（备份时刻的binlog位置） [root@db01 full]# cat xtrabackup_binlog_info mysql-bin.000003 536749 79de40d3-5ff3-11e9-804a-000c2928f5dd:1-7 记录的是备份时刻，binlog的文件名字和当时的结束的position，可以用来作为截取binlog时的起点。 xtrabackup_checkpoints ： backup_type = full-backuped from_lsn = 0 上次所到达的LSN号(对于全备就是从0开始,对于增量有别的显示方法) to_lsn = 160683027 备份开始时间(ckpt)点数据页的LSN last_lsn = 160683036 备份结束后，redo日志最终的LSN compact = 0 recover_binlog_info = 0 （1）备份时刻，立即将已经commit过的，内存中的数据页刷新到磁盘(CKPT).开始备份数据，数据文件的LSN会停留在to_lsn位置。 （2）备份时刻有可能会有其他的数据写入，已备走的数据文件就不会再发生变化了。 （3）在备份过程中，备份软件会一直监控着redo的undo，如果一旦有变化会将日志也一并备走，并记录LSN到last_lsn。 从to_lsn ----》last_lsn 就是，备份过程中产生的数据变化. innobackupex 增量备份(incremental) （1）增量备份的方式，是基于上一次备份进行增量。 （2）增量备份无法单独恢复。必须基于全备进行恢复。 （3）所有增量必须要按顺序合并到全备中。 增量备份命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 （1）删掉原来备份 略. （2）全备（周日） [root@db01 backup]# innobackupex --user=root --password --no-timestamp /backup/full \u0026gt;\u0026amp;/tmp/xbk_full.log （3）模拟周一数据变化 db01 [(none)]\u0026gt;create database cs charset utf8; db01 [(none)]\u0026gt;use cs db01 [cs]\u0026gt;create table t1 (id int); db01 [cs]\u0026gt;insert into t1 values(1),(2),(3); db01 [cs]\u0026gt;commit; （4）第一次增量备份（周一） innobackupex --user=root --password=123 --no-timestamp --incremental --incremental-basedir=/backup/full /backup/inc1 \u0026amp;\u0026gt;/tmp/inc1.log （5）模拟周二数据 db01 [cs]\u0026gt;create table t2 (id int); db01 [cs]\u0026gt;insert into t2 values(1),(2),(3); db01 [cs]\u0026gt;commit; （6）周二增量 innobackupex --user=root --password=123 --no-timestamp --incremental --incremental-basedir=/backup/inc1 /backup/inc2 \u0026amp;\u0026gt;/tmp/inc2.log （7）模拟周三数据变化 db01 [cs]\u0026gt;create table t3 (id int); db01 [cs]\u0026gt;insert into t3 values(1),(2),(3); db01 [cs]\u0026gt;commit; db01 [cs]\u0026gt;drop database cs; 恢复到周三误drop之前的数据状态\n1 2 3 4 5 6 7 恢复思路： 1. 挂出维护页，停止当天的自动备份脚本 2. 检查备份：周日full+周一inc1+周二inc2，周三的完整二进制日志 3. 进行备份整理（细节），截取关键的二进制日志（从备份——误删除之前） 4. 测试库进行备份恢复及日志恢复 5. 应用进行测试无误，开启业务 6. 此次工作的总结 恢复过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 1. 检查备份 1afe8136-601d-11e9-9022-000c2928f5dd:7-9 2. 备份整理（apply-log）+合并备份（full+inc1+inc2） (1) 全备的整理 [root@db01 one]# innobackupex --apply-log --redo-only /data/backup/full (2) 合并inc1到full中 [root@db01 one]# innobackupex --apply-log --redo-only --incremental-dir=/data/backup/inc1 /data/backup/full (3) 合并inc2到full中 [root@db01 one]# innobackupex --apply-log --incremental-dir=/data/backup/inc2 /data/backup/full (4) 最后一次整理全备 [root@db01 backup]# innobackupex --apply-log /data/backup/full 3. 截取周二 23:00 到drop 之前的 binlog [root@db01 inc2]# mysqlbinlog --skip-gtids --include-gtids=\u0026#39;1afe8136-601d-11e9-9022-000c2928f5dd:7-9\u0026#39; /data/binlog/mysql-bin.000009 \u0026gt;/data/backup/binlog.sql 4. 进行恢复 [root@db01 backup]# mkdir /data/mysql/data2 -p [root@db01 full]# cp -a * /data/mysql/data2 [root@db01 backup]# chown -R mysql. /data/* [root@db01 backup]# systemctl stop mysqld vim /etc/my.cnf datadir=/data/mysql/data2 systemctl start mysqld Master [(none)]\u0026gt;set sql_log_bin=0; Master [(none)]\u0026gt;source /data/backup/binlog.sql 下面是我画的思维导图，可以很快加强记忆。\n","date":"2020-05-28T17:18:38Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/202005281718/","title":"MySQL--备份恢复【Mysqdump+xtrabackup（XBK）】"},{"content":"Mysql日志 1.错误日志 1.作用 - 排查MySQL运行过程的故障.\n2.默认配置 1 1.默认就开启了 1 2.默认路径和名字: datadir/hostname.err 3.人为定制位置 1 - log_error=/tmp/mysql.log 重启生效\n1 show variables like \u0026#39;log_error\u0026#39;; 二进制日志(binlog) 1.作用 - (1)备份恢复必须依赖二进制日志\n- (2)主从环境必须依赖二进制日志\n2. binlog配置 (5.7必须加server_id) 1 2 3 4 5 6 7 8 9 1.默认：8.0版本以前，没有开启（注意：MySQL默认是没有开启二进制日志的 说明：和数据盘分开，防止数据盘损坏，导致binlong无法恢复 2.默认配置方法 server_id=6 # 主机编号。主从使用， 5.7以后开启binlog要加此参数 log_bin=/data/binlog/mysql-bin #日志存放目录+日志名前缀，例如：mysql-bin.000001 sync_binlog=1 #binlog日志刷盘策略，双一的第二个1。每次事务提交立即刷写binlog到磁盘 binlog_format=row #binlog的记录格式为row模式 重启完成 - 基础参数查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 - 开关（1：表示开启） - select @@log_bin; - 日志路径及名字 - select @@log_bin_basename; - 服务ID号 - select @@server_id; - 二进制日志格式 - select @@binlog_format; - 双一标准之二 - select @@sync_binlog; 3.二进制内置查看命令 查看一目前有几个日志binlog 1 show binary logs; 查看当前在使用的binlog 1 show master status; 查看二进制事件 1 show binlog events in \u0026#39;mysql-bin.000001 \u0026#39;; binlog文件内容详细查看 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 - mysql -uroot -proot -e \u0026#34;show binlog events in \u0026#39;mysql-bin.000001\u0026#39;\u0026#34; - 普通查看 mysqlbinlog - mysqlbinlog /mysql/mysql/data/binlog/mysql-bin.000001 \u0026gt; a.sql #把binlog到成sql文件 - 翻译查看 - mysqlbinlog --base64-output=decode-rows -vvv /data/binlog/mysql-bin.000003 - 基于时间查看 - mysqlbinlog --start-datetime=\u0026#39;2019-05-06 17:00:00\u0026#39; --stop-datetime=\u0026#39;2019-05-06 17:01:00\u0026#39; /data/binlog/mysql-bin.000004 - 基于Position号进行日志截取 - mysqlbinlog --start-position=219 --stop-position=1347 /data/binlog/mysql-bin.000003 \u0026gt;/tmp/bin.sql - 数据恢复 - set sql_log_bin=0; source /tmp/bin.sql binlog维护操作 - 1.日志滚动\n1 2 3 4 5 6 - flush logs； - mysqladmin -uroot -proot flush-logs - 自动滚动，默认1G滚动一次，可以设置参数 - select @@max_binlog_size; - mysqldump -F - 重启数据自动滚动 - 2.日志的删除\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - 注意：不要使用rm命令删除日志 - 自动删除（默认：0 永不删除，单位是天） - select @@expire_logs_days; - 问题：到底设置多少天合适？ 一个全备周期（7+1）天，一般生产一遍建议2个全备周期+1 - show variables like \u0026#39;%expire%\u0026#39;; expire_logs_days 0 自动清理时间,是要按照全备周期+1 set global expire_logs_days=8; 永久生效: my.cnf expire_logs_days=15; 企业建议,至少保留两个全备周期+1的binlog 1 2 3 4 5 6 7 8 9 10 - 手工删除 - PURGE BINARY LOGS TO \u0026#39;mysql-bin.010\u0026#39;; - PURGE BINARY LOGS BEFORE \u0026#39;2008-04-02 22:46:26\u0026#39;; - 全部清空 - reset master; - 比较危险，在主从执行此操作，主从必宕机 binlog日志的GTID新特性 1.GTID 介绍 1 2 3 4 5 6 7 8 9 10 - 5.6 版本新加的特性,5.7中做了加强 5.6 中不开启,没有这个功能. 5.7 中的GTID,即使不开也会有自动生成 SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; - 对于一个已提交事务的编号，并且是一个全局唯一的编号。 它的官方定义如下： GTID = source_id ：transaction_id 7E11FA47-31CA-19E1-9E56-C43AA21293967:29 2.开启参数 1 2 3 4 5 开启的参数 gtid-mode=on enforce-gtid-consistency=true 查看 - select @@gtid_mode; 3.具备GTID后,截取查看某些事务日志 1 2 3 --include-gtids --exclude-gtids --skip-gtids 4.数据截取 mysqlbinlog --skip-gtids --include-gtids='3ca79ab5-3e4d-11e9-a709-000c293b577e:1-9' mysql-bin.000002 mysql-bin.000003 \u0026gt; /tmp/gtid.sql 5.数据恢复 1 2 set sql_log_bin=0; source /tmp/bin.sql 慢日志(slow-log) 1 作用：记录运行较慢的语句,优化过程中常用的工具日志. 2.配置方法（默认没开启） 1 2 3 select @@slow_query_log; #0表示关闭 select @@slow_query_log_file; #文件存放文字 ** 开关:**\n1 slow_query_log=1 文件位置及名字\n1 slow_query_log_file=/data/mysql/slow.log 设定慢查询时间:\n1 long_query_time=0.1 没走索引的语句也记录:\n1 log_queries_not_using_indexes 1 2 3 4 5 6 vim /etc/my.cnf slow_query_log=1 slow_query_log_file=/data/mysql/slow.log long_query_time=0.1 log_queries_not_using_indexes systemctl restart mysqld 3.mysqldumpslow 分析慢日志 mysqldumpslow -s c -t 10 /data/mysql/slow.log 第三方工具(自己扩展) 1 2 https://www.percona.com/downloads/percona-toolkit/LATEST/ yum install perl-DBI perl-DBD-MySQL perl-Time-HiRes perl-IO-Socket-SSL perl-Digest-MD5 1 - toolkit工具包中的命令: 1 2 ./pt-query-diagest /data/mysql/slow.log Anemometer基于pt-query-digest将MySQL慢查询可视化 ","date":"2020-05-27T17:51:57Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202005271751/","title":"Mysql日志分析（错误日志，Binlog日志，慢日志），有惊喜哦"},{"content":"目录\n多实例的应用\n1、准备多个目录\n2、准备配置文件\n3、初始化三套数据\n4、systemd管理多实例\n5、授权\n6、启动\n7、验证多实例\nMysql下载\nhttps://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.26-linux-glibc2.12-x86_64.tar.gz\nMysql版本：5.7.26\nMysql安装方式：二进制\n多实例的应用 1、准备多个目录 1 mkdir -p /mysql/330{7,8,9}/data 2、准备配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 cat \u0026gt; /mysql/3307/my.cnf \u0026lt;\u0026lt;EOF [mysqld] basedir=/application/mysql #mysql安装路径 datadir=/mysql/3307/data #mysql数据存放目录 socket=/mysql/3307/mysql.sock #mysql的socket log_error=/mysql/3307/mysql.log port=3307 server_id=7 log_bin=/mysql/3307/mysql-bin EOF cat \u0026gt; /mysql/3308/my.cnf \u0026lt;\u0026lt;EOF [mysqld] basedir=/application/mysql datadir=/mysql/3308/data socket=/mysql/3308/mysql.sock log_error=/mysql/3308/mysql.log port=3308 server_id=8 log_bin=/mysql/3308/mysql-bin EOF cat \u0026gt; /mysql/3309/my.cnf \u0026lt;\u0026lt;EOF [mysqld] basedir=/application/mysql datadir=/mysql/3309/data socket=/mysql/3309/mysql.sock log_error=/mysql/3309/mysql.log port=3309 server_id=9 log_bin=/mysql/3309/mysql-bin EOF 3、初始化三套数据 1 2 3 mysqld --initialize-insecure --user=mysql --datadir=/mysql/3307/data --basedir=/application/mysql mysqld --initialize-insecure --user=mysql --datadir=/mysql/3308/data --basedir=/application/mysql mysqld --initialize-insecure --user=mysql --datadir=/mysql/3309/data --basedir=/application/mysql 4、systemd管理多实例 1 2 3 4 5 6 7 8 9 10 11 cd /etc/systemd/system cp mysqld.service mysqld3307.service cp mysqld.service mysqld3308.service cp mysqld.service mysqld3309.service vim mysqld3307.service ExecStart=/app/mysql/bin/mysqld --defaults-file=/mysql/3307/my.cnf vim mysqld3308.service ExecStart=/app/mysql/bin/mysqld --defaults-file=/mysql/3308/my.cnf vim mysqld3309.service ExecStart=/app/mysql/bin/mysqld --defaults-file=/mysql/3309/my.cnf 5、授权 1 chown -R mysql.mysql /mysql/* 6、启动 1 2 3 systemctl start mysqld3307.service systemctl start mysqld3308.service systemctl start mysqld3309.service 7、验证多实例 1 2 3 4 netstat -lnp|grep 330 mysql -S /mysql/3307/mysql.sock -e \u0026#34;select @@server_id\u0026#34; mysql -S /mysql/3308/mysql.sock -e \u0026#34;select @@server_id\u0026#34; mysql -S /mysql/3309/mysql.sock -e \u0026#34;select @@server_id\u0026#34; ","date":"2020-05-19T17:56:00Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/202005191756/","title":"Mysql多实例启动"},{"content":"目录\n前言\n如何入手监控\n一、zabbix 快速监控主机\n1.安装zabbix-agent\n2.配置zabbix-agent(修改配置)\n3.启动zabbix-agent并检查\n4.zabbix-web界面，添加主机\n二、自定义监控主机小试身手\n1.监控需求\n2.命令行实现\n3.编写zabbix监控文件(传参形式)\n4.server端进行测试\n5.web端添加\n前言 Centos7安装Zabbix服务端、Zabbix客户端和Win客户端配置（源码编译安装）\n前面介绍怎么源码编译安装Zabbix和主机添加。下面要介绍怎么添加模板和自定义监控\n如何入手监控 1.硬件监控 路由器、交换机、防火墙\n2.系统监控 CPU、内存、磁盘、网络、进程、 TCP\n3.服务监控 nginx、 php、 tomcat、 redis、 memcache、 mysql\n4.WEB 监控 请求时间、响应时间、加载时间、\n5.日志监控 ELk（收集、存储、分析、展示） 日志易\n6.安全监控 Firewalld、 WAF(Nginx+lua)、安全宝、牛盾云、安全狗\n7.网络监控 smokeping 多机房\n8.业务监控 活动引入多少流量、产生多少注册量、带来多大价值\n一、zabbix 快速监控主机 1.安装zabbix-agent 1 rpm -ivh https://mirror.tuna.tsinghua.edu.cn/zabbix/zabbix/4.0/rhel/7/x86_64/zabbix-agent-4.0.4-1.el7.x86_64.rpm 2.配置zabbix-agent(修改配置) 1 2 3 4 5 6 7 8 [root@kvm zabbix]# grep \u0026#34;^[a-Z]\u0026#34; /etc/zabbix/zabbix_agentd.conf PidFile=/var/run/zabbix/zabbix_agentd.pid LogFile=/var/log/zabbix/zabbix_agentd.log LogFileSize=0 Server=192.168.1.10 ServerActive=192.168.1.10 Hostname=Zabbix_kvm Include=/etc/zabbix/zabbix_agentd.d/*.conf 3.启动zabbix-agent并检查 4.zabbix-web界面，添加主机 这边配置自动发现和自动注册（可以自动发现和添加主机和模板）\n二、自定义监控主机小试身手 1.监控需求 监控TCP3种状态集\n2.命令行实现 1 2 3 4 [root@kvm zabbix]# netstat -ant|grep -c TIME_WAIT 1 [root@kvm zabbix]# netstat -ant|grep -c LISTEN 13 3.编写zabbix监控文件(传参形式) 1 2 3 [root@kvm zabbix]# cat /etc/zabbix/zabbix_agentd.d/tcp_status.conf UserParameter=tcp_state[*],netstat -ant|grep -c $1 [root@kvm zabbix]# systemctl restart zabbix-agent.service 4.server端进行测试 1 2 3 4 [root@master zabbix]# /data/zabbix/bin/zabbix_get -s 192.168.1.100 -k tcp_state[TIME_WAIT] 6 [root@master zabbix]# /data/zabbix/bin/zabbix_get -s 192.168.1.100 -k tcp_state[LISTEN] 13 5.web端添加 数据出来了\n","date":"2020-05-13T17:22:05Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/202005131722/","title":"Zabbix服务自定义监控和模板"},{"content":"目录\n1、宝塔安装和配置环境\n2、安装依赖和编译安装\n3、配置数据库\n4、配置Zabbix服务端\n5、配置Zabbix的Web端\n6、配置Linux客户端\n7、配置Windows客户端\n8、在zabbix服务器添加主机\nCentos7上使用宝塔面板配置LNMP环境安装zabbix4.2\n1、宝塔安装和配置环境 Centos7安装宝塔控制面板\nZabbix4.2.2的源码包（包括Win）\nL：Linux\nN：Nginx\nM：Mysql\nP：PHP\n/2‘’\n2、安装依赖和编译安装 1 yum install -y wget telnet net-tools python-paramiko gcc gcc-c++ dejavu-sans-fonts python-setuptools python-devel sendmail mailx net-snmp net-snmp-devel net-snmp-utils freetype-devel libpng-devel perl unbound libtasn1-devel p11-kit-devel OpenIPMI unixODBC libevent-devel 首先添加zabbix用户和zabbix组\n1 2 groupadd zabbix useradd -g zabbix -s /sbin/nologin zabbix 然后下载zabbix4.2编译安装包\n执行\n1 2 3 wget https://jaist.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/4.2.4/zabbix-4.2.4.tar.gz tar -zxvf zabbix-4.2.4.tar.gz cd zabbix-4.2.4 执行帮助查看编译安装选项\n1 ./configure --help 我采取的是尽量多安装模块\n1 ./configure --prefix=/data/zabbix --enable-server --enable-proxy --enable-agent --enable-ipv6 --with-mysql --with-net-snmp --with-libcurl --with-openipmi --with-openssl --with-libcurl --with-libxml2 说明：\n1、对于虚拟机监视\u0026ndash;with-libcurl和\u0026ndash;with-libxml2配置选项是必需的\n2、enable proxy,agent是启用代理\n3、with-net-snmp with-mysql是配置snmp和mysql支持\n4、在编译过程中，如果提示错误，则是某些扩展包没有安装，进行yum安装即可\n在检查配置无误后，执行安装\n1 make install 3、配置数据库 然后配置数据库。在宝塔面板中可以查看和修改数据库root密码\n完成数据创建后，导入数据库\n1 2 3 mysql -uzabbix -pzabbix -hlocalhost zabbix \u0026lt; database/mysql/schema.sql mysql -uzabbix -pzabbix -hlocalhost zabbix \u0026lt; database/mysql/images.sql mysql -uzabbix -pzabbix -hlocalhost zabbix \u0026lt; database/mysql/data.sql 导入完毕后，可以在宝塔面板的phpMyadmin中查看数据库详细\n4、配置Zabbix服务端 然后进入zabbix安装目录/usr/local/zabbix配置zabbix.conf配置文件\n1 vim /data/zabbix/etc/zabbix_server.conf 然后关闭centos上防火墙，selinux等\n1 2 systemctl stop firewalld systemctl disable firewalld Zabbix前端是用PHP编写的，因此要运行它需要PHP支持的Web服务器。只需将PHP文件从frontends / php复制到webserver HTML文档目录即可完成安装。\n在使用宝塔面板安装LNMP环境后，会自动配置nginx，同时会在跟目录下创建WWW目录，存放WEB服务器等信息。\n5、配置Zabbix的Web端 再宝塔面板网站中，添加新的站点\n说明\n1、域名一般使用公网域名\n2、没有公网域名，内网中使用.lcoal或者其他不冲突的域名格式代替即可\n3、使用ip地址业务可以\n完成域名配置后，将zabbix-4.2.4目录中的frontends / php/下的文件复制到站点目录\n1 2 cd zabbix-4.2.4 cp -r frontends/php/* /www/wwwroot/zabbix/ #此文件就是之前创建的站点 完成之后，再软件商店中调整以下php设置\n根据zabbix要求，调整max_input_time 由60改为300,同时调整时区date.timezone为.Asia/Shanghai,然后保存设置\n然后启动zabbix和zabbix-agent\n1 2 /data/zabbix/sbin/zabbix_server /data/zabbix/sbin/zabbix_agentd 然后再浏览器中输入ip/setup.php(服务器IP地址），进行配置zabbix\n提示缺少php ldap的警告\n无视，点击下一步\n配置mysql\n然后这里提示报错。我们需要按照提将文件下载保存为/www/wwwroot/zabbix/conf/zabbix.conf.php\u0026quot;\n完成后，zabbix配置完成\n以后对 zabbix的维护，包括安全加固，数据备份，新能调优等等，都可以通过宝塔面板进行\n6、配置Linux客户端 1.添加zabbix用户和组。\n1 2 groupadd zabbix useradd zabbix -g zabbix -s /sbin/nologin 2.安装zabbix客户端。\n1 2 3 tar xvf zabbix-4.2.4.tar.gz cd zabbix-4.2.4 ./configure --prefix=/data/zabbix_agent --enable-agent 1 make \u0026amp;\u0026amp; make install 3.添加服务端口和修改启动脚本。\n1 2 3 4 echo \u0026#39;zabbix-agent 10050/tcp #Zabbix Agent\u0026#39; \u0026gt;\u0026gt; /etc/services echo \u0026#39;zabbix-agent 10050/udp #Zabbix Agent\u0026#39; \u0026gt;\u0026gt; /etc/services cp zabbix-4.2.2/misc/init.d/Fedora/core/zabbix_agentd /etc/init.d/ sed -i \u0026#39;s/BASEDIR=\\/usr\\/local/BASEDIR=\\/data\\/zabbix/g\u0026#39; /etc/init.d/zabbix_agentd Zabbix agentd使用 chkconfig 将其加入 init 的启动服务\n1 2 chkconfig --add zabbix_agentd chkconfig --level 345 zabbix_agentd on 使用 chkconfig --list 检查一下\n1 chkconfig --list | grep zabbix 4.修改zabbix_agent配置文件。\n1 vim /data/zabbix/etc/zabbix_agentd.conf 1 2 3 4 5 Server=192.168.1.83 //配置zabbix_server服务端服务器的IP地址 ServerActive=192.168.1.83 Hostname=linux_server1 //配置主机名 PidFile=/var/tmp/zabbix_agentd.pid //指定pid路径 LogFile=/var/log/zabbix/zabbix_agentd.log //指定日志文件 保存退出\n1 2 3 mkdir /var/log/zabbix touch /var/log/zabbix/zabbix_agentd.log chown -R zabbix.zabbix /var/log/zabbix 5.启动客户端服务并进程测试。\n1 /etc/init.d/zabbix_agentd start 1 ps aux| grep zabbix 在zabbix的服务端执行下面的命令测试与客户端是否联通\n1 /data/zabbix/bin/zabbix_get -s 192.168.1.160 -p10050 -k”net.if.in[eth0,bytes]” 可以得到网卡信息说明客户端与服务端可以正常通信。\n7、配置Windows客户端 1.从官方下载Zabbix Agent后，压缩包里面有2个目录\n在C盘下创建一个为zabbix的目录，在bin文件夹下有一个为win32和win64两个目录，每个目录下应该有3个.exe程序，分别为：zabbix_agentd.exe zabbix_get.exe zabbix_sender.exe\n2.根据自己的操作系统复制相应的win32/win64里边的数据到刚创建好的c:\\zabbix目录下\n3.复制解压后zabbix_agents_2.4.4.win文件夹conf里的在C盘的zabbix目录下的conf文件夹下有个zabbix_agentd.win.conf修改一下内容重命名zabbix_agentd.conf到c:\\zabbix下\nLogFile=c:\\zabbix\\zabbix_agentd.log\nServer=\u0026lt;服务端IP地址\u0026gt;\nHostname=win_server1\n4.安装zabbix客户端。依次执行 开始–\u0026gt;运行–\u0026gt;cmd(也可以使用win+R快捷键直接打开)，在打开的命令提示符下执行下面的命令\ncd c:\\zabbix\nzabbix_agentd.exe –c c:\\zabbix\\zabbix_agentd.conf -i\n看到上面的信息说明agent已经安装成功了。\n5.启动客户端\nzabbix_agentd.exe –s\n如果在启动的时候报错，说cannot open config file[C:\\zabbix_agentd.conf]: [2] No such file or directory，把配置文件复制到c:\\一份即可\nzabbix_agentd.exe可用参数介绍：\n-c 指定配置文件所在位置\n-i 安装客户端\n-s 启动客户端\n-x 停止客户端\n-d 卸载/删除客户端\n可以看到客户端已经监听在了10050端口上。打开windows管理工具—\u0026gt;服务，查看一下\n8、在zabbix服务器添加主机 ","date":"2020-05-09T18:03:18Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202005091803/","title":"Centos7安装Zabbix服务端、Zabbix客户端和Win客户端配置（源码编译安装）"},{"content":"目录\n第一章 初入公司\n第2章 第一阶段：解决物理服务器单电问题\n第3章 第二阶段：解决服务器虚拟化问题\n第4章 第三阶段：数据库备份\n第5章 第四阶段：解决数据库单点\n第6章 第五阶段：完善监控项\n第7章 第六阶段：统一服务的安装方式\n第8章 第七阶段：关键服务 NFS 迁移备份\n第9章 第八阶段：服务拆分-用户和爬虫流量分离-ELK 日志收集\n第10章 第九阶段：增加第二机房-大数据服务\n完整架构图\n整体结构演变\n第一章 初入公司 1.1 架构拓扑图 1.2 存在的问题汇总 1.物理服务器电源单电，机房电力切割时需要关闭所有服务器，导致服务中断 4-6 小时 2.物理服务器配置不当，有的服务器系统盘用 SSD，数据盘反而用机械盘 3.服务器虚拟化 ESXi 上的虚拟机经常无故变成系统只读导致服务不可用 4.操作系统不统一，有 debian，有 FreeBSD 5.数据库单点，且没有备份以及恢复计划 6.keepalived 配置错误导致 HA 高可用没有生效 7.重要的 NFS 服务器 5T 的数据没有备份，且操作系统为 FreeBSD 8.zabbix 监控项过多，报警内容太多，关键报警被淹没 9.软件部署方式不统一，有编译/脚本/tar 包/deb 包多种安装方式 10.脚本不统一，脚本随处存放，命名混乱，很多不知道有没有用 11.没有批量操作工具，依靠纯手工或脚本操作 第2章 第一阶段：解决物理服务器单电问题 2.1 问题现象 1.物理服务器电源单模块，如果机房进行电力切割，必须关闭所有服务器，等机房电力切割结束再把所有服务器开机\n2.2 导致后果 1.电力切割期间所有的业务完全中断，用户在此期间无法访问\n2.服务没有做开机自启动，重新开机之后需要手动起服务\n3.因为服务器老旧，关机之后可能起不来\n4.因为关机时暴力 kill 了服务，导致服务器重启后数据损坏\n2.3 解决方案 1.因为服务器型号老旧，厂商已经停产，买不到新电源\n2.退而求次，淘宝联系卖二手服务器的卖家，购买服务器相同型号的电源\n3.服务器电源属于热插拔，可以直接插上测试\n4.验收方法为：\n- 两个电源模块都插上电，然后拔掉第一个电源模块，查看服务器有没有重启。\n- 如果服务器没有重启，再全部插上电源，然后再拔掉另一个电源，查看有没有重启\n- 如果都没有重启，就证明服务器双电模块运行正常\n2.4 价值体现 1.由原来业务需要中断 4-6 小时缩短为 0 中断\n2.给公司减少了由于业务中断导致的经济损失\n3.运维再也不用在机房通宵熬夜等待电力切割了\n第3章 第二阶段：解决服务器虚拟化问题 3.1 问题现象 1.服务器虚拟化采用的是 VMware ESXi 虚拟化平台，运行的虚拟机经常无故的系统变成只读，导致服务中断\n3.2 导致后果 1.虚拟机运行的服务不可用，导致业务中断，影响用户体验\n3.3 解决方案 1.找台新服务器安装部署新版本的 ESXi,安装 VCenter 管理中心\n2.迁移旧服务到新服务器\n3.持续观察检验\n3.4 价值体现 1.新版本虚拟机运行稳定，业务不会中断\n2.服务器虚拟化充分利用了硬件资源，为公司节省了购买 3 台物理服务器的价格\n第4章 第三阶段：数据库备份 4.1 问题现象 1.mysql 数据库只有一台主从，没有备份\n2.redis 和 mongo 单点，且没有备份\n3.数据库服务器数据盘均为单块 SSD 固态硬盘，一旦损坏没有修复的希望\n4.2 导致后果 1.如果数据库服务器硬件或者硬盘损坏，没有数据可以提供恢复，后果不堪设想\n4.3 解决方案 1.这个阶段重点是先备份数据\n2.新增加一个 mysql 从库，使用 xtrabackup 在从库上每天定时备份数据\n3.新增加一个 redis 从库，每天物理备份 redis 持久化的 RDB 文件\n4.新增加一个 mongo 从库，每天使用 mongodump 导出数据\n4.4 价值体现 1.在服务器有限和时间有限的情况下，按照紧急度优先解决数据库备份\n2.降低了因为物理服务器损坏或者人为操作失误导致的数据灾难性丢失情况\n3.为下一步数据库服务集群化提供了基础保障\n4.5 拓扑图 第5章 第四阶段：解决数据库单点 5.1 问题现象 1.数据库故障到完全修复过程中，业务会中断，用户不能访问\n5.2 导致后果 1.虽然已经做了数据库备份，但是一旦数据库损坏，恢复起来依然需要很多时间\n2.由于业务代码写死了 IP 地址，一旦数据库损坏，在其他机器上恢复了，业务代码也需要变更 IP 地\n址，重新发布，耗时太长\n5.3 解决方案 1.对目前单点数据库升级到集群架构\n2.新增加 mysql 从库。\n3.redis 由单节点升级为 redis cluster 集群\n4.mongo 由主从复制升级为 3 节点副本集\n5.新部署 elasticsearch 服务集群\n5.4 实施难点 1.对于运维来说，安装部署不复杂，但是难题在于由于数据库架构升级，从而导致业务代码连接方\n式发生改变\n2.因为现在的业务代码都是采用框架开发，所以很多数据库插件需要升级，比如 redis 和 mongo 都需\n要升级插件才能支持集群化连接\n3.需要提前在开发环境搭建部署好。然后开发测试没有问题之后，再采用轮训升级的方式逐步升级\n升级数据库以及代码框架\n5.5 价值体现 1.对数据库的稳定性以及安全性有了质的改变\n2.由于数据库集群化，性能得到了大大的提升，用户打开网站访问速度更好，体验更好\n3.由于数据库集群化，提供了冗余，所以即使服务器损坏或者维护也不会影响架构改变，用户访问\n也不会中断\n4.减少了因为数据库宕机或恢复导致的经济损失\n5.6 拓扑图 第6章 第五阶段：完善监控项 6.1 问题现象 1.监控项报警内容太多，有时候几分钟上百条\n6.2 导致后果 1.关键的报警信息被不重要的报警淹没，以至于不能第一时间发现关键问题\n2.由于报警监控项配置不当，导致连锁反应，牵动很多不需要的警告\n3.由于短时间大量的报警产生，对 zabbix 服务器造成很大压力，甚至导致 zabbix 服务宕机\n4.如果配置了短信报警，那么大量的非关键报警导致消耗很多短信条目，很花钱！\n6.3 解决方案 1.优化报警内容，只监控必须要监控的，模版自带的一些不需要的监控项停掉\n2.报警分级，将报警信息按紧急度拆分，所有报警都发邮件，紧急的报警追加发送到微信\n3.取消短信报警，改为邮件和微信报警\n4.报警内容优化，尽量看标题就能知道报警内容是什么\n6.4 价值体现 1.接受的报警量大大减少，运维的注意力可以更集中在关键的问题处理上\n2.通过不同的发送介质第一时间了解问题的紧急度，比如发送到微信上的都是需要立刻处理的\n3.取消短信报警为公司节省开销\n第7章 第六阶段：统一服务的安装方式 7.1 问题现象 1.服务安装混乱，有脚本安装，有 tar 包安装，有 deb 包安装，有编译安装\n2.软件存放目录不统一，数据存放目录不统一，脚本存放目录不统一\n3.防火墙规则不统一，有一些不用的规则没有清理\n7.2 导致后果 1.一些操作无法进行批量执行\n2.安装服务基本需要手动操作\n3.文档缺失，不知道以前怎么装的，不知道以前配了什么参数\n4.防火墙规则混乱，有一些已经不用的服务，但是规则没有删除，看起来很乱\n7.3 解决方案 1.引入 ansible 批量管理工具，统一服务安装方式，新服务统一使用 playbook 安装\n2.统一服务安装目录，数据目录，日志目录\n3.清理防火墙不用的规则，只保留必须的规则\n4.编写运维文档，制定运维规范\n7.4 价值体现 1.新服务安装部署由原来的按小时缩短为分钟级别，提高了运维效率\n2.避免了因为手动操作敲错命令导致的不可预料后果，实现了一次编写重复运行\n3.编写运维文档，为部门同事和公司留下可持续的价值输出\n第8章 第七阶段：关键服务 NFS 迁移备份 8.1 问题现象 1.公司成立以来所有的图片数据都保存在一台安装 FreeBSD 操作系统的老旧服务器上且没有备份\n8.2 导致后果 1.NFS 服务器的数据高达 5T，且没有备份，一旦服务器或硬盘损坏，后果不堪设想\n2.由于 FreeBSD 操作系统的原因，没有办法使用 sersync 来进行实时同步备份\n3.一旦服务器发生损坏，所有业务网站的图片服务都将失效，短时间没有替代方案\n8.3 解决方案 1.虽然 FreeBSD 系统不能使用 sersync，但是可以使用 rsync\n2.找一台性能足够服务器，使用四块 6T 磁盘制作 RAID10，安装部署 rsync 服务端和 NFS 服务端\n3.分目录进行同步，总目录数据量太大，同步起来时间按天算，拆分成一个个小目录分批进行同步\n4.统计记录每次同步的时间，最高在半夜业务低峰期进行同步\n5.和开发沟通切换方案，开发修改代码，用户上传数据同时往 2 台服务器上写入，保证数据一致性\n6.业务低峰期进行切换，在前端代理轮流更新 WEB 服务器的卸载旧 NFS，然后重新挂载新 NFS\n7.修改个别权限不对的目录。测试所有上传业务是否可以正常的写入\n8.为了防止意料不到的意外情况，旧服务器数据继续保留 1 个月。确定一切正常之后重做旧服务器\n9.然后使用 sersync 同步数据到新服务器，持续观察，保证同步正常工作\n8.4 价值体现 1.在业务不中断，数据不丢失的情况下为公司的宝贵资产提供了可靠的备份方案\n2.烫手的山芋在运维手里得到了解决，荣誉感，成就感大增\n3.在公司领导和同事心中留下了可靠可信赖的印象\n8.5 拓扑图 第9章 第八阶段：服务拆分-用户和爬虫流量分离-ELK 日志收集 9.1 问题现象 1.因为公司有论坛，需要爬虫来爬取\n2.但是爬虫的流量和用户的流量混合在了一起，且没有统一的日志管理平台查看过滤\n9.2 导致后果 1.由于流量没有分离，导致当有大量的爬虫访问的时候，对服务器和数据库造成非常大的压力\n2.当爬虫访问频次过高时，会导致数据库连接数变多，系统负载变高，影响用户正常服务\n3.同样，由于爬虫流量和用户流量混在了一起，没有很好的办法分析压力高的原因\n4.开发或者运营想看一些日志数据只能找运维解决，每次分析都要花很久，分析脚本没有办法复用\n9.3 解决方案 1.新增加一台 WEB 服务器专门给爬虫使用，页面做静态化，优化爬虫抓去效率\n2.数据库读写分离，爬虫抓取如果需要查询数据库，把流量指向没有业务访问的从库\n3.前端代理根据访问的请求报文里的 user-agent 来分离爬虫和用户的访问流量，如果是爬虫就把流量引到给爬虫抓取的 WEB 服务器上，用户正常的流量还是正常访问 WEB 集群。\n4.搭建部署 ELK 日志收集平台，集中收集所有 Nginx 访问日志，使用 kibana 作为过滤搜索和展示，\n制作好用户展示数据的图表，培训部门同事 ELK 的使用和查询方法\n9.4 价值体现 1.将用户和爬虫流量分离，当爬虫大量访问时候正常用户访问也不会受到影响，用户体验大大提升\n2.搭建部署了 ELK 日志分析平台，方便部门同事查询数据，定位故障时间大大地缩减，大大提高了工作效率\n9.5 拓扑图 第10章 第九阶段：增加第二机房-大数据服务 10.1 项目需求 1.业务发展，需要部署 Hadoop 平台和 kafka+zookeeper 服务\n2.由于 IDC 机房只有一个机架，所以需要在第二机房部署\n10.2 价值体现 1.测试环境测试没问题后使用 ansible 批量部署到新的大数据服务器，节省了部署时间\n2.大数据服务可以更好的分析用户访问行为，根据分析结果可以更精确的给用户推送推荐内容\n10.3 拓扑图 完整架构图 整体结构演变 ","date":"2020-04-29T17:20:47Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/202004291720/","title":"运维工作经验汇总-高级运维工程师"},{"content":"手误【删库】 == 跑路，不存在的 ——删瓦辛格 前言 今天公司服务器的宝塔打不开，让我去修（ps：宝宝委屈）\n打开找一下问题所在\n问题： 发现是宝塔官方的cdn好像挂掉了\n解决思路： （1）本地重新搞个服务器装宝塔，发现没问题 （2）比较一下，发现线上的确实cdn有问题 （3）把本地的可以用的cdn，放到线上就行。 解决： 卧槽，我发下，我根本不了解宝塔的目录结构【ps：解决毛线，下班走人】\n正文 别眨眼，接下的我sb的操作，真的亮瞎我狗眼【ps：别害怕，也会亮瞎你的狗眼的，嘿嘿。。。】\n常规操作： （1）备份要修改的文件，这是职业本能，OK，没毛病，我喜欢。\n（2）删除那个没用的文件 【文件：你才没用，看老子的移魂大法】\n重点： 备份一切over，删除时，我手贱多打了个 * 。\n**然后没思考，一个回车，那感觉爽啊 **\n求德玛得\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;..雅蠛蝶\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip; 对，ctrl +c 你已经看出我的绝望了 说时迟那时快，掏出手机，抢票一张去往非洲的机票 我多打一个 * ，导致把线上环境的 nginx服务，php环境，redis缓存，mysql数据库，全部删掉了\n不多说，赶紧跑路了\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;.. 飞机延时，公司发现 跑路失败，当场被捕 数据恢复 （1）停止对服务器进行一切的写入操作，卸载挂载 （2）安装extundelete，进行扫描 （3）恢复数据，环境再次重新工作 PS：因为这个服务器没有搞本地备份和异地本地，所以才有下面这些破事，再不是，直接备份恢复就行了\n（1）停止对服务器进行一切的写入操作，卸载挂载\n当发现误删除文件时，为了尽可能的恢复数据，先要关掉所有的正在进行的服务，不要再进行数据的写入，要不然恢复的概率那就低了。我们可以直接\n1 # killall 进程名 或者\n1 # kill -9 pid 然后把误删除的文件所在分区，重新挂载成只读的\n1 # mount -o ro /dev/sdb /data/ （2）安装extundelete\n1 2 yum install bzip2 gcc-c++ e2fsprogs* -y wget http://nchc.dl.sourceforge.net/project/extundelete/extundelete/0.2.4/extundelete-0.2.4.tar.bz2 1 2 tar jxvf extundelete-0.2.4.tar.bz2 cd extundelte-0.2.4 1 2 ./configure make \u0026amp;\u0026amp; make install 验证安装结果\n1 extundelete -v （3）进行扫描，恢复数据\n恢复指定文件：\n**原理：**从根节点(inode=2)开始找到被删除文件的i节点，然后recover i节点。\n删除目录：/www/server\n先检测被删除的文件有哪些：\n1 extundelete /dev/mapper/centos-root --inode 2 1 extundelete /dev/mapper/centos-root --inode 1703938 1 extundelete /dev/mapper/centos-root --inode 1703940 1 extundelete /dev/mapper/centos-root --restore-directory /www/server 1 2 3 4 5 6 7 8 #查看能恢复的数据： [root@localhost ~]# extundelete /dev/sdc1 --inode 2 #恢复单个文件 [root@localhost ~]# extundelete /dev/sdc1 --restore-file somefile #恢复目录 [root@localhost ~]# extundelete /dev/sdc1 --restore-directory /somedir #恢复所有文件 [root@localhost ~]# extundelete /dev/sdb1 --restore-all 能恢复多少，就靠运气 后记： 运气较好，恢复了数据\n把生成环境搭建出来，跑上面完全没有问题\n切记 （1）记得备份 （2）不要用rm （3）干啥要三思 下次更新，“垃圾rm，毁我青春，我直接把你删掉” 预知后事，且听下回分解【关注我，期待下次分享】 ","date":"2020-04-22T18:42:41Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/202004221842/","title":"手误【删库】 ==  跑路，不存在的  ——删瓦辛格"},{"content":"概述 集群对时间同步要求高，实际使用环境中必须确保集群中所有系统时间保持一致\nChrony是一个开源的自由软件，像CentOS 7或基于RHEL 7操作系统，已经是默认服务，默认配置文件在 /etc/chrony.conf 它能保持系统时间与时间服务器（NTP）同步，让时间始终保持同步。相对于NTP时间同步软件，占据很大优势。\n需要注意的是，配置完/etc/chrony.conf后，需重启chrony服务，否则可能会不生效\n主机名称IP地址Master192.168.1.10Node1192.168.1.20Node2192.168.1.30 1、安装chrony CentOS7中已经默认安装了chrony，其配置文件路径在\n1 /etc/chrony.conf 如果系统内没有chrony，请按照如下进行安装，启动并检查相关状态\n1 2 3 4 5 yum install chrony -y systemctl enable chronyd.service systemctl restart chronyd.service systemctl status chronyd.service 在防火墙内放行，因NTP使用123/UDP端口协议，所以允许NTP服务即可。（如果已关闭防火墙请无视）\n1 2 firewall-cmd --add-service=ntp --permanent firewall-cmd --reload 2、检查设置时区 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # timedatectl Local time: Fri 2018-2-29 13:31:04 CST Universal time: Fri 2018-2-29 05:31:04 UTC RTC time: Fri 2018-2-29 08:17:20 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a #如果你当前的时区不正确，请按照以下操作设置。 #查看所有可用的时区： # timedatectl list-timezones #筛选式查看在亚洲S开的上海可用时区： # timedatectl list-timezones | grep -E \u0026#34;Asia/S.*\u0026#34; Asia/Sakhalin Asia/Samarkand Asia/Seoul Asia/Shanghai Asia/Singapore Asia/Srednekolymsk #设置当前系统为Asia/Shanghai上海时区： # timedatectl set-timezone Asia/Shanghai #设置完时区后，强制同步下系统时钟： # chronyc -a makestep 200 OK 3、Master节点配置chrony 此处选用的是阿里云的ntp服务server，地址为\n1 ntp1.aliyun.com 当前是Master，ip地址为192.168.1.10，网段是192.168.1.0/24，配置详情如下，红色为更改部分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 vim /etc/chrony.conf server ntp1.aliyun.com iburst # Record the rate at which the system clock gains/losses time. driftfile /var/lib/chrony/drift # Allow the system clock to be stepped in the first three updates # if its offset is larger than 1 second. makestep 1.0 3 # Enable kernel synchronization of the real-time clock (RTC). rtcsync # Enable hardware timestamping on all interfaces that support it. #hwtimestamp * # Increase the minimum number of selectable sources required to adjust # the system clock. #minsources 2 # Allow NTP client access from local network. #allow 192.168.0.0/16 allow 192.168.1.0/24 # Serve time even if not synchronized to a time source. local stratum 10 # Specify file containing keys for NTP authentication. #keyfile /etc/chrony.keys # Specify directory for log files. logdir /var/log/chrony # Select which information is logged. #log measurements statistics tracking 重启\n1 2 # systemctl restart chronyd.service # systemctl status chronyd.service 1 2 3 4 5 6 7 8 强制同步下系统时钟： # chronyc -a makestep 查看时间同步源状态： # chronyc sourcestats 查看时间同步源： # chronyc sources -v 4、Node节点配置chrony node节点(192.168.26.136)只需要注释掉原来的ip，新增Master主机的IP地址即可（记得重启chrony服务）\n1 2 # systemctl restart chronyd.service # systemctl status chronyd.service 1 2 3 4 5 6 7 8 强制同步下系统时钟： # chronyc -a makestep 查看时间同步源状态： # chronyc sourcestats 查看时间同步源： # chronyc sources -v Node2也是以上步骤\n5、 chrony.conf文件详解 以下是系统默认配置文件的说明(CentOS7)，了解一下各个配置是做什么。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # 使用pool.ntp.org项目中的公共服务器。以server开，理论上你想添加多少时间服务器都可以。 # Please consider joining the pool (http://www.pool.ntp.org/join.html). server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst # 根据实际时间计算出服务器增减时间的比率，然后记录到一个文件中，在系统重启后为系统做出最佳时间补偿调整。 driftfile /var/lib/chrony/drift # chronyd根据需求减慢或加速时间调整， # 在某些情况下系统时钟可能漂移过快，导致时间调整用时过长。 # 该指令强制chronyd调整时期，大于某个阀值时步进调整系统时钟。 # 只有在因chronyd启动时间超过指定的限制时（可使用负值来禁用限制）没有更多时钟更新时才生效。 makestep 1.0 3 # 将启用一个内核模式，在该模式中，系统时间每11分钟会拷贝到实时时钟（RTC）。 rtcsync # Enable hardware timestamping on all interfaces that support it. # 通过使用hwtimestamp指令启用硬件时间戳 #hwtimestamp eth0 #hwtimestamp eth1 #hwtimestamp * # Increase the minimum number of selectable sources required to adjust # the system clock. #minsources 2 # 指定一台主机、子网，或者网络以允许或拒绝NTP连接到扮演时钟服务器的机器 #allow 192.168.0.0/16 #deny 192.168/16 # Serve time even if not synchronized to a time source. local stratum 10 # 指定包含NTP验证密钥的文件。 #keyfile /etc/chrony.keys # 指定日志文件的目录。 logdir /var/log/chrony # Select which information is logged. #log measurements statistics tracking 6、chrony的优势 chrony的优势更快的同步只需要数分钟而非数小时时间，从而最大程度减少了时间和频率误差，这对于并非全天 24 小时运行的台式计算机或系统而言非常有用。\n能够更好地响应时钟频率的快速变化，这对于具备不稳定时钟的虚拟机或导致时钟频率发生变化的节能技术而言非常有用。\n在初始同步后，它不会停止时钟，以防对需要系统时间保持单调的应用程序造成影响。\n在应对临时非对称延迟时（例如，在大规模下载造成链接饱和时）提供了更好的稳定性。\n无需对服务器进行定期轮询，因此具备间歇性网络连接的系统仍然可以快速同步时钟。\n","date":"2020-04-20T11:50:43Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/202004201150/","title":"Centos7集群时间同步（Chrony）"},{"content":" 近期写一个Python系统硬件监控，准备发布到Linux上。\n下面一起看看怎么把项目发布到Linux上吧。\n环境要求 Python版本：3.7\nWindows运行项目 Centos7运行项目 因为centos7的python环境是2.75的。\n所以我们首先把Python环境换成3.7的才行。\nCentos7升级Python3.7.3版本\n上面是教程。\n（1）打包Python项目的依赖（也就是本地安装的项目依赖） 1 pip3 freeze \u0026gt; requirements.txt （2）压缩Python项目和上传到服务器，解压 zip包的解压命令：unzip 包名\n（3）cd到项目里，安装依赖 1 pip3 install -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com -r requirements.txt （4）运行项目启动文件 1 python3 manage.py 项目已经运行成功，还有最简便的方法就是运行在docker容器里，更加方便，后续会更新\n","date":"2020-04-18T17:04:28Z","image":"https://www.ownit.top/title_pic/23.jpg","permalink":"https://www.ownit.top/p/202004181704/","title":"Python项目打包发布Linux线上"},{"content":"购物车练习程序 （1）可以显示商品列表\n（2）根据商品id进行购买\n（3）根据输入的工作来判断是否有足够的钱购买\n（4）退出时，显示购买的商品和卡中的余额\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 购物车练习程序 product_list = [ (\u0026#39;iphone\u0026#39;, 5000), (\u0026#39;Mac\u0026#39;, 9000), (\u0026#39;Bike\u0026#39;, 800), (\u0026#39;watch\u0026#39;, 600), (\u0026#39;book\u0026#39;, 600), ] shopping_list = [] def shop(): salay = input(\u0026#34;请输入你的工资:\u0026#34;) if salay.isdigit(): salay = int(salay) while True: for index, itme in enumerate(product_list): #print(product_list.index(itme),itme) print(index,itme) user_choice = input(\u0026#34;选择要买的商品？请选择购买的编号！\u0026#34;) if user_choice.isdigit(): user_choice = int(user_choice) if user_choice \u0026lt; len(product_list) and user_choice \u0026gt;-1: p_itme = product_list[user_choice] if p_itme[1] \u0026lt;= salay:#买的起 shopping_list.append(p_itme) salay-=p_itme[1] print(\u0026#34;Added %s into shopping cart ,you current balance is \\033[31;1m%s\\033[0m\u0026#34; %(p_itme,salay)) else: print(\u0026#34;\\033[41;1m你的余额不足，只剩[%s]\\033[0m\u0026#34; % salay) else: print(\u0026#34;请输入正确商品编号\u0026#34;) elif user_choice == \u0026#39;q\u0026#39;: print(\u0026#39;----------------------shopping list----------------------------\u0026#39;) for p in shopping_list: print(p) print(\u0026#34;\\033[41;1m你的余额[%s]\\033[0m\u0026#34; % salay) exit() else: print(\u0026#34;请输入正确的编号\u0026#34;) else: print(\u0026#34;你输入的工资格式不对，请输入正确的格式\u0026#34;) shop() shop() 学生管理系统 欢迎使用[学生管理系统] V1.0 1.显示所有学生信息 2.新建学生信息 3.查询学生信息 4.修改学生信息 5.删除学生信息 0.退出系统\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 \u0026#34;\u0026#34;\u0026#34; 欢迎使用[学生管理系统] V1.0 1.显示所有学生信息 2.新建学生信息 3.查询学生信息 4.修改学生信息 5.删除学生信息 0.退出系统 \u0026#34;\u0026#34;\u0026#34; # 模拟学生数据 from datetime import datetime student_data = [ { \u0026#39;name\u0026#39;: \u0026#39;南宫乘风\u0026#39;, \u0026#39;sex\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;address\u0026#39;: \u0026#39;西安\u0026#39;, \u0026#39;birthday\u0026#39;: \u0026#39;20000229\u0026#39; }, { \u0026#39;name\u0026#39;: \u0026#39;乘风\u0026#39;, \u0026#39;sex\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;address\u0026#39;: \u0026#39;洋县\u0026#39;, \u0026#39;birthday\u0026#39;: \u0026#39;20101229\u0026#39; } ] # 学生类 class Student: # 学生初始化 def __init__(self, name, sex, address, birthday): self.name = name self.sex = sex self.address = address self.birthday = birthday # 获取学生年龄 def get_age(self): if self.birthday: age = datetime.now().year - int(self.birthday[:4]) return age else: print(\u0026#34;不知道\u0026#34;) # 学生管理系统类 class System: # 初始化 def __init__(self, name): self.name = name self.data = [] # 美化输出打印 def beauty_print(self, data_list): for index, student in enumerate(data_list): print(f\u0026#34;序号：{index}\u0026#34;, end=\u0026#39;\\t\u0026#39;) print(f\u0026#34;姓名：{student.name}\u0026#34;, end=\u0026#39;\\t\u0026#39;) print(f\u0026#34;性别：{student.sex:2}\u0026#34;, end=\u0026#39;\\t\u0026#39;) print(f\u0026#34;地址：{student.address}\u0026#34;, end=\u0026#39;\\t\u0026#39;) print(f\u0026#34;年龄：{student.get_age()}\u0026#34;) # 加载数据 def load_data(self): for item in student_data: student = Student(item[\u0026#39;name\u0026#39;], item[\u0026#39;sex\u0026#39;], item[\u0026#39;address\u0026#39;], item[\u0026#39;birthday\u0026#39;]) self.data.append(student) # 显示菜单 def show_menu(self): # f-string print(f\u0026#34;\u0026#34;\u0026#34; ****************************** 欢迎使用[{self.name}] V2.0 1.显示所有学生信息 2.新建学生信息 3.查询学生信息 4.修改学生信息 5.删除学生信息 0.退出系统 ****************************** \u0026#34;\u0026#34;\u0026#34;) # 启动学生管理系统 def start(self): # 加载数据 self.load_data() while True: self.show_menu() op = input(\u0026#34;选择操作\u0026#34;) if op == \u0026#39;1\u0026#39;: self.show_all_student() elif op == \u0026#39;2\u0026#39;: self.create_student() elif op == \u0026#39;3\u0026#39;: self.find_student() elif op == \u0026#39;4\u0026#39;: self.modify_student() elif op == \u0026#39;5\u0026#39;: self.remove_student() elif op == \u0026#39;6\u0026#39;: print(\u0026#39;退出程序\u0026#39;) break else: print(\u0026#34;请输入正常的操作\u0026#34;) # 选择性别 def choose_sex(self): sex = input(\u0026#34;请选择性别：（1）:男|(2):女\u0026#34;).strip() if sex == \u0026#39;1\u0026#39;: return \u0026#39;男\u0026#39; elif sex == \u0026#39;2\u0026#39;: return \u0026#39;女\u0026#39; else: return \u0026#39;未知\u0026#39; # 判断名字 def input_name(self): while True: name = input(\u0026#34;请输入名字：\u0026#34;).strip() if name: return name else: continue # 根据名字查询 def find_student_name(self): name = self.input_name() find_list = [] for student in self.data: if name.lower() in student.name.lower(): find_list.append(student) if find_list: return find_list else: print(f\u0026#34;没有找到学生：{name}\u0026#34;) # 1.显示所有学生信息 def show_all_student(self): self.beauty_print(self.data) # 2.新建学生信息 def create_student(self): name = self.input_name() sex = self.choose_sex() address = input(\u0026#34;请输入地址\u0026#34;) birthday = input(\u0026#34;请输入生日\u0026#34;) student = Student(name, sex, address, birthday) self.data.append(student) # 3.查询学生信息 def find_student(self): # name = self.input_name() # for student in self.data: # if name.lower() in student.name.lower(): # self.beauty_print([student]) find_list = self.find_student_name() self.beauty_print(find_list) # 4.修改学生信息 def modify_student(self): find_list = self.find_student_name() if find_list: self.beauty_print(find_list) index = int(input(\u0026#34;选择序号:\u0026#34;)) student = find_list[index] print(\u0026#39;当前修改的是：\u0026#39;) self.beauty_print([student]) student.name = input(\u0026#39;输入新的名字：\u0026#39;).strip() student.sex = self.choose_sex() student.address = input(\u0026#34;请输入地址：\u0026#34;) student.birthday = input(\u0026#34;请输入生日：\u0026#34;) print(f\u0026#39;{student.name}已经修改\u0026#39;) return # 5.删除学生信息 def remove_student(self): find_list = self.find_student_name() if find_list: self.beauty_print(find_list) index = int(input(\u0026#34;选择序号:\u0026#34;)) student = find_list[index] print(\u0026#39;当前删除的是：\u0026#39;) self.beauty_print([student]) self.data.remove(student) return else: print(f\u0026#39;没有的人\u0026#39;) # 0.退出系统 if __name__ == \u0026#39;__main__\u0026#39;: student_sys = System(\u0026#39;乘风系统\u0026#39;) student_sys.start() ","date":"2020-04-14T15:53:27Z","image":"https://www.ownit.top/title_pic/36.jpg","permalink":"https://www.ownit.top/p/202004141553/","title":"Python购物车和学生管理系统"},{"content":"目录\n前言：\n正文：\n1.下载Python3.7.3的镜像\n2、解压 tar -xzvf Python-3.7.3.tgz\n3、cd Python-3.7.3\n4、安装到/usr/local目录中\n5、make \u0026amp;\u0026amp; make altinstall\n6、验证\n7、cd /usr/bin\n8、备份之前的python 9、创建软连接 问题\n1、更改yum脚本的python依赖​\n2、修改urlgrabber配置文件\n备注：\n前言： 我们使用的centos7镜像，里面都内置的Python，但都是python2的版本，比较落后。\n现在有的有Python3已经出来，有的程序运行需要Python3的环境支持。\n安装下面操作，能够正确安装和替换Python2，如果操作有问题请下方留言\n正文： 1.下载Python3.7.3的镜像 1 wget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz 2、解压 tar -xzvf Python-3.7.3.tgz 1 tar -xzvf Python-3.7.3.tgz 3、cd Python-3.7.3 1 cd Python-3.7.3 4、安装到/usr/local目录中 1 ./configure --prefix=/usr/local/python3 执行这步后，会检测程序。等检测完毕，是否有报错或者依赖没安装。\n5、make \u0026amp;\u0026amp; make altinstall 1 make \u0026amp;\u0026amp; make altinstall 执行这步后，会进行编译，然后安装程序到指定的目录\n6、验证 直接先运行python3，再确认一下版本信息：\n7、cd /usr/bin 8、备份之前的python 1 mv python python.bak 9、创建软连接 1 ln -s /usr/local/python3/bin/python3.7 /usr/bin/python 问题 1、更改yum脚本的python依赖 1 vi /usr/bin/yum #!/usr/bin/python 改为 #!/usr/bin/python2\n2、修改urlgrabber配置文件 1 vi /usr/libexec/urlgrabber-ext-down #!/usr/bin/python 改为 #!/usr/bin/python2\n备注： 1、3.6的依赖 没有执行 报错了 后续有需要再逐步加上这些依赖吧\n1 yum install openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel 2、3.7的依赖包（一定要在安装前先install 否则安装会报错）\n1 yum install openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel libffi-devel 测试：输入python 查看最新的版本\n已经完成，可以正常使用python3了\n","date":"2020-04-10T10:47:58Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/202004101047/","title":"Centos7升级Python3.7.3版本"},{"content":"docker-compose安装配置 二进制安装\n1、下载最新版的 docker-compose 二进制执行文件。\n1 sudo curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose 2、配置可执行权限。\n1 sudo chmod +x /usr/local/bin/docker-compose 3、测试是否安装成功。\n1 docker-compose --version 使用docker-compose启动yml，一键配置和启动容器\n对于容器开机启动，本地数据持续化，保留数据\n安装docker-compose\n运行：\n1 docker-compose up -d 但是这样运行的话，可能jenkins会运行，但是会报错，因为jenkins_home的权限不够，导致无法写入数据。\n编写一个shell脚本，和这个docker-compose.yml放在一个目录下\n直接运行这个脚本，就可以实现gitlab和jenkins的完美运行。\n两个 开始消耗的内存和cpu过大，需要等待一会就可以了。\ngit.sh\n1 2 3 4 #!/bin/bash mkdir -p /usr/local/docker/jenkins/jenkins_home chown -R 1000:1000 /usr/local/docker/jenkins/jenkins_home docker-compose up -d docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 version: \u0026#39;3\u0026#39; services: gitlab: image: twang2218/gitlab-ce-zh:9.4 restart: always hostname: 192.168.1.100 environment: TZ: \u0026#39;Asia/Shanghai\u0026#39; GITLAB_OMNIBUS_CONFIG: | external_url \u0026#39;http://192.168.1.100:8080\u0026#39; gitlab_rails[\u0026#39;gitlab_shell_ssh_port\u0026#39;] = 2222 unicorn[\u0026#39;port\u0026#39;] = 8888 nginx[\u0026#39;listen_port\u0026#39;] = 8080 ports: - \u0026#39;8080:8080\u0026#39; - \u0026#39;8443:443\u0026#39; - \u0026#39;2222:22\u0026#39; volumes: - /usr/local/docker/gitlab/config:/etc/gitlab - /usr/local/docker/gitlab/repo:/var/opt/gitlab - /usr/local/docker/gitlab/logs:/var/log/gitlab jenkins: restart: always image: jenkins/jenkins container_name: docker_jenkins ports: - \u0026#39;8081:8080\u0026#39; - \u0026#39;50000:50000\u0026#39; volumes: - /usr/local/docker/jenkins/jenkins_home:/var/jenkins_home environment: JAVA_OPTS: \u0026#39;-Djava.util.logging.config.file=/var/jenkins_home/log.properties\u0026#39; ","date":"2020-03-31T15:34:57Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/202003311534/","title":"Docker一键部署GitLab+Jenkins【本地持久化】"},{"content":"自动增加公钥 需求：\n提示要输入对方的ip和root密码，然后可以自动把本机的公钥增加到对方机器上，从而实现密钥认证。\n1.在使用之前，先安装epel源，yum install expect -y\n2.写分发脚本，后缀为exp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/usr/bin/expect#!/bin/bash #name:南宫乘风 #email：heian99@163.com #自动添加公钥到指定的服务器 set host_ip [lindex $argv 0] spawn ssh-copy-id -i /root/.ssh/id_rsa.pub $host_ip expect { -timeout 60 \u0026#34;(yes/no)?\u0026#34; { send \u0026#34;yes\\n\u0026#34;;exp_continue} \u0026#34;password:\u0026#34; { send \u0026#34;root\\n\u0026#34;} #填写服务器的同一密码 timeout {puts \u0026#34;Connect timeout!\u0026#34;;return} } expect eof exit -onexit { send_user \u0026#34;Job has finished!\u0026#34; } 注：set的作用是设置变量，spawn设置执行命令时，可以引用变量；变量的第一个参数为0\n编写ip.txt,存放ip地址\n3.执行以下命令开始分发\n1 for ip in `cat /root/ip.txt`;do expect /root/ssh.exp $ip ;done 如果密码不一样，也可以定义到ip.txt的文本里面，通过awk获取到。\n然后传值给expect。可以实现不同ip和密码的自动批量秘钥传输。\n","date":"2020-03-30T13:41:03Z","image":"https://www.ownit.top/title_pic/24.jpg","permalink":"https://www.ownit.top/p/202003301341/","title":"Linux自动批量增加公钥"},{"content":"单个服务器监控 监控远程的一台机器(假设ip为192.168.1.100)的存活状态，当发现宕机时发一封邮件给你自己\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash #name:南宫乘风 #email：heian99@163.com ip=\u0026#34;192.168.1.100\u0026#34; while true ; do # 利用ping检查主机是否存活 ping -c 4 $ip \u0026gt; /dev/null 2\u0026gt; /dev/null if [ $? != \u0026#34;0\u0026#34; ]; then # 失败提示，可以通过邮件发送信息（mailx） echo \u0026#34;$ip已经挂掉\u0026#34; #echo \u0026#34;服务器$ip坏掉，请及时处理\u0026#34; | mail -s \u0026#34;$ip服务器挂掉\u0026#34; 1794748404@qq.com else echo \u0026#34;$ip正常\u0026#34; fi sleep 3s done 批量服务器监控 监控多台服务器，可以使用文本记录ip或者使用数组\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #name:南宫乘风 #email：heian99@163.com #此处也可以使用文本，写入ip地址 #ip=$(cat /data/ip.txt) ip=\u0026#34;192.168.1.100 192.168.1.111 192.168.1.99\u0026#34; while true ; do for i in $ip; do # 利用ping检查主机是否存活 echo \u0026#34;$i\u0026#34; ping -c 4 $i \u0026gt; /dev/null 2\u0026gt; /dev/null if [ $? != \u0026#34;0\u0026#34; ]; then # 失败提示，可以通过邮件发送信息（mailx） echo \u0026#34;$i已经挂掉\u0026#34; #echo \u0026#34;服务器$i坏掉，请及时处理\u0026#34; | mail -s \u0026#34;$i服务器挂掉\u0026#34; 1794748404@qq.com else echo \u0026#34;$i正常\u0026#34; fi sleep 3s done done ","date":"2020-03-28T17:59:29Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/202003281759/","title":"主机存活监控"},{"content":"Linux常见问题及命令总结 1.查询Linux系统相关的 查看linux内核版本 1 cat /etc/version 查看内核版本号 1 uname -r 查看内核/操作系统的信息 1 uname -a 查看系统版本号 1 lsb_release -a 手动释放cache缓存 1 echo 3 \u0026gt; /proc/sys/vm/drop_caches 2.操作文件相关的 查找目录下文件内字符串 1 grep -rn \u0026#34;welcome\u0026#34; *;//查找当前目录下\u0026#34;welcome\u0026#34;字符串, *表示当前目录下所有文件,也可以是文件名 查找指定文件内字符串 1 grep -rn /usr/local/ -e \u0026#34;20003\u0026#34;;//查找在/usr/local目录下文件中包含20003关键字的文件 搜索所有以index开头的文件 1 find /home/tom -name \u0026#39;index*\u0026#39; # 搜索所有以index开头的文件 在/home目录下搜索所有大小超过10000K的文件 1 find /home -size +10000k # 在/home目录下搜索所有大小超过10000K的文件 在/home/software 下查找名字 1 find /home/software/ -type f -name \u0026#34;splunk.tgz\u0026#34;; //在/home/software 下查找名字为\u0026#34;splunk.tgz\u0026#34;的文件 解压redis.tar.gz的文件 1 tar -zxvf redis.tar.gz;//解压redis.tar.gz的文件 解压到指定目录（-C） 1 tar -zxvf jdk-8u72-linux-x64.tar.gz -C /usr/local;//将jdk-8u72-linux-x64.tar.gz 解压到/usr/local目录中 解压到指定目录（-d） 1 unzip gradle-3.3-bin.zip -d /usr/local/; 将gradle-3.3-bin.zip解压到/usr/local目录 zip压缩文件 1 zip -r test.zip test;将test文件夹打包成test.zip 查看尾部内容 1 tail -f access.log;//查看文件尾部内容 进程后台运行 1 nohup java -jar jenkins.war \u0026amp;;//进程后台程序 将文件权限分配用户 1 chown -R cdn:cdn common/;//将common文件夹权限分给cdn 查询2017年6月22日日志 1 sed -n \u0026#39;/22\\/Jun\\/2017/\u0026#39;p access.log\u0026gt;\u0026gt;20170622.log;//查询2017年6月22日日志 截取某段时间的日志 1 sed -n \u0026#39;/2017-06-15 00:00:00/,/2017-06-15 24:00:00/p\u0026#39; catalina.out \u0026gt;\u0026gt; 20170615.log;//截取某个时间段的日志 1 sed -n \u0026#39;/2017-07-05 09:[0-9][0-9]:[0-9][0-9]/,/2017-07-05 16:[0-9][0-9]:[0-9][0-9]/p\u0026#39; catalina.out 创建文件软链接 1 ln -s /usr/mengqc/mub1 /usr/liu/abc; 将/usr/mengqc/mub1代表的路径将存放在名为/usr/liu/abc的文件中。 删除软链接 1 rm -rf /usr/softlink ;删除软链接 注意后面不用加／ 创建目录引向某个文件 1 ln –s /var/www/test /var/test ; 创建/var/test 引向/var/www/test 文件夹 3.查询进程相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 netstat -lp|grep memcached; //查看启动的memcache服务 netstat -nltp|grep 8080;//查询8080端口是否监听 ps -aux;//查看所有的进程 ps -ef|grep java; //查看java的进程号 history 1000|grep pip;//列出最近使用pip 命令的1000条记录 ps -ef|grep jetty|grep -v grep|awk \u0026#39;{print $2}\u0026#39;;//查看jetty进程号 ps -ef|grep -v grep|grep jetty-avene|grep jetty|grep -v python|awk \u0026#39;{print $2}\u0026#39;;//查询jetty项目名称为jetty－avene的进程号 ps -ef|grep -v grep|grep jetty-avene|grep jetty|grep -v python|awk \u0026#39;{print $2}\u0026#39; 4.安装软件相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 rpm --install couchbase-server-enterprise-3.0.3-centos6.x86_64.rpm;//解压rpm文件 rpm -qa;//查看所有安装的软件包 rpm -qa|grep kernel;//查询系统所有内核 yum remove kernel-headers-3.10.0-327.el7.x86_64;//删除内核kernel-headers-3.10.0-327.el7.x86_64 rpm -l pkgname.rpm；//安装rpm包 rpm -e pkgname;//删除rmp包 从源码安装 ./configure make make install 5.防火墙相关的(centos6) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 chkconfig --list;//列出系统所有服务启动情况 service iptables status;//查看防火墙状态 service iptables start;//打开防火墙 service iptables stop;//关闭防火墙 为防火墙添加访问端口和ip，在vi /etc/sysconfig/iptables目录下编辑: iptables -A INPUT -p tcp --dport 10006 -j ACCEPT; //允许端口10006访问 iptables -A INPUT -s 192.168.50.87 -p tcp -j ACCEPT；//允许192.168.50.87地址能访问 6.操作磁盘相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 df -h;//查看磁盘的使用情况 du -h;//查看目录的大小 du –sh *；//查看某个目录下所有文件及文件的大小： du -sh *|sort -nr;//定位那个目录最大 du –sh * |sort –n;//按照文件大小排序 fdisk -l;//可以查看到当前的所有分区，比如boot分区，该分区存档linux的grub以及内核源码 vim /etc/fstab ;//修改fstab内容 7.网络相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 nslookup www.baidu.com;//查询域名对应的ip地址 dig www.baidu.com;//查询域名对应的ip地址 lsof -i:4080;//查看4080端口是否被占用 curl ifconfig.me;//查出外网的ip地址 netstat -tnlp|grep redis; ifconfig -a; //列出所有网络端口和IP地址 iftop //监控网络带宽 ifconfig eth0 //列出指定以太网端口对应的IP地址和详细信息 ethtool eth0 //查看以太网状态 ping host whois domain //获取指定域名的信息 dig domain //获取指定域名的DNS信息 dig -x host //根据主机地址反向查找 host goole.com //根据域名查找DNS IP地址 wget file //下载文件 netstat -tupl //列出系统的活跃连接 8.文件传输相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 scp file.txt server2:/tmp //安全拷贝file.txt到远程主机的/tmp目录下 scp noodle@server2:/www/*.html /www/tmp //拷贝远程主机的/www/目录下的所有HTML文件到本地的/www/tmp目录 scp -r noodle@server2:/www /www/tmp //递归拷贝远程主机/www目录下的所有文件和文件夹到本地/www/tmp目录 scp -P 2244 client.xml datasources.xml server.xml root@139.136.218.194:/data/appdatas/cat;//远程机器访问端口为2244 # rsync rsync -a /home/apps /backup/ # 源目录和目标目录同步 rsync -avz /home/apps noodle@192.168.10.1:/backup //本地目录和远程主机目录同步，启用压缩 //模拟请求 curl -i -X POST -H \u0026#39;Content-type\u0026#39;:\u0026#39;application/json\u0026#39; -d \u0026#39;{\u0026#34;customerId\u0026#34;:3,\u0026#34;recNum\u0026#34;:\u0026#34;18862285367\u0026#34;}\u0026#39; http://10.105.31.109:10000/sms/sendCoupenCodeSms 9.硬件相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dmesg //监测硬件和启动消息 cat /proc/cpuinfo //CPU信息 cat /proc/meminfo //硬件内存信息 free -m //已使用的和可用内存，-m表示单位为M lspci -tv //显示PCI设备信息 lsusb -tv //显示USB设备信息 hdparm -l /dev/sda //显示sda硬盘信息 hdparm -tT /dev/sda //对sda硬盘进行读取速度测试 hdparm -s /dev/sda //测试sda硬盘上不可读的块 10.统计相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 top //显示并不断更新最耗CPU的进程 mpstat 1 //显示CPU统计信息 vmstat 2 //显示虚拟内存统计信息 iostat 2 //显示IO统计信息（2s采样间隔） tcpdump -i eth1 //捕获eth1网络接口上的所有数据包 tcpdump -i eth0 \u0026#39;port 80\u0026#39; //监控80端口的网络流量 lsof //列出所有活跃进程打开的文件 lsof -u testuser //列出所有testuser用户打开的文件 wc -l filename;//统计行数 wc -c filename;//统计字节数 wc -m filename;//统计字符数 wc -w filename;//统计单词数 ls -l|wc -l 用来统计当前目录下的文件数 11.nginx统计相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 1.根据访问IP统计UV awk \u0026#39;{print $1}\u0026#39; access.log|sort | uniq -c |wc -l 2.统计访问URL统计PV awk \u0026#39;{print $7}\u0026#39; access.log|wc -l 3.查询访问最频繁的URL awk \u0026#39;{print $7}\u0026#39; access.log|sort | uniq -c |sort -n -k 1 -r|more 4.查询访问最频繁的IP awk \u0026#39;{print $1}\u0026#39; access.log|sort | uniq -c |sort -n -k 1 -r|more 5.根据时间段统计查看日志 cat access.log| sed -n \u0026#39;/14\\/Mar\\/2015:21/,/14\\/Mar\\/2015:22/p\u0026#39;|more 12.nmap相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 nmap 192.168.102.10; 侦测ip地址 nmap weixin.hao.cn;侦测域名 nmap -sU -sS -F weixin.hao.cn;//-F 快速扫描模式，扫描最可能开放的前100个端口 nmap -sV 192.168.102.10; nmap -A 192.168.102.10;执行全网扫描 nmap -O weixin.hao.cn;//侦测操作系统的信息 nmap -sP 192.168.102.*;//找出网络中的在线主机 nmap -V;//查询nmap版本 nmap -p 8080 weixin.hao.cn;//扫描特定端口 13.用户相关的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 adduser newname // 新建用户newname passwd newname //设置用户名和密码 userdel newname //删除用户 deluser –remove-home newname //删除home目录的数据 sudo addgroup siatstudent //创建组 groupadd testgroup groupmod -n test2group testgroup //修改组 delgroup happy //删除分组 groups #查看当前登陆用户所在的组 groups testnewuser #查看testnewuser 所在的组 cat /etc/group #查看所有组 14.Linux内存清理命令 1 2 3 4 5 6 7 free -m;//清理内存前 查看内存使用情况 echo 1 \u0026gt; /proc/sys/vm/drop_caches;//开始清理 free -m;//清理之后查看内存使用情况 dmidecode | grep -A16 \u0026#34;Memory Device$\u0026#34;;//查看内存条数 ","date":"2020-03-25T17:09:05Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/202003251709/","title":"Linux常见问题及命令"},{"content":"使用Docker测试静态网站 **将Docker作为本地Web开发环境是Docker的一个最简单的应用场景。 这样的环境可以完全复制生产环境，并确保用户开发的东西在生产环境中也能运行。下面从将Nginx Web服务器安装到容器来架构一个简 单的网站开始。这个网站暂且命名为Sample。 **\nSample网站的初始Dockerfile 为了完成网站开发，从这个简单的Dockerfile开始。先来创建一个 目录，保存Dockerfile\n1 2 3 $ mkdir sample $ cd sample $ touch Dockerfile 现在还需要一些Nginx配置文件，才能运行这个网站。首先在这个示例 所在的目录里创建一个名为nginx的目录，用来存放这些配置文件。 然后我们可以从GitHub上下载作者准备好的示例文件\n1 2 3 4 $ mkdir nginx \u0026amp;\u0026amp; cd nginx $ wget https://raw.githubusercontent.com/jamtur01/dockerbook-code/master/code/5/sample/nginx/global.conf $ wget https://raw.githubusercontent.com/jamtur01/dockerbook-code/master/code/5/sample/nginx/nginx.conf $ cd .. 现在看一下我们将要为Sample网站创建的Dockerfile，如代码清单\n1 2 3 4 5 6 7 8 FROM centos MAINTAINER heian99 \u0026#34;heian99@163.com\u0026#34; ENV REFRESHED_AT 2014-06-01 RUN yum -y update \u0026amp;\u0026amp; yum -y install nginx RUN mkdir -p /var/www/html/website ADD nginx/global.conf /etc/nginx/conf.d/ ADD nginx/nginx.conf /etc/nginx/nginx.conf EXPOSE 80 这个简单的Dockerfile内容包括以下几项。 安装Nginx。\n在容器中创建一个目录/var/www/html/website/。\n将来自我们下载的本地文件的Nginx配置文件添加到镜像中。\n公开镜像的80端口。\n这个Nginx配置文件是为了运行Sample网站而配置的。将文件 nginx/global.conf用ADD指令复制到/etc/nginx/conf.d/目 录中。配置文件\nglobal.conf 1 2 3 4 5 6 7 8 9 10 server { listen 0.0.0.0:80; server_name _; root /var/www/html/website; index index.html index.htm; access_log /var/log/nginx/default_access.log; error_log /var/log/nginx/default_error.log; } 这个文件将Nginx设置为监听80端口，并将网络服务的根路径设置 为/var/www/ html/website，这个目录是我们用RUN指令创建 的。\n我们还需要将Nginx配置为非守护进程的模式，这样可以让Nginx在 Docker容器里工作。将文件nginx/nginx.conf复制 到/etc/nginx目录就可以达到这个目的\nnginx.conf的清单 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 user www-data;#把这个删除掉，不然后面docker里面的nginx无法启动 worker_processes 4; pid /run/nginx.pid; daemon off; events { } http { sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; gzip on; gzip_disable \u0026#34;msie6\u0026#34;; include /etc/nginx/conf.d/*.conf; } 在这个配置文件里，daemon off;选项阻止Nginx进入后台，强制其 在前台运行。这是因为要想保持Docker容器的活跃状态，需要其中运 行的进程不能中断。默认情况下，Nginx会以守护进程的方式启动，这 会导致容器只是短暂运行，在守护进程被fork启动后，发起守护进程 的原始进程就会退出，这时容器就停止运行了\n这个文件通过ADD指令复制到/etc/nginx/nginx.conf。\n第一个指令以 目录/etc/nginx/ conf.d/结束，而第二个指令指定了文 件/etc/nginx/nginx.conf。将文件复制到Docker镜像时，这两 种风格都是可以用的。\n构建Sample网站和Nginx镜像 利用之前的Dockerfile，可以用docker build命令构建出新的镜 像，并将这个镜像命名为heian/nginx\n1 docker build -t heian/nginx . 这将构建并命名一个新镜像。下面来看看构建的执行步骤。使用 docker history命令查看构建新镜像的步骤和层级\n从Sample网站和Nginx镜像构建容器 现在可以使用heian/nginx镜像，并开始从这个镜像构建可以用 来测试Sample网站的容器。为此，需要添加Sample网站的代码。现在\n注意提示下载这段代码到sample目录\n1 2 3 $ mkdir website \u0026amp;\u0026amp; cd website $ wget http://raw.githubusercontent.com/jamtur01/dockerbook-code/master/code/5/sample/website/index.html $ cd.. 现在来看看如何使用docker run命令来运行一个容器\n1 docker run -d -p 80 --name website -v $PWD/website:/var/www/html/website heian/nginx nginx 上面命令如果有问题，那就执行下面的命令\n1 docker run -d -p 80 --name nginx -v $PWD/website:/var/www/html/website heian/nginx /bin/bash -c \u0026#34;tail -f /dev/null;/usr/sbin/nginx\u0026#34; 可以看到，在执行docker run时传入了nginx作为容器的启动命令。一般情况下， 这个命令无法让Nginx以交互的方式运行。我们已经在提供给Docker的配置里加入了指令 daemon off，这个指令让Nginx启动后以交互的方式在前台运行\n可以看到，我们使用docker run命令从heian/nginx镜像创建 了一个名为website的容器。读者已经见过了大部分选项，不过-v选 项是新的。-v这个选项允许我们将宿主机的目录作为卷，挂载到容器 里。\n回到刚才的例子。当我们因为某些原因不想把应用或者代码构建到镜 像中时，就体现出卷的价值了。例如：\n希望同时对代码做开发和测试； 代码改动很频繁，不想在开发过程中重构镜像； 希望在多个容器间共享代码 -v选项通过指定一个目录或者登上与容器上与该目录分离的本地宿主 机来工作，这两个目录用:分隔。如果容器目录不存在，Docker会自 动创建一个。\n1 2 3 docker run -d -p 80 --name website \\ -v $PWD/website:/var/www/html/website:ro \\ heian/nginx nginx 这将使目的目录/var/www/html/website变成只读状态。\n在Nginx网站容器里，我们通过卷将$PWD/website挂载到容器 的/var/www/ html/website目录，顺利挂载了正在开发的本地网 站。在Nginx配置里（在配置文 件/etc/ngingx/conf.d/global.conf中），已经指定了这个目 录为Nginx服务器的工作目录。\n现在，如果使用docker ps命令查看正在运行的容器，可以看到名为 website的容器正处于活跃状态，容器的80端口被映射到宿主机的 49161端口\n修改网站 我们已经得到了一个可以工作的网站！现在，如果要修改网站，该怎 么办？可以直接打开本地宿主机的website目录下的index.html文 件并修改\n**可以看到，Sample网站已经更新了。显然这个修改太简单了，不过可 以看出，更复杂的修改也并不困难。更重要的是，正在测试网站的运 行环境，完全是生产环境里的真实状态。现在可以给每个用于生产的 网站服务环境（如Apache、Nginx）配置一个容器，给不同开发框架 的运行环境（如PHP或者Ruby on Rails）配置一个容器，或者给后端 数据库配置一个容器，等等。 **\n","date":"2020-03-23T11:28:44Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/202003231128/","title":"Docker测试一个静态网站"},{"content":"Docker网络（host、bridge、none）详细介绍 Docker容器间通信 前面我们已经解决了容器间通信的问题，接下来讨论容器如何与外部世界通信。\n这里涉及两个方向：\n（1）容器访问外部世界。\n（2）外部世界访问容器。\n容器访问外部世界 在我们当前的实验环境下，docker host是可以访问外网的\n我们看一下容器是否也能访问外网呢？\n可见，容器默认就能访问外网。\n请注意：这里外网指的是容器网络以外的网络环境，并非特指Internet.\n现象很简单，但更重要的：我们应该理解现象下的本质。\n在上面的例子中，busybox位于dockero这个私有bridge网络中（172.17.0.0/16），当busybox从容器向外ping时，数据包是怎样到达bing.com的呢？\n这里的关键就是NAT，我们查看一下docker host上的iptables规则\n在NAT表中，有这么一条规则： 1 -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE 其含义是：如果网桥docker0收到来自172.17.0.0/16网段的外出包，把它交给MASQUERADE处理。而且MASQUERADE的处理方式是将\n包的原地址替换成host的地址发送出去，机 做了一次网络地址转换（NAT）\n下面我们通过tcpdump查看地址是如何转换的。先查看docker host的路由表\n默认路由通过ens192发出去，所以我们同时要监控ens192和docker0上的icmp（ping）的数据包。\n当busybox ping www.baidu.com时，tcpdumo输出如下\n1 tcpdump -i docker0 -n icmp docker0收到busybox的ping包，源地址为容器的IP：172.17.0.2，这没问题，交给MASQUERADE处理。这时，在看ens192的变化。\nping包的源地址变成ens192的192.168.1.100\n这就是iptables的NAT规则的处理结果，从而保证数据包能够到达外网。\n下面这张图来说明结果。\n（1）busybox发送ping包：172.17.0.2 \u0026gt; www.bing.com。 （2）dockero收到包，发现是发送到外网的，交给NAT处理。 （3）NAT将源地址换成ens192的IP：10.0.2.15 \u0026gt;www.bing.com. （4）ping从 ens192出去，达www.bing.como 通过NAT，docker实现了容器对外网的访问。 外部世界访问容器 下面我们来讨论另一个方向：外网如何访问到容器？\n答案是：端口映射。\ndocker可将容器对外提供服务的端口映射到host的某个端口，外网通过该端口访问容器。容器启动时通过-p参数映射端口\n1 2 3 4 5 6 7 8 [root@kvm ~]# docker run -d -p 80 httpd 6b89d7e3f47a425256af6934689c0009fbbad9099bf31c2e6148a6cb236a9655 [root@kvm ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6b89d7e3f47a httpd \u0026#34;httpd-foreground\u0026#34; 5 seconds ago Up 2 seconds 0.0.0.0:32768-\u0026gt;80/tcp stoic_bartik 78de9710bce0 busybox \u0026#34;sh\u0026#34; 19 minutes ago Up 19 minutes hei [root@kvm ~]# docker port 6b89d7e3f47a 80/tcp -\u0026gt; 0.0.0.0:32768 容器启动后，可通过docker ps或者docker port查看到host映射的端口。在上面的例子中，httpd容器的80端口被映射到host 32768上，这样就可以通过\u0026lt;host ip\u0026gt;：\u0026lt;32773\u0026gt;访问容器的Web服务了\n除了映射动态端口，也可在-p中指定映射到host某个特定端口，例如可将80端口映射到host的8080端口\n1 2 3 4 5 [root@kvm ~]# docker run -d -p 8080:80 httpd 8d7db7b2c7a6fc03cd219d6d3c07f15261d8a12efda3be25fdb60576c91c1a7e [root@kvm ~]# curl 192.168.1.100:8080 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; [root@kvm ~]# 每一个映射的端口，host都会启动一个docker-proxy进程来处理访问容器的流量。\n以0.0.0.0：32773-280/tcp为例分析整个过程 （1）docker-proxy监听host的32773端口。 （2）当curl访问10.0.2.15：32773时，docker-proxy转发给容器172.170.2：80。 （3）httpd容器响应请求并返回结果。 ","date":"2020-03-20T16:38:14Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202003201638/","title":"Docker容器访问外部世界"},{"content":"IP通信 从前面的例子可以得出这样一个结论：两个容器要能通信，必须要有属于同一个网络的网卡。满足这个条件后，容器就可以通过IP交互了。具体做法是在容器创建时通过-network指定相应的网络，或者通过docker network connect将现有容器加入到指定网络。可参考上一节\nDocker网络（host、bridge、none）详细介绍\nDocker DNS Server 通过IP访问容器虽然满足了通信的需求，但还是不够灵活。因为在部署应用之前可能无法确定IP，部署之后再指定要访问的IP会比较麻烦。对于这个问题，可以通过docker自带的DNS服务解决。\n从Docker 1.10版本开始，docker daemon实现了一个内嵌的DNS server，使容器可以直接1通过“容器名”通信。方法很简单，只要在启动时用-name为容器命名就可以了。\n1 2 docker run -it --network=my_net2 --name=bbox1 busybox docker run -it --network=my_net2 --name=bbox2 busybox 然后，bbox2就可以直接ping到bbox1了\n使用docker DNS有个限制：只能在user-defined网络中使用。也就是说，默认的bridge网络是无法使用DNS的。\n下面验证一下：创建bbox3和bbox4，均连接到bridge网络。\n1 2 docker run -it --name=bbox3 busybox docker run -it --name=bbox4 busybox bbox4无法ping到bbox3\nJoined容器 joined容器非常特别，它可以使两个或多个容器共享一个网络栈，共享网卡和配置信息，joined容器之间可以通过127.0.01直接通信。\n请看下面的例子：先创建一个httpd容器，名字为web1。\n1 docker run -d -it --name=web1 =httpd 然后创建busybox容器并通过-network-container:webl指定joined容器为webl，\n1 docker run -it --network=container:web1 busybox 请注意busybox器中网配置息，下面我们查看一下webl的网络，\n看！busybox和webl的网卡mac地址与IP完全一样，它们共享了相同的网络栈。\nbusybox可以直接用127.0.0.1访问webl的http服务\njoined容器非常适合以下场景： 不同容器中的程序希望通过loopback高效快速地通信，比如web Server与App Server. 希望监控其他容器的网络流量，比如运行在独立容器中的网络监控程序。 ","date":"2020-03-18T14:47:52Z","image":"https://www.ownit.top/title_pic/07.jpg","permalink":"https://www.ownit.top/p/202003181447/","title":"Docker容器间通信"},{"content":"Docker网络（host、bridge、none） 我们会首先学习Docker提供的几种原生网络，以及如何创建自定义网络；然后探讨容器之间如何通信，以及容器与外界如何交互。\nDocker网络从覆盖范围可分为单个host上的容器网络和跨多个host的网络，本章重点讨论前一种。对于更为复杂的多host容器网络，我们会在后面进阶技术章节单独讨论。\nDocker 安装时会自动在host 上创建三个网络，我们可用docker network ls命令查看\n1 docker network ls none网络 顾名思义，none网络就是什么都没有的网络。挂在这个网络下的容器除了1o，没有其他任何网卡。容器创建时，可以通过-network=none指定使用none网络，如图\n1 docker run -it --network=none busybox 我们不禁会问，这样一个封闭的网络有什么用呢？\n其实还真有应用场景。封闭意味着隔离，一些对安全性要求高并且不需要联网的应用可以使用none网络。\n比如某个容器的唯一用途是生成随机密码，就可以放到none网络中避免密码被窃取。\n当然大部分容器是需要网络的，我们接着看host网络。\nhost网络 连接到host网络的容器共享Docker host的网络栈，容器的网络配置与host完全一样。\n可以通过-network-host 指定使用host网络\n1 docker run -it --network=host busybox 在容器中可以看到host的所有网卡，并且连hostmame 也是host的。host网络的使用场景又是什么呢？ 直接使用Docker host的网络最大的好处就是性能，如果容器对网络传输效率有较高要求，则可以选择host网络。当然不便之处就是牺牲一些灵活性，比如要考虑端口冲突问题，Docker host 上已经使用的端口就不能再用了。\nDocker host的另一个用途是让容器可以直接配置host网路，比如某些跨host的网络解决方案，其本身也是以容器方式运行的，这些方案需要对网络进行配置，比如管理iptables\nbridge网络 Docker安装时会创建一个命名为docker0的Linux bridge。如果不指定-network，创建的容器默认都会挂到docker0上\n1 brctl show 当前docker0上没有任何其他网络设备，我们创建一个容器看看有什么变化\n一个新的网络接口vethb56e98c被挂到了docker0上，vethb56e98c就是新创建容器的虚拟网卡。\n下面看一下容器的网络配置\n容器有一个网卡eth0@if34，为什么不是vethb56e98c呢？\n实际上eth0@if34 和veth28c57df 是一对veth pair。 veth pair 是一种成对出现的特殊网络设备，可以把它们想象成由一根虚拟网线连接起来的一对网卡， 网卡的一头(eth0@if34) 在容器中，另一头(veth28c57df) 挂在网桥docker0 上，其效果就是将eth0@if34 也挂在了docker0上。\n我们还看到eth0@if34 已经配置了IP 172.17.0.2， 为什么是这个网段呢?让我们通过docker network inspect bridge看- -下bridge 网络的配置信息\n原来bridge 网络配置的subnet 就是172.17.0.0/16, 并且网关是172.17.0.1. 这个网关在哪儿呢?大概你已经猜出来了，就是docker0\n容器网络拓扑图\n容器创建时，docker 会自动从172.17.0.0/16 中分配-个IP,这里16 位的掩码保证有足够多的IP可以供容器使用。\nuser-defined网络 除了none、host、bridge这三个自动创建的网络，用户也可以根据业务需要创建user-defined网络。\nDocker提供三种user-defined 网络驱动：bridge、overlay 和macvlan。overlay 和macvlan用于创建跨主机的网络\n我们可通过bridge 驱动创建类似前面默认的bridge 网络\n1 docker network create --driver bridge my_net 1 2 3 4 5 6 7 [root@kvm ~]# docker network create --driver bridge my_net 7428a4d14c67b44be3a7184a5a065303d3688b5b13883526989b7173af881170 [root@kvm ~]# brctl show bridge name\tbridge id\tSTP enabled\tinterfaces br-7428a4d14c67\t8000.024221339eb6\tno\tdocker0\t8000.024280532eb0\tno\tveth6281a8d virbr0\t8000.52540015904b\tyes\tvirbr0-nic 新增了一个网桥br-7428a4d14c67，这里7428a4d14c67正好是新建bridge网络my_net的短id。执行docker network inspect查看一下my_net的配置信息，\n这里172.18.0.0/16是Docker自动分配的IP网段。\n我们可以自己指定IP网段吗？\n答案是：可以。\n只需在创建网段时指定\u0026ndash;subnet和-gateway参数\n1 docker network create --driver bridge --subnet 172.22.16.0/24 --gateway 172.22.16.1 my_net2 这里我们创建了新的bridge网络my_net2，网段为172.22.16.0/24，网关为172.22.16.1与前面一样，网关在my net2对应的网桥br-5d863e9778b6上\n容器要使用新的网络，需要在启动时通过\u0026ndash;network指定\n1 docker run -it --network=my_net2 busybox 容器分配到的IP为172.22.16.20到目前为止，容器的IP都是docker自动从subnet中分配，我们能否指定一个静态IP呢？\n答案是：可以，通过-p指定\n1 docker run -it --network=my_net2 --ip 172.22.16.8 busybox 好了，我们来看看当前docker host的网络拓扑结构 两个busybox容器都挂在mynet2上，应该能够互通\n可见同一网络中的容器、网关之间都是可以通信的。\nmy_net2与默认bridge网络能通信吗？\n从拓扑图可知，两个网络属于不同的网桥，应该不能通信，我们通过实验验证一下，让busybox容器ping 不同网段容器\n确实 ping不通，符合预期。\n“等等！不同的网络如果加上路由应该就可以通信了吧？”我已经听到有读者在建议了这是一个非常非常好的想法。\n确实，如果host上对每个网络都有一条路由，同时操作系统上打开了ip forwarding，host就成了一个路由器，挂接在不同网桥上的网络就能够相互通信。下面我们来看看docker host是否满足这些条件呢？\nip r查看host上的路由表：\n172.17.0.0/16和172.22.16.0/24两个网络的路由都定义好了。再看看ip forwarding：\n1 2 [root@kvm ~]# sysctl net.ipv4.ip_forward net.ipv4.ip_forward = 1 ip forwarding也已经启用了。条件都满足，为什么不能通行呢？\n我们还得看看iptables：\n1 2 3 4 iptavles-save -A DOCKER-ISOLATION -i br-13ceb40bd8e8 -o docker0 -j DROP -A DOCKER-ISOLATION -i docker0 -o br-13ceb40bd8e8 -j DROP 原因就在这里了：iptables DROP掉了网桥dockero与br-13ceb40bd8e8之间双向的流量。\n从规则的命名DOCKER-ISOLATION可知docker在设计上就是要隔离不同的netwrok\n那么接下来的问题是：怎样才能让busybox与httpd 通信呢？\n答案是：为httpd容器添加一块net_my2的网卡。这个可以通过docker network connect命令实现，\n1 docker network connect my_net2 655643ea6894 我们在docker0网段器中查看一下网络配置\n容器中增加了一个网卡ethl，分配了my_net2的IP 172.22.16.3。现在busybox应该能够访问docker0网段了，验证一下\nbusybox能够ping到httpd，并且可以访问httpd的Web服务所示。\n（因为这里我在docker0网段，用busybox代替的httpd）\n学习了Docker各种类型网络之后，接下来我们讨论容器与容器、容器与外界的连通问题\n","date":"2020-03-17T11:35:03Z","image":"https://www.ownit.top/title_pic/24.jpg","permalink":"https://www.ownit.top/p/202003171135/","title":"Docker网络（host、bridge、none）详细介绍"},{"content":"**一个docker host. 上会运行若干容器，每个容器都需要CPU、内存和I0资源。对于KVM、VMware 等虚拟化技术，用户可以控制分配多少CPU、内存资源给每个虚拟机。对于容器，Docker 也提供了类似的机制避免某个容器因占用太多资源而影响其他容器乃至整个host 的性能。**\n内存限额 与操作系统类似，容器可以使用的内存包括两部分：物理内存和Swap。\nDocker通过下面两组参数来控制容器内存的使用量\n（1）-m 或 --memory ：设置内存的使用限额，例如100MB，2GB\n（2）\u0026ndash;memory-swap：设置内存+swawp的使用限额\n当我们执行如下的命令时\n1 docker run -m 200M --memory-swap=300M ubuntu 其含义是允许该容器最多使用200MB的内存和100MB 的swap。默认情况下，上面两组参数为-1, 即对容器内存和swap的使用没有限制。\n下面我们将使用progrium/stress 镜像来学习如何为容器分配内存。该镜像可用于对容器执行压力测试。执行如下命令:\n1 docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 208M --vm1:启动1个内存工作线程。 --vm-bytes 280M:每个线程分配280MB内存。 运行如下图结果\n因为280MB在可分配的范围(300MB) 内，所以工作线程能够正常工作，其过程是:\n(1)分配280MB内存。\n(2)释放280MB内存。\n(3)再分配280MB内存。\n(4)再释放280MB内存。\n(5)一-直循环\u0026hellip;..\n如果让工作线程分配的内存超过300MB,结果如图\n1 docker run -it -m 200M --memory-swap=300M progrium/stress --vm 1 --vm-bytes 310M 分配的内存超过限额，stress 线程报错，容器退出。\n如果在启动容器时只指定-m而不指定-memoryswap, 那么-memory-swap 默认为-m的两倍，比如:\n1 docker run -it -m 200M ubuntu 容器最多使用200M绒里内存和200swap\nCPU限额 默认设置下，所有容器可以平等地使用host CPU资源并且没有限制。\nDocker可以通过-c或-pu-shares设置容器使用CPU的权重。如果不指定，默认值为1024。\n与内存限额不同，通过-c设置的cpu share 并不是CPU资源的绝对数量，而是一个相对的权重值。某个容器最终能分配到的CPU资源取决于它的cpu share占所有容器cpu share总和的比例。\n换句话说:通过cpu share可以设置容器使用CPU的优先级。\n比如在host中启动了两个容器:\n1 docker run --name \u0026#34;cont_A\u0026#34; -c 1024 ubuntu docker run --name \u0026#34;cont_B\u0026#34; -c 512 ubuntu containerA的cpu share 1024， 是containerB 的两倍。当两个容器都需要CPU资源时，containerA可以得到的CPU是containerB 的两倍。\n需要特别注意的是，这种按权重分配CPU只会发生在CPU资源紧张的情况下。如果containerA处于空闲状态，这时，为了充分利用CPU资源，containerB 也可以分配到全部可用的CPU.\n下面我们继续用progrium/stress 做实验。\n(1)启动(container_ A, cpu share为1024\n1 docker run --name \u0026#34;cont_A\u0026#34; -it -c 1024 progrium/stress --cpu 1 --cpu用来设置工作线程的数量。因为当前host 只有1颗CPU,所以一个工作线程就能将CPU压满。如果host有多颗CPU,则需要相应增加\u0026ndash;cpu的数量。\n(2)启动(container_B, cpu share为512 1 docker run --name \u0026#34;cont_B\u0026#34; -it -c 512 progrium/stress --cpu 1 (3)在host中执行top, 查看容器对CPU的使用情况， 1 ps aux|head -1;ps aux|sort -k3nr |head -4 containerA消耗的CPU是containerB 的两倍。 (4)现在暂停container. A (5) top 显示containerB在containerA空闲的情况下能够用满整颗CPU Block IO 带宽限额 Block 10是另一种可以限制容器使用的资源。Block I0指的是磁盘的读写，docker 可通过设置权重、限制bps和iops 的方式控制容器读写磁盘的带宽，下 面分别讨论。\n注:目前Block I0限额只对direct IO (不使用文件缓存)有效。\nBlock IO权重\n默认情况下，所有容器能平等地读写磁盘，可以通过设置**-blkio-weight**参数来改变容器block Io的优先级。\n-blkio-weight与\u0026ndash;cpu-shares 类似，设置的是相对权重值，默认为500。 在下面的例子中，containerA 读写磁盘的带宽是containerB 的两倍。\n1 2 docker run -it --name cont_A --blkip-weight 600 ubuntu docker run -it --name cont_B --blkip-weight 300 ubuntu 限制bps和iops bps是 byte per second ，每秒读写的数量\niops是 io per second ，每秒IO的次数\n可以同过下面的参数控制容器的bps和iops；\n--device-read-bps:限制读某个设备的bps. --devce-write-bps:限制写某个设备的bps. --device- read-iops:限制读某个设备的iops. --device-write-iops: 限制写某个设备的iops。\n下面这个例子限制容器写/dev/sda 的速率为30 MB/s:\n1 2 3 4 5 6 7 8 9 10 [root@kvm ~]# docker run -it --device-write-bps /dev/sda:30MB ubuntu root@10845a98036e:/# time dd if=/dev/zero of=test.out bs=1M count=800 oflag=direct 800+0 records in 800+0 records out 838860800 bytes (839 MB, 800 MiB) copied, 26.6211 s, 31.5 MB/s real\t0m26.623s user\t0m0.000s sys\t0m0.106s root@10845a98036e:/# 1 docker run -it --device-write-bps /dev/sda:30MB ubuntu 有限制\n没有限制\n通过dd测试在容器中写磁盘的速度。因为容器的文件系统是在host /dev/sda. 上的，在容器中写文件相当于对host /dev/sda 进行写操作。另外，oflag= -direct指定用direct I0方式写文件，这样\u0026ndash;device-write-bps才能生效。\n看到，没有限速的话，速度很快，\n其他参数，大家也可以试试\n","date":"2020-03-16T17:43:53Z","image":"https://www.ownit.top/title_pic/11.jpg","permalink":"https://www.ownit.top/p/202003161743/","title":"Docker的资源限制（内存、CPU、IO）详细篇"},{"content":"linux的磁盘容量扩容，基于lvm，即逻辑卷管理。具体是什么请百度，这里不细述。\n此次操作的目的是为了给已存在的linux主机的其中一个数据分区扩容。\n环境：esxi6.5 虚拟机系统centos7\n简单来说，扩容这件事分三步\n一、从esxi中为此虚拟机增加硬盘，并让centos系统识别出此硬盘\n二、将此硬盘进行分区、格式化（重点是这里的分区不是类似于windows，分完就能用了，而它需要一个挂载的过程，要么单独挂载，要么加入lvm挂载，否则在linux中是无法访问的）\n三、卷组管理\n1、将分好区的硬盘创建为物理卷 2、将此物卷直接进行挂载到文件系统 3、或将此物理卷加入到lvm卷组中 4、对加入到卷组的空间进行逻辑卷扩容或是创建为逻辑卷再进行扩容等操作 以下是本次操作的过程记录\n1、首先看一下未添加硬盘前的系统磁盘状态， 2、在esxi中添加硬盘的过程就不说了，添加过硬盘，需要对scsi接口进行扫描，就相当于扫描新硬件 端口太多，一个个扫描太慢，我就写个简单脚本执行。 1 ls | sort \u0026gt; /opt/host.txt 批量扫描脚本 1 2 3 4 5 6 7 8 9 #!/bin/bash DIR=\u0026#34;/sys/class/scsi_host/\u0026#34; for i in `cat host.txt` do echo \u0026#34;- - -\u0026#34; \u0026gt; $DIR$i/scan echo $DIR$i/scan done rm -rf /opt/host.txt 运行脚本 1 2 3 4 5 [root@kvm opt]# ls backup host.sh host.txt rh [root@kvm opt]# pwd /opt [root@kvm opt]# bash host.sh 3、可以看到新加的10G硬盘已经被识别为/dev/sdb 4、查看一下scsi的状态，以上都是准备工作，状态都对，后面操作就容易 1 cat /proc/scsi/scsi 5、对新硬盘进行分区，此处是新建了一个主分区，默认id为1，所以分好后就是sdb1 6、我们的目的是为了用lvm进行管理，所以在分完区后，要将分区属性标记为lvm的8e 7、分完后可以看到分区信息，/dev/sdb1的8e,然后重读一下分区表，刷新 已经分区成功了\n8、上面分完区，下面当然就是加入卷了，先把sdb1做成一个新的物理卷 9、用vgdisplay查看一下卷组的状态，可以看到原先centos组里面没有空余，那么我们要做的就是把刚加的磁盘，刚分好的区，然后刚创建成的物理卷加入到这个centos组里去，加进组才能在组里进行分配嘛。所以vgextend centos /dev/sdb1,加完再看vgdisplay，空余空间为10G，很明显，新加的磁盘已处理待分配状态 1 2 [root@kvm dev]# vgextend centos_kvm /dev/sdb1 Volume group \u0026#34;centos_kvm\u0026#34; successfully extended 10、最后就是剑指黄龙，我要给var进行扩容，lvresize -L +10G /dev/centos/var 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@kvm centos_kvm]# lvresize -L +20G /dev/centos_kvm/root Size of logical volume centos_kvm/root changed from \u0026lt;26.00 GiB (6655 extents) to \u0026lt;46.00 GiB (11775 extents). Logical volume centos_kvm/root successfully resized. [root@kvm centos_kvm]# vgdisplay --- Volume group --- VG Name centos_kvm System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 5 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 128.99 GiB PE Size 4.00 MiB Total PE 33022 Alloc PE / Size 12543 / \u0026lt;49.00 GiB Free PE / Size 20479 / \u0026lt;80.00 GiB VG UUID K35BzQ-nhXT-zFsf-W1kp-T9iq-kcFx-hfTdTN 11、上面那一步不算完，记得不，之前我虽然分区了，创建卷了，加入卷组，但实际上我没格式化。那么OK，这里用xfs_growfs /dev/centos/var重新识别一下新卷的容量，是扩容后的哦，扩容时加上的新磁盘也就同时被格式化了。 xfs_growfs 是centos7的命令，在centos6.X中是resize2fs，其实还是6.x的命令好记。\n此处讲的是直接将新加的磁盘扩容到已有分区，还可以做的是，在将新分区加入卷组后：\n1、创建需要大小的独立逻辑卷，将它进行单独挂载使用。（别忘了改一下/etc/fstab，不然下次重启还要手动挂载）\n1 2 3 lvcreate -L 4G -n newlv centos 在centos卷组的空闲空间中划出4G的新逻辑卷，起名为newlv mkfs.xfs /dev/centos/newlv 将新的newlv格式化为xfs文件系统 1、不创新需要大小的独立逻辑卷，将自由空间扩容到现有的分区挂载点\n基本就这些了。lvm管理说实话真挺爽的。特别是在esxi主机上使用，无需停机，直接加装扩容。\n有正就有反，能装就得能卸，能扩就得能减。\n1、直接扩容原有逻辑卷大小的卸载新加容量\nlvreduce -L -10G /dev/centos/var 先把扩容的容量减掉\n如果是创建成为一个独立的逻辑卷，则\nlvremove /dev/centos/newlv1\n2、从卷组中删掉加入的磁盘分区\nvgreduce centos /dev/sdb1\n3、从物理卷中卸掉sdb1\npvremove /dev/sdb1 最后就是在esxi中删硬件了。\n","date":"2020-03-16T11:53:10Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/202003161153/","title":"esxi中CentOS7不停机加磁盘并扩容现有分区"},{"content":"VMware三个版本\nworkstation： 单机级，用在个人桌面系统中，需要操作系统支持\nservier：工作组级，用于服务器，需要操作系统支持\nesxi：企业级，用于服务器，不需要操作系统支持\nExsi 是一款虚拟化系统，与VMware，VirtualBox不同，它不需要安装在其他操作系统上，直接运行在裸机上；占用系统资源很小，易于管理，所以被大多数中小型公司所使用；\n安装图解： 镜像已上传到服务器，CentOS7镜像包\n新建虚拟机\n选择存储\n自定义设置\n如果配置不够，可进行修改\n将镜像加载到新建的虚拟机里，CD/DVD驱动器选择数据存储ISO文件\n现在开始运行，首先打开电源,打开控制台\n下面步骤就可安装Centos7一样了。\n选择自己需求的东西就行，然后开始安装。\n","date":"2020-03-16T09:59:18Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/202003160959/","title":"ESXI6.5安装CentOS7教程"},{"content":"占内存的程序命令\n1 ps aux | head -1;ps aux|sort -k4nr|head -5 占CPU的程序命令\n1 ps aux | head -1;ps aux|sort -k3nr|head -5 ","date":"2020-03-14T21:26:47Z","image":"https://www.ownit.top/title_pic/36.jpg","permalink":"https://www.ownit.top/p/202003142126/","title":"Linux查看占用CPU和内存的 的程序"},{"content":"\n环境准备：三台虚拟机\n1）此环境是针对内部服务的LVS架构，如数据库，缓存，共享存储等业务。\n虚拟机角色IP地址备注LVS负载均衡器192.168.116.129VIP地址：192.168.116.100http服务器RS1192.168.116.130\u0026nbsp;http服务器RS2192.168.116.131\u0026nbsp; LVS负载均衡器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 vim /usr/local/sbin/lvs_dr.sh #!/bin/bash yum install -y net-tools ipvsadm echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward ipv=/usr/sbin/ipvsadm vip=192.168.116.100 rs1=192.168.116.130 rs2=192.168.116.131 #注意这里的网卡名字 ifconfig ens33:2 $vip broadcast $vip netmask 255.255.255.255 up route add -host $vip dev ens33:2 $ipv -C $ipv -A -t $vip:80 -s wrr $ipv -a -t $vip:80 -r $rs1:80 -g -w 1 $ipv -a -t $vip:80 -r $rs2:80 -g -w 1 http服务器RS1、http服务器RS2 1 2 3 4 5 6 7 8 9 10 11 12 13 vim /usr/local/sbin/lvs_dr.sh #/bin/bash yum install -y net-tools vip=192.168.116.100 #把vip绑定在lo上，是为了实现rs直接把结果返回给客户端 ifconfig lo:0 $vip broadcast $vip netmask 255.255.255.255 up route add -host $vip lo:0 #以下操作为更改arp内核参数，目的是为了让rs顺利发送mac地址给客户端 echo \u0026#34;1\u0026#34; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_ignore echo \u0026#34;2\u0026#34; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo \u0026#34;1\u0026#34; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo \u0026#34;2\u0026#34; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_announce 运行脚本 1 bash /usr/local/sbin/lvs_dr_rs.sh 在httpd服务器创建文件测试\n1 yum install -y httpd \u0026amp;\u0026amp; echo \u0026#34;this is one\u0026#34; \u0026gt;\u0026gt; /var/www/html/index.html \u0026amp;\u0026amp; systemctl restart httpd ","date":"2020-02-20T23:23:23Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/202002202323/","title":"Centos7使用脚本搭建LVS的DR模式。"},{"content":"节点亲和性 pod.spec.nodeAﬃnity\npreferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 requiredDuringSchedulingIgnoredDuringExecution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: affinity labels: app: node-affinity-pod spec: containers: - name: with-node-affinity image: wangyanglinux/myapp:v1 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - node02 preferredDuringSchedulingIgnoredDuringExecution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Pod metadata: name: affinity labels: app: node-affinity-pod spec: containers: - name: with-node-affinity image: wangyanglinux/myapp:v1 affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: kubernetes.io/hostname operator: In values: - node3 先硬后软 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: v1 kind: Pod metadata: name: affinity labels: app: node-affinity-pod spec: containers: - name: with-node-affinity image: hub.atguigu.com/library/myapp:v1 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - k8s-node02 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: source operator: In values: - qikqiak 键值运算关系 In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在\nPod 亲和性 pod.spec.aﬃnity.podAﬃnity/podAntiAﬃnity\npreferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: Pod metadata: name: pod-3 labels: app: pod-3 spec: containers: - name: pod-3 image: wangyanglinux/myapp:v1 affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - pod-1 topologyKey: kubernetes.io/hostname podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - pod-2 topologyKey: kubernetes.io/hostname 亲和性/反亲和性调度策略比较如下：\n调度策略匹配 标签匹配 标签操作符拓扑域 支持调度目标nodeAﬃnity\u0026nbsp;主机In, NotIn, Exists, DoesNotExist, Gt, Lt否指向主机podAﬃnity\u0026nbsp;PODIn, NotIn, Exists, DoesNotExist是POD与指定POD同一拓 扑域podAnitAﬃnity\u0026nbsp;PODIn, NotIn, Exists, DoesNotExist是POD与指定POD不在同 一拓扑域","date":"2020-02-11T16:02:24Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/202002111602/","title":"Kubernetes（k8s）的调度器 - 调度亲和性详细介绍"},{"content":"简介 Scheduler 是 kubernetes 的调度器，主要的任务是把定义的 pod 分配到集群的节点上。听起来非常简单，但有 很多要考虑的问题：\n公平：如何保证每个节点都能被分配资源 资源高效利用：集群所有资源最大化被使用 效率：调度的性能要好，能够尽快地对大批量的 pod 完成调度工作 灵活：允许用户根据自己的需求控制调度的逻辑\nSheduler 是作为单独的程序运行的，启动之后会一直坚挺 API Server，获取 PodSpec.NodeName 为空的 pod， 对每个 pod 都会创建一个 binding，表明该 pod 应该放到哪个节点上 调度过程 调度分为几个部分：首先是过滤掉不满足条件的节点，这个过程称为 predicate ；然后对通过的节点按照优先级 排序，这个是 priority ；最后从中选择优先级最高的节点。如果中间任何一步骤有错误，就直接返回错误 **Predicate 有一系列的算法可以使用： **\nPodFitsResources ：节点上剩余的资源是否大于 pod 请求的资源 PodFitsHost ：如果 pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配 PodFitsHostPorts ：节点上已经使用的 port 是否和 pod 申请的 port 冲突 PodSelectorMatches ：过滤掉和 pod 指定的 label 不匹配的节点 NoDiskConflict ：已经 mount 的 volume 和 pod 指定的 volume 不冲突，除非它们都是只读\n如果在 predicate 过程中没有合适的节点，pod 会一直在 pending 状态，不断重试调度，直到有节点满足条件。 经过这个步骤，如果有多个节点满足条件，就继续 priorities 过程： 按照优先级大小对节点排序\n优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重（该项的重要性）。这些优先级选项包括：\nLeastRequestedPriority ：通过计算 CPU 和 Memory 的使用率来决定权重，使用率越低权重越高。换句话****说，这个优先级指标倾向于资源使用比例更低的节点 BalancedResourceAllocation ：节点上 CPU 和 Memory 使用率越接近，权重越高。这个应该和上面的一起****使用，不应该单独使用 ImageLocalityPriority ：倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高\n通过算法对所有的优先级项目和权重进行计算，得出最终的结果\n自定义调度器 除了 kubernetes 自带的调度器，你也可以编写自己的调度器。通过 spec:schedulername 参数指定调度器的名 字，可以为 pod 选择某个调度器进行调度。比如下面的 pod 选择 my-scheduler 进行调度，而不是默认的 default-scheduler ：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: annotation-second-scheduler labels: name: multischeduler-example spec: schedulername: my-scheduler containers: - name: pod-with-second-annotation-container image: wangyanglinux/myapp:v1 ","date":"2020-02-11T14:45:11Z","image":"https://www.ownit.top/title_pic/01.jpg","permalink":"https://www.ownit.top/p/202002111445/","title":"Kubernetes（k8s）的调度器详细介绍"},{"content":"概念 PersistentVolume （PV） 是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、 iSCSI 或特定于云供应商的存储系统 PersistentVolumeClaim （PVC） 是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源 （CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂载）\n静态 pv 集群管理员创建一些 PV。它们带有可供群集用户使用的实际存储的细节。它们存在于 Kubernetes API 中，可用 于消费\n动态 当管理员创建的静态 PV 都不匹配用户的 PersistentVolumeClaim 时，集群可能会尝试动态地为 PVC 创建卷。此 配置基于StorageClasses ：PVC 必须请求 [存储类]，并且管理员必须创建并配置该类才能进行动态创建。声明该 类为 \u0026quot;\u0026quot; 可以有效地禁用其动态配置\n要启用基于存储级别的动态存储配置，集群管理员需要启用 API server 上的 DefaultStorageClass [准入控制器] 。例如，通过确保 DefaultStorageClass 位于 API server 组件的 --admission-control 标志，使用逗号分隔的 有序值列表中，可以完成此操作\n绑定 master 中的控制环路监视新的 PVC，寻找匹配的 PV（如果可能），并将它们绑定在一起。如果为新的 PVC 动态 调配 PV，则该环路将始终将该 PV 绑定到 PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求 的数量。一旦 PV 和 PVC 绑定后， PersistentVolumeClaim 绑定是排他性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射 持久化卷声明的保护 PVC 保护的目的是确保由 pod 正在使用的 PVC 不会从系统中移除，因为如果被移除的话可能会导致数据丢失\n当启用PVC 保护 alpha 功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除。PVC 的 删除将被推迟，直到 PVC 不再被任何 pod 使用\n持久化卷类型 **PersistentVolume 类型以插件形式实现。Kubernetes 目前支持以下插件类型： **\nGCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath VMware Photon Portworx Volumes ScaleIO Volumes StorageOS 持久卷演示代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 PV 访问模式 PersistentVolume 可以以资源提供者支持的任何方式挂载到主机上。如下表所示，供应商具有不同的功能，每个 PV 的访问模式都将被设置为该卷支持的特定模式。例如，NFS 可以支持多个读/写客户端，但特定的 NFS PV 可能 以只读方式导出到服务器上。每个 PV 都有一套自己的用来描述特定功能的访问模式 ReadWriteOnce——该卷可以被单个节点以读/写模式挂载 ReadOnlyMany——该卷可以被多个节点以只读模式挂载 ReadWriteMany——该卷可以被多个节点以读/写模式挂载 在命令行中，访问模式缩写为：\nRWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany 回收策略 Retain（保留）——手动回收 Recycle（回收）——基本擦除（ rm -rf /thevolume/* ） Delete（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和 OpenStack Cinder 卷）将被删除 当前，只有 NFS 和 HostPath 支持回收策略。AWS EBS、GCE PD、Azure Disk 和 Cinder 卷支持删除策略\n状态 卷可以处于以下的某种状态：\nAvailable（可用）——一块空闲资源还没有被任何声明绑定 Bound（已绑定）——卷已经被声明绑定 Released（已释放）——声明被删除，但是资源还未被集群重新声明 Failed（失败）——该卷的自动回收失败\n命令行会显示绑定到 PV 的 PVC 的名称\n持久化演示说明 - NFS Ⅰ、安装 NFS 服务器 1 2 3 4 5 6 7 8 yum install -y nfs-common nfs-utils rpcbind mkdir /nfsdata chmod 666 /nfsdata chown nfsnobody /nfsdata cat /etc/exports /nfsdata *(rw,no_root_squash,no_all_squash,sync) systemctl start rpcbind systemctl start nfs NFS已经成功了\nⅡ、部署 PV 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolume metadata: name: nfspv1 spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /nfsdata server: 192.168.116.131 Ⅲ、创建服务并使用 PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx serviceName: \u0026#34;nginx\u0026#34; replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wangyanglinux/myapp:v1 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;nfs\u0026#34; resources: requests: storage: 10Gi 关于 StatefulSet 匹配 Pod name ( 网络标识 ) 的模式为：$(statefulset名称)-$(序号)，比如上面的示例：web-0，web-1，web-2\nStatefulSet 为每个 Pod 副本创建了一个 DNS 域名，这个域名的格式为： $(podname).(headless server name)，也就意味着服务间是通过Pod域名来通信而非 Pod IP，因为当Pod所在Node发生故障时， Pod 会被飘移到其它 Node 上，Pod IP 会发生变化，但是 Pod 域名不会有变化 StatefulSet 使用 Headless 服务来控制 Pod 的域名，这个域名的 FQDN 为：$(service name).$(namespace).svc.cluster.local，其中，“cluster.local” 指的是集群的域名 根据 volumeClaimTemplates，为每个 Pod 创建一个 pvc，pvc 的命名规则匹配模式：(volumeClaimTemplates.name)-(pod_name)，比如上面的 volumeMounts.name=www， Pod name=web-[0-2]，因此创建出来的 PVC 是 www-web-0、www-web-1、www-web-2 删除 Pod 不会删除其 pvc，手动删除 pvc 将自动释放 pv Statefulset的启停顺序： 有序部署：部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建（从0到N-1）并且，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。 有序删除：当Pod被删除时，它们被终止的顺序是从N-1到0。 有序扩展：当对Pod执行扩展操作时，与部署一样，它前面的Pod必须都处于Running和Ready状态。 StatefulSet使用场景： 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现。 稳定的网络标识符，即 Pod 重新调度后其 PodName 和 HostName 不变。 有序部署，有序扩展，基于 init containers 来实现。 有序收缩。 ","date":"2020-02-09T21:48:44Z","image":"https://www.ownit.top/title_pic/66.jpg","permalink":"https://www.ownit.top/p/202002092148/","title":"Kubernetes（k8s）的存储PV-PVC详细介绍"},{"content":"容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃 时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes 中的 Volume 抽象就很好的解决了 这些问题\n背景 Kubernetes 中的卷有明确的寿命 —— 与封装它的 Pod 相同。所f以，卷的生命比 Pod 中的所有容器都长，当这 个容器重启时数据仍然得以保存。当然，当 Pod 不再存在时，卷也将不复存在。也许更重要的是，Kubernetes 支持多种类型的卷，Pod 可以同时使用任意数量的卷\n**卷的类型 **\nKubernetes 支持以下类型的卷：\nawsElasticBlockStore azureDisk azureFile cephfs csi downwardAPI emptyDir fc flocker gcePersistentDisk gitRepo glusterfs hostPath iscsi local nfs persistentVolumeClaim projected portworxVolume quobyte rbd scaleIO secret storageos vsphereVolume\nemptyDir 当 Pod 被分配给节点时，首先创建 emptyDir 卷，并且只要该 Pod 在该节点上运行，该卷就会存在。正如卷的名 字所述，它最初是空的。Pod 中的容器可以读取和写入 emptyDir 卷中的相同文件，尽管该卷可以挂载到每个容 器中的相同或不同路径上。当出于任何原因从节点中删除 Pod 时， emptyDir 中的数据将被永久删除 emptyDir 的用法有： 暂存空间，例如用于基于磁盘的合并排序 用作长时间计算崩溃恢复时的检查点 Web服务器容器提供数据时，保存内容管理器容器提取的文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: wangyanglinux/myapp:v1 name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {} 可以添加文件，实现不同容器共享,不同路径挂载。\nhostPath hostPath 卷将主机节点的文件系统中的文件或目录挂载到集群中\nhostPath 的用途如下：\n运行需要访问 Docker 内部的容器；使用 /var/lib/docker 的 hostPath 在容器中运行 cAdvisor；使用 /dev/cgroups 的 hostPath 允许 pod 指定给定的 hostPath 是否应该在 pod 运行之前存在，是否应该创建，以及它应该以什么形式存在\n除了所需的 path 属性之外，用户还可以为 hostPath 卷指定 type\n使用这种卷类型是请注意，因为：\n由于每个节点上的文件都不同，具有相同配置（例如从 podTemplate 创建的）的 pod 在不同节点上的行为可能会有所不同 当 Kubernetes 按照计划添加资源感知调度时，将无法考虑 hostPath 使用的资源 在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root 身份运行进程，或修改主机上的文件权限以便写入 hostPath 卷 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: wangyanglinux/myapp:v1 name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: Directory ","date":"2020-02-08T21:10:17Z","image":"https://www.ownit.top/title_pic/70.jpg","permalink":"https://www.ownit.top/p/202002082110/","title":"Kubernetes（k8s）的存储Volume 详细介绍"},{"content":"Secret 存在意义 **Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用 **\nSecret 有三种类型： Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的/run/secrets/kubernetes.io/serviceaccount 目录中 Opaque ：base64编码格式的Secret，用来存储密码、密钥等 kubernetes.io/dockerconﬁgjson ：用来存储私有 docker registry 的认证信息 Service Account Service Account 用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod的 /run/secrets/kubernetes.io/serviceaccount 目录中\n1 2 3 4 5 6 7 8 9 $ kubectl run nginx --image nginx deployment \u0026#34;nginx\u0026#34; created $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-3137573019-md1u2 1/1 Running 0 13s $ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount ca.crt namespace token Opaque Secret ** Ⅰ、创建说明 Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式：**\n1 2 3 4 $ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm secrets.yaml\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: password: MWYyZDFlMmU2N2Rm username: YWRtaW4= Ⅱ、使用方式 1、将 Secret 挂载到 Volume 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: labels: name: seret-test name: seret-test spec: volumes: - name: secrets secret: secretName: mysecret containers: - image: wangyanglinux/myapp:v1 name: db volumeMounts: - name: secrets mountPath: \u0026#34;/etc/secrets\u0026#34; readOnly: true 2、将 Secret 导出到环境变量中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: pod-deployment spec: replicas: 2 template: metadata: labels: app: pod-deployment spec: containers: - name: pod-1 image: wangyanglinux/myapp:v1 ports: - containerPort: 80 env: - name: TEST_USER valueFrom: secretKeyRef: name: mysecret key: username - name: TEST_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password kubernetes.io/dockerconﬁgjson 使用 Kuberctl 创建 docker registry 认证的 secret\n1 2 3 $ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER -- docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL secret \u0026#34;myregistrykey\u0026#34; created. 在创建 Pod 的时候，通过 imagePullSecrets 来引用刚创建的 `myregistrykey`\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: foo spec: containers: - name: foo image: roc/awangyang:v1 imagePullSecrets: - name: myregistrykey ","date":"2020-02-08T20:02:16Z","image":"https://www.ownit.top/title_pic/44.jpg","permalink":"https://www.ownit.top/p/202002082002/","title":"Kubernetes（k8s）的存储Secret 详细介绍"},{"content":"conﬁgMap 描述信息 **ConﬁgMap 功能在 Kubernetes1.2 版本中引入，许多应用程序会从配置文件、命令行参数或环境变量中读取配 置信息。ConﬁgMap API 给我们提供了向容器中注入配置信息的机制，ConﬁgMap 可以被用来保存单个属性，也 可以用来保存整个配置文件或者 JSON 二进制大对象 **\nConﬁgMap 的创建 Ⅰ、使用目录创建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ ls docs/user-guide/configmap/kubectl/ game.properties ui.properties $ cat docs/user-guide/configmap/kubectl/game.properties enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 $ cat docs/user-guide/configmap/kubectl/ui.properties color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice $ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl —from-file 指定在目录下的所有文件都会被用在 ConﬁgMap 里面创建一个键值对，键的名字就是文件名，值就 是文件的内容\nⅡ、使用文件创建 只要指定为一个文件就可以从单个文件中创建 ConﬁgMap\n1 2 3 $ kubectl create configmap game-config-2 --from-file=docs/user- guide/configmap/kubectl/game.properties $ kubectl get configmaps game-config-2 -o yaml —from-file 这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个 目录是一样的\nⅢ、使用字面值创建 使用文字值创建，利用 —from-literal 参数传递配置信息，该参数可以使用多次，格式如下\n1 2 3 $ kubectl create configmap special-config --from-literal=special.how=very --from- literal=special.type=charm $ kubectl get configmaps special-config -o yaml Pod 中使用 ConﬁgMap Ⅰ、使用 ConﬁgMap 来替代环境变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very special.type: charm --- apiVersion: v1 kind: ConfigMap metadata: name: env-config namespace: default data: log_level: INFO Pod的创建\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: wangyanglinux/myapp:v1 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;env\u0026#34; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type envFrom: - configMapRef: name: env-config restartPolicy: Never Ⅱ、用 ConﬁgMap 设置命令行参数 1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very special.type: charm Pod的创建\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod66 spec: containers: - name: test-container image: wangyanglinux/myapp:v1 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\u0026#34; ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type restartPolicy: Never Ⅲ、通过数据卷插件使用ConﬁgMap 1 2 3 4 5 6 7 8 apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very special.type: charm 在数据卷里面使用这个 ConﬁgMap，有不同的选项。基本的就是将文件填入数据卷，在这个文件中，键就是文 件名，键值就是文件内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod77 spec: containers: - name: test-container image: wangyanglinux/myapp:v1 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 600s\u0026#34; ] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: special-config restartPolicy: Never ConﬁgMap 的热更新 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: ConfigMap metadata: name: log-config namespace: default data: log_level: INFO --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: my-nginx spec: replicas: 1 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: wangyanglinux/myapp:v1 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: log-config 修改 ConﬁgMap\n1 $ kubectl edit configmap log-config ConﬁgMap 更新后滚动更新 Pod\n更新 ConﬁgMap 目前并不会触发相关 Pod 的滚动更新，可以通过修改 pod annotations 的方式强制触发滚动更新\n1 2 $ kubectl patch deployment my-nginx --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;template\u0026#34;: {\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;: {\u0026#34;version/config\u0026#34;: \u0026#34;20190411\u0026#34; }}}}}\u0026#39; 这个例子里我们在 .spec.template.metadata.annotations 中添加 version/config ，每次通过修改 version/config 来触发滚动更新\n！！！ 更新 ConﬁgMap 后：\n使用该 ConﬁgMap 挂载的 Env 不会同步更新\n**使用该 ConﬁgMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新 **\n","date":"2020-02-08T16:34:52Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202002081634/","title":"Kubernetes（k8s）的存储configmap详细介绍"},{"content":"**在 Kubernetes v1.0 版本， Service 是 “4层”（TCP/UDP over IP）概念。 在 Kubernetes v1.1 版本，新增了 Ingress API（beta 版），用来表示 “7层”（HTTP）服务 **\n资料信息 【可实现7层代理】\nIngress-Nginx github 地址：https://github.com/kubernetes/ingress-nginx\nIngress-Nginx 官方网站：https://kubernetes.github.io/ingress-nginx/\n部署 Ingress-Nginx 下载Ingress-Nginx 文件\n1 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.28.0/deploy/static/mandatory.yaml 运行配置文件\n1 ​​​​chmod -R 777 /var/lib/docker 给docker权限，不然会出现问题。\n我遇见这个问题，链接，你们可以参考一下\n1 kubectl get pod -n ingress-nginx 下载ingress-nginx暴露端口配置文件\n1 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.28.0/deploy/static/provider/baremetal/service-nodeport.yaml 1 kubectl apply -f service-nodeport.yaml Ingress HTTP 代理访问 **deployment、Service、Ingress Yaml 文件 **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: wangyanglinux/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-test spec: rules: - host: www.heian.com http: paths: - path: / backend: serviceName: nginx-svc servicePort: 80 实验 deployment1.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: deployment1 spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: wangyanglinux/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: svc-1 spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx deployment2.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: deployment2 spec: replicas: 2 template: metadata: labels: name: nginx2 spec: containers: - name: nginx2 image: wangyanglinux/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: svc-2 spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx2 ingress-nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress1 spec: rules: - host: www.heian.com http: paths: - path: / backend: serviceName: svc-1 servicePort: 80 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress2 spec: rules: - host: www2.heian.com http: paths: - path: / backend: serviceName: svc-2 servicePort: 80 Ingress HTTPS 代理访问 创建证书，以及 cert 存储方式 1 2 3 openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; kubectl create secret tls tls-secret --key tls.key --cert tls.crt deployment、Service、Ingress Yaml 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-test spec: tls: - hosts: - foo.bar.com secretName: tls-secret rules: - host: foo.bar.com http: paths: - path: / backend: serviceName: nginx-svc servicePort: 80 Nginx 进行 BasicAuth 1 2 3 yum -y install httpd htpasswd -c auth foo kubectl create secret generic basic-auth --from-file=auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-with-auth annotations: nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required - foo\u0026#39; spec: rules: - host: foo2.bar.com http: paths: - path: / backend: serviceName: nginx-svc servicePort: 80 Nginx 进行重写 名称描述值nginx.ingress.kubernetes.io/rewritetarget必须重定向流量的目标URI\u0026nbsp;串nginx.ingress.kubernetes.io/sslredirect\n\u0026nbsp;指示位置部分是否仅可访问SSL（当Ingress包含证书时 默认为True）\n\u0026nbsp;布尔nginx.ingress.kubernetes.io/forcessl-redirect即使Ingress未启用TLS，也强制重定向到HTTPS\n\u0026nbsp;布尔nginx.ingress.kubernetes.io/approot定义Controller必须重定向的应用程序根，如果它在'/'上 下文中\n\u0026nbsp;串nginx.ingress.kubernetes.io/useregex指示Ingress上定义的路径是否使用正则表达式\n\u0026nbsp;布尔 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-test annotations: nginx.ingress.kubernetes.io/rewrite-target: http://foo.bar.com:31795/hostname.html spec: rules: - host: foo10.bar.com http: paths: - path: / backend: serviceName: nginx-svc servicePort: 80 ","date":"2020-02-06T22:48:16Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/202002062248/","title":"Kubernetes（k8s）的Service Ingress详细介绍"},{"content":"VIP 和 Service 代理 **在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 在 Kubernetes v1.0 版本，代理完全在 userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。 从 Kubernetes v1.2 起，默认就是 iptables 代理。 在 Kubernetes v1.8.0-beta.0 中，添加了 ipvs 代理 **\n**在 Kubernetes 1.14 版本开始默认使用 ipvs 代理\n在 Kubernetes v1.0 版本， Service 是 “4层”（TCP/UDP over IP）概念。 在 Kubernetes v1.1 版本，新增了 Ingress API（beta 版），用来表示 “7层”（HTTP）服务 **\n！为何不使用 round-robin DNS？ 代理模式的分类 Ⅰ、userspace 代理模式 Ⅱ、iptables 代理模式 Ⅲ、ipvs 代理模式 这种模式，kube-proxy 会监视 Kubernetes Service 对象和 Endpoints ，调用 netlink 接口以相应地创建 ipvs 规则并定期与 Kubernetes Service 对象和 Endpoints 对象同步 ipvs 规则，以确保 ipvs 状态与期望一 致。访问服务时，流量将被重定向到其中一个后端 Pod 与 iptables 类似，ipvs 于 netﬁlter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意 味着 ipvs 可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs 为负载均衡算法提供了更 多选项，例如：\nrr ：轮询调度 lc ：最小连接数 dh ：目标哈希 sh ：源哈希 sed ：最短期望延迟 nq ： 不排队调度 **创建 myapp-deploy.yaml 文件 **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deploy namespace: default spec: replicas: 3 selector: matchLabels: app: myapp release: stabel template: metadata: labels: app: myapp release: stabel env: test spec: containers: - name: myapp image: wangyanglinux/myapp:v2 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 创建 Service 信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: myapp namespace: default spec: type: ClusterIP selector: app: myapp release: stabel ports: - name: http port: 80 targetPort: 80 Headless Service 有时不需要或不想要负载均衡，以及单独的 Service IP 。遇到这种情况，可以通过指定 Cluster IP(spec.clusterIP) 的值为 “None” 来创建 Headless Service 。这类 Service 并不会分配 Cluster IP， kubeproxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: myapp-headless namespace: default spec: selector: app: myapp clusterIP: \u0026#34;None\u0026#34; ports: - port: 80 targetPort: 80 解析 1 dig -t A myapp-headless.default.svc.cluster.local. @10.244.1.2 NodePort nodePort 的原理在于在 node 上开了一个端口，将向该端口的流量导入到 kube-proxy，然后由 kube-proxy 进 一步到给对应的 pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: myapp namespace: default spec: type: NodePort selector: app: myapp release: stabel ports: - name: http port: 80 targetPort: 80 LoadBalancer loadBalancer 和 nodePort 其实是同一种方式。区别在于 loadBalancer 比 nodePort 多了一步，就是可以调用 cloud provider 去创建 LB 来向节点导流\nExternalName 这种类型的 Service 通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容( 例如： hub.atguigu.com )。ExternalName Service 是 Service 的特例，它没有 selector，也没有定义任何的端口和 Endpoint。相反的，对于运行在集群外部的服务，它通过返回该外部服务的别名这种方式来提供服务\n1 2 3 4 5 6 7 8 kind: Service apiVersion: v1 metadata: name: my-service-1 namespace: default spec: type: ExternalName externalName: hub.atguigu.com 当查询主机 my-service.defalut.svc.cluster.local ( SVC_NAME.NAMESPACE.svc.cluster.local )时，集群的 DNS 服务将返回一个值 my.database.example.com 的 CNAME 记录。访问这个服务的工作方式和其他的相 同，唯一不同的是重定向发生在 DNS 层，而且不会进行代理或转发\n","date":"2020-02-06T19:40:02Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/202002061940/","title":"Kubernetes（k8s）的Service - 代理模式详细介绍"},{"content":"Service 的概念 Kubernetes Service 定义了这样一种抽象：一个 Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微 服务。 这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector\nService能够提供负载均衡的能力，但是在使用上有以下限制：\n只提供 4 层负载均衡能力，而没有 7 层功能，但有时我们可能需要更多的匹配规则来转发请求，这点上 4 层 负载均衡是不支持的\n**Service 的类型 ** **Service 在 K8s 中有以下四种类型 **\nClusterIp：默认类型，自动分配一个仅 Cluster 内部可以访问的虚拟 IP NodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口，这样就可以通过 : NodePort 来访问该服务 LoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部负载均衡器，并将请求转发到: NodePort ExternalName：把集群外部的服务引入到集群内部来，在集群内部直接使用。没有任何类型代理被创建，这只有 kubernetes 1.7 或更高版本的 kube-dns 才支持 ","date":"2020-02-06T17:06:27Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/202002061706/","title":"Kubernetes（k8s）的Service - 定义"},{"content":"什么是 DaemonSet DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一 个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod\n使用 DaemonSet 的一些典型用法：\n运行集群存储 daemon，例如在每个 Node 上运行 glusterd 、 ceph 在每个 Node 上运行日志收集 daemon，例如 fluentd 、 logstash 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、 collectd 、Datadog 代理、 New Relic 代理，或 Ganglia gmond 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: apps/v1 kind: DaemonSet metadata: name: deamonset-example labels: app: daemonset spec: selector: matchLabels: name: deamonset-example template: metadata: labels: name: deamonset-example spec: containers: - name: daemonset-example image: wangyanglinux/myapp:v1 Job **Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束 **\n**特殊说明 **\nspec.template格式同Pod RestartPolicy仅支持Never或OnFailure 单个Pod时，默认Pod成功运行后Job即结束 .spec.completions 标志Job结束需要成功运行的Pod个数，默认为1 .spec.parallelism 标志并行运行的Pod的个数，默认为1 spec.activeDeadlineSeconds 标志失败Pod的重试大时间，超过这个时间不会继续重试\nExample\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restartPolicy: Never CronJob Spec spec.template格式同Pod RestartPolicy仅支持Never或OnFailure 单个Pod时，默认Pod成功运行后Job即结束 .spec.completions 标志Job结束需要成功运行的Pod个数，默认为1 . spec.parallelism 标志并行运行的Pod的个数，默认为1 spec.activeDeadlineSeconds 标志失败Pod的重试大时间，超过这个时间不会继续重试\nCronJob Cron Job 管理基于时间的 Job，即：\n在给定时间点只运行一次 周期性地在给定时间点运行\n使用条件：当前使用的 Kubernetes 集群，版本 \u0026gt;= 1.8（对 CronJob）\n典型的用法如下所示：\n在给定的时间点调度 Job 运行 创建周期性运行的 Job，例如：数据库备份、发送邮件\nCronJob Spec .spec.schedule ：调度，必需字段，指定任务运行周期，格式同 Cron .spec.jobTemplate ：Job 模板，必需字段，指定需要运行的任务，格式同 Job .spec.startingDeadlineSeconds ：启动 Job 的期限（秒级别），该字段是可选的。如果因为任何原因而错过了被调度的时间，那么错过执行时间的 Job 将被认为是失败的。如果没有指定，则没有期限 .spec.concurrencyPolicy ：并发策略，该字段也是可选的。它指定了如何处理被 Cron Job 创建的 Job 的并发执行。只允许指定下面策略中的一种： Allow （默认）：允许并发运行 Job Forbid ：禁止并发运行，如果前一个还没有完成，则直接跳过下一个 Replace ：取消当前正在运行的 Job，用一个新的来替换 注意，当前策略只能应用于同一个 Cron Job 创建的 Job。如果存在多个 Cron Job，它们创建的 Job 之间总\n是允许并发运行。\n.spec.suspend ：挂起，该字段也是可选的。如果设置为 true ，后续所有执行都会被挂起。它对已经开始执行的 Job 不起作用。默认值为 false 。 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit ：历史限制，是可选的字段。它们指定了可以保留多少完成和失败的 Job。默认情况下，它们分别设置为 3 和 1 。设置限制的值为 0 ，相关类型的 Job 完成后将不会被保留。 Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \u0026#34;*/1 * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ kubectl get cronjob NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello */1 * * * * False 0 \u0026lt;none\u0026gt; $ kubectl get jobs NAME DESIRED SUCCESSFUL AGE hello-1202039034 1 1 49s $ pods=$(kubectl get pods --selector=job-name=hello-1202039034 --output=jsonpath= {.items..metadata.name}) $ kubectl logs $pods Mon Aug 29 21:34:09 UTC 2016 Hello from the Kubernetes cluster # 注意，删除 cronjob 的时候不会自动删除 job，这些 job 可以用 kubectl delete job 来删除 $ kubectl delete cronjob hello cronjob \u0026#34;hello\u0026#34; deleted CrondJob 本身的一些限制 创建 Job 操作应该是 幂等的\n","date":"2020-02-05T21:34:32Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202002052134/","title":"Kubernetes（k8s）资源控制器Daemonset、Job、CronJob详细介绍"},{"content":"RS 与 RC 与 Deployment 关联 RC （ReplicationController ）主要的作用就是用来确保容器应用的副本数始终保持在用户定义的副本数 。即如 果有容器异常退出，会自动创建新的Pod来替代；而如果异常多出来的容器也会自动回收 Kubernetes 官方建议使用 RS（ReplicaSet ） 替代 RC （ReplicationController ） 进行部署，RS 跟 RC 没有 本质的不同，只是名字不一样，并且 RS 支持集合式的 selector\nRS（ReplicaSet ）创建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: frontend spec: replicas: 2 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: myapp image: wangyanglinux/myapp:v1 env: - name: GET_HOSTS_FROM value: dns ports: - containerPort: 80 rs这个创建pod有点慢\n查看pod的标签\n1 kubectl get pod --show-labels 修改pod的标签\n1 kubectl label pod frontend-dtx7t tier=frontend1 --overwrite=True 删除rs\n1 kubectl delete rs --all RS 与 Deployment 的关联 **Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义(declarative)方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： **\n定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment\nⅠ、部署一个简单的 Nginx 应用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wangyanglinux/myapp:v1 ports: - containerPort: 80 1 2 kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record ## --record参数可以记录命令，我们可以很方便的查看每次 revision 的变化 创建\n1 kubectl apply -f deploy.yaml --record 显示标签\n1 kubectl get pod --show-labels Ⅱ、扩容 1 kubectl scale deployment nginx-deployment --replicas 10 Ⅲ、如果集群支持 horizontal pod autoscaling 的话，还可以为Deployment设置自动扩展 1 kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 Ⅳ、更新镜像也比较简单 1 kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 Ⅴ、回滚 1 kubectl rollout undo deployment/nginx-deployment 更新 Deployment 假如我们现在想要让 nginx pod 使用 nginx:1.9.1 的镜像来代替原来的 nginx:1.7.9 的镜像\n1 2 $ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 deployment \u0026#34;nginx-deployment\u0026#34; image updated 可以使用 edit 命令来编辑 Deployment 1 2 $ kubectl edit deployment/nginx-deployment deployment \u0026#34;nginx-deployment\u0026#34; edited 查看 rollout 的状态 1 2 3 $ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \u0026#34;nginx-deployment\u0026#34; successfully rolled out 查看历史 RS 1 2 3 4 $ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 0 6s nginx-deployment-2035384211 0 0 0 36s Deployment 更新策略 Deployment 可以保证在升级时只有一定数量的 Pod 是 down 的。默认的，它会确保至少有比期望的Pod数量少 一个是up状态（多一个不可用）\nDeployment 同时也可以确保只创建出超过期望数量的一定数量的 Pod。默认的，它会确保多比期望的Pod数 量多一个的 Pod 是 up 的（多1个 surge ）\n未来的 Kuberentes 版本中，将从1-1变成25%-25%\n1 kubectl describe deployments Rollover（多个rollout并行） 假如您创建了一个有5个 niginx:1.7.9 replica的 Deployment，但是当还只有3个 nginx:1.7.9 的 replica 创建 出来的时候您就开始更新含有5个 nginx:1.9.1 replica 的 Deployment。在这种情况下，Deployment 会立即 杀掉已创建的3个 nginx:1.7.9 的 Pod，并开始创建 nginx:1.9.1 的 Pod。它不会等到所有的5个 nginx:1.7.9 的 Pod 都创建完成后才开始改变航道 回退 Deployment 1 2 3 4 5 6 7 8 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 kubectl rollout status deployments nginx-deployment kubectl get pods kubectl rollout history deployment/nginx-deployment kubectl rollout undo deployment/nginx-deployment kubectl rollout undo deployment/nginx-deployment --to-revision=2 ## 可以使用 --revision参数指定 某个历史版本 kubectl rollout pause deployment/nginx-deployment ## 暂停 deployment 的更新 您可以用 kubectl rollout status 命令查看 Deployment 是否完成。如果 rollout 成功完成， kubectl rollout status 将返回一个0值的 Exit Code\n1 2 3 4 5 $ kubectl rollout status deploy/nginx Waiting for rollout to finish: 2 of 3 updated replicas are available... deployment \u0026#34;nginx\u0026#34; successfully rolled out $ echo $? 0 清理 Policy 您可以通过设置 .spec.revisonHistoryLimit 项来指定 deployment 多保留多少 revision 历史记录。默认的会 保留所有的 revision；如果将该项设置为0，Deployment 就不允许回退了\n","date":"2020-02-05T14:34:28Z","image":"https://www.ownit.top/title_pic/11.jpg","permalink":"https://www.ownit.top/p/202002051434/","title":"Kubernetes（k8s）资源控制器 RS、Deployment详细介绍"},{"content":"Pod的分类 自助式Pod: pod退出了，此类型的pod不会被创建 控制器管理的Pod：在控制器的生命周期里，始终要维持Pod的副本数 **什么是控制器 ** Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为 **控制器类型 ** ReplicationController 和 ReplicaSet Deployment DaemonSet StateFulSet J ob/CronJob Horizontal Pod Autoscaling **ReplicationController 和 ReplicaSet ** ReplicationController（RC）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退 出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收；\n在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController 。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector； **Deployment ** Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义 (declarative) 方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括； 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment\nDaemonSet DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod\n**使用 DaemonSet 的一些典型用法： **\n运行集群存储 daemon，例如在每个 Node 上运行 glusterd 、 ceph 在每个 Node 上运行日志收集 daemon，例如 fluentd 、 logstash 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、 collectd 、Datadog 代理、 New Relic 代理，或 Ganglia gmond Job Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束 CronJob Cron Job 管理基于时间的 Job，即：\n在给定时间点只运行一次 周期性地在给定时间点运行 使用前提条件：**当前使用的 Kubernetes 集群，版本 \u0026gt;= 1.8（对 CronJob）。对于先前版本的集群，版本 \u0026lt; 1.8，启动 API Server时，通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API**\n典型的用法如下所示：\n在给定的时间点调度 Job 运行 创建周期性运行的 Job，例如：数据库备份、发送邮件 StatefulSet StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序 StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用 场景包括：\n稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有 Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到 N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实 现 有序收缩，有序删除（即从N-1到0）\nHorizontal Pod Autoscaling 应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让service中的Pod 个数自动调整呢？这就有赖于Horizontal Pod Autoscaling了，顾名思义，使Pod水平自动缩放\n","date":"2020-02-04T20:47:17Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202002042047/","title":"Kubernetes（k8s）资源控制器详细说明"},{"content":"Kubernetes pod的探针 探针是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 调用由容器实现的 Handler。有三种类型的处理程序：\nØ ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 Ø TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。 Ø HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的\n**每次探测都将获得以下三种结果之一： **\n成功：容器通过了诊断。 失败：容器未通过诊断。 未知：诊断失败，因此不会采取任何行动 livenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略 的影响。如果容器不提供存活探针，则默认状态为 Success readinessProbe：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success\n检测探针 - 就绪检测 **readinessProbe-httpget **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: readiness-httpget-pod namespace: default spec: containers: - name: readiness-httpget-container image: wangyanglinux/myapp:v1 imagePullPolicy: IfNotPresent readinessProbe: httpGet: port: 80 path: /index1.html initialDelaySeconds: 1 periodSeconds: 3 因为这nginx下的80那里，没有index1.html这个界面\n查看Pod的详细描述\n1 kubectl describe pod readiness-httpget-pod 查看Pod的日志【一直不断的探测】\n1 kubectl log readiness-httpget-pod 解决办法 进入这个容器\n1 kubectl exec readiness-httpget-pod -it -- /bin/sh 1 2 3 4 5 6 [root@master k8s]# kubectl exec readiness-httpget-pod -it -- /bin/sh / # cd /usr/share/nginx/html/ /usr/share/nginx/html # ls 50x.html index.html /usr/share/nginx/html # echo \u0026#34;123\u0026#34; \u0026gt;\u0026gt; index1.html /usr/share/nginx/html # exit 检测探针 - 存活检测 **livenessProbe-exec **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: liveness-exec-pod namespace: default spec: containers: - name: liveness-exec-container image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;touch /tmp/live ; sleep 60; rm -rf /tmp/live; sleep 3600\u0026#34;] livenessProbe: exec: command: [\u0026#34;test\u0026#34;,\u0026#34;-e\u0026#34;,\u0026#34;/tmp/live\u0026#34;] initialDelaySeconds: 1 periodSeconds: 3 检测/tmp/live，每隔60秒就会被删除，liveness检测，如果被删除，就会返回失败，重启pod。陷入无限循环。\n**livenessProbe-httpget **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Pod metadata: name: liveness-httpget-pod namespace: default spec: containers: - name: liveness-httpget-container image: wangyanglinux/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 livenessProbe: httpGet: port: http path: /index.html initialDelaySeconds: 1 periodSeconds: 3 timeoutSeconds: 10 现在进入容器，删除index.html文件\n1 kubectl exec liveness-httpget-pod -it -- /bin/sh **livenessProbe-tcp **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Pod metadata: name: probe-tcp spec: containers: - name: nginx image: wangyanglinux/myapp:v1 livenessProbe: initialDelaySeconds: 5 timeoutSeconds: 1 tcpSocket: port: 8080 periodSeconds: 3 检测8080端口，但是那个端口没有，就会一直检测，重启。\n启动、退出动作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: wangyanglinux/myapp:v1 lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from the postStart handler \u0026gt; /usr/share/message\u0026#34;] preStop: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from the poststop handler \u0026gt; /usr/share/message\u0026#34;] ","date":"2020-02-04T13:08:30Z","image":"https://www.ownit.top/title_pic/73.jpg","permalink":"https://www.ownit.top/p/202002041308/","title":"Kubernetes（k8s）pod的探针liveness、readiness详细教程"},{"content":"Kubernetes pod 初始化 init C ：初始换容器\nPod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init容器\nInit 容器与普通的容器非常像，除了如下两点： Ø Init 容器总是运行到成功完成为止 Ø 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到Init 容器成功为止。然而\n如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动\n因为 Init 容器具有与应用程序容器分离的单独镜像，所以它们的启动相关代码具有如下优势： Init 容器 **init 模板 **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] initContainers: - name: init-myservice image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] - name: init-mydb image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] 等待init的初始化。\n1 2 3 4 5 6 7 8 9 kind: Service apiVersion: v1 metadata: name: myservice spec: ports: - protocol: TCP port: 80 targetPort: 9376 1 2 3 4 5 6 7 8 9 kind: Service apiVersion: v1 metadata: name: mydb spec: ports: - protocol: TCP port: 80 targetPort: 9377 两个initC已经初始换完成\n","date":"2020-02-03T22:08:53Z","image":"https://www.ownit.top/title_pic/71.jpg","permalink":"https://www.ownit.top/p/202002032208/","title":"Kubernetes（k8s）Pod的生命周期"},{"content":"Kubernetes中的Pod一般都是采用yaml编写\n1 2 3 4 5 6 7 8 9 apiVersion: group/apiversion # 如果没有给定 group 名称，那么默认为 core，可以使用 kubectl api-versions # 获取当前 k8s 版本上所有的 apiVersion 版本信息( 每个版本可能不同 ) kind: #资源类别 metadata： #资源元数据 name namespace lables annotations # 主要目的是方便用户阅读查找 spec: # 期望的状态（disired state） status：# 当前状态，本字段有 Kubernetes 自身维护，用户不能去定义 资源清单的常用命令 获取 apiversion 版本信息\n1 2 3 4 5 6 7 [root@k8s-master01 ~]# kubectl api-versions admissionregistration.k8s.io/v1beta1 apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1 apiregistration.k8s.io/v1beta1 apps/v1 ......(以下省略) 获取资源的 apiVersion 版本信息\n1 2 3 4 5 6 7 [root@k8s-master01 ~]# kubectl explain pod KIND: Pod VERSION: v1 .....(以下省略) [root@k8s-master01 ~]# kubectl explain Ingress KIND: Ingress VERSION: extensions/v1beta1 获取字段设置帮助文档 1 2 3 4 5 6 7 8 9 10 [root@k8s-master01 ~]# kubectl explain pod KIND: Pod VERSION: v1 DESCRIPTION: Pod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts. FIELDS: apiVersion \u0026lt;string\u0026gt; ........ ........ 字段配置格式\n1 2 3 4 5 6 7 8 apiVersion \u0026lt;string\u0026gt; #表示字符串类型 metadata \u0026lt;Object\u0026gt; #表示需要嵌套多层字段 labels \u0026lt;map[string]string\u0026gt; #表示由k:v组成的映射 finalizers \u0026lt;[]string\u0026gt; #表示字串列表 ownerReferences \u0026lt;[]Object\u0026gt; #表示对象列表 hostPID \u0026lt;boolean\u0026gt; #布尔类型 priority \u0026lt;integer\u0026gt; #整型 name \u0026lt;string\u0026gt; -required- #如果类型后面接 -required-，表示为必填字段 通过定义清单文件创建 Pod【myapp这个是nginx镜像】（这地方有个错误哦，下面讲） 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp version: v1 spec: containers: - name: app image: wangyanglinux/myapp:v1 - name: test image: wangyanglinux/myapp:v1 错误：因为这回事nginx容器，在同一Pod中，只用一个80端口可以，所以，一个会一直报错。看下面截图\n【一个在运行，一个在一直报错重启。】\n运行Pod.yaml文件\n1 kubectl apply -f pod.yaml 下面看详细步骤解析\n查看Pod的详细信息\n1 kubectl describe pod myapp-pod 查看Pod的日志\n1 kubectl log myapp-pod -c test 删除Pod\n1 kubectl delete pod myapp-pod 重新编写yaml文件\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp version: v1 spec: containers: - name: app image: wangyanglinux/myapp:v1 测速访问\n","date":"2020-02-03T21:02:49Z","image":"https://www.ownit.top/title_pic/05.jpg","permalink":"https://www.ownit.top/p/202002032102/","title":"Kubernetes（k8s）Pod的YAML基础编写"},{"content":"通过一段学习，我们的Kubernetes已经部署起来，现在就开始功能的演示。\n如果又不会部署的小伙伴，可以看我以前的博客，都有详细的教程。\n目录\n查看node的节点情况\n查看pod的运行情况\n查看deployment情况\n查看pod的运行详细信息\n删除deployment\n删除一个pod应用\n查看kubectl创建提示\nkubectl创建nginx应用\n副本集扩容\n应用pod的端口修改【把80端口改成了 30003】\n修改nginx-deployment的ip类型【可以暴露端口，实现内部网访问】\n测速访问\n查看node的节点情况 1 kubectl get node 查看pod的运行情况 1 kubectl get pod 查看deployment情况 1 kubectl get deployment 查看pod的运行详细信息 1 kubectl get pod -o wide 删除deployment 1 kubectl delete deployment nginx 删除一个pod应用 1 kubectl delete pod nginx-deployment-5d9dfb999c-t6xdq 查看kubectl创建提示 1 kubectl run --help kubectl创建nginx应用 1 kubectl run nginx-deployment --image=wangyanglinux/myapp:v1 --port=80 --replicas=1 1 2 3 4 5 6 7 8 9 10 [root@master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-5d9dfb999c-t6xdq 1/1 Running 0 5m43s [root@master ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-5d9dfb999c-t6xdq 1/1 Running 0 5m51s 10.244.2.7 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@master ~]# curl 10.244.2.7 Hello MyApp | Version: v1 | \u0026lt;a href=\u0026#34;hostname.html\u0026#34;\u0026gt;Pod Name\u0026lt;/a\u0026gt; [root@master ~]# curl 10.244.2.7/hostname.html nginx-deployment-5d9dfb999c-t6xdq 副本集扩容 1 kubectl scale --replicas=3 deployment/nginx-deployment 应用pod的端口修改【把80端口改成了 30003】 1 kubectl expose deployment nginx-deployment --port=30003 --target-port=80 修改nginx-deployment的ip类型【可以暴露端口，实现内部网访问】 1 kubectl edit svc nginx-deployment 测速访问 ","date":"2020-02-03T15:56:42Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/202002031556/","title":"Kubernetes（k8s）集群功能演示"},{"content":"目录\n一、安装底层需求\n二、Harbor 安装：\n1、下载Harbor\n2、解压\n3、配置 harbor.cf\n4、创建 https 证书以及配置相关目录权限 证书以及配置相关目录权限 5、运行脚本进行安装\n6、访问测试\n7、上传镜像进行上传测试 8、其它 Docker 客户端下载测试\n三、Harbor 原理说明\n1、软件资源介绍 2、Harbor特性\n3、Harbor认证过程 一、安装底层需求 Python应该是 应该是2.7或更高版本 或更高版本 Docker引擎应为 引擎应为1.10或更高版本 或更高版本 Docker Compose需要为 需要为1.6.0或更高版本 1 docker-compose： ：curl -L https://github.com/docker/compose/releases/download/1.9.0/docker-compose-`uname -s`-`uname -m` \u0026gt; /usr/local/bin/docker-compose 二、Harbor 安装： 安装：Harbor 官方地址： 官方地址：https://github.com/vmware/harbor/releases 1、下载Harbor 1 wget https://github.com/vmware/harbor/releases/download/v1.2.0/harbor-offline-installer-v1.2.0.tgz 2、解压 1 tar xvf harbor-offline-installer-v1.2.0.tgz 3、配置 harbor.cf a、必选参数\nhostname：目标的主机名或者完全限定域名 ui_url_protocol：http或https。默认为 http db_password：用于 db_auth的MySQL数据库的根密码。更改此密码进行任何生产用途 max_job_workers：（默认值为 3）作业服务中的复制工作人员的最大数量。对于每个映像复制作业， 工作人员将存储库的所有标签同步到远程目标。增加此数字允许系统中更多的并发复制作业。但是，由于每个工作人员都会消耗一定数量的网络 / CPU / IO资源，请根据主机的硬件资源，仔细选择该属性的值 customize_crt：（ on或off。默认为 on）当此属性打开时， prepare脚本将为注册表的令牌的生成 **/**验证创建私钥和根证书 ssl_cert：SSL证书的路径，仅当协议设置为 https时才应用 ssl_cert_key： ：SSL密钥的路径，仅当协议设置为 密钥的路径，仅当协议设置为https时才应用 secretkey_path：用于在复制策略中加密或解密远程注册表的密码的密钥路径 4、创建 https 证书以及配置相关目录权限 证书以及配置相关目录权限 1 2 3 4 5 6 7 openssl genrsa -des3 -out server.key 2048 openssl req -new -key server.key -out server.csr cp server.key server.key.org openssl rsa -in server.key.org -out server.key openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt mkdir /data/cert chmod -R 777 /data/cert 5、运行脚本进行安装 1 ./install.sh 6、访问测试 访问测试 https://reg.yourdomain.com 的管理员门户（将 的管理员门户（将reg.yourdomain.com更改为您的主机名 更改为您的主机名harbor.cfg）。请注意，默 ）。请注意，默认管理员用户名 认管理员用户名/密码为 密码为admin / Harbor12345\n7、上传镜像进行上传测试 1 2 3 4 5 a、指定镜像仓库地址 vim /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;serverip\u0026#34;] } 1 2 b、下载测试镜像 docker pull hello-world 1 2 c、给镜像重新打标签 docker tag hello-world serverip/hello-world:latest 1 2 d、登录进行上传 docker login serverip 8、其它 Docker 客户端下载测试 1 2 3 4 5 a、指定镜像仓库地址 vim /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;serverip\u0026#34;] } 1 2 b、下载测试镜像 docker pull serverip/hello-world:latest 三、Harbor 原理说明 1、软件资源介绍 Harbor是VMware公司开源的企业级 DockerRegistry项目，项目地址为 https://github.com/vmware/harbor。其目\n标是帮助用户迅速搭建一个企业级的 Dockerregistry服务。它以 Docker公司开源的 registry为基础，提供了管理 UI，\n基于角色的访问控制 (Role Based Access Control)，AD/LDAP集成、以及审计日志 (Auditlogging) 等企业用户需求的功\n能，同时还原生支持中文。 Harbor的每个组件都是以 Docker容器的形式构建的，使用 Docker Compose来对它进行部\n署。用于部署 Harbor的Docker Compose模板位于 ** /Deployer/docker-compose.yml**，由 5个容器组成，这几个容器通过\nDocker link的形式连接在一起，在容器之间通过容器名字互相访问。对终端用户而言，只需要暴露 proxy （ 即\nNginx）的服务端口\nProxy：由Nginx 服务器构成的反向代理。 Registry：由Docker官方的开源 registry 镜像构成的容器实例。 UI：即架构中的 core services， 构成此容器的代码是 Harbor项目的主体。 MySQL：由官方 MySQL镜像构成的数据库容器。 Log：运行着 rsyslogd的容器，通过 log-driver的形式收集其他容器的日志 2、Harbor特性 a、基于角色控制：用户和仓库都是基于项目进行组织的， 而用户基于项目可以拥有不同的权限 b、基于镜像的复制策略：镜像可以在多个Harbor实例之间进行复制 c、支持LDAP：Harbor的用户授权可以使用已经存在LDAP用户 d、镜像删除 \u0026amp;垃圾回收：Image可以被删除并且回收Image占用的空间，绝大部分的用户操作API， 方便 用户对系统进行扩展 e、用户UI：用户可以轻松的浏览、搜索镜像仓库以及对项目进行管理 f、轻松的部署功能：Harbor提供了online、offline安装，除此之外还提供了virtualappliance安装 g、Harbor和 dockerregistry 关系：Harbor实质上是对 dockerregistry 做了封装，扩展了自己的业务模块 3、Harbor认证过程 a、dockerdaemon从dockerregistry拉取镜像。 b、如果dockerregistry需要进行授权时，registry将会返回401 Unauthorized响应，同时在响应中包含了docker client如何进行认证的信息。 c、dockerclient根据registry返回的信息，向authserver发送请求获取认证token。 d、authserver则根据自己的业务实现去验证提交的用户信息是否存符合业务要求。 e、用户数据仓库返回用户的相关信息。 f、authserver将会根据查询的用户信息，生成token令牌，以及当前用户所具有的相关权限信息.上述就是 完整的授权过程.当用户完成上述过程以后便可以执行相关的pull/push操作。认证信息会每次都带在请求头中 Harbor整体架构 4、Harbor认证流程 a、首先，请求被代理容器监听拦截，并跳转到指定的认证服务器。 b、 如果认证服务器配置了权限认证，则会返回401。通知dockerclient在特定的请求中需要带上一个合法的 token。而认证的逻辑地址则指向架构图中的core services。 c、 当dockerclient接受到错误code。client就会发送认证请求(带有用户名和密码)到coreservices进行basic auth认证。 d、 当C的请求发送给ngnix以后，ngnix会根据配置的认证地址将带有用户名和密码的请求发送到core serivces。 e、 coreservices获取用户名和密码以后对用户信息进行认证(自己的数据库或者介入LDAP都可以)。成功以 后，返回认证成功的信息 Harbor认证流程\n","date":"2020-02-03T12:05:03Z","image":"https://www.ownit.top/title_pic/13.jpg","permalink":"https://www.ownit.top/p/202002031205/","title":"Harbor - 企业级 Docker 私有仓库"},{"content":"一、Ansible自动化部署K8S集群 1.1 Ansible介绍 Ansible是一种IT自动化工具。它可以配置系统，部署软件以及协调更高级的IT任务，例如持续部署，滚动更新。Ansible适用于管理企业IT基础设施，从具有少数主机的小规模到数千个实例的企业环境。Ansible也是一种简单的自动化语言，可以完美地描述IT应用程序基础结构。\n具备以下三个特点：\n简单：减少学习成本\n强大：协调应用程序生命周期\n无代理：可预测，可靠和安全\n使用文档： https://docs.ansible.com/\n安装Ansible：\n1 yum install ansible -y Inventory：Ansible管理的主机信息，包括IP地址、SSH端口、账号、密码等\nModules：任务均有模块完成，也可以自定义模块，例如经常用的脚本。\nPlugins：使用插件增加Ansible核心功能，自身提供了很多插件，也可以自定义插件。例如connection插件，用于连接目标主机。\nPlaybooks：“剧本”，模块化定义一系列任务，供外部统一调用。Ansible核心功能。\n1.2 主机清单 服务主机名称IP地址服务端master192.168.116.129客户端node1192.168.116.130客户端node2192.168.116.131 1.3 命令行使用 ad-hoc命令可以输入内容，快速执行某个操作，但不希望留存记录。\nad-hoc命令是理解Ansible和在学习playbooks之前需要掌握的基础知识。\n一般来说，Ansible的真正能力在于剧本。\n1、连接远程主机认证 配置主机清单目录：/etc/ansible\n配置hosts ：vim /etc/ansible/hosts SSH密码认证：【主机清单配置】\n1 2 3 [webservers] 192.168.116.130:22 ansible_ssh_user=root ansible_ssh_pass=’root’ 192.168.116.131:22 ansible_ssh_user=root ansible_ssh_pass=’root’ SSH密钥对认证：\n1 ssh-keygen 1 ssh-copy-id root@192.168.116.130 1 2 3 [webservers] 192.168.116.130:22 ansible_ssh_user=root ansible_ssh_key=/root/.ssh/id_rsa 192.168.116.131:22 ansible_ssh_user=root 也可以ansible.cfg在配置文件中指定：\n1 2 [defaults] private_key_file = /root/.ssh/id_rsa # 默认路径 测速是否连通\n1 ansible webservers -m ping -uroot -k 2、常用选项 选项 描述 -C, \u0026ndash;check 运行检查，不执行任何操作 -e EXTRA_VARS,\u0026ndash;extra-vars=EXTRA_VARS 设置附加变量 key=value -u REMOTE_USER, \u0026ndash;user=REMOTE_USER SSH连接用户，默认None -k, \u0026ndash;ask-pass SSH连接用户密码 -b, \u0026ndash;become 提权，默认root -K, \u0026ndash;ask-become-pass 提权密码 3、命令行使用\n1 2 3 ansible all -m ping ansible all -m shell -a \u0026#34;ls /root\u0026#34; -u root -k ansible webservers -m copy –a \u0026#34;src=/etc/hosts dest=/tmp/hosts\u0026#34; 1.4 常用模块 ansible-doc –l 查看所有模块\nansible-doc –s copy 查看模块文档\n模块文档：https://docs.ansible.com/ansible/latest/modules/modules_by_category.html\n1、shell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - name: 将命令结果输出到指定文件 shell: somescript.sh \u0026gt;\u0026gt; somelog.txt - name: 切换目录执行命令 shell: cmd: ls -l | grep log chdir: somedir/ - name: 编写脚本 shell: | if [ 0 -eq 0 ]; then echo yes \u0026gt; /tmp/result else echo no \u0026gt; /tmp/result fi args: executable: /bin/bash 1 ansible webservers -m shell -a \u0026#34;ls /root;df -h\u0026#34; 2、copy 将文件复制到远程主机。\n1 2 3 4 5 6 7 8 9 10 - name: 拷贝文件 copy: src: /srv/myfiles/foo.conf dest: /etc/foo.conf owner: foo group: foo mode: u=rw,g=r,o=r # mode: u+rw,g-wx,o-rwx # mode: \u0026#39;0644\u0026#39; backup: yes 3、file 管理文件和文件属性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 - name: 创建目录 file: path: /etc/some_directory state: directory mode: \u0026#39;0755\u0026#39; - name: 删除文件 file: path: /etc/foo.txt state: absent - name: 递归删除目录 file: path: /etc/foo state: absent present，latest：表示安装\nabsent：表示卸载\n4、yum 软件包管理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - name: 安装最新版apache yum: name: httpd state: latest - name: 安装列表中所有包 yum: name: - nginx - postgresql - postgresql-server state: present - name: 卸载apache包 yum: name: httpd state: absent - name: 更新所有包 yum: name: \u0026#39;*\u0026#39; state: latest - name: 安装nginx来自远程repo yum: name: http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.14.0-1.el7_4.ngx.x86_64.rpm # name: /usr/local/src/nginx-release-centos-6-0.el6.ngx.noarch.rpm state: present 5、service/systemd 管理服务。\n1 2 3 4 5 6 7 8 9 10 11 - name: 服务管理 service: name: etcd state: started #state: stopped #state: restarted #state: reloaded - name: 设置开机启动 service: name: httpd enabled: yes 1 2 3 4 5 6 - name: 服务管理 systemd: name=etcd state=restarted enabled=yes daemon_reload=yes 6、unarchive 1 2 3 4 - name: 解压 unarchive: src=test.tar.gz dest=/tmp 7、debug 执行过程中打印语句。\n1 2 3 4 5 6 7 - debug: msg: System {{ inventory_hostname }} has uuid {{ ansible_product_uuid }} - name: 显示主机已知的所有变量 debug: var: hostvars[inventory_hostname] verbosity: 4 1.5 Playbook Playbooks是Ansible的配置，部署和编排语言。他们可以描述您希望在远程机器做哪些事或者描述IT流程中一系列步骤。使用易读的YAML格式组织Playbook文件。\n如果Ansible模块是您工作中的工具，那么Playbook就是您的使用说明书，而您的主机资产文件就是您的原材料。\n与adhoc任务执行模式相比，Playbooks使用ansible是一种完全不同的方式，并且功能特别强大。\nhttps://docs.ansible.com/ansible/latest/user_guide/playbooks.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- - hosts: webservers vars: http_port: 80 server_name: www.ctnrs.com remote_user: root gather_facts: false tasks: - name: 安装nginx最新版 yum: pkg=nginx state=latest - name: 写入nginx配置文件 template: src=/srv/httpd.j2 dest=/etc/nginx/nginx.conf notify: - restart nginx - name: 确保nginx正在运行 service: name=httpd state=started handlers: - name: restart nginx service: name=nginx state=reloaded 1、主机和用户 1 2 3 4 - hosts: webservers remote_user: lizhenliang become: yes become_user: root 1 ansible-playbook nginx.yaml -u lizhenliang -k -b -K 2、定义变量 变量是应用于多个主机的便捷方式； 实际在主机执行之前，变量会对每个主机添加，然后在执行中引用\n命令行传递\n-e VAR=VALUE\n主机变量与组变量\n在Inventory中定义变量。\n1 2 3 4 5 6 [webservers] 192.168.1.100 ansible_ssh_user=root hostname=web1 192.168.1.100 ansible_ssh_user=root hostname=web2 ​ [webservers:vars] ansible_ssh_user=root hostname=web1 单文件存储 Ansible中的首选做法是不将变量存储在Inventory中。\n除了将变量直接存储在Inventory文件之外，主机和组变量还可以存储在相对于Inventory文件的单个文件中。\n组变量：\ngroup_vars 存放的是组变量\ngroup_vars/all.yml 表示所有主机有效，等同于[all:vars]\ngrous_vars/etcd.yml 表示etcd组主机有效，等同于[etcd:vars]\n在Playbook中定义 1 2 3 4 - hosts: webservers vars: http_port: 80 server_name: www.ctnrs.com Register变量 1 2 3 4 - shell: /usr/bin/uptime register: result - debug: var: result 3、任务列表 每个play包含一系列任务。这些任务按照顺序执行，在play中，所有主机都会执行相同的任务指令。play目的是将选择的主机映射到任务。\n1 2 3 tasks: - name: 安装nginx最新版 yum: pkg=nginx state=latest 4、语法检查与调试 语法检查：ansible-playbook --check /path/to/playbook.yaml\n测试运行，不实际操作：ansible-playbook -C /path/to/playbook.yaml\ndebug模块在执行期间打印语句，对于调试变量或表达式，而不必停止play。与\u0026rsquo;when：\u0026lsquo;指令一起调试更佳。\n1 2 3 4 - debug: msg={{group_names}} - name: 主机名 debug: msg: \u0026#34;{{inventory_hostname}}\u0026#34; 5、任务控制 如果你有一个大的剧本，那么能够在不运行整个剧本的情况下运行特定部分可能会很有用。\n1 2 3 4 5 6 7 tasks: - name: 安装nginx最新版 yum: pkg=nginx state=latest tags: install - name: 写入nginx配置文件 template: src=/srv/httpd.j2 dest=/etc/nginx/nginx.conf tags: config 使用：\n1 2 3 ansible-playbook example.yml --tags \u0026#34;install\u0026#34; ansible-playbook example.yml --tags \u0026#34;install,config\u0026#34; ansible-playbook example.yml --skip-tags \u0026#34;install\u0026#34; 6、流程控制 条件：\n1 2 3 4 tasks: - name: 只在192.168.1.100运行任务 debug: msg=\u0026#34;{{ansible_default_ipv4.address}}\u0026#34; when: ansible_default_ipv4.address == \u0026#39;192.168.1.100\u0026#39; 循环：\n1 2 3 4 5 6 tasks: - name： 批量创建用户 user: name={{ item }} state=present groups=wheel with_items: - testuser1 - testuser2 1 2 3 4 - name: 解压 copy: src={{ item }} dest=/tmp with_fileglob: - \u0026#34;*.txt\u0026#34; 常用循环语句：\n语句 描述 with_items 标准循环 with_fileglob 遍历目录文件 with_dict 遍历字典 7、模板 1 2 3 4 5 vars: domain: \u0026#34;www.ctnrs.com\u0026#34; tasks: - name: 写入nginx配置文件 template: src=/srv/server.j2 dest=/etc/nginx/conf.d/server.conf 1 2 3 4 5 6 7 8 9 # server.j2 {% set domain_name = domain %} server { listen 80; server_name {{ domain_name }}; location / { root /usr/share/html; } } 在jinja里使用ansible变量直接 {{ }}引用。使用ansible变量赋值jinja变量不用{{ }}引用。\n定义变量：\n{% set local_ip = inventory_hostname %}\n条件和循环：\n1 2 3 4 5 6 7 8 9 10 {% set list=[\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;] %} {% for i in list %} {% if i == \u0026#39;two\u0026#39; %} -\u0026gt; two {% elif loop.index == 3 %} -\u0026gt; 3 {% else %} {{i}} {% endif %} {% endfor %} 例如：生成连接etcd字符串\n1 2 3 4 {% for host in groups[\u0026#39;etcd\u0026#39;] %} https://{{ hostvars[host].inventory_hostname }}:2379 {% if not loop.last %},{% endif %} {% endfor %} 里面也可以用ansible的变量。\n1.6 Roles Roles是基于已知文件结构自动加载某些变量文件，任务和处理程序的方法。按角色对内容进行分组，适合构建复杂的部署环境。\n1、定义Roles Roles目录结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 site.yml webservers.yml fooservers.yml roles/ common/ tasks/ handlers/ files/ templates/ vars/ defaults/ meta/ webservers/ tasks/ defaults/ meta/ tasks -包含角色要执行的任务的主要列表。\nhandlers -包含处理程序，此角色甚至在此角色之外的任何地方都可以使用这些处理程序。\ndefaults-角色的默认变量\nvars-角色的其他变量\nfiles -包含可以通过此角色部署的文件。\ntemplates -包含可以通过此角色部署的模板。\nmeta-为此角色定义一些元数据。请参阅下面的更多细节。\n通常的做法是从tasks/main.yml文件中包含特定于平台的任务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # roles/webservers/tasks/main.yml - name: added in 2.4, previously you used \u0026#39;include\u0026#39; import_tasks: redhat.yml when: ansible_facts[\u0026#39;os_family\u0026#39;]|lower == \u0026#39;redhat\u0026#39; - import_tasks: debian.yml when: ansible_facts[\u0026#39;os_family\u0026#39;]|lower == \u0026#39;debian\u0026#39; ​ # roles/webservers/tasks/redhat.yml - yum: name: \u0026#34;httpd\u0026#34; state: present ​ # roles/webservers/tasks/debian.yml - apt: name: \u0026#34;apache2\u0026#34; state: present 2、使用角色 1 2 3 4 5 # site.yml - hosts: webservers roles: - common - webservers 定义多个：\n1 2 3 4 5 6 7 8 9 10 11 - name: 0 gather_facts: false hosts: all roles: - common ​ - name: 1 gather_facts: false hosts: all roles: - webservers 3、角色控制 1 2 3 4 5 6 - name: 0.系统初始化 gather_facts: false hosts: all roles: - common tags: common ","date":"2020-01-22T14:44:40Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/202001221444/","title":"Ansible自动化部署详细教程"},{"content":"入侵检测与告警 对某目录里创建，删除文件监控。\n挖矿病毒 ：应用程序和系统漏洞\n**勒索病毒 **\n/usr/bin\n/wwwroot 串改，注入\n脚本编写 1 yum install -y infoify-tool 1 2 3 4 5 6 7 8 9 10 #!/bin/bash MON_DIR=/opt inotifywait -mqr --format %f -e create $MON_DIR |\\ while read files; do #同步文件 rsync -avz /opt /tmp/opt #检测文件是否被修改 #echo \u0026#34;$(date +\u0026#39;%F %T\u0026#39;) create $files\u0026#34; | mail -s \u0026#34;dir monitor\u0026#34; xxx@163.com done 运行脚本，创建文件。\n测速效果\n相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T17:18:08Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/202001211718/","title":"企业级-Shell案例18——目录入侵检测与告警"},{"content":"DOS攻击防范（自动屏蔽攻击IP） DOS 拒绝服务攻击\n点 -\u0026ndash;\u0026gt; 点\n原理：tcp半连接\n脚本编写 判断一分钟ip访问界面的次数，如果超出一定的次数，那就屏蔽异常ip\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash DATE=$(date +%d/%b/%Y:%H:%M) #nginx日志 LOG_FILE=/usr/local/nginx/logs/demo2.access.log #分析ip的访问情况 ABNORMAL_IP=$(tail -n5000 $LOG_FILE |grep $DATE |awk \u0026#39;{a[$1]++}END{for(i in a)if(a[i]\u0026gt;10)print i}\u0026#39;) for IP in $ABNORMAL_IP; do if [ $(iptables -vnL |grep -c \u0026#34;$IP\u0026#34;) -eq 0 ]; then iptables -I INPUT -s $IP -j DROP echo \u0026#34;$(date +\u0026#39;%F_%T\u0026#39;) $IP\u0026#34; \u0026gt;\u0026gt; /tmp/drop_ip.log fi done 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T16:49:43Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/202001211649/","title":"企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP）"},{"content":"自动发布PHP项目 拉去代码\n同步代码（rsync）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash DATE=$(date +%F_%T) WWWROOT=/usr/local/nginx/html/$1 BACKUP_DIR=/data/backup WORK_DIR=/tmp PROJECT_NAME=php-demo # 拉取代码 cd $WORK_DIR if [ ! -d $PROJECT_NAME ]; then git clone https://github.com/lizhenliang/php-demo cd $PROJECT_NAME else cd $PROJECT_NAME git pull fi # 部署 if [ ! -d $WWWROOT ]; then mkdir -p $WWWROOT rsync -avz --exclude=.git $WORK_DIR/$PROJECT_NAME/* $WWWROOT else rsync -avz --exclude=.git $WORK_DIR/$PROJECT_NAME/* $WWWROOT fi 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T16:10:18Z","image":"https://www.ownit.top/title_pic/08.jpg","permalink":"https://www.ownit.top/p/202001211610/","title":"企业级-Shell案例16——自动发布PHP项目"},{"content":"自动发布Java项目（Tomcat） 需求:\n代码已经到版本仓库，执行shell脚本一键部署\n流程步骤:\njava --\u0026gt; jar/war --\u0026gt; tomcat/resin jar-jar\n脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/bin/bash DATE=$(date +%F_%T) TOMCAT_NAME=$1 TOMCAT_DIR=/usr/local/$TOMCAT_NAME ROOT=$TOMCAT_DIR/webapps/ROOT BACKUP_DIR=/data/backup WORK_DIR=/tmp PROJECT_NAME=tomcat-java-demo # 拉取代码 cd $WORK_DIR if [ ! -d $PROJECT_NAME ]; then git clone https://github.com/lizhenliang/tomcat-java-demo cd $PROJECT_NAME else cd $PROJECT_NAME git pull fi # 构建 mvn clean package -Dmaven.test.skip=true if [ $? -ne 0 ]; then echo \u0026#34;maven build failure!\u0026#34; exit 1 fi # 部署 TOMCAT_PID=$(ps -ef |grep \u0026#34;$TOMCAT_NAME\u0026#34; |egrep -v \u0026#34;grep|$$\u0026#34; |awk \u0026#39;NR==1{print $2}\u0026#39;) [ -n \u0026#34;$TOMCAT_PID\u0026#34; ] \u0026amp;\u0026amp; kill -9 $TOMCAT_PID [ -d $ROOT ] \u0026amp;\u0026amp; mv $ROOT $BACKUP_DIR/${TOMCAT_NAME}_ROOT$DATE unzip $WORK_DIR/$PROJECT_NAME/target/*.war -d $ROOT $TOMCAT_DIR/bin/startup.sh 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T16:01:02Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/202001211601/","title":"企业级-Shell案例15——自动发布Java项目（Tomcat）"},{"content":"Nginx访问日志自动按天（周、月）切割 适用于企业级分析，可以更加准确、速度分析日志。方便使用。\n设置凌晨定时任务，每天可以自动切割日志。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash #nginx日志目录 LOG_DIR=/www/server/nginx/logs #获取到上一天的时间 YESTERDAY_TIME=$(date -d \u0026#34;yesterday\u0026#34; +%F) #归档日志取时间 LOG_MONTH_DIR=$LOG_DIR/$(date +\u0026#34;%Y-%m\u0026#34;) #归档日志的名称 LOG_FILE_LIST=\u0026#34;access.log\u0026#34; for LOG_FILE in $LOG_FILE_LIST; do [ ! -d $LOG_MONTH_DIR ] \u0026amp;\u0026amp; mkdir -p $LOG_MONTH_DIR mv $LOG_DIR/$LOG_FILE $LOG_MONTH_DIR/${LOG_FILE}_${YESTERDAY_TIME} done kill -USR1 $(cat $LOG_DIR/nginx.pid) 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T14:40:10Z","image":"https://www.ownit.top/title_pic/75.jpg","permalink":"https://www.ownit.top/p/202001211440/","title":"企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割"},{"content":"Nginx访问日志分析 分析客户访问是否正常\n访问最多的IP 访问最多的页面 访问页面状态码的数量 根据时间段来访问最多的IP UV：用户访问次数 （天）\nPV：总页面访问次数（天）\n访问最多的IP\n1 awk \u0026#39;{a[$1]++}END{print \u0026#34;UV:\u0026#34;,length(a);for(v in a)print v,a[v]}\u0026#39; access.log |sort -k2 -nr |head -10 统计时间段访问最多的IP\n1 awk \u0026#39;$4\u0026gt;=\u0026#34;[01/Dec/2018:13:20:25\u0026#34; \u0026amp;\u0026amp; $4\u0026lt;=\u0026#34;[27/Nov/2018:16:20:49\u0026#34;{a[$1]++}END{for(v in a)print v,a[v]}\u0026#39; $LOG_FILE |sort -k2 -nr|head -10 访问最多的10个页面\n1 awk \u0026#39;{a[$7]++}END{print \u0026#34;PV:\u0026#34;,length(a);for(v in a){if(a[v]\u0026gt;5)print v,a[v]}}\u0026#39; access.log |sort -k2 -nr 统计访问页面状态码数量\n1 awk \u0026#39;{a[$7\u0026#34; \u0026#34;$9]++}END{for(v in a){if(a[v]\u0026gt;5)print v,a[v]}}\u0026#39; $LOG_FILE |sort -k3 -nr 脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash # 日志格式: $remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34; LOG_FILE=$1 echo \u0026#34;统计访问最多的10个IP\u0026#34; awk \u0026#39;{a[$1]++}END{print \u0026#34;UV:\u0026#34;,length(a);for(v in a)print v,a[v]}\u0026#39; $LOG_FILE |sort -k2 -nr |head -10 echo \u0026#34;----------------------\u0026#34; echo \u0026#34;统计时间段访问最多的IP\u0026#34; awk \u0026#39;$4\u0026gt;=\u0026#34;[01/Dec/2018:13:20:25\u0026#34; \u0026amp;\u0026amp; $4\u0026lt;=\u0026#34;[27/Nov/2018:16:20:49\u0026#34;{a[$1]++}END{for(v in a)print v,a[v]}\u0026#39; $LOG_FILE |sort -k2 -nr|head -10 echo \u0026#34;----------------------\u0026#34; echo \u0026#34;统计访问最多的10个页面\u0026#34; awk \u0026#39;{a[$7]++}END{print \u0026#34;PV:\u0026#34;,length(a);for(v in a){if(a[v]\u0026gt;10)print v,a[v]}}\u0026#39; $LOG_FILE |sort -k2 -nr echo \u0026#34;----------------------\u0026#34; echo \u0026#34;统计访问页面状态码数量\u0026#34; awk \u0026#39;{a[$7\u0026#34; \u0026#34;$9]++}END{for(v in a){if(a[v]\u0026gt;5)print v,a[v]}}\u0026#39; $LOG_FILE |sort -k3 -nr 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T14:14:12Z","image":"https://www.ownit.top/title_pic/26.jpg","permalink":"https://www.ownit.top/p/202001211414/","title":"企业级-Shell案例13——Nginx访问日志分析"},{"content":"MySql数据库备份脚本 mysql备份数据库，使用企业级，可以防止数据库出错。\n分库备份\n1 mysqldump -uroot -pxxx -B A \u0026gt; A.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash DATE=$(date +%F_%H-%M-%S) HOST=localhost USER=backup PASS=123.com BACKUP_DIR=/data/db_backup DB_LIST=$(mysql -h$HOST -u$USER -p$PASS -s -e \u0026#34;show databases;\u0026#34; 2\u0026gt;/dev/null |egrep -v \u0026#34;Database|information_schema|mysql|performance_schema|sys\u0026#34;) for DB in $DB_LIST; do BACKUP_NAME=$BACKUP_DIR/${DB}_${DATE}.sql if ! mysqldump -h$HOST -u$USER -p$PASS -B $DB \u0026gt; $BACKUP_NAME 2\u0026gt;/dev/null; then echo \u0026#34;$BACKUP_NAME 备份失败!\u0026#34; fi done 分表备份\n1 mysqldump -uroot -pxxx -A t \u0026gt; t.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash DATE=$(date +%F_%H-%M-%S) HOST=localhost USER=backup PASS=123.com BACKUP_DIR=/data/db_backup DB_LIST=$(mysql -h$HOST -u$USER -p$PASS -s -e \u0026#34;show databases;\u0026#34; 2\u0026gt;/dev/null |egrep -v \u0026#34;Database|information_schema|mysql|performance_schema|sys\u0026#34;) for DB in $DB_LIST; do BACKUP_DB_DIR=$BACKUP_DIR/${DB}_${DATE} [ ! -d $BACKUP_DB_DIR ] \u0026amp;\u0026amp; mkdir -p $BACKUP_DB_DIR \u0026amp;\u0026gt;/dev/null TABLE_LIST=$(mysql -h$HOST -u$USER -p$PASS -s -e \u0026#34;use $DB;show tables;\u0026#34; 2\u0026gt;/dev/null) for TABLE in $TABLE_LIST; do BACKUP_NAME=$BACKUP_DB_DIR/${TABLE}.sql if ! mysqldump -h$HOST -u$USER -p$PASS $DB $TABLE \u0026gt; $BACKUP_NAME 2\u0026gt;/dev/null; then echo \u0026#34;$BACKUP_NAME 备份失败!\u0026#34; fi done done 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-21T13:42:41Z","image":"https://www.ownit.top/title_pic/73.jpg","permalink":"https://www.ownit.top/p/202001211342/","title":"企业级-Shell案例12——MySql数据库备份脚本"},{"content":"监控MySQL主从同步状态是否异常脚本 流程图\n主从同步\nmaster binlog\nsave\n写 --\u0026gt; master --\u0026gt; binlong --\u0026gt; relaylog --\u0026gt;slave\n脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash HOST=localhost USER=root PASSWD=123.com IO_SQL_STATUS=$(mysql -h$HOST -u$USER -p$PASSWD -e \u0026#39;show slave status\\G\u0026#39; 2\u0026gt;/dev/null |awk \u0026#39;/Slave_.*_Running:/{print $1$2}\u0026#39;) for i in $IO_SQL_STATUS; do THREAD_STATUS_NAME=${i%:*} THREAD_STATUS=${i#*:} if [ \u0026#34;$THREAD_STATUS\u0026#34; != \u0026#34;Yes\u0026#34; ]; then echo \u0026#34;Error: MySQL Master-Slave $THREAD_STATUS_NAME status is $THREAD_STATUS!\u0026#34; |mail -s \u0026#34;Master-Slave Staus\u0026#34; xxx@163.com fi done 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-19T14:53:24Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202001191453/","title":"企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本"},{"content":"一键部署LNMP网站平台脚本 网站浏览流程图\nL ：Linux\nN ： Nginx\nM ：Mysql\nP ：PHP\nuser --\u0026gt; Nginx --\u0026gt; PHP --\u0026gt; Mysql\nCentos软件安装\n1、yum安装\n2、源码编译\n1）./configure 2）make 3）make install 3、二进制安装\n脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 #!/bin/bash NGINX_V=1.15.6 PHP_V=5.6.36 TMP_DIR=/tmp INSTALL_DIR=/usr/local PWD_C=$PWD echo echo -e \u0026#34;\\tMenu\\n\u0026#34; echo -e \u0026#34;1. Install Nginx\u0026#34; echo -e \u0026#34;2. Install PHP\u0026#34; echo -e \u0026#34;3. Install MySQL\u0026#34; echo -e \u0026#34;4. Deploy LNMP\u0026#34; echo -e \u0026#34;9. Quit\u0026#34; function command_status_check() { if [ $? -ne 0 ]; then echo $1 exit fi } function install_nginx() { cd $TMP_DIR yum install -y gcc gcc-c++ make openssl-devel pcre-devel wget wget http://nginx.org/download/nginx-${NGINX_V}.tar.gz tar zxf nginx-${NGINX_V}.tar.gz cd nginx-${NGINX_V} ./configure --prefix=$INSTALL_DIR/nginx \\ --with-http_ssl_module \\ --with-http_stub_status_module \\ --with-stream command_status_check \u0026#34;Nginx - 平台环境检查失败！\u0026#34; make -j 4 command_status_check \u0026#34;Nginx - 编译失败！\u0026#34; make install command_status_check \u0026#34;Nginx - 安装失败！\u0026#34; mkdir -p $INSTALL_DIR/nginx/conf/vhost alias cp=cp ; cp -rf $PWD_C/nginx.conf $INSTALL_DIR/nginx/conf rm -rf $INSTALL_DIR/nginx/html/* echo \u0026#34;ok\u0026#34; \u0026gt; $INSTALL_DIR/nginx/html/status.html echo \u0026#39;\u0026lt;?php echo \u0026#34;ok\u0026#34;?\u0026gt;\u0026#39; \u0026gt; $INSTALL_DIR/nginx/html/status.php $INSTALL_DIR/nginx/sbin/nginx command_status_check \u0026#34;Nginx - 启动失败！\u0026#34; } function install_php() { cd $TMP_DIR yum install -y gcc gcc-c++ make gd-devel libxml2-devel \\ libcurl-devel libjpeg-devel libpng-devel openssl-devel \\ libmcrypt-devel libxslt-devel libtidy-devel wget http://docs.php.net/distributions/php-${PHP_V}.tar.gz tar zxf php-${PHP_V}.tar.gz cd php-${PHP_V} ./configure --prefix=$INSTALL_DIR/php \\ --with-config-file-path=$INSTALL_DIR/php/etc \\ --enable-fpm --enable-opcache \\ --with-mysql --with-mysqli --with-pdo-mysql \\ --with-openssl --with-zlib --with-curl --with-gd \\ --with-jpeg-dir --with-png-dir --with-freetype-dir \\ --enable-mbstring --enable-hash command_status_check \u0026#34;PHP - 平台环境检查失败！\u0026#34; make -j 4 command_status_check \u0026#34;PHP - 编译失败！\u0026#34; make install command_status_check \u0026#34;PHP - 安装失败！\u0026#34; cp php.ini-production $INSTALL_DIR/php/etc/php.ini cp sapi/fpm/php-fpm.conf $INSTALL_DIR/php/etc/php-fpm.conf cp sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm chmod +x /etc/init.d/php-fpm /etc/init.d/php-fpm start command_status_check \u0026#34;PHP - 启动失败！\u0026#34; } read -p \u0026#34;请输入编号：\u0026#34; number case $number in 1) install_nginx;; 2) install_php;; 3) install_mysql;; 4) install_nginx install_php ;; 9) exit;; esac 测试截图\n相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-19T14:32:45Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/202001191432/","title":"企业级-Shell案例10——一键部署LNMP网站平台脚本"},{"content":"批量主机远程执行命令脚本 多台主机同时执行命令\nexpect 脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash COMMAND=$* HOST_INFO=host.info for IP in $(awk \u0026#39;/^[^#]/{print $1}\u0026#39; $HOST_INFO); do USER=$(awk -v ip=$IP \u0026#39;ip==$1{print $2}\u0026#39; $HOST_INFO) PORT=$(awk -v ip=$IP \u0026#39;ip==$1{print $3}\u0026#39; $HOST_INFO) PASS=$(awk -v ip=$IP \u0026#39;ip==$1{print $4}\u0026#39; $HOST_INFO) expect -c \u0026#34; spawn ssh -p $PORT $USER@$IP expect { \\\u0026#34;(yes/no)\\\u0026#34; {send \\\u0026#34;yes\\r\\\u0026#34;; exp_continue} \\\u0026#34;password:\\\u0026#34; {send \\\u0026#34;$PASS\\r\\\u0026#34;; exp_continue} \\\u0026#34;$USER@*\\\u0026#34; {send \\\u0026#34;$COMMAND\\r exit\\r\\\u0026#34;; exp_continue} } \u0026#34; echo \u0026#34;-------------------\u0026#34; done 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-19T13:54:33Z","image":"https://www.ownit.top/title_pic/23.jpg","permalink":"https://www.ownit.top/p/202001191354/","title":"企业级-Shell案例9——批量主机远程执行命令脚本"},{"content":"批量检测网站是否异常脚本 检测网站运行是否正常，如果不能正常访问，发送邮件通知管理员\n1 curl -o /de/dev/null -s -w \u0026#34;%{http_code}\u0026#34; www.baidu.com 访问失败，也又可能和网络等等原因有关。\n所以我们要进行次数判断，超出一定的次数。那就发送邮件。\n脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash URL_LIST=\u0026#34;www.baidu.com www.ctnrs.com www.der-matech.net.cn www.der-matech.com.cn www.der-matech.cn www.der-matech.top www.der-matech.org\u0026#34; for URL in $URL_LIST; do FAIL_COUNT=0 for ((i=1;i\u0026lt;=3;i++)); do HTTP_CODE=$(curl -o /dev/null --connect-timeout 3 -s -w \u0026#34;%{http_code}\u0026#34; $URL) if [ $HTTP_CODE -eq 200 ]; then echo \u0026#34;$URL OK\u0026#34; break else echo \u0026#34;$URL retry $FAIL_COUNT\u0026#34; let FAIL_COUNT++ fi done if [ $FAIL_COUNT -eq 3 ]; then echo \u0026#34;Warning: $URL Access failure!\u0026#34; echo \u0026#34;网站$URL坏掉，请及时处理\u0026#34; | mail -s \u0026#34;$URL网站高危\u0026#34; 1794748404@qq.com fi done 测试效果图 在设置个定时任务，10分钟一次。\n然后就可以了。\n相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T18:14:53Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/202001181814/","title":"企业级-Shell案例8——批量检测网站是否异常并邮件通知"},{"content":"监控多台服务器磁盘利用率脚本 SSH 1 ssh root@192.168.1.99 \u0026#34;df -h\u0026#34; 但每次要使用密码，不推荐使用。\n可以使用秘钥登录。\n创建秘钥【一直回车就行】 1 ssh-keygen 把公钥复制到需要被控的服务器 1 ssh-copy-id root@192.168.1.99 在被传公钥的服务器的root的.ssh下\n1 ls .ssh/ 私钥登录公钥服务器 1 ssh -i .ssh/id_rsa root@192.168.1.99 脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/bin/bash HOST_INFO=host.info for IP in $(awk \u0026#39;/^[^#]/{print $1}\u0026#39; $HOST_INFO); do #取出用户名和端口 USER=$(awk -v ip=$IP \u0026#39;ip==$1{print $2}\u0026#39; $HOST_INFO) PORT=$(awk -v ip=$IP \u0026#39;ip==$1{print $3}\u0026#39; $HOST_INFO) #创建临时文件，保存信息 TMP_FILE=/tmp/disk.tmp #通过公钥登录获取主机磁盘信息 ssh -p $PORT $USER@$IP \u0026#39;df -h\u0026#39; \u0026gt; $TMP_FILE #分析磁盘占用空间 USE_RATE_LIST=$(awk \u0026#39;BEGIN{OFS=\u0026#34;=\u0026#34;}/^\\/dev/{print $NF,int($5)}\u0026#39; $TMP_FILE) #循环磁盘列表，进行判断 for USE_RATE in $USE_RATE_LIST; do #取出等号（=）右边的值 挂载点名称 PART_NAME=${USE_RATE%=*} #取出等号（=）左边的值 磁盘利用率 USE_RATE=${USE_RATE#*=} #进行判断 if [ $USE_RATE -ge 80 ]; then echo \u0026#34;Warning: $PART_NAME Partition usage $USE_RATE%!\u0026#34; echo \u0026#34;服务器$IP的磁盘空间占用过高，请及时处理\u0026#34; | mail -s \u0026#34;空间不足警告\u0026#34; 你的qq@qq.com else echo \u0026#34;服务器$IP的$PART_NAME目录空间良好\u0026#34; fi done done 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T17:16:17Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/202001181716/","title":"企业级-Shell案例7——监控多台服务器磁盘利用率脚本"},{"content":"查看网卡的实时流量 监控流量\n脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash eth0=$1 echo -e \u0026#34;流量进入--流量传出 \u0026#34; while true; do old_in=$(cat /proc/net/dev |grep $eth0 |awk \u0026#39;{print $2}\u0026#39;) old_out=$(cat /proc/net/dev |grep $eth0 |awk \u0026#39;{print $10}\u0026#39;) sleep 1 new_in=$(cat /proc/net/dev |grep $eth0 |awk \u0026#39;{print $2}\u0026#39;) new_out=$(cat /proc/net/dev |grep $eth0 |awk \u0026#39;{print $10}\u0026#39;) in=$(printf \u0026#34;%.1f%s\u0026#34; \u0026#34;$((($new_in-$old_in)/1024))\u0026#34; \u0026#34;KB/s\u0026#34;) out=$(printf \u0026#34;%.1f%s\u0026#34; \u0026#34;$((($new_out-$old_out)/1024))\u0026#34; \u0026#34;KB/s\u0026#34;) echo \u0026#34;$in $out\u0026#34; done 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T15:50:52Z","image":"https://www.ownit.top/title_pic/69.jpg","permalink":"https://www.ownit.top/p/202001181550/","title":"企业级-Shell案例6——查看网卡的实时流量"},{"content":"找出占用CPU 内存过高的进程脚本 背景：服务器CPU占用高，找出最高的分析，看是否进程正确，是否是垃圾进程\n分析占用CPU最高的应用 1 ps -eo user,pid,pcpu,pmem,args --sort=-pcpu |head -n 10 分析占用内存最高的应用 1 ps -eo user,pid,pcpu,pmem,args --sort=-pmem |head -n 10 整合脚本 1 2 3 4 5 #!/bin/bash echo \u0026#34;-------------------CUP占用前10排序--------------------------------\u0026#34; ps -eo user,pid,pcpu,pmem,args --sort=-pcpu |head -n 10 echo \u0026#34;-------------------内存占用前10排序--------------------------------\u0026#34; ps -eo user,pid,pcpu,pmem,args --sort=-pmem |head -n 10 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T14:40:38Z","image":"https://www.ownit.top/title_pic/21.jpg","permalink":"https://www.ownit.top/p/202001181440/","title":"企业级-Shell案例5——找出占用CPU 内存过高的进程"},{"content":"一键查看服务器利用率 背景：web访问过慢，服务器内存搞 cpu 60% 内存 利用率 硬盘 利用率 TCP连接状态 脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #!/bin/bash function cpu(){ util=$(vmstat | awk \u0026#39;{if(NR==3)print $13+$14}\u0026#39;) iowait=$(vmstat | awk \u0026#39;{if(NR==3)print $16}\u0026#39;) echo \u0026#34;CPU -使用率：${util}% ,等待磁盘IO相应使用率：${iowait}:${iowait}%\u0026#34; } function memory (){ total=`free -m |awk \u0026#39;{if(NR==2)printf \u0026#34;%.1f\u0026#34;,$2/1024}\u0026#39;` used=`free -m |awk \u0026#39;{if(NR==2) printf \u0026#34;%.1f\u0026#34;,($2-$NF)/1024}\u0026#39;` available=`free -m |awk \u0026#39;{if(NR==2) printf \u0026#34;%.1f\u0026#34;,$NF/1024}\u0026#39;` echo \u0026#34;内存 - 总大小: ${total}G , 使用: ${used}G , 剩余: ${available}G\u0026#34; } disk(){ fs=$(df -h |awk \u0026#39;/^\\/dev/{print $1}\u0026#39;) for p in $fs; do mounted=$(df -h |awk \u0026#39;$1==\u0026#34;\u0026#39;$p\u0026#39;\u0026#34;{print $NF}\u0026#39;) size=$(df -h |awk \u0026#39;$1==\u0026#34;\u0026#39;$p\u0026#39;\u0026#34;{print $2}\u0026#39;) used=$(df -h |awk \u0026#39;$1==\u0026#34;\u0026#39;$p\u0026#39;\u0026#34;{print $3}\u0026#39;) used_percent=$(df -h |awk \u0026#39;$1==\u0026#34;\u0026#39;$p\u0026#39;\u0026#34;{print $5}\u0026#39;) echo \u0026#34;硬盘 - 挂载点: $mounted , 总大小: $size , 使用: $used , 使用率: $used_percent\u0026#34; done } function tcp_status() { summary=$(ss -antp |awk \u0026#39;{status[$1]++}END{for(i in status) printf i\u0026#34;:\u0026#34;status[i]\u0026#34; \u0026#34;}\u0026#39;) echo \u0026#34;TCP连接状态 - $summary\u0026#34; } cpu memory disk tcp_status 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T14:21:08Z","image":"https://www.ownit.top/title_pic/37.jpg","permalink":"https://www.ownit.top/p/202001181421/","title":"企业级-Shell案例4——一键查看服务器利用率"},{"content":"批量创建多少个用户并设置密码 背景：多名新人入职 单个用户创建 添加\n1 useradd zhang 改密码\n1 passwd zhang 脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash USER_LIST=$@ USER_FILE=./user.info for USER in $USER_LIST;do if ! id $USER \u0026amp;\u0026gt;/dev/null; then PASS=$(echo $RANDOM |md5sum |cut -c 1-8) useradd $USER echo $PASS | passwd --stdin $USER \u0026amp;\u0026gt;/dev/null echo \u0026#34;$USER $PASS\u0026#34; \u0026gt;\u0026gt; $USER_FILE echo \u0026#34;$USER User create successful.\u0026#34; else echo \u0026#34;$USER User already exists!\u0026#34; fi done 1 ./user.sh li zhang wei wu yi 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T11:45:08Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/202001181145/","title":"企业级-Shell案例3——批量创建多个用户并设置密码"},{"content":"发送告警邮件 安装软件 1 yum install mailx -y 配置文件 进入qq邮箱首页，点击设置\u0026gt;账户，然后找到下图截取的地方（需要设置的，如图）\n设置完之后呢，就要把生成的授权码作为邮箱的password的啦~\n配置/etc/mail.rc文件【下面的配置qq是假的，别用】\n1 2 3 4 5 6 7 8 9 10 #设置发件人名称 set from=1832025651@qq.com #设置邮件服务器 set smtp=smtp.qq.com #填写自己邮箱地址 set smtp-auth-user=1832025651@qq.com #输入邮箱验证码 set smtp-auth-password=pfljngafoqaxecff #smtp的认证方式，默认是login set smtp-auth=login 测试【已经完成】 1 echo \u0026#34;admin ,文件内容\u0026#34; | mail -s \u0026#34;标题\u0026#34; 你的qq@qq.com 后续会用到这个。 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T11:15:22Z","image":"https://www.ownit.top/title_pic/73.jpg","permalink":"https://www.ownit.top/p/202001181115/","title":"企业级-Shell案例2——发送告警邮件"},{"content":"服务器系统配置初始化 背景：新购买10台服务器并已安装Linux操作系统 需求： 安装系统新能分析工具已经其他的工具 设置时区并同步时间 禁用selinux 清空防火墙默认策源 历史命令显示操作时间 禁止root远程登录 禁止定时任务发送邮件 设置最大打开文件数 减少Swap使用 系统内核参数的优化 脚本编写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #/bin/bash # 安装系统性能分析工具及其他 yum install gcc make autoconf vim sysstat net-tools iostat iftop iotp wget lrzsz lsof unzip openssh-clients net-tool vim ntpdate -y # 设置时区并同步时间 ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime if ! crontab -l |grep ntpdate \u0026amp;\u0026gt;/dev/null ; then (echo \u0026#34;* 1 * * * ntpdate time.windows.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\u0026#34;;crontab -l) |crontab fi # 禁用selinux sed -i \u0026#39;/SELINUX/{s/permissive/disabled/}\u0026#39; /etc/selinux/config # 关闭防火墙 if egrep \u0026#34;7.[0-9]\u0026#34; /etc/redhat-release \u0026amp;\u0026gt;/dev/null; then systemctl stop firewalld systemctl disable firewalld elif egrep \u0026#34;6.[0-9]\u0026#34; /etc/redhat-release \u0026amp;\u0026gt;/dev/null; then service iptables stop chkconfig iptables off fi # 历史命令显示操作时间 if ! grep HISTTIMEFORMAT /etc/bashrc; then echo \u0026#39;export HISTTIMEFORMAT=\u0026#34;%Y-%m-%d %H:%M:%S `whoami` \u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/bashrc fi # SSH超时时间 if ! grep \u0026#34;TMOUT=600\u0026#34; /etc/profile \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;export TMOUT=600\u0026#34; \u0026gt;\u0026gt; /etc/profile fi # 禁止root远程登录 切记给系统添加普通用户，给su到root的权限 sed -i \u0026#39;s/#PermitRootLogin yes/PermitRootLogin no/\u0026#39; /etc/ssh/sshd_config # 禁止定时任务向发送邮件 sed -i \u0026#39;s/^MAILTO=root/MAILTO=\u0026#34;\u0026#34;/\u0026#39; /etc/crontab # 设置最大打开文件数 if ! grep \u0026#34;* soft nofile 65535\u0026#34; /etc/security/limits.conf \u0026amp;\u0026gt;/dev/null; then cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; EOF * soft nofile 65535 * hard nofile 65535 EOF fi # 系统内核优化 cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; EOF net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_tw_buckets = 20480 net.ipv4.tcp_max_syn_backlog = 20480 net.core.netdev_max_backlog = 262144 net.ipv4.tcp_fin_timeout = 20 EOF # 减少SWAP使用 echo \u0026#34;0\u0026#34; \u0026gt; /proc/sys/vm/swappiness 相关博文： 企业级-Shell案例1——服务器系统配置初始化 企业级-Shell案例2——发送告警邮件 企业级-Shell案例3——批量创建多个用户并设置密码 企业级-Shell案例4——一键查看服务器利用率 企业级-Shell案例5——找出占用CPU 内存过高的进程 企业级-Shell案例6——查看网卡的实时流量 企业级-Shell案例7——监控多台服务器磁盘利用率脚本 企业级-Shell案例8——批量检测网站是否异常并邮件通知 企业级-Shell案例9——批量主机远程执行命令脚本 企业级-Shell案例10——一键部署LNMP网站平台脚本 企业级-Shell案例11——监控MySQL主从同步状态是否异常脚本 企业级-Shell案例12——MySql数据库备份脚本 企业级-Shell案例13——Nginx访问日志分析 企业级-Shell案例14——Nginx访问日志自动按天（周、月）切割 企业级-Shell案例15——自动发布Java项目（Tomcat） 企业级-Shell案例16——自动发布PHP项目 企业级-Shell案例17——DOS攻击防范（自动屏蔽攻击IP） 企业级-Shell案例18——目录入侵检测与告警","date":"2020-01-18T10:48:05Z","image":"https://www.ownit.top/title_pic/25.jpg","permalink":"https://www.ownit.top/p/202001181048/","title":"企业级-Shell案例1——服务器系统配置初始化"},{"content":" swarm介绍 Swarm这个项目名称特别贴切。在Wiki的解释中，Swarm behavior是指动物的群集行 为。比如我们常见的蜂群，鱼群，秋天往南飞的雁群都可以称作Swarm behavior。 Swarm项目正是这样，通过把多个Docker Engine聚集在一起，形成一个大的dockerengine，对外提供容器的集群服务。同时这个集群对外提供Swarm API（命令，docker engine的命令），用户可以像使用Docker Engine一样使用Docker集群。\nSwarm是Docker公司在2014年12月初发布的容器管理工具，和Swarm一起发布的 Docker管理工具还有Machine以及Compose。Swarm是一套较为简单的工具，用以管理 Docker集群，使得Docker集群暴露给用户时相当于一个虚拟的整体。Swarm将一群 Docker宿主机变成一个单一的，虚拟的主机。Swarm使用标准的Docker API接口作为其 前端访问入口，换言之，各种形式的Docker Client(docker client in Go, docker_py, docker等)均可以直接与Swarm通信。Swarm几乎全部用Go语言来完成开发，Swarm0.2 版本增加了一个新的策略来调度集群中的容器，使得在可用的节点上传播它们，以及支\n持更多的Docker命令以及集群驱动。Swarm deamon只是一个调度器（Scheduler）加 路由器(router)，Swarm自己不运行容器，它只是接受docker客户端发送过来的请求， 调度适合的节点来运行容器，这意味着，即使Swarm由于某些原因挂掉了，集群中的节 点也会照常运行，当Swarm重新恢复运行之后，它会收集重建集群信息。 docker swarm特点： 1) 对外以Docker API接口呈现，这样带来的好处是，如果现有系统使用Docker Engine， 则可以平滑将Docker Engine切到Swarm上，无需改动现有系统。 2) Swarm对用户来说，之前使用Docker的经验可以继承过来。非常容易上手，学习成本 和二次开发成本都比较低。同时Swarm本身专注于Docker集群管理，非常轻量，占用资 源也非常少。简单说，就是插件化机制，Swarm中的各个模块都抽象出了API，可以根据 自己一些特点进行定制实现。 3) Swarm自身对Docker命令参数支持的比较完善，Swarm目前与Docker是同步发布 的。Docker的新功能，都会第一时间在Swarm中体现。 **docker swarm架构 ** Swarm作为一个管理Docker集群的工具，首先需要将其部署起来，可以单独将Swarm部 署于一个节点。另外，自然需要一个Docker集群，集群上每一个节点均安装有Docker。\n相关术语： Swarm Manager：集群的管理工具，通过swarm manager管理多个节点。 Node：是已加入到swarm的Docker引擎的实例 。 manager nodes：也就是管理节点 ，执行集群的管理功能，维护集群的状态， 选举一个leader节点去执 行调度任务 worker nodes，也就是工作节点 ，接收和执行任务。参与容器集群负载调度， 仅用于承载task 一个服务是工作节点上执行任务的定义。创建一个服务，指定了容器所使用的镜像和 容器运行的命令。service是运行在worker nodes上的task的描述，service的描述包 括使用哪个docker 镜像，以及在使用该镜像的容器中执行什么命令。\ntask任务：一个任务包含了一个容器及其运行的命令。task是service的执行实体， task启动docker容器并在容器中执行任务\ndocker swarm使用 搭建步骤：\n1、环境准备：\n1.1、准备三台已近安装docker engine的centos/Ubuntu系统主机（docker版本必须在 1.12以上的版本，老版本不支持swarm）\n1.2、docker容器主机的ip地址固定，集群中所有工作节点必须能访问该管理节点\n1.3、集群管理节点必须使用相应的协议并且保证端口可用 集群管理通信：\nTCP，端口2377 节点通信：TCP和UDP，端口7946 覆盖型网络(docker网络)：UDP，端口4789 overlay驱动 说明：三台容器主机的ip地址分别为： 192.168.200.162（管理节点） 192.168.200.163（工作节点） 192.168.200.158（工作节点）\n主机名称分别为：manager1、work1以及work2\n1 vim /etc/hostname (修改完成后需要重启) 2、创建docker swarm 2.1、在manager1机器上创建docker swarm集群 1 docker swarm init ‐‐advertise‐addr 192.168.200.162 （‐‐advertise‐addr将该IP地址的机器设置为集群管理节点；如果是单节点，无需该参 数 2.2、查看管理节点集群信息： 1 docker node ls 3、向docker swarm中添加工作节点：在两个工作节点中分别执行如下命令，ip地址是 manager节点的 3.1、添加两个work节点 1 2 docker swarm join ‐‐token xxx 192.168.200.138:2377 （worker1） docker swarm join ‐‐token xxx 192.168.200.138:2377 （worker2） （‐‐token xxx:向指定集群中加入工作节点的认证信息，xxx认证信息是在创建docker swarm时产生的） 3.2、继续查看管理节点集群信息与之前的区别 1 docker node ls 4、在docker swarm中部署服务 在Docker Swarm集群中部署服务时，既可以使用Docker Hub上自带的镜像来启动服务，也\n5 docker compose编排工具 5.1 docker compose介绍 可以使用自己通Dockerfile构建的镜像来启动服务。如果使用自己通过Dockerfile构建的 镜像来启动服务那么必须先将镜像推送到Docker Hub中心仓库。为了方便读者的学习，这里 以使用Docker Hub上自带的alpine镜像为例来部署集群服务 4.1、部署服务 1 docker service create ‐‐replicas 1 ‐‐name helloworld alpine ping docker.com docker service create指令：用于在Swarm集群中创建一个基于alpine镜像的服务 ‐‐replicas参数：指定了该服务只有一个副本实例 ‐‐name参数：指定创建成功后的服务名称为helloworld ping docker.com指令：表示服务启动后执行的命令 5.查看docker swarm集群中的服务 1 2 3 查看服务列表：docker service ls 查看部署具体服务的详细信息：docker service inspect 服务名称 查看服务在集群节点上的分配以及运行情况：docker service ps 服务名称 6、修改副本数量 在manager1上，更改服务副本的数量（创建的副本会随机分配到不同的节点） 1 docker service scale helloworld=5 7、删除服务（在管理节点） 1 docker service rm 服务名称 8、访问服务 8.1、查看集群环境下的网络列表：\n1 docker network ls 8.2、在manager1上创建一overlay为驱动的网络（默认使用的网络连接ingress） 1 docker network create ‐d=overlay my‐multi‐host‐network 8.3、在集群管理节点manager1上部署一个nginx服务 1 2 3 4 5 6 docker service create \\ ‐‐network my‐multi‐host‐network \\ ‐‐name my‐web \\ ‐p 8080:80 \\ ‐‐replicas 2 \\ nginx 8.3、在管理节点查看服务的运行情况： 1 docker service ps my‐web ","date":"2020-01-17T15:20:40Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/202001171520/","title":"Centos搭建docker swarm集群详细教程"},{"content":"1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能 目录\n1、Centos7安装Promethus(普罗米修斯）监控系统完整版\n2、Promethus(普罗米修斯）监控Mysql数据库\n3、Promethus(普罗米修斯）安装Grafana可视化图形工具\n4、Promethus的Grafana图形显示MySQL监控数据\nGrafana+onealert报警\n1、 先在onealert里添加grafana应用(申请onealert账号)\n2、在Grafana中配置Webhook URL\n现在可以去设置一个报警来测试了(这里以我们前面加的cpu负载监控来 做测试)\n最终的邮件报警效果：\n测试mysql链接数报警\n总结报警不成功的可能原因\n扩展\nGrafana+onealert报警 prometheus报警需要使用alertmanager这个组件，而且报警规则需要手 动编写(对运维来说不友好)。所以我这里选用grafana+onealert报警。\n注意: 实现报警前把所有机器时间同步再检查一遍.\n1 ntpdate time.windows.com 1、 先在onealert里添加grafana应用(申请onealert账号) https://caweb.aiops.com/\n2、在Grafana中配置Webhook URL 1、在Grafana中创建Notification channel，选择类型为Webhook；\n2、推荐选中Send on all alerts和Include image，Cloud Alert体验更佳；\n3、将第一步中生成的Webhook URL填入Webhook settings Url；\n4、Http Method选择POST；\n5、Send Test\u0026amp;Save；\n现在可以去设置一个报警来测试了(这里以我们前面加的cpu负载监控来 做测试) 配置\n保存后就可以测试了\n如果node1上的cpu负载还没有到0.5，你可以试试0.1,或者运行一些程序 把node1负载调大。最终能测试报警成功\n模拟cpu负载\n1 cat /dev/urandom | md5sum 最终的邮件报警效果： 测试mysql链接数报警 总结报警不成功的可能原因 各服务器之间时间不同步，这样时序数据会出问题，也会造成报警出问 题 必须写通知内容，留空内容是不会发报警的 修改完报警配置后，记得要点右上角的保存 保存配置后，需要由OK状态变为alerting状态才会报警(也就是说，你 配置保存后，就已经是alerting状态是不会报警的) grafana与onealert通信有问题 扩展 prometheus目前还在发展中，很多相应的监控都需要开发。但在官网的 dashboard库中,也有一些官方和社区开发人员开发的dashboard可以直接 拿来用。\n相关博文 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能","date":"2020-01-13T16:52:18Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/202001131652/","title":"Promethus(普罗米修斯）的Grafana+onealert实现报警功能"},{"content":"相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能 目录\nGrafana图形显示MySQL监控数据\n① 在grafana上修改配置文件,并下载安装mysql监控的dashboard（包含 相关json文件，这些json文件可以看作是开发人员开发的一个监控模板)\n② 在grafana图形界面导入相关json文件\nGrafana图形显示MySQL监控数据 ① 在grafana上修改配置文件,并下载安装mysql监控的dashboard（包含 相关json文件，这些json文件可以看作是开发人员开发的一个监控模板) 参考网址: https://github.com/percona/grafana-dashboards\n在grafana配置文件里最后加上以下三行\n1 2 3 4 vim /etc/grafana/grafana.ini [dashboards.json] enabled = true path = /var/lib/grafana/dashboards 下载配置模板\n1 cd /var/lib/grafana/ 1 git clone https://github.com/percona/grafana-dashboards.git 1 cp -r /var/lib/grafana/grafana-dashboards-master/dashboards/ /var/lib/grafana/ 重启grafana服务\n1 systemctl restart grafana-server ② 在grafana图形界面导入相关json文件 ③ 点import导入后,报prometheus数据源找不到,因为这些json文件里默认 要找的就是叫Prometheus的数据源，但我们前面建立的数据源却是叫 prometheus_data(坑啊)\n那么请自行把原来的prometheus_data源改名为Prometheus即可(注意: 第一个字母P是大写)\n然后再回去刷新,就有数据了(如下图所示)\n相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能","date":"2020-01-13T15:42:22Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/202001131542/","title":"Promethus的Grafana图形显示MySQL监控数据"},{"content":"相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能 教程使用的软件：链接: https://pan.baidu.com/s/1QV4KYZksyIp65UsScioq4Q 提取码: vcej\n环境配置\n服务器IP地址Prometneus服务器192.168.116.129被监控服务器（mysql）192.168.116.130grafana服务器192.168.116.131 1、什么是Grafana Grafana是一个开源的度量分析和可视化工具，可以通过将采集的数据分 析，查询，然后进行可视化的展示,并能实现报警。\n网址: https://grafana.com/\n2、使用Grafana连接Prometheus ① 在grafana服务器上安装grafana 下载地址:https://grafana.com/grafana/download\n上传grafana-5.3.4-1.x86_64.rpm\n我这里选择的rpm包，下载后直接rpm -ivh安装就OK【失败原因缺少组件，可以yum安装组件】\n1 rpm -ivh /root/Desktop/grafana-5.3.41.x86_64.rpm 或者第二种方法【yum安装会自动安装缺少的组件的】\n1 yum install -y grafana-5.3.4-1.x86_64.rpm 启动服务\n1 2 systemctl start grafana-server systemctl enable grafana-server 确认端口(3000)\n1 lsof -i:3000 ② 通过浏览器访问 http:// grafana服务器IP:3000就到了登录界面,使用默 认的admin用户,admin密码就可以登陆了 ③ 下面我们把prometheus服务器收集的数据做为一个数据源添加到 grafana,让grafana可以得到prometheus的数据。 ④ 然后为添加好的数据源做图形显示 ⑤ 保存 ⑥ 最后在dashboard可以查看到 ⑦ 匹配条件显示\n相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能","date":"2020-01-13T14:27:19Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/202001131427/","title":"Promethus(普罗米修斯）安装Grafana可视化图形工具"},{"content":"相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能 Promethus(普罗米修斯）监控Mysql数据库\n这个是基于上面环境搭建的，需要的可以访问方面连接查看。\n教程使用的软件：链接: https://pan.baidu.com/s/1QV4KYZksyIp65UsScioq4Q 提取码: vcej\n监控远程MySQL\n服务器IP地址Prometneus服务器192.168.116.129被监控服务器（mysql）192.168.116.130grafana服务器192.168.116.131 ① 在被管理机agent1上安装mysqld_exporter组件\n下载地址: https://prometheus.io/download/\n上传mysqld_exporter组件\n安装mysqld_exporter组件\n1 2 3 tar xf mysqld_exporter-0.11.0.linux-amd64.tar.gz -C /usr/local/ mv /usr/local/mysqld_exporter-0.11.0.linux-amd64/ /usr/local/mysqld_exporter ls /usr/local/mysqld_exporter 安装mariadb数据库,并授权\n1 2 3 4 yum install mariadb\\* -y systemctl restart mariadb systemctl enable mariadb mysql 1 MariaDB [(none)]\u0026gt; grant select,replication client,process ON *.* to \u0026#39;mysql_monitor\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;123\u0026#39;; (注意:授权ip为localhost，因为不是prometheus服务器来直接找mariadb 获取数据，而是prometheus服务器找mysql_exporter,mysql_exporter 再找mariadb。所以这个localhost是指的mysql_exporter的IP)\n1 2 MariaDB [(none)]\u0026gt; flush privileges; MariaDB [(none)]\u0026gt; quit 创建一个mariadb配置文件，写上连接的用户名与密码(和上面的授权的用户名 和密码要对应)\n1 vim /usr/local/mysqld_exporter/.my.cnf 1 2 3 [client] user=mysql_monitor password=123 启动mysqld_exporter\n1 nohup /usr/local/mysqld_exporter/mysqld_exporter --config.my-cnf=/usr/local/mysqld_exporter/.my.cnf \u0026amp; 确认端口(9104)\n② 回到prometheus服务器的配置文件里添加被监控的mariadb的配置段\n在主配置文件最后再加上下面三行\n1 vim /usr/local/prometheus/prometheus.yml 1 2 3 - job_name: \u0026#39;mariadb\u0026#39; static_configs: - targets: [\u0026#39;192.168.116.130:9104\u0026#39;] 1 2 3 4 - job_name: \u0026#39;agent1_mariadb\u0026#39; # 取一个job 名称来代表被监控的mariadb static_configs: - targets: [\u0026#39;10.1.1.14:9104\u0026#39;] # 这里改成 被监控机器的IP，后面端口接9104 改完配置文件后,重启服务\n1 pkill prometheus 1 /usr/local/prometheus/prometheus --config.file=\u0026#34;/usr/local/prometheus/prometheus.yml\u0026#34; \u0026amp; ③ 回到web管理界面 --》点Status --》点Targets --》可以看到监控 mariadb了\n相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能","date":"2020-01-13T13:59:14Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/202001131359/","title":"Promethus(普罗米修斯）监控Mysql数据库"},{"content":"相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能 目录\n一、普罗米修斯概述\n二、时间序列数据\n1、什么是序列数据\n2、时间序列数据特点\n3、Prometheus的主要特征\n4、普罗米修斯原理架构图\n三、实验环境准备\n1、安装prometheus\n2、prometheus界面\n3、主机数据展示\n4、监控远程Linux主机\n一、普罗米修斯概述 Prometheus(由go语言(golang)开发)是一套开源的监控\u0026amp;报警\u0026amp;时间序列数 据库的组合。适合监控docker容器。因为kubernetes(俗称k8s)的流行带动 了prometheus的发展。\nhttps://prometheus.io/docs/introduction/overview/\n二、时间序列数据 1、什么是序列数据 时间序列数据(TimeSeries Data) : 按照时间顺序记录系统、设备状态变化 的数据被称为时序数据。\n应用的场景很多, 如：\n无人驾驶车辆运行中要记录的经度，纬度，速度，方向，旁边物体的距 离等等。每时每刻都要将数据记录下来做分析。 某一个地区的各车辆的行驶轨迹数据 传统证券行业实时交易数据 实时运维监控数据等 2、时间序列数据特点 性能好 关系型数据库对于大规模数据的处理性能糟糕。NOSQL可以比较好的处理 大规模数据，让依然比不上时间序列数据库。\n存储成本低 高效的压缩算法，节省存储空间，有效降低IO\nPrometheus有着非常高效的时间序列数据存储方法，每个采样数据仅仅占 用3.5byte左右空间，上百万条时间序列，30秒间隔，保留60天，大概花了 200多G（来自官方数据)\n3、Prometheus的主要特征 多维度数据模型 灵活的查询语言 不依赖分布式存储，单个服务器节点是自主的 以HTTP方式，通过pull模型拉去时间序列数据 也可以通过中间网关支持push模型 通过服务发现或者静态配置，来发现目标服务对象 支持多种多样的图表和界面展示\n4、普罗米修斯原理架构图 三、实验环境准备 服务器IP地址Prometneus服务器192.168.116.129被监控服务器192.168.116.130grafana服务器192.168.116.131 教程使用的软件：链接: https://pan.baidu.com/s/1QV4KYZksyIp65UsScioq4Q 提取码: vcej\n失效可联系我\n1. 静态ip(要求能上外网)\n2. 主机名\n1 2 3 4 5 6 7 各自配置好主机名 # hostnamectl set-hostname --static server.cluster.com 三台都互相绑定IP与主机名 # vim /etc/hosts 192.168.116.129 master 192.168.116.130 node1 192.168.116.131 node2 1 2 3 echo \u0026#34;192.168.116.129 master 192.168.116.130 node1 192.168.116.131 node2\u0026#34;\u0026gt;\u0026gt;/etc/hosts 3. 时间同步(时间同步一定要确认一下)\n1 yum install -y ntpdate \u0026amp;\u0026amp; ntpdate time.windows.com 4. 关闭防火墙,selinux\n1 2 3 # systemctl stop firewalld # systemctl disable firewalld # iptables -F 1、安装prometheus 从 https://prometheus.io/download/ 下载相应版本，安装到服务器上\n官网提供的是二进制版，解压就能用，不需要编译\n上传prometheus-2.5.0.linux-amd64.tar.gz\n1 2 tar -zxvf prometheus-2.5.0.linux-amd64.tar.gz -C /usr/local/ mv /usr/local/prometheus-2.5.0.linux-amd64/ /usr/local/prometheus 直接使用默认配置文件启动\n1 /usr/local/prometheus/prometheus --config.file=\u0026#34;/usr/local/prometheus/prometheus.yml\u0026#34; \u0026amp; 确认端口(9090)\n1 ss -anltp | grep 9090 2、prometheus界面 通过浏览器访问http://服务器IP:9090就可以访问到prometheus的主界面\n默认只监控了本机一台，点Status --》点Targets --》可以看到只监控了本 机\n3、主机数据展示 通过http://服务器IP:9090/metrics可以查看到监控的数据\n在web主界面可以通过关键字查询监控项\n4、监控远程Linux主机 ① 在远程linux主机(被监控端agent1)上安装node_exporter组件\n下载地址: https://prometheus.io/download/\n上传node_exporter-0.16.0.linux-amd64.tar.gz\n1 2 tar -zxvf node_exporter-0.16.0.linux-amd64.tar.gz -C /usr/local/ mv /usr/local/node_exporter-0.16.0.linux-amd64/ /usr/local/node_exporter 里面就一个启动命令node_exporter,可以直接使用此命令启动\n1 nohup /usr/local/node_exporter/node_exporter \u0026amp; 确认端口(9100)\n扩展: nohup命令: 如果把启动node_exporter的终端给关闭,那么进程也会 随之关闭。nohup命令会帮你解决这个问题。\n② 通过浏览器访问http://被监控端IP:9100/metrics就可以查看到 node_exporter在被监控端收集的监控信息\n③ 回到prometheus服务器的配置文件里添加被监控机器的配置段\n在主配置文件最后加上下面三行\n1 vim /usr/local/prometheus/prometheus.yml 1 2 3 - job_name: \u0026#39;node1\u0026#39; static_configs: - targets: [\u0026#39;192.168.116.130:9100\u0026#39;] 1 2 3 - job_name: \u0026#39;agent1\u0026#39; # 取一个job名称来代 表被监控的机器 static_configs: - targets: [\u0026#39;10.1.1.14:9100\u0026#39;] # 这里改成被监控机器 的IP，后面端口接9100 改完配置文件后,重启服务\n1 pkill prometheus 确认端口没有进程占用\n1 /usr/local/prometheus/prometheus --config.file=\u0026#34;/usr/local/prometheus/prometheus.yml\u0026#34; \u0026amp; 确认端口被占用，说 明重启成功\n④ 回到web管理界面 --》点Status --》点Targets --》可以看到多了一台监 控目标\n练习: 加上本机prometheus的监控\n答: 在本机安装node_exporter，也使用上面的方式监控起来。\n相关博文： 1、Centos7安装Promethus(普罗米修斯）监控系统完整版 2、Promethus(普罗米修斯）监控Mysql数据库 3、Promethus(普罗米修斯）安装Grafana可视化图形工具 4、Promethus的Grafana图形显示MySQL监控数据 5、Promethus(普罗米修斯）的Grafana+onealert实现报警功能","date":"2020-01-13T10:40:01Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/202001131040/","title":"Centos7安装Promethus(普罗米修斯）监控系统完整版"},{"content":"和Mavan的管理一样，Dockers不仅提供了一个中央仓库，同时也允许我们使用registry搭建本地私有仓库。使用私有仓库有许多优点：\n一、节省网络带宽，针对于每个镜像不用每个人都去中央仓库上面去下载，只需要从私有仓库中下载即可；\n二、提供镜像资源利用，针对于公司内部使用的镜像，推送到本地的私有仓库中，以供公司内部相关人员使用。\n接下来我们就大致说一下如何在本地搭建私有仓库。\n1.拉取镜像仓库 1 docker pull registry 2.查看所有镜像 1 docker images 3.启动镜像服务器registry 首先在在主机上新建一个目录，供存储镜像\n1 2 cd /usr/local/ mkdir docker_registry 启动镜像\n1 docker run -d -p 5000:5000 --name=jackspeedregistry --restart=always --privileged=true -v /usr/local/docker_registry:/var/lib/registry docker.io/registry 解释：\n-p 5000:5000 端口\n--name=jackspeedregistry 运行的容器名称\n--restart=always 自动重启\n--privileged=true centos7中的安全模块selinux把权限禁止了，加上这行是给容器增加执行权限\n-v /usr/local/docker_registry:/var/lib/registry 把主机的/usr/local/docker_registry 目录挂载到registry容器的/var/lib/registry目录下，假如有删除容器操作，我们的镜像也不会被删除\ndocker.io/registry 镜像名称\n查看启动的容器\n4.从公有仓库拉取一个镜像下来，然后push到私有仓库中进行测试 当前用nginx镜像做测试 1 2 docker pull nginx docker images 5.给docker注册https协议，支持https访问 1 vim /etc/docker/daemon.json 如果daemon文件不存在，vim会自己创建一个，假如一下代码，\n{\n\u0026ldquo;insecure-registries\u0026rdquo;:[\u0026ldquo;主机的IP地址或者域名:5000\u0026rdquo;],\n\u0026ldquo;registry-mirrors\u0026rdquo;: [\u0026quot;https://registry.docker-cn.com\u0026quot;]\n}\n注释：\ninsecure-registries\u0026mdash;\u0026ndash;\u0026gt;开放注册https协议\nregistry-mirrors\u0026mdash;\u0026ndash;\u0026gt;仓库源\n6.新建一个tag，把docker.io/nginx名称变成域名或者IP/镜像名称 1 docker tag docker.io/nginx ip或者域名:5000/nginx 推送到本地仓库\n1 docker push ip或者域名:5000/nginx 7.进入刚才新建的nginx仓库目录得到 删除刚刚tag的镜像 （192.*******:5000/nginx刚才创建的镜像的tag） 1 2 3 docker rmi 196.*******:5000/nginx docker rmi nginx docker images 拉取刚刚自己创建的镜像\n1 docker pull 192.168.116.131:5000/nginx:1.2 已经完成\n","date":"2020-01-11T16:17:03Z","image":"https://www.ownit.top/title_pic/59.jpg","permalink":"https://www.ownit.top/p/202001111617/","title":"Centos7下使用Docker搭建本地私有仓库"},{"content":"kubeadm部署Kubernetes（k8s）完整版详细教程 容易配置，但出问题却很难发现。\n二进制包安装Kubernetes集群环境完整版\n配置麻烦，但不容易出现问题，也容易排查。\n对于上面安装Kubernetes方法，有兴趣的可以参考一下。\n下面这种方法，容易配置，也不容易出现问题。\n环境配置 准备3台服务器（我用的是CentOS7系统）：\nMaster：192.168.116.129\nNode1：192.168.116.130\nNode2：192.168.116.131\nk8s的全生命周期管理： 在k8s进行管理应用的时候，基本步骤是：创建集群，部署应用，发布应用，扩展应用，更新应用\nk8s的主要组件，以及它们主要是用来干什么的： **etcd：**一款开源软件。提供可靠的分布式数据存储服务，用于持久化存储K8s集群的配置和状态\n**apiservice：**用户程序（如kubectl）、K8s其它组件之间通信的接口。K8s其它组件之间不直接通信，而是通过API server通信的。这一点在上图的连接中可以体现，例如，只有API server连接了etcd，即其它组件更新K8s集群的状态时，只能通过API server读写etcd中的数据。\n**Scheduler：**排程组件，为用户应用的每一可部署组件分配工作结点。\n**controller-manager：**执行集群级别的功能，如复制组件、追踪工作结点状态、处理结点失败等。Controller Manager组件是由多个控制器组成的，其中很多控制器是按K8s的资源类型划分的，如Replication Manager（管理ReplicationController 资源），ReplicaSet Controller，PersistentVolume controller。\n**kube-proxy：**在应用组件间负载均衡网络流量。\n**kubelet：**管理工作结点上的容器。\nContriner runtime Docker， rkt等实际运行容器的组件\n上面都是些k8s集群所要用到的组件\nmaster主机上必须要有的组件： etcd ：提供分布式数据存储的数据库吧，用于持久化存储k8s集群的配置和状态\nkube-apiserver：api service提供了http rest接口，是整个集群的入口，K8s其它组件之间不直接通信，而是通过API server通信的。（只有API server连接了etcd，即其它组件更新K8s集群的状态时，只能通过API server读写etcd中的数据）\nkube-scheduler：scheduler负责资源的调度\nkube-controller-manager：整个集群的管理控制中心，此组件里面是由多个控制器组成的，如：Replication Manager（管理ReplicationController 资源），ReplicaSet Controller，PersistentVolume controller。主要作用用来复制组件、追踪工作结点状态、处理失败结点\nnode节点机上必须要有的组件： **flannel：**好像是用来支持网络通信的吧\n**kube-proxy：**用来负载均衡网络流量\n**kubelet：**用来管理node节点机上的容器\n**docker：**运行项目镜像容器的组件\nk8s的整个集群运行原理： master主机上的kube-controller-manager是整个集群的控制管理中心，kube-controler-manager中的node controller模块 通过apiservice提供的监听接口，实时监控node机的状态信息。\n当某个node机器宕机，controller-manager就会及时排除故障并自动修复。\nnode节点机上的kubelet进程每隔一段时间周期就会调用一次apiservice接口报告自身状态，apiservice接口接受到这些信息后将节点状态更新到ectd中。kubelet也通过apiservice的监听接口监听pod信息，如果监控到新的pod副本被调度绑定到本节点，则执行pod对应的容器的创建和启动，如果监听到pod对象被删除，则删除本节点对应的pod容器。\nKubernetes安装步骤： 1、所有机器上执行以下命令，准备安装环境：(注意是所有机器，主机master，从机node都要安装) 安装epel-release源\n1 yum -y install epel-release 所有机器关闭防火墙\n1 2 3 4 5 6 7 8 systemctl stop firewalld systemctl disable firewalld setenforce 0 #查看防火墙状态 firewall-cmd --state 关闭swap\n1 swapoff -a 2、现在开始master主机上安装kubernetes Master 1 yum -y install etcd kubernetes-master etcd.conf 编辑：vi /etc/etcd/etcd.conf文件，修改结果如下：\napiserver 配置：vi /etc/kubernetes/apiserver文件，配置结果如下：\n启动etcd、kube-apiserver、kube-controller-manager、kube-scheduler等服务，并设置开机启动。\n1 for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler; do systemctl restart $SERVICES;systemctl enable $SERVICES;systemctl status $SERVICES ; done 在etcd中定义flannel网络\n1 etcdctl mk /atomic.io/network/config \u0026#39;{\u0026#34;Network\u0026#34;:\u0026#34;172.17.0.0/16\u0026#34;}\u0026#39; 以上master主机上的配置安装什么的都弄完 3、在node机上安装kubernetes Node和flannel组件应用 1 yum -y install flannel kubernetes-node flanneld 为flannel网络指定etcd服务，修改**/etc/sysconfig/flanneld**文件，配置结果如下图：\nconfig 修改：vi /etc/kubernetes/config文件，配置结果如下图：\nkubelet 修改node机的kubelet配置文件**/etc/kubernetes/kubelet**\nnode节点机上启动kube-proxy,kubelet,docker,flanneld等服务，并设置开机启动。\n1 for SERVICES in kube-proxy kubelet docker flanneld;do systemctl restart $SERVICES;systemctl enable $SERVICES;systemctl status $SERVICES; done 4、在master主机上执行如下命令，查看运行的node节点机器： 1 kubectl get nodes k8s的安装算是完成 5、部署 Dashboard 也可以使用我给配置文件 https://www.lanzous.com/i8jjpij\n默认镜像国内无法访问，修改镜像地址为： lizhenliang/kubernetes-dashboard-amd64:v1.10.1\n默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：\n先Docker拉去镜像\n1 docker pull lizhenliang/kubernetes-dashboard-amd64:v1.10.1 执行kubernetes-dashboard.yaml 文件\n1 kubectl apply -f kubernetes-dashboard.yaml 安装成功\n查看暴露的端口\n1 kubectl get pods,svc -n kube-system 6. 访问 Dashboard的web界面 访问地址：https://NodeIP:30001 【必须是https】\n创建service account并绑定默认cluster-admin管理员集群角色：【依次执行】\n1 kubectl create serviceaccount dashboard-admin -n kube-system 1 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 1 kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 已经部署完成。\n","date":"2020-01-11T12:04:17Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/202001111204/","title":"CentOS7的Yum安装Kubernetes（k8s）完整版详细教程"},{"content":"Kubernetes 概述 1. Kubernetes是什么 Kubernetes是Google在2014年开源的一个容器集群管理系统，Kubernetes简称K8S。 K8S用于容器化应用程序的部署，扩展和管理。 K8S提供了容器编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能。 Kubernetes目标是让部署容器化应用简单高效。 官方网站：http://www.kubernetes.io\n2. Kubernetes特性 自我修复\n在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端请求，确保线上服务不中断。\n** 弹性伸缩**\n使用命令、UI或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行服务。\n** 自动部署和回滚**\nK8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不受影响业务。\n服务发现和负载均衡\nK8S为多个容器提供一个统一访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。\n** 机密和配置管理**\n管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。\n** 存储编排**\n挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网络存储（如NFS、GlusterFS、Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。\n批处理\n提供一次性任务，定时任务；满足批量数据处理和分析的场景。\n3. Kubernetes集群架构与组件 Master组件 kube-apiserver Kubernetes API 集群的统一入口，各组件协调者，以 RESTful API提供接口服务，所有对象资源的增删改查和监听 操作都交给APIServer处理后再提交给Etcd存储。 kube-controller-manager 处理集群中常规后台任务，一个资源对应一个控制器，而 ControllerManager就是负责管理这些控制器的。 kube-scheduler 根据调度算法为新创建的Pod选择一个Node节点，可以任意 部署,可以部署在同一个节点上,也可以部署在不同的节点上。 etcd 分布式键值存储系统。用于保存集群状态数据，比如Pod、 Service等对象信息。 Node组件 kubelet kubelet 是Master在Node节点上的Agent，管理本机运行容器 的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获 取容器和节点状态等工作。 kubelet 将每个Pod转换成一组容器。 kube-proxy 在Node节点上实现Pod网络代理，维护网络规则和四层负载均 衡工作。 docker或rocket 容器引擎，运行容器 4. Kubernetes核心概念 Pod 最小部署单元 一组容器的集合 一个Pod中的容器共享网络命名空间 Pod是短暂的 Controllers ReplicaSet ： 确保预期的Pod副本数量 Deployment ： 无状态应用部署 StatefulSet ： 有状态应用部署 DaemonSet ： 确保所有Node运行同一个Pod Job ： 一次性任务 Cronjob ： 定时任务 更高级层次对象，部署和管理Pod Service 防止Pod失联 定义一组Pod的访问策略 Label 标签，附加到某个资源上，用于关联对象、查询和筛选 Namespace 命名空间，将对象逻辑上隔离 搭建一个 Kubernetes 集群 1. 官方提供的三种部署方式 minikube Minikube是一个工具，可以在本地快速运行一个单点的Kubernetes，尝试Kubernetes或日常开发的用户使用。不能用于生产环境。\n官方地址：https://kubernetes.io/docs/setup/minikube/\nkubeadm Kubeadm也是一个工具，提供kubeadm init和kubeadm join，用于快速部署Kubernetes集群。\n官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/\n二进制包 从官方下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。\n小结：\n生产环境中部署Kubernetes集群，只有Kubeadm和二进制包可选，Kubeadm降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。我们这里使用二进制包部署Kubernetes集群，我也是推荐大家使用这种方式，虽然手动部署麻烦点，但学习很多工作原理，更有利于后期维护。\n2. Kubernetes集群环境规划 软件环境\n| 软件\n|\n版本\n操作系统\n|\nCentOS7.5_x64\n| |\nDocker\n|\n18-ce\n| |\nKubernetes\n|\n1.12\n|\n服务器角色\n| 角色\n|\nIP\n|\n组件\nk8s-master\n|\n192.168.116.129\n|\nkube-apiserver，kube-controller-manager，kube-scheduler，etcd\n| |\nk8s-node1\n|\n192.168.116.130\n|\nkubelet，kube-proxy，docker，flannel，etcd\n| |\nk8s-node2\n|\n192.168.116.131\n|\nkubelet，kube-proxy，docker，flannel，etcd\n|\n需要的软件和脚本，失效可联系我\n**链接: https://pan.baidu.com/s/1Vr8jOFgo4b8tQM9AUtXC_A 提取码: yhhn **\n架构图\n3. HTTPS证书介绍 1、关闭selinux\n1 2 sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 2、关闭防火墙\n1 2 systemctl stop firewalld systemctl disable firewalld 3、关闭swap\n1 swapoff -a 4、修改主机名\n1 hostnamectl set-hostname 名字 5、同步时间\n1 yum install -y ntpdate \u0026amp;\u0026amp; ntpdate time.windows.com 以上步骤可以参考下方链接\nkubeadm部署Kubernetes（k8s）完整版详细教程\n自签SSL证书 准备使用etcd-cert生成证书\n上传cfssl.sh，运行脚本\n生成下面两个文件，记住是tab补全查看\n执行下面命令，生成两个文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 cat \u0026gt; ca-config.json \u0026lt;\u0026lt;EOF { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;www\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34;, \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ] } } } } EOF cat \u0026gt; ca-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;etcd CA\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;Beijing\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;Beijing\u0026#34; } ] } EOF 执行\n1 cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 1 cat etcd-cert.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat \u0026gt; server-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;etcd\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;192.168.116.129\u0026#34;, \u0026#34;192.168.116.130\u0026#34;, \u0026#34;192.168.116.131\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } EOF 颁发证书\n1 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server 4. Etcd数据库集群部署 上传etcd安装包 【etcd-v3.3.10-linux-amd64.tar.gz】\n解压\n1 tar zxvf etcd-v3.3.10-linux-amd64.tar.gz 为了方便安装，创建下面目录安装。\n1 2 3 4 5 6 [root@master etcd-v3.3.10-linux-amd64]# mkdir -p /opt/etcd/{cfg,bin,ssl} [root@master etcd-v3.3.10-linux-amd64]# ls /opt/etcd/ bin cfg ssl [root@master etcd-v3.3.10-linux-amd64]# mv etcd etcdctl /opt/etcd/bin/ [root@master etcd-v3.3.10-linux-amd64]# ls /opt/etcd/bin/ etcd etcdctl 使用脚本部署etcd 1 chmod +x etcd.sh 运行脚本 1 ./etcd.sh etcd01 192.168.116.129 etcd02=https://192.168.116.130:2380,etcd03=https://192.168.116.131:2380 可以查看etcd的运行服务文件\n1 cat /usr/lib/systemd/system/etcd.service 拷贝证书\n1 cp /root/k8s/etcd-cert/{ca,server-key,server}.pem /opt/etcd/ssl/ 启动etcd 1 systemctl start etcd 已经部署一个成功，但是还有两个机器没有部署etcd。需要把/opt/的etcd目录移到其余节点配置\n还有/usr/lib/systemd/system/etcd.service这个配置文件\n拷贝过去还需要修改配置文件才能启动。\n1 vi /opt/etcd/cfg/etcd 这几个需要修改本机对于的ip\n切忌要给下方的文件执行权限，不然会报错\n1 /opt/etcd/bin 启动\n1 systemctl daemon-reload 1 systemctl start etcd 已经完成启动\n在node2也按照上方部署就行。\n查看先前日志【没有报任何错误，已经成功搭建集群】\n1 tail /var/log/messages -f 测试 1 /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem --endpoints=\u0026#34;https://192.168.116.129:2379,https://192.168.116.130:2379,https://192.168.116.131:2379\u0026#34; cluster-health 5. Node安装Docker 安装依赖软件\n1 yum install -y yum-utils device-mapper-persistent-data lvm2 1 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装docker-ce\n1 yum install docker-ce -y 配置docker加速器\n1 curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://bc437cce.m.daocloud.io 启动docker\n1 systemctl start docker 开机自启\n1 systemctl enable docker 6. Flannel容器集群网络部署 Kubernetes网络模型设计基本要求：\n一个Pod一个IP 每个Pod独立IP，Pod内所有容器共享网络（同一个IP） 所有容器都可以与所有其他容器通信 所有节点都可以与所有有容器通信 工作原理：\n添加网段\n1 /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem --endpoints=\u0026#34;https://192.168.116.129:2379,https://192.168.116.130:2379,https://192.168.116.131:2379\u0026#34; set /coreos.com/network/config \u0026#39;{ \u0026#34;Network\u0026#34;: \u0026#34;172.17.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: {\u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;}}\u0026#39; 1 /opt/etcd/bin/etcdctl --ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem --endpoints=\u0026#34;https://192.168.116.129:2379,https://192.168.116.130:2379,https://192.168.116.131:2379\u0026#34; get /coreos.com/network/config 上传Flannel【在node上部署flannel】\n方便管理flannel，创建下面文件\n1 mkdir -p /opt/kubernetes/{bin,cfg,ssl} 使用flannel连接etcd形式执行脚本\n1 ./flannel.sh https://192.168.116.129:2379,https://192.168.116.130:2379,https://192.168.116.131:2379 解压flannel-v0.10.0-linux-amd64.tar.gz\n1 tar zxvf flannel-v0.10.0-linux-amd64.tar.gz 1 mv flanneld mk-docker-opts.sh /opt/kubernetes/bin/ 启动flannel，成功\n1 systemctl start flanneld 重启docker\n1 systemctl restart docker 安装openssh-clients，把flannel的配置文件复制到Node2上。\n1 yum install -y openssh-clients 传输到Node2上\n1 scp -r /opt/kubernetes/ root@192.168.116.131:/opt/ 1 scp /usr/lib/systemd/system/{flanneld,docker}.service root@192.168.116.131:/usr/lib/systemd/system/ 在Node2上启动flanneld\n1 2 systemctl start flanneld systemctl start docker 7. 部署Master组件 1. kube-apiserver\n2. kube-controller-manager\n3. kube-scheduler\n配置文件 -\u0026gt; systemd管理组件 -\u0026gt; 启动\n上传Master\n1 unzip master.zip 上传kubernetes-server-linux-amd64.tar.gz\n1 tar zxvf kubernetes-server-linux-amd64.tar.gz 创建安装目录，方便使用\n1 mkdir -p /opt/kubernetes/{bin,cfg,ssl} 来到/root/k8s/soft/kubernetes/server/bin目录下拷贝文件\n1 cp kube-apiserver kube-controller-manager kube-scheduler /opt/kubernetes/bin/ 执行apiserver.sh 【脚本 masterip etcd的ip】\n1 ./apiserver.sh 192.168.116.129 https://192.168.116.129:2379,https://192.168.116.130:2379,https://192.168.116.131:2379 查看生成的配文件\n1 cat /opt/kubernetes/cfg/kube-apiserver 上传k8s-cert.sh，生成证书\n1 bash k8s-cert.sh 复制证书到/opt/kubernetes/ssl/里\n1 cp ca.pem ca-key.pem server.pem server-key.pem /opt/kubernetes/ssl/ 上传token配置文件，生成token。\n1 2 3 4 5 BOOTSTRAP_TOKEN=0fb61c46f8991b718eb38d27b605b008 cat \u0026gt; token.csv \u0026lt;\u0026lt;EOF ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,\u0026#34;system:kubelet-bootstrap\u0026#34; EOF 生成token.csv 文件移动 1 mv token.csv /opt/kubernetes/cfg/ 启动kube-apiserver\n1 systemctl restart kube-apiserver 查看监听端口\n1 2 3 4 5 6 [root@master k8s-cert]# netstat -ano | grep 8080 tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN off (0.00/0/0) [root@master k8s-cert]# netstat -ano | grep 6443 tcp 0 0 192.168.116.129:6443 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 192.168.116.129:44594 192.168.116.129:6443 ESTABLISHED keepalive (9.93/0/0) tcp 0 0 192.168.116.129:6443 192.168.116.129:44594 ESTABLISHED keepalive (109.64/0/0) 运行controller-manager.sh\n1 2 ./controller-manager.sh 127.0.0.1 ./scheduler.sh 127.0.0.1 1 2 3 4 5 6 [root@master k8s]# ls apiserver.sh controller-manager.sh etcd-cert etcd.sh flannel.sh k8s-cert master.zip scheduler.sh soft [root@master k8s]# ./controller-manager.sh 127.0.0.1 Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service. [root@master k8s]# ./scheduler.sh 127.0.0.1 Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service. 创建kubectl管理工具\n1 cp kubectl /usr/bin/ 查看集群健康状态\n1 kubectl get cs 查看资源名字简写\n1 kubectl api-resources 8. 部署Node组件 1. 将kubelet-bootstrap用户绑定到系统集群角色 1 2 3 kubectl create clusterrolebinding kubelet-bootstrap \\ --clusterrole=system:node-bootstrapper \\ --user=kubelet-bootstrap 2. 创建kubeconfig文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #---------------------- APISERVER=$1 SSL_DIR=$2 #需要先前创建的token BOOTSTRAP_TOKEN=0fb61c46f8991b718eb38d27b605b008 # 创建kubelet bootstrapping kubeconfig export KUBE_APISERVER=\u0026#34;https://$APISERVER:6443\u0026#34; # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=$SSL_DIR/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=bootstrap.kubeconfig # 设置客户端认证参数 kubectl config set-credentials kubelet-bootstrap \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=bootstrap.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig #---------------------- # 创建kube-proxy kubeconfig文件 kubectl config set-cluster kubernetes \\ --certificate-authority=$SSL_DIR/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy \\ --client-certificate=$SSL_DIR/kube-proxy.pem \\ --client-key=$SSL_DIR/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 执行脚本\n1 bash kubeconfig.sh 192.168.116.129 /root/k8s/k8s-cert/ 把这两个文件拷贝到2个Node的/opt/kubernetes/cfg/\n1 2 scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.116.130:/opt/kubernetes/cfg/ scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.116.131:/opt/kubernetes/cfg/ 3. 部署kubelet，kube-proxy组件\n上传node包\n把matser里面的kubelet和kube-poxy拷贝过来\n1 2 scp kubelet kube-proxy root@192.168.116.130:/opt/kubernetes/bin/ scp kubelet kube-proxy root@192.168.116.131:/opt/kubernetes/bin/ 运行 kubelet.sh 1 bash kubelet.sh 192.168.116.130 Master节点颁发csr 1 kubectl get csr 1 kubectl certificate approve node-csr-96wka1dqU6bV1MMXD1nNUw14Av8NU6H6IW11ecJlYM0 运行proxy.sh\n1 bash proxy.sh 192.168.116.130 9. 部署多个Node操作 复制粘贴Node1已经部署好的文件\n1 scp -r /opt/kubernetes/ root@192.168.116.131:/opt/ 1 scp /usr/lib/systemd/system/{kubelet,kube-proxy}.service root@192.168.116.131:/usr/lib/systemd/system/ 删除复制过来的证书\n修改Node2上配置文件的IP地址\n修改完毕，重启即可。\n1 2 3 [root@node2 cfg]# systemctl restart kubelet [root@node2 cfg]# systemctl restart kube-proxy [root@node2 cfg]# ps -ef | grep kube 去Master节点颁发证书\n1 kubectl certificate approve node-csr-96wka1dqU6bV1MMXD1nNUw14Av8NU6H6IW11ecJlYM0 ","date":"2020-01-10T19:37:49Z","image":"https://www.ownit.top/title_pic/63.jpg","permalink":"https://www.ownit.top/p/202001101937/","title":"二进制包安装Kubernetes集群环境完整版"},{"content":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。\n这个工具能通过两条指令完成一个kubernetes集群的部署：\n1 2 3 4 5 # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join \u0026lt;Master节点的IP和端口 \u0026gt; 1. 安装要求 在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap分区 环境： master 192.168.116.129【最低是2核的，不然安装会保存】\nnode1 192.168.116.130\nnode2 192.168.116.131\n2. 学习目标 在所有节点上安装Docker和kubeadm 部署Kubernetes Master 部署容器网络插件 部署 Kubernetes Node，将节点加入Kubernetes集群中 部署Dashboard Web页面，可视化查看Kubernetes资源 3. 准备环境 1、关闭防火墙：\n1 2 systemctl stop firewalld systemctl disable firewalld 2、关闭selinux：\n1 2 sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 3、关闭swap：\n1 2 swapoff -a # 临时关闭 sed -ri \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab #永久关闭 4、修改主机名称\n1 hostnamectl set-hostname 名字 5、添加主机名与IP对应关系（记得设置主机名）：\n1 2 3 4 cat /etc/hosts 192.168.116.129 master 192.168.116.130 note1 192.168.116.131 note2 6、将桥接的IPv4流量传递到iptables的链\n1 2 3 4 cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF 1 sysctl --system 4. 所有节点安装****Docker/kubeadm/kubelet xshell可以使用发送键盘所有会话来安装。比较省事\nKubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\n1、安装Docker 安装Docker源\n1 yum install -y wget \u0026amp;\u0026amp; wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 安装Docker\n1 yum -y install docker-ce-18.06.1.ce-3.el7 开启自启和启动\n1 systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 查看版本\n1 docker --version 2、安装kubeadm，kubelet和****kubectl 1、添加阿里云YUM的软件源\n1 2 3 4 5 6 7 8 9 cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2、安装kubeadm，kubelet和kubectl\n由于版本更新频繁，这里指定版本号部署：\n1 yum install -y kubelet-1.15.0 kubeadm-1.15.0 kubectl-1.15.0 设置开机自启\n1 systemctl enable kubelet 5、部署Kubernetes Master 在192.168.116.129（Master）执行\n1 2 3 4 5 6 kubeadm init \\ --apiserver-advertise-address=192.168.116.129 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.15.0 \\ --service-cidr=10.1.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。\n已经初始化完成\n使用kubectl工具：\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 6. 安装Pod网络插件（CNI） 1 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 确保能够访问到quay.io这个registery。\n如果下载失败，可以改成这个镜像地址：lizhenliang/flannel:v0.11.0-amd64\n也可以使用我给配置文件 https://www.lanzous.com/i8jjpij\n这里，我是用的是我的配置文件。\n1 kubectl apply -f kube-flannel.yml 7. 加入****Kubernetes Node 在node上安装flannel\n1 docker pull lizhenliang/flannel:v0.11.0-amd64 在192.168.116.130（Node）执行。\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\n1 2 kubeadm join 192.168.116.129:6443 --token iz96vy.f5ukew9geeome5is \\ --discovery-token-ca-cert-hash sha256:72b689426bfc34512294c29b39ea3b2af3a94e39f62c4434f3a49f16d51a1382 查看Node\n1 kubectl get node 添加一下node2，命令如上。切记，先执行在node上安装flannel\n已经完全准备完成。\n1 kubectl get pods -n kube-system 8. 测试kubernetes集群 在Kubernetes集群中创建一个pod，验证是否正常运行：\n创建nginx容器\n1 kubectl create deployment nginx --image=nginx 暴露对外端口\n1 kubectl expose deployment nginx --port=80 --type=NodePort 查看nginx是否运行成功\n1 kubectl get pod,svc 在浏览器访问。三个结点都可访问，说明集群已经搭建完成，\n扩容nginx副本wei3个，成功\n1 kubectl scale deployment nginx --replicas=3 1 kubectl get pods 9. 部署 Dashboard 先前已经给配置配置文件。\n也可以使用我给配置文件 https://www.lanzous.com/i8jjpij\n默认镜像国内无法访问，修改镜像地址为： lizhenliang/kubernetes-dashboard-amd64:v1.10.1\n默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：\n先Docker拉去镜像\n1 docker pull lizhenliang/kubernetes-dashboard-amd64:v1.10.1 执行kubernetes-dashboard.yaml 文件\n1 kubectl apply -f kubernetes-dashboard.yaml 安装成功\n查看暴露的端口\n1 kubectl get pods,svc -n kube-system 10. 访问 Dashboard的web界面 访问地址：https://NodeIP:30001 【必须是https】\n创建service account并绑定默认cluster-admin管理员集群角色：【依次执行】\n1 kubectl create serviceaccount dashboard-admin -n kube-system 1 kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin 1 kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 已经部署完成。\n","date":"2020-01-08T16:49:12Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/202001081649/","title":"kubeadm部署Kubernetes（k8s）完整版详细教程"},{"content":"本地镜像发布到阿里云流程 镜像的生成方法 1、前面的DockerFile\n2、从容器创建一个新的镜像\n1 docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] OPTIONS说明：\n-a :提交的镜像作者；\n-m :提交时的说明文字；\n将本地镜像推送到阿里云 本地镜像素材原型\n阿里云开发者平台\nhttps://dev.aliyun.com/search.html\n创建仓库镜像\n命名空间 仓库名称 将镜像推送到registry\n公有云可以查询到\n查看详情\n将阿里云上的镜像下载到本地\n","date":"2020-01-04T10:04:42Z","image":"https://www.ownit.top/title_pic/01.jpg","permalink":"https://www.ownit.top/p/202001041004/","title":"本地镜像发布到阿里云"},{"content":"目录\n总体步骤\n安装tomcat\n安装mysql\n安装redis\n总体步骤 1、搜索镜像 2、拉取镜像 3、查看镜像 4、启动镜像 5、停止容器 6、移除容器 安装tomcat docker hub上面查找tomcat镜像\n1 docker search tomcat 从docker hub上拉取tomcat镜像到本地\n1 docker pull tomcat docker images查看是否有拉取到的tomcat\n使用tomcat镜像创建容器(也叫运行镜像)\n1 docker run -it -p 8080:8080 tomcat -p 主机端口:docker容器端口 -P 随机分配端口 i:交互 t:终端 安装mysql docker hub上面查找mysql镜像\n1 docker search mysql 从docker hub上(阿里云加速器)拉取mysql镜像到本地标签为5.6\n1 docker pull mysql:5.6 使用mysql5.6镜像创建容器(也叫运行镜像)\n1 docker run -p 12345:3306 --name mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -v /zzyyuse/mysql/logs:/logs -v /zzyyuse/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 命令说明：\n-p 12345:3306：将主机的12345端口映射到docker容器的3306端口。\n--name mysql：运行服务名字\n-v /zzyyuse/mysql/conf:/etc/mysql/conf.d ：将主机/zzyyuse/mysql录下的conf/my.cnf 挂载到容器的 /etc/mysql/conf.d\n-v /zzyyuse/mysql/logs:/logs：将主机/zzyyuse/mysql目录下的 logs 目录挂载到容器的 /logs。\n-v /zzyyuse/mysql/data:/var/lib/mysql ：将主机/zzyyuse/mysql目录下的data目录挂载到容器的 /var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。\n-d mysql:5.6 : 后台程序运行mysql5.6\n1 docker exec -it MySQL运行成功后的容器ID /bin/bash 外部Win10也来连接运行在dokcer上的mysql服务\n数据备份小测试\n1 docker exec myql服务容器ID sh -c \u0026#39; exec mysqldump --all-databases -uroot -p\u0026#34;123456\u0026#34; \u0026#39; \u0026gt; /zzyyuse/all-databases.sql 安装redis hub上(阿里云加速器)拉取redis镜像到本地标签为3.2\n1 docker pull redis:3.2 使用redis3.2镜像创建容器(也叫运行镜像)\n使用镜像\n1 docker run -p 6379:6379 -v /zzyyuse/myredis/data:/data -v /zzyyuse/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf --appendonly yes 在主机/zzyyuse/myredis/conf/redis.conf目录下新建redis.conf文件\n1 vim /zzyyuse/myredis/conf/redis.conf/redis.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 # Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k =\u0026gt; 1000 bytes # 1kb =\u0026gt; 1024 bytes # 1m =\u0026gt; 1000000 bytes # 1mb =\u0026gt; 1024*1024 bytes # 1g =\u0026gt; 1000000000 bytes # 1gb =\u0026gt; 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \u0026#34;include\u0026#34; won\u0026#39;t be rewritten by command \u0026#34;CONFIG REWRITE\u0026#34; # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you\u0026#39;d better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf ################################## NETWORK ##################################### # By default, if no \u0026#34;bind\u0026#34; configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \u0026#34;bind\u0026#34; configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 lookback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \u0026#34;bind\u0026#34; directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \u0026#34;bind\u0026#34; directive. protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use \u0026#39;yes\u0026#39; if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. #daemonize no # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \u0026#34;process is ready.\u0026#34; # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \u0026#34;/var/run/redis.pid\u0026#34;. # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \u0026#34;\u0026#34; # To enable logging to the system logger, just set \u0026#39;syslog-enabled\u0026#39; to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT \u0026lt;dbid\u0026gt; where # dbid is a number between 0 and \u0026#39;databases\u0026#39;-1 databases 16 ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \u0026#34;save\u0026#34; lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \u0026#34;\u0026#34; save 120 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that\u0026#39;s set to \u0026#39;yes\u0026#39; as it\u0026#39;s almost always a win. # If you want to save some CPU in the saving child set it to \u0026#39;no\u0026#39; but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the \u0026#39;dbfilename\u0026#39; configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./ ################################# REPLICATION ################################# # Master-Slave replication. Use slaveof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of slaves. # 2) Redis slaves are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition slaves automatically try to reconnect to masters # and resynchronize with them. # # slaveof \u0026lt;masterip\u0026gt; \u0026lt;masterport\u0026gt; # If the master is password protected (using the \u0026#34;requirepass\u0026#34; configuration # directive below) it is possible to tell the slave to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the slave request. # # masterauth \u0026lt;master-password\u0026gt; # When a slave loses its connection with the master, or when the replication # is still in progress, the slave can act in two different ways: # # 1) if slave-serve-stale-data is set to \u0026#39;yes\u0026#39; (the default) the slave will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if slave-serve-stale-data is set to \u0026#39;no\u0026#39; the slave will reply with # an error \u0026#34;SYNC with master in progress\u0026#34; to all the kind of commands # but to INFO and SLAVEOF. # slave-serve-stale-data yes # You can configure a slave instance to accept writes or not. Writing against # a slave instance may be useful to store some ephemeral data (because data # written on a slave will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default slaves are read-only. # # Note: read only slaves are not designed to be exposed to untrusted clients # on the internet. It\u0026#39;s just a protection layer against misuse of the instance. # Still a read only slave exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only slaves using \u0026#39;rename-command\u0026#39; to shadow all the # administrative / dangerous commands. slave-read-only yes # Replication SYNC strategy: disk or socket. # # ------------------------------------------------------- # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY # ------------------------------------------------------- # # New slaves and reconnecting slaves that are not able to continue the replication # process just receiving differences, need to do what is called a \u0026#34;full # synchronization\u0026#34;. An RDB file is transmitted from the master to the slaves. # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the slaves incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to slave sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more slaves # can be queued and served with the RDB file as soon as the current child producing # the RDB file finishes its work. With diskless replication instead once # the transfer starts, new slaves arriving will be queued and a new transfer # will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple slaves # will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the slaves. # # This is important since once the transfer starts, it is not possible to serve # new slaves arriving, that will be queued for the next RDB transfer, so the server # waits a delay in order to let more slaves arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # Slaves send PINGs to server in a predefined interval. It\u0026#39;s possible to change # this interval with the repl_ping_slave_period option. The default value is 10 # seconds. # # repl-ping-slave-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of slave. # 2) Master timeout from the point of view of slaves (data, pings). # 3) Slave timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-slave-period otherwise a timeout will be detected # every time there is low traffic between the master and the slave. # # repl-timeout 60 # Disable TCP_NODELAY on the slave socket after SYNC? # # If you select \u0026#34;yes\u0026#34; Redis will use a smaller number of TCP packets and # less bandwidth to send data to slaves. But this can add a delay for # the data to appear on the slave side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \u0026#34;no\u0026#34; the delay for data to appear on the slave side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and slaves are many hops away, turning this to \u0026#34;yes\u0026#34; may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # slave data when slaves are disconnected for some time, so that when a slave # wants to reconnect again, often a full resync is not needed, but a partial # resync is enough, just passing the portion of data the slave missed while # disconnected. # # The bigger the replication backlog, the longer the time the slave can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a slave connected. # # repl-backlog-size 1mb # After a master has no longer connected slaves for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last slave disconnected, for # the backlog buffer to be freed. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The slave priority is an integer number published by Redis in the INFO output. # It is used by Redis Sentinel in order to select a slave to promote into a # master if the master is no longer working correctly. # # A slave with a low priority number is considered better for promotion, so # for instance if there are three slaves with priority 10, 100, 25 Sentinel will # pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the slave as not able to perform the # role of master, so a slave with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. slave-priority 100 # It is possible for a master to stop accepting writes if there are less than # N slaves connected, having a lag less or equal than M seconds. # # The N slaves need to be in \u0026#34;online\u0026#34; state. # # The lag in seconds, that must be \u0026lt;= the specified value, is calculated from # the last ping received from the slave, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough slaves # are available, to the specified number of seconds. # # For example to require at least 3 slaves with a lag \u0026lt;= 10 seconds use: # # min-slaves-to-write 3 # min-slaves-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-slaves-to-write is set to 0 (feature disabled) and # min-slaves-max-lag is set to 10. # A Redis master is able to list the address and port of the attached # slaves in different ways. For example the \u0026#34;INFO replication\u0026#34; section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover slave instances. # Another place where this info is available is in the output of the # \u0026#34;ROLE\u0026#34; command of a masteer. # # The listed IP and address normally reported by a slave is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the slave to connect with the master. # # Port: The port is communicated by the slave during the replication # handshake, and is normally the port that the slave is using to # list for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the slave may be actually reachable via different IP and port # pairs. The following two options can be used by a slave in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # slave-announce-ip 5.5.5.5 # slave-announce-port 1234 ################################## SECURITY ################################### # Require clients to issue AUTH \u0026lt;PASSWORD\u0026gt; before processing any other # commands. This might be useful in environments in which you do not trust # others with access to the host running redis-server. # # This should stay commented out for backward compatibility and because most # people do not need auth (e.g. they run their own servers). # # Warning: since Redis is pretty fast an outside user can try up to # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. # # requirepass foobared # Command renaming. # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \u0026#34;\u0026#34; # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to slaves may cause problems. ################################### LIMITS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error \u0026#39;max number of clients reached\u0026#39;. # # maxclients 10000 # Don\u0026#39;t use more memory than the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can\u0026#39;t remove keys according to the policy, or if the policy is # set to \u0026#39;noeviction\u0026#39;, Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU cache, or to set # a hard memory limit for an instance (using the \u0026#39;noeviction\u0026#39; policy). # # WARNING: If you have slaves attached to an instance with maxmemory on, # the size of the output buffers needed to feed the slaves are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of slaves is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have slaves attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for slave # output buffers (but this is not needed if the policy is \u0026#39;noeviction\u0026#39;). # # maxmemory \u0026lt;bytes\u0026gt; # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select among five behaviors: # # volatile-lru -\u0026gt; remove the key with an expire set using an LRU algorithm # allkeys-lru -\u0026gt; remove any key according to the LRU algorithm # volatile-random -\u0026gt; remove a random key with an expire set # allkeys-random -\u0026gt; remove a random key, any key # volatile-ttl -\u0026gt; remove the key with the nearest expire time (minor TTL) # noeviction -\u0026gt; don\u0026#39;t expire at all, just return an error on write operations # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs a bit more CPU. 3 is very fast but not very accurate. # # maxmemory-samples 5 ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \u0026#34;appendonly.aof\u0026#34;) appendfilename \u0026#34;appendonly.aof\u0026#34; # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don\u0026#39;t fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \u0026#34;everysec\u0026#34;, as that\u0026#39;s usually the right compromise between # speed and data safety. It\u0026#39;s up to you to understand if you can relax this to # \u0026#34;no\u0026#34; that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that\u0026#39;s snapshotting), # or on the contrary, use \u0026#34;always\u0026#34; that\u0026#39;s very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \u0026#34;everysec\u0026#34;. # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it\u0026#39;s possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \u0026#34;appendfsync none\u0026#34;. In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \u0026#34;yes\u0026#34;. Otherwise leave it as # \u0026#34;no\u0026#34; that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can\u0026#39;t happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \u0026#34;redis-check-aof\u0026#34; utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn\u0026#39;t want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 ################################ REDIS CLUSTER ############################### # # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however # in order to mark it as \u0026#34;mature\u0026#34; we need to wait for a non trivial percentage # of users to deploy it in production. # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # # Normal Redis instances can\u0026#39;t be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A slave of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a slave to actually have a exact measure of # its \u0026#34;data age\u0026#34;, so the following two checks are performed: # # 1) If there are multiple slaves able to failover, they exchange messages # in order to try to give an advantage to the slave with the best # replication offset (more data from the master processed). # Slaves will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single slave computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \u0026#34;connected\u0026#34; state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the slave will not try to failover # at all. # # The point \u0026#34;2\u0026#34; can be tuned by user. Specifically a slave will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * slave-validity-factor) + repl-ping-slave-period # # So for example if node-timeout is 30 seconds, and the slave-validity-factor # is 10, and assuming a default repl-ping-slave-period of 10 seconds, the # slave will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large slave-validity-factor may allow slaves with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a slave at all. # # For maximum availability, it is possible to set the slave-validity-factor # to a value of 0, which means, that slaves will always try to failover the # master regardless of the last time they interacted with the master. # (However they\u0026#39;ll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-slave-validity-factor 10 # Cluster slaves are able to migrate to orphaned masters, that are masters # that are left without working slaves. This improves the cluster ability # to resist to failures as otherwise an orphaned master can\u0026#39;t be failed over # in case of failure if it has no working slaves. # # Slaves migrate to orphaned masters only if there are still at least a # given number of other working slaves for their old master. This number # is the \u0026#34;migration barrier\u0026#34;. A migration barrier of 1 means that a slave # will migrate only if there is at least 1 other working slave for its master # and so forth. It usually reflects the number of slaves you want for every # master in your cluster. # # Default is 1 (slaves migrate only if their masters remain with at least # one slave). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don\u0026#39;t have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \u0026#34;CONFIG SET latency-monitor-threshold \u0026lt;milliseconds\u0026gt;\u0026#34; if needed. latency-monitor-threshold 0 ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \u0026#34;foo\u0026#34; stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@\u0026lt;db\u0026gt;__ prefix. # E Keyevent events, published with __keyevent@\u0026lt;db\u0026gt;__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # A Alias for g$lshzxe, so that the \u0026#34;AKE\u0026#34; string means all the events. # # The \u0026#34;notify-keyspace-events\u0026#34; takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don\u0026#39;t need # this feature and the feature has some overhead. Note that if you don\u0026#39;t # specify at least one of K or E, no events will be delivered. notify-keyspace-events \u0026#34;\u0026#34; ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb \u0026lt;-- not recommended for normal workloads # -4: max size: 32 Kb \u0026lt;-- not recommended # -3: max size: 16 Kb \u0026lt;-- probably not recommended # -2: max size: 8 Kb \u0026lt;-- good # -1: max size: 4 Kb \u0026lt;-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \u0026#34;don\u0026#39;t start compressing until after 1 node into the list, # going from either the head or tail\u0026#34; # So: [head]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]-\u0026gt;[next]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[prev]-\u0026gt;[tail] # 2 here means: don\u0026#39;t compress head or head-\u0026gt;next or tail-\u0026gt;prev or tail, # but compress all nodes between them. # 3: [head]-\u0026gt;[next]-\u0026gt;[next]-\u0026gt;node-\u0026gt;node-\u0026gt;...-\u0026gt;node-\u0026gt;[prev]-\u0026gt;[prev]-\u0026gt;[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \u0026#34;steps\u0026#34; are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \u0026#34;activerehashing no\u0026#34; if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \u0026#34;activerehashing yes\u0026#34; if you don\u0026#39;t have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can\u0026#39;t consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -\u0026gt; normal clients including MONITOR clients # slave -\u0026gt; slave clients # pubsub -\u0026gt; clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit \u0026lt;class\u0026gt; \u0026lt;hard limit\u0026gt; \u0026lt;soft limit\u0026gt; \u0026lt;soft seconds\u0026gt; # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don\u0026#39;t receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and slave clients, since # subscribers and slaves receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \u0026#34;hz\u0026#34; value. # # By default \u0026#34;hz\u0026#34; is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes 测试redis-cli连接上来\n1 docker exec -it 运行着Rediis服务的容器ID redis-cli 测试持久化文件生成\n","date":"2020-01-03T18:06:43Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/202001031806/","title":"Docker常用安装(tomcat、mysql、redis)"},{"content":"目录\n下载tomcat容器卷\n1、创建文件夹\n2、在上述目录下touch c.txt\n3、将jdk和tomcat安装的压缩包拷贝进上一步目录\n4、在/heian/mydockerfile/tomcat9目录下新建Dockerfile文件\n5、构建\n6、run\n7、验证\n8、结合前述的容器卷将测试的web服务test发布\n总结\n下载tomcat容器卷 1、创建文件夹 1 mkdir -p /heian/mydockerfile/tomcat9 2、在上述目录下touch c.txt 1 touch c.txt 3、将jdk和tomcat安装的压缩包拷贝进上一步目录 apache-tomcat-9.0.8.tar.gz\njdk-8u171-linux-x64.tar.gz\n4、在/heian/mydockerfile/tomcat9目录下新建Dockerfile文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 FROM centos MAINTAINER heian\u0026lt;1794748404@126.com\u0026gt; #把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下 COPY c.txt /usr/local/cincontainer.txt #把java与tomcat添加到容器中 ADD jdk-8u152-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-8.5.24.tar.gz /usr/local/ #安装vim编辑器 RUN yum -y install vim #设置工作访问时候的WORKDIR路径，登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配置java与tomcat环境变量 ENV JAVA_HOME /usr/local/jdk1.8.0_152 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-8.5.24 ENV CATALINA_BASE /usr/local/apache-tomcat-8.5.24 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [\u0026#34;/usr/local/apache-tomcat-8.5.24/bin/startup.sh\u0026#34; ] # CMD [\u0026#34;/usr/local/apache-tomcat-8.5.24/bin/catalina.sh\u0026#34;,\u0026#34;run\u0026#34;] CMD /usr/local/apache-tomcat-8.5.24/bin/startup.sh \u0026amp;\u0026amp; tail -F /usr/local/apache-tomcat-8.5.24/bin/logs/catalina.out 5、构建 1 docker build -f Dockerfile -t heiantomcat9 . 构建完成\n6、run 1 docker run -d -p 9080:8080 --name myt9 -v /heian/mydockerfile/tomcat9/test:/usr/local/apache-tomcat-9.0.8/webapps/test -v /heian/mydockerfile/tomcat9/tomcat9logs/:/usr/local/apache-tomcat-9.0.8/logs --privileged=true heiantomcat9 Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied\n解决办法：在挂载目录后多加一个\u0026ndash;privileged=true参数即可\n7、验证 8、结合前述的容器卷将测试的web服务test发布 a.jsp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34; pageEncoding=\u0026#34;UTF-8\u0026#34;%\u0026gt; \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; -----------welcome------------ \u0026lt;%=\u0026#34;i am in docker tomcat self \u0026#34;%\u0026gt; \u0026lt;br\u0026gt; \u0026lt;br\u0026gt; \u0026lt;% System.out.println(\u0026#34;=============docker tomcat self\u0026#34;);%\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; web.xml\n1 2 3 4 5 6 7 8 9 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://java.sun.com/xml/ns/javaee\u0026#34; xsi:schemaLocation=\u0026#34;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\u0026#34; id=\u0026#34;WebApp_ID\u0026#34; version=\u0026#34;2.5\u0026#34;\u0026gt; \u0026lt;display-name\u0026gt;test\u0026lt;/display-name\u0026gt; \u0026lt;/web-app\u0026gt; 测试\n总结 ","date":"2020-01-03T16:33:10Z","image":"https://www.ownit.top/title_pic/27.jpg","permalink":"https://www.ownit.top/p/202001031633/","title":"DockerFile自定义镜像Tomcat9"},{"content":"作用：都是指定一个容器启动时要运行的命令 CMD Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换\n实例\ntomcat的讲解演示\n1 docker run -it -p 8080:8080 tomcat 1 docker run -it -p 8080:8080 tomcat ls -l **ENTRYPOINT ** docker run 之后的参数会被当做参数传递给 ENTRYPOINT，之后形成新的命令组合\n实例\n制作CMD版可以查询IP信息的容器\n1 2 3 FROM centos RUN yum install -y curl CMD [ \u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://ip.cn\u0026#34; ] crul命令解释\ncurl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。\n如果系统没有curl可以使用yum install curl安装，也可以下载安装。\ncurl是将下载文件输出到stdout\n使用命令：curl http://www.baidu.com\n执行后，www.baidu.com的html就会显示在屏幕上了\n这是最简单的使用方法。用这个命令获得了http://curl.haxx.se指向的页面，同样，如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地。如果下载的是HTML文档，那么缺省的将只显示文件头部，即HTML文档的header。要全部显示，请加参数 -i\n问题 如果我们希望显示 HTTP 头信息，就需要加上 -i 参数\nWHY\n我们可以看到可执行文件找不到的报错，executable file not found。\n之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。\n因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://ip.cn 后面。而 -i 根本不是命令，所以自然找不到。\n那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令：\n1 $ docker run myip curl -s http://ip.cn -i 制作ENTROYPOINT版查询IP信息的容器\n1 2 3 FROM centos RUN yum install -y curl ENTRYPOINT [ \u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://ip.cn\u0026#34; ] ","date":"2020-01-03T14:49:12Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/202001031449/","title":"DockerFile的CMD/ENTRYPOINT 镜像案例"},{"content":"Base镜像(scratch) Docker Hub 中 99% 的镜像都是通过在 base 镜像中安装和配置需要的软件构建出来的\n自定义镜像mycentos 1、编写 自定义mycentos目的使我们自己的镜像具备如下：\n登陆后的默认路径 vim编辑器 查看网络配置ifconfig支持 准备编写DockerFile文件\nmyCentOS内容DockerFile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #基于本地的centos FROM centos #作者、邮件 MAINTAINER cf\u0026lt;1794748404@qq.com\u0026gt; #来设置环境变量 ENV MYPATH /uer/local #登录进去的路径 WORKDIR $MYPATH ##安装下面的软件 RUN yum -y install vim RUN yum -y install net-tools #暴露80端口 EXPOSE 80 #打印信息 CMD echo $MYPATH CMD echo \u0026#34;success-----------------ok\u0026#34; #使用bash CMD /bin/bash 2、构建 1 docker build -f /root/docker/dockerfile1 -t mycentos:1.3 . 3、运行 1 docker run -it 新镜像名字:TAG 可以看到，我们自己的新镜像已经支持vim/ifconfig命令，扩展成功了\n4、列出镜像的变更历史 docker history 镜像名\n","date":"2020-01-03T14:13:50Z","image":"https://www.ownit.top/title_pic/50.jpg","permalink":"https://www.ownit.top/p/202001031413/","title":"DockerFile自定义镜像centos"},{"content":"DockerFile体系结构(保留字指令) FROM：基础镜像，当前新镜像是基于哪个镜像的\nMAINTAINER：镜像维护者的姓名和邮箱地址\nRUN：容器构建时需要运行的命令\nEXPOSE：当前容器对外暴露出的端口\nWORKDIR：指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点\nENV：用来在构建镜像过程中设置环境变量\nENV MY_PATH /usr/mytest\n这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样；\n也可以在其它指令中直接使用这些环境变量，\n比如：WORKDIR $MY_PATH\nADD**:将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包**\nCOPY**:类似ADD，拷贝文件和目录到镜像中。\n将从构建上下文目录中 \u0026lt;源路径\u0026gt; 的文件/目录复制到新的一层的镜像内的 \u0026lt;目标路径\u0026gt; 位置**\nCOPY src dest\nCOPY [\u0026ldquo;src\u0026rdquo;, \u0026ldquo;dest\u0026rdquo;]\nVOLUME**:容器数据卷，用于数据保存和持久化工作**\nCMD**:指定一个容器启动时要运行的命令**\nCMD容器启动命令\nCND指令的格式和RUN 相似，也是两种格式:\n●shell 格式: CMD \u0026lt;命令\u0026gt; ●exec格式: CID [\u0026ldquo;可执行文件”， “参数1\u0026rdquo;， “参数2”..]\n.参数列表格式: CNO [“参数\u0026quot;,“参数2”..]. 在指定了EITRYPOIT 指令后,用CID指定具体的参\n数。\nDockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换\nENTRYPOINT :指定一个容器启动时要运行的命令\nENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数\nONBUILD:当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发\n","date":"2020-01-03T12:01:16Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/202001031201/","title":"DockerFile体系结构(保留字指令)"},{"content":"1、是什么 Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。\n构建三步骤 编写Dockerfile文件 docker build docker run 文件什么样？？？ 以我们熟悉的CentOS为例 https://hub.docker.com/_/centos/\n1 2 3 4 5 6 7 8 FROM scratch ADD centos- 7-docker.tar.xz / LABEL org. label-schema. schema-version = \u0026#34;1.0\u0026#34; \\ org. label-schema. name=\u0026#34;CentOS Base Image\u0026#34; \\ org. label-schema. vendor=\u0026#34;Cent0S\u0026#34; \\ org. label-schema. license=\u0026#34;GPLv2\u0026#34; \\ org. label-schema. build-date=\u0026#34;20180531\u0026#34; CMD [\u0026#34;/bin/bash\u0026#34;] 2、DockerFile构建过程解析 Dockerfile内容基础知识 1：每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2：指令按照从上到下，顺序执行 3：#表示注释 4：每条指令都会创建一个新的镜像层，并对镜像进行提交 Docker执行Dockerfile的大致流程 （1）docker从基础镜像运行一个容器 （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （4）docker再基于刚提交的镜像运行一个新容器 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段，\n* Dockerfile是软件的原材料\n* Docker镜像是软件的交付品\n* Docker容器则可以认为是软件的运行态。\nDockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。\n1 Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等;\n2 Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时，会真正开始提供服务;\n3 Docker容器，容器是直接提供服务的。\n","date":"2020-01-03T11:29:01Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/202001031129/","title":"DockerFile解析"},{"content":"运行原理流程图\n什么是内网穿透?\n1. 内网穿透是我们在进行网络连接时的一种术语，也叫做NAT穿透，即在计算机是局域网内的时候，外网与内网的计算机的节点进行连接时所需要的连接通信，有时候就会出现内网穿透不支的情况。内网穿透的功能就是，当我们在端口映射时设置时，内网穿透起到了地址转换的功能，也就是把公网的地址进行翻译，转成为一种私有的地址，然后再采用路由的方式ADSL的宽带路由器，具有一个动态或者是固定的公网IP，最后ADSL直接在 交换机 上，这样所有的电脑都可以共享上网。内网穿透除了可以实现内网之间机器的网络通信功通之外，还可以解决UDP中出现的数据传输不稳定问题。\n2. 简单说, 就是A客户端要访问B客户端，通过一台服务器进行桥接，桥接有两种方式，一种是相互转发，另一个是告诉对方的地址，自己就当一个介绍人的角色。\n** 网云穿是一款可以在包括但不限于Windows、Mac、Linux、群辉、树莓派、威联通上使用的内网穿透，它可以很便捷的帮助你将本地内网的应用发布出去，比如：网站、数据库、硬盘文件、远程桌面、应用、游戏(如我的世界)等等，这样您就可以很方便的微信调试、自建云盘、异地办公等等，它不需要您有公网IP、不需要您有服务器、也不需要您设置任何路由器，只需要您安装软件进行简单设置即可直接使用。**\n如何实现?\n大部分内网穿透都是通过第三方工具实现, 市面上也有很多第三个工具, 花生壳, Ngrok等… 但本团队在使用过之后, 发现这些第三方工具在价钱和使用方面不太符合预期的效果, 在这里小蚂蚁内网穿透团队致力打造一款更加方便, “亲民” 的一款内网穿透工具, 详情请了解官网:\n网云穿\n费话不多说，直接上干货 _ SpringBoot 使用网云穿对外发布项目_ 我的程序运行的jar包，需要发布到外网，但是没有服务器，只能通过穿透来。\n下面就是教程\n1、下载网云穿pc安装包，解压。 下载\n2、登录账号（没有的需要自己去官网注册） 3、查询本机需要映射的IP 4、配置需要映射的端口信息 1、点击web\n2、点击隧道管理\n3、配置需要穿透的本机IP和端口\n5、启动SpringBoot项目 6、返回网云穿Pc端 7、查看效果 完美穿透，真是方便，以后我的电脑都可以搞成服务器了。\n","date":"2019-12-24T15:00:48Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/201912241500/","title":"SpringBoot项目映射"},{"content":"Centos7系统Docker安装 目录\nCentos7系统Docker安装\n目录\nCentos7系统Docker安装\n1、下载mysql镜像\n2、创建Master实例并启动\n参数说明\n3、创建Slave实例并启动\n4、添加master主从复制部分配置\n5、添加Slave主从复制部分配置\n6、为master授权用户来他的同步数据\n1、下载mysql镜像 1 docker search mysql 1 docker pull mysql:5.7 1 docker images 2、创建Master实例并启动 1 2 3 4 5 6 docker run -p 3307:3306 --name mysql-master \\ -v /mydata/mysql/master/log:/var/log/mysql \\ -v /mydata/mysql/master/data:/var/lib/mysql \\ -v /mydata/mysql/master/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=root \\ -d mysql:5.7 参数说明 -p 3307:3306：将容器的3306端口映射到主机的3307端口 -v /mydata/mysql/master/conf:/etc/mysql：将配置文件夹挂在到主机 -v /mydata/mysql/master/log:/var/log/mysql：将日志文件夹挂载到主机 -v /mydata/mysql/master/data:/var/lib/mysql/：将配置文件夹挂载到主机 -e MYSQL_ROOT_PASSWORD=root：初始化root用户的密码 修改master****基本配置\n1 vim /mydata/mysql/master/conf/my.cnf 1 2 3 4 5 6 7 8 9 10 11 12 13 [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] init_connect=\u0026#39;SET collation_connection = utf8_unicode_ci\u0026#39; init_connect=\u0026#39;SET NAMES utf8\u0026#39; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake skip-name-resolve 注意：skip-name-resolve一定要加，不然连接mysql会超级慢\n3、创建Slave实例并启动 1 2 3 4 5 6 docker run -p 3316:3306 --name mysql-slaver-01 \\ -v /mydata/mysql/slaver/log:/var/log/mysql \\ -v /mydata/mysql/slaver/data:/var/lib/mysql \\ -v /mydata/mysql/slaver/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=root \\ -d mysql:5.7 修改slave****基本配置\n1 vim /mydata/mysql/slaver/conf/my.cnf 1 2 3 4 5 6 7 8 9 10 11 12 13 [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] init_connect=\u0026#39;SET collation_connection = utf8_unicode_ci\u0026#39; init_connect=\u0026#39;SET NAMES utf8\u0026#39; character-set-server=utf8 collation-server=utf8_unicode_ci skip-character-set-client-handshake skip-name-resolve 4、添加master****主从复制部分配置 1 vim /mydata/mysql/master/conf/my.cnf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 server_id=1 log-bin=mysql-bin read-only=0 binlog-do-db=gmall_ums binlog-do-db=gmall_pms binlog-do-db=gmall_oms binlog-do-db=gmall_sms binlog-do-db=gmall_cms replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema 重启容器\n5、添加Slave主从复制部分配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 server_id=2 log-bin=mysql-bin read-only=1 binlog-do-db=gmall_ums binlog-do-db=gmall_pms binlog-do-db=gmall_oms binlog-do-db=gmall_sms binlog-do-db=gmall_cms replicate-ignore-db=mysql replicate-ignore-db=sys replicate-ignore-db=information_schema replicate-ignore-db=performance_schema 重启容器\n6、为master授权用户来他的同步数据 1、进入主库\n1 docker exec -it 4fdd7f265228 /bin/bash 2、进入主库mysql数据库\n1 mysql -u root -p ** 1）、授权root可以远程访问（ 主从无关，为了方便我们远程连接mysql）** 1 grant all privileges on *.* to \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;root\u0026#39; with grant option; 1 flush privileges; ** 2）、添加用来同步的用户** 1 GRANT REPLICATION SLAVE ON *.* to \u0026#39;backup\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;123456\u0026#39;; 3）、查看数据库的状态 1 show master status\\G; 3、进入从库mysql数据库\n** 1）、授权root可以远程访问（ 主从无关，为了方便我们远程连接mysql）** 1 grant all privileges on *.* to \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;root\u0026#39; with grant option; 1 flush privileges; ** 2）、设置主库连接** 1 change master to master_host=\u0026#39;192.168.116.129\u0026#39;,master_user=\u0026#39;backup\u0026#39;,master_password=\u0026#39;123456\u0026#39;,master_log_file=\u0026#39;mysql-bin.000001\u0026#39;,master_log_pos=0,master_port=3307; ** 3）、启动从库同步** 1 start slave; ** 4）、查看从库状态** 1 show slave status\\G; 至此主从配置完成；\n总结：\n** 1）、主从数据库在自己配置文件中声明需要同步哪个数据库，忽略哪个数据库等信息。并且server-id不能一样**\n** 2）、主库授权某个账号密码来同步自己的数据**\n** 3）、从库使用这个账号密码连接主库来同步数据**\n演示效果\n","date":"2019-12-19T11:16:59Z","image":"https://www.ownit.top/title_pic/09.jpg","permalink":"https://www.ownit.top/p/201912191116/","title":"Docker安装MySQL集群【读写分离】"},{"content":"\nSpringBoot 是 SpringMVC 的升级，对于编码、配置、部署和监控，更加简单\n微服务\nSpring 为 微服务提供了一整套的组件-SpringClound , SpirngBoot 就是该基础。\n第一个SpringBoot程序 maven配置的中央仓库阿里云镜像\nsetting.xml\n1 2 3 4 5 6 \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;nexus-aliyun\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;Nexus aliyun\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;\thttps://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 使用IDEA创建SpringBoot项目\n项目结构为：\n项目默认的 maven pom.xml文件\n运行SpirngbootdemoApplication的main方法，就能开始运行。\n控制台输出：\n从这里可以看到 Tomcat 的端口号，因为还没有自定义Controller，所以还没有视图，下面来创建一个输出Hello SpringBoot!的视图。\n创建一个HelloController，位于controller包下\nHelloController.java\n1 2 3 4 5 6 7 8 9 10 11 12 package com.jxust.controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class HelloController { @RequestMapping(\u0026#34;/hello\u0026#34;) public String say(){ return \u0026#34;Hello SpringBoot!\u0026#34;; } } @RestController Spring4 之后新加的注解,原来返回json需要@ResponseBody配合@Controller,现在一个顶俩\n在浏览器中输入http://localhost:8080/hello就能输出Hello SpringBoot!这句话。\n自定义属性配置\n用到的是application.properties这个文件\n配置端口号和访问前缀\napplication.properties\n1 2 server.port=8081 server.context-path=/springboot 除了使用.properties格式的文件，还可以使用.yml格式的配置文件(推荐)，更加简便\napplication.yml\n把原来的application.properties文件删除\n注意格式，空格不能少\n获取配置文件中的属性值\n我们也可以在配置文件中，配置数据，在 Controller 中获取,比如：\n1 2 3 4 5 6 server: port: 8080 servlet: context-path: /springboot heian : 乘风破浪 HelloController 获取配置文件中的值\nHelloController.java\n1 2 3 4 5 6 7 8 9 @RestController public class HelloController { @Value(\u0026#34;${heian}\u0026#34;) private String heian; @RequestMapping(value = \u0026#34;/heain\u0026#34;,method = RequestMethod.GET) public String heian(){ return heian; } } 返回的为heian的值\n配置文件中值配置方式的多样化\n配置文件的值可以是多个，也可以是组合，如：\napplication.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 name: 乘风 age: 22 或者 name: 乘风 age: 22 content: \u0026#34;name: ${name},age: ${age}\u0026#34; 或者 server: port: 8080 context-path: /springboot person: name: 乘风 age: 22 前两种配置获取值的方式都是一样的,但是对于这种方式，person 有相应的两个属性，需要这样处理\nPersonProperties.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package com.example.demo.controller; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.stereotype.Component; /** * @Author:南宫乘风 * @Date:2019/12/18 16:01 */ @Component @ConfigurationProperties(prefix = \u0026#34;person\u0026#34;) public class PersonProperties { private String name; private String age; public String getName() { return name; } public void setName(String name) { this.name = name; } public String getAge() { return age; } public void setAge(String age) { this.age = age; } } Alt+insert快捷键提示生成 Getter and Setter\npom.xml需要加入下面的依赖,处理警告\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-configuration-processor\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; HelloController.java\n1 2 3 4 5 6 7 @Autowired PersonProperties personProperties; @RequestMapping(value = \u0026#34;/heian\u0026#34;, method = RequestMethod.GET) public String hello() { return \u0026#34;名字： \u0026#34;+personProperties.getName()+\u0026#34; 年龄： \u0026#34;+personProperties.getAge(); } 关于配置文件application.yml的多套配置\n类似 il8n 文件国际化的配置方式i18n_en_US.properties和i18n_zh_CN.properties\n这样能解决，需要频繁修改配置的尴尬\n由application.yml配置文件决定使用那套配置文件。\napplication.yml\n1 2 3 spring: profiles: active: a application-a.yml\n1 2 3 4 5 6 7 8 server: port: 8080 servlet: context-path: /springboot person : name : 南宫乘风A age : 20 application-b.yml\n1 2 3 4 5 6 7 8 server: port: 8080 servlet: context-path: /springboot person : name : 南宫乘风B age : 22 SpringBoot增删改查实例 完整的项目结构\nController的使用\n@Controller 处理http请求 @RestController Spring4 之后新加的注解，原来返回json需要@ResponseBody配合@Controller @RequestMapping 配置url映射 对于 REST 风格的请求\n对于 Controller 中的方法上的注解\n1 2 3 4 @RequestMapping(value = “/hello”,method = RequestMethod.GET) @RequestMapping(value = “/hello”,method = RequestMethod.POST) @RequestMapping(value = “/hello”,method = RequestMethod.DELETE) @RequestMapping(value = “/hello”,method = RequestMethod.PUT) SpringBoot 对上面的注解进行了简化\n1 2 3 4 @GetMapping(value = “/girls”) @PostMapping(value = “/girls”) @PutMapping(value = “/girls/{id}”) @DeleteMapping(value = “/girls/{id}”) 浏览器需要发送不同方式的请求，可以安装HttpRequester插件，火狐浏览器可以直接搜索该组件安装。\nspring-data-jpa\nJPA全称Java Persistence API.JPA通过JDK 5.0注解或XML描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。\nHibernate3.2+、TopLink 10.1.3以及OpenJPA都提供了JPA的实现。\n利用JPA创建MySQL数据库\npom.xml加入JPA和MySQL的依赖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.38\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置JPA和数据库\napplication.yml\n1 2 3 4 5 6 7 8 9 10 11 12 spring: profiles: active: a datasource: driver-class-name: com.mysql.jdbc.Driver url : jdbc:mysql://127.0.0.1:3306/heian username: root password: root jpa: hibernate: ddl-auto: update show-sql: true 格式很重要\n需要自己手动去创建 db_person 数据库\n创建与数据表对应的实体类Person\nPerson.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 package com.example.demo.entity; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.Id; /** * @Author:南宫乘风 * @Date:2019/12/18 16:38 */ @Entity public class Person { @Id @GeneratedValue private Integer id; private String name; private Integer age; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } } 运行项目后，查看数据库，会自动创建表 person\n接下来就可以进行person表的增删改查了\n创建控制器PersonController.java\n首先创建一个接口PersonMapper，位于dao包下,PersonController调用该接口继承自JpaRepository的方法，来实现和数据库交互\n这个PersonMapper接口的功能，与SSM框架中 dao 层接口功能有异曲同工之妙；在SSM框架中，Service层通过该接口，间接执行Mybatis数据库映射文件(.xml)里的相应sql语句，执行数据库增删改查的操作。(Mapper自动实现DAO接口)\nPersonMapper.java\n1 2 3 4 5 6 7 8 9 package com.example.demo.dao; import com.example.demo.entity.Person; import org.springframework.data.jpa.repository.JpaRepository; /** * @Author:南宫乘风 * @Date:2019/12/18 16:38 */ public interface PersonMapper extends JpaRepository\u0026lt;Person,Integer\u0026gt; { } PersonController.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com.example.demo.controller; import com.example.demo.dao.PersonMapper; import com.example.demo.entity.Person; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import java.util.List; /** * @Author:南宫乘风 * @Date:2019/12/18 16:39 */ @RestController public class PersonController { @Autowired PersonMapper personMapper; @GetMapping(value = \u0026#34;/person\u0026#34;) public List\u0026lt;Person\u0026gt; personList(){ return personMapper.findAll(); } } 在数据库中添加两条数据\n启动项目执行请求http://localhost:8080/person\n控制台输出的sql语句：\n其他增删改查的方法\nPersonController.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 /** * 添加一个人员 * @param name * @param age * @return */ @PostMapping(value = \u0026#34;/person\u0026#34;) public Person add(@RequestParam(\u0026#34;name\u0026#34;) String name, @RequestParam(\u0026#34;age\u0026#34;) Integer age) { Person person = new Person(); person.setName(name); person.setAge(age); return personMapper.save(person); } 利用ApiPost测试【已经成功】\n更新其他方法\n","date":"2019-12-18T17:45:19Z","image":"https://www.ownit.top/title_pic/38.jpg","permalink":"https://www.ownit.top/p/201912181745/","title":"Idea SpringBoot 对数据库实例详解"},{"content":"企业真实场景由于硬盘常年大量读写，经常会出现坏盘，需要更换硬盘。或者由于磁盘空间不足，需添加新硬盘，新添加的硬盘需要经过格式化、分区才能被 Linux 系统所使用。\n虚拟机 CentOS 7 Linux 模拟DELL R730 真实服务器添加一块新硬盘，不需要关机，直接插入用硬盘即可，一般硬盘均支持热插拔功能。企业中添加新硬盘的操作流程如下：\n（1） 检测Linux系统识别的硬盘设备，新添加硬盘被识别为/dev/sdb，如果有多块硬盘，会依次识别成/dev/sdc、/dev/sdd 等设备名称\n1 fdisk -l （2） 基于新硬盘/dev/sdb设备，创建磁盘分区/dev/sdb1\n1 fdisk /dev/sdb （3） fdisk 分区命令参数如下，常用参数包括m、n、p、e、d、w。\n（4） 创建/dev/sdb1 分区方法，fdisk /dev/sdb，然后按 n-p-1-Enter 键+20G-Enter 键-w，最后执行 fdisk –l|tail -10\n（5） mkfs.ext4 /dev/sdb1 格式化磁盘分区\n1 mkfs.ext4 /dev/sdb1 （6） /dev/sdb1 分区格式化，使用mount 命令挂载到/data/目录\nmkdir -p /data/ 创建/data/数据目录 mount /dev/sdb1 /data 挂载/dev/sdb1 分区至/data/目录 df -h 查看磁盘分区详情 echo \u0026ldquo;mount /dev/sdb1 /data\u0026rdquo; \u0026raquo;/etc/rc.local 将 挂 载 分 区 命 令 加 入/etc/rc.local 开机启动 （7） 自动挂载分区除了可以加入到/etc/rc.local 开机启动之外，还可以加入到/etc/fstab文件中\n/dev/sdb1 /data/ ext4 defaults 0 0 mount -o rw,remount / 重新挂载/系统，检测/etc/fstab 是否有误 ","date":"2019-12-16T23:48:41Z","image":"https://www.ownit.top/title_pic/48.jpg","permalink":"https://www.ownit.top/p/201912162348/","title":"Linux下磁盘实战操作命令"},{"content":"是什么 一句话：有点类似我们Redis里面的rdb和aof文件\n先来看看Docker的理念：\n* 将运用与运行的环境打包形成容器运行 ，运行可以伴随着容器，但是我们对数据的要求希望是持久化的\n* 容器之间希望有可能共享数据\nDocker容器产生的数据，如果不通过docker commit生成新的镜像，使得数据做为镜像的一部分保存下来，\n那么当容器删除后，数据自然也就没有了。\n为了能保存数据在docker中我们使用卷。\n能干嘛 容器的持久化 容器间继承+共享数据 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性：\n卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不会在容器删除时删除其挂载的数据卷\n特点：\n1：数据卷可在容器之间共享或重用数据 2：卷中的更改可以直接生效 3：数据卷中的更改不会包含在镜像的更新中 4：数据卷的生命周期一直持续到没有容器使用它为止 数据卷 直接命令添加 1、命令 1 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 1 docker run -it -v /wei:/wei centos 2、查看数据卷是否挂载成功 1 docker inspect df6f397beedd 3、容器和宿主机之间数据共享 4、容器停止退出后，主机修改后数据是否同步【完全同步】 5、命令(带权限) 1 docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 DockerFile添加 1、根目录下新建mydocker文件夹并进入\n2、可在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷\nVOLUME[\u0026quot;/dataVolumeContainer\u0026quot;,\u0026quot;/dataVolumeContainer2\u0026quot;,\u0026quot;/dataVolumeContainer3\u0026quot;]\n说明：\n出于可移植和分享的考虑，用-v 主机目录:容器目录这种方法不能够直接在Dockerfile中实现。\n由于宿主机目录是依赖于特定宿主机的，并不能够保证在所有的宿主机上都存在这样的特定目录。\n3、File构建\n1 2 3 4 5 6 7 8 9 # volume test FROM centos VOLUME [\u0026#34;/dataVolumeContainer1\u0026#34;,\u0026#34;/dataVolumeContainer2\u0026#34;] CMD echo \u0026#34;finished,--------success1\u0026#34; CMD /bin/bash 4、build后生成镜像\n1 docker build --f /root/heian -t heian/centos . 5、run容器\n6、通过上述步骤，容器内的卷目录地址已经知道对应的主机目录地址哪？？\n7、主机对应默认地址 数据完全可以同步\n备注 Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied\n解决办法：在挂载目录后多加一个\u0026ndash;privileged=true参数即可\n数据卷容器 是什么 命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器\n总体介绍 1、以上一步新建的镜像heian/centos为模板并运行容器dc01/dc02/dc03\n2、它们已经具有容器卷\n/dataVolumeContainer1 /dataVolumeContainer2 容器间传递共享(\u0026ndash;volumes-from) 1、先启动一个父容器dc01，在dataVolumeContainer2新增内容\n2、dc02/dc03继承自dc01\n1 docker run -it --name dc02 --volumes-from dc01 zzyy/centos dc02/dc03分别在dataVolumeContainer2各自新增内容\n3、回到dc01可以看到02/03各自添加的都能共享了\n4、删除dc01，dc02修改后dc03可否访问\n5、删除dc02后dc03可否访问\n再进一步\n6、新建dc04继承dc03后再删除dc03\n结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止\n","date":"2019-12-12T11:41:16Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/201912121141/","title":"Docker容器数据卷介绍和命令"},{"content":"目录\n是什么\nUnionFS（联合文件系统）\nDocker镜像加载原理\n分层的镜像\n为什么 Docker 镜像要采用这种分层结构呢\n特点\nDocker镜像commit操作补充\n案例演示\n1、从Hub上下载tomcat镜像到本地并成功运行\n2、故意删除上一步镜像生产tomcat容器的文档\n3、commit一个没有doc的tomcat新镜像\n4.启动我们的新镜像并和原来的对比\n是什么 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。\nUnionFS（联合文件系统） UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。\n特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录\nDocker镜像加载原理 docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。\nbootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。\nrootfs (root file system) ，在bootfs之上。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。\n** 平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？**\n分层的镜像 以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载\n为什么 Docker 镜像要采用这种分层结构呢 最大的一个好处就是 - 共享资源\n比如：有多个镜像都从相同的 base 镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像，\n同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n特点 Docker镜像都是只读的\n当容器启动时，一个新的可写层被加载到镜像的顶部。\n这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。\nDocker镜像commit操作补充 docker commit提交容器副本使之成为一个新的镜像\ndocker commit -m=“提交的描述信息” -a=“作者” 容器ID 要创建的目标镜像名:[标签名]\n案例演示 1、从Hub上下载tomcat镜像到本地并成功运行 -p 主机端口:docker容器端口【指定端口】\n1 docker run -it -p 8080:8080 tomcat -P 随机分配端口\n1 docker run -it -P tomcat i:交互\nt:终端\n2、故意删除上一步镜像生产tomcat容器的文档 3、commit一个没有doc的tomcat新镜像 也即当前的tomcat运行实例是一个没有文档内容的容器，\n以它为模板commit一个没有doc的tomcat新镜像heian/tomcat02\n1 docker commit -a \u0026#34;wei\u0026#34; -m \u0026#34;del tomcat docs\u0026#34; 63982bc3e2d9 heian/mytomcat:1.2 4.启动我们的新镜像并和原来的对比 启动heian/tomcat02，它没有docs\n1 docker run -it -p 7777:8080 heian/mytomcat:1.2 新启动原来的tomcat，它有docs\n","date":"2019-12-12T10:19:06Z","image":"https://www.ownit.top/title_pic/04.jpg","permalink":"https://www.ownit.top/p/201912121019/","title":"Docker 镜像介绍和命令"},{"content":"\nattach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 **build ** Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container changes # 提交当前容器为新的镜像 **cp ** Copy files/folders from the containers filesystem to the host path #从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container\u0026rsquo;s filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应export] **info ** Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 **ps ** List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至docker源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 **rmi ** Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 **tag ** Tag an image into a repository # 给源中镜像打标签 **top ** Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 ","date":"2019-12-10T15:13:46Z","image":"https://www.ownit.top/title_pic/52.jpg","permalink":"https://www.ownit.top/p/201912101513/","title":"Docker命令总结"},{"content":"目录\n启动守护式容器\n查看容器日志\ndocker后台运行\n查看容器内运行的进程\n​查看容器内部细节\n进入正在运行的容器并以命令行交互\n重新进入\n上述两个区别\n从容器内拷贝文件到主机上\n启动守护式容器 1 docker run -d 容器名 #使用镜像centos:latest以后台模式启动一个容器\n1 docker run -d centos 问题：然后docker ps -a 进行查看, 会发现容器已经退出\n很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程.\n容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的。\n这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如\n1 service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用,\n这样的容器后台启动后,会立即自杀因为他觉得他没事可做了.\n所以，最佳的解决方案是,将你要运行的程序以前台进程的形式运行\n查看容器日志 1 docker logs -f -t --tail 容器ID * -t 是加入时间戳 * -f 跟随最新的日志打印 * \u0026ndash;tail 数字 显示最后多少条 docker后台运行 1 docker run -d centos /bin/sh -c \u0026#34;while true;do echo hello zzyy;sleep 2;done\u0026#34; 查看容器内运行的进程 1 docker top 容器ID 查看容器内部细节 1 docker inspect 容器ID 进入正在运行的容器并以命令行交互 1 docker exec -it 容器ID bashShell 重新进入 1 docker attach 容器ID 上述两个区别 attach： 直接进入容器启动命令的终端，不会启动新的进程 exec： 是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到主机上 1 docker cp 容器ID:容器内路径 目的主机路径 ","date":"2019-12-10T15:01:46Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201912101501/","title":"Docker启动守护式容器"},{"content":"目录\n新建并启动容器\nOPTIONS说明\n启动交互式容器\n列出当前所有正在运行的容器\n退出容器\nexit\nctrl+P+Q\n进入出容器\n启动容器\n停止容器\n强制停止容器\n删除已停止的容器\n一次性删除多个容器\n有镜像才能创建容器，这是根本前提(下载一个CentOS镜像演示)\n1 docker pull centos 新建并启动容器 docker run [OPTIONS] IMAGE [COMMAND] [ARG\u0026hellip;]\nOPTIONS说明 OPTIONS说明（常用）：有些是一个减号，有些是两个减号\n--name=\u0026ldquo;容器新名字\u0026rdquo;: 为容器指定一个名称； -d: 后台运行容器，并返回容器ID，也即启动守护式容器； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P: 随机端口映射； -p: 指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 启动交互式容器 #使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。\n1 docker run -it centos /bin/bash 列出当前所有正在运行的容器 docker ps [OPTIONS]\nOPTIONS说明（常用）：\n-a :列出当前所有正在运行的容器+历史上运行过的（docker ps） -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 --no-trunc :不截断输出 退出容器 exit 容器停止退出 ctrl+P+Q 容器不停止退出 进入出容器 docker attach 名称\n1 docker attach edc486762ad2 启动容器 1 docker start 容器ID或者容器名 重启容器\n1 docker restart 容器ID或者容器名 停止容器 1 docker stop 容器ID或者容器名 强制停止容器 1 docker kill 容器ID或者容器名 删除已停止的容器 1 docker rm 容器ID 一次性删除多个容器 1 docker rm -f $(docker ps -a -q) 1 docker ps -a -q | xargs docker rm ","date":"2019-12-10T14:15:02Z","image":"https://www.ownit.top/title_pic/15.jpg","permalink":"https://www.ownit.top/p/201912101415/","title":"Docker容器常用命令"},{"content":"目录\n帮助命令\n镜像命令\n列出本地主机上的镜像\ndocker search 某个XXX镜像名字\ndocker pull 某个XXX镜像名字\n删除镜像\n帮助命令 1 docker version 1 docker info 1 docker --help 镜像命令 1 docker images 列出本地主机上的镜像 各个选项说明:\nREPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 -a :列出本地所有的镜像（含中间映像层） -q :只显示镜像ID。 --digests :显示镜像的摘要信息 --no-trunc :显示完整的镜像信息 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。\n如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像\ndocker search 某个XXX镜像名字 网站\nhttps://hub.docker.com\n命令\n1 docker search [OPTIONS] 镜像名字 OPTIONS说明：\n--no-trunc : 显示完整的镜像描述 -s : 列出收藏数不小于指定值的镜像。 --automated : 只列出 automated build类型的镜像； docker pull 某个XXX镜像名字 下载镜像\n1 docker pull 镜像名字[:TAG] 1 docker pull tomcat 删除镜像 docker rmi 某个XXX镜像名字ID\n删除单个\n1 docker rmi -f 镜像ID 删除多个\n1 docker rmi -f 镜像名1:TAG 镜像名2:TAG 1 docker rmi -f hello-world nginx 删除全部\n1 docker rmi -f $(docker images -qa) ","date":"2019-12-09T17:24:35Z","image":"https://www.ownit.top/title_pic/49.jpg","permalink":"https://www.ownit.top/p/201912091724/","title":"Docker镜像常用命令"},{"content":"目录\n前期说明\n安装步骤\n1、官网中文安装参考手册\n2、确定你是CentOS7及以上版本\n3、yum安装gcc相关\n4、卸载旧版本\n5、安装需要的软件包\n6、设置stable镜像仓库\n7、更新yum软件包索引\n8、安装DOCKER-CE\n9、启动docker\n10、测试\n11、配置镜像加速\n12、卸载\n底层原理\n1、Docker是怎么工作的\n2、为什么Docker比较比VM快\n前期说明 CentOS Docker 安装\nDocker支持以下的CentOS版本：\nCentOS 7 (64-bit)\nCentOS 6.5 (64-bit) 或更高的版本\n前提条件\n目前，CentOS 仅发行版本中的内核支持 Docker。\nDocker 运行在 CentOS 7 上，要求系统为64位、系统内核版本为 3.10 以上。\nDocker 运行在 CentOS-6.5 或更高的版本的 CentOS 上，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。\n查看自己的内核\nuname命令用于打印当前系统相关信息（内核版本号、硬件架构、主机名称和操作系统类型等）。\n查看已安装的CentOS版本信息（CentOS6.8有，CentOS7无该命令）\nCentos7\n安装步骤 1、官网中文安装参考手册 https://docs.docker.com/install/linux/docker-ce/centos/\n2、确定你是CentOS7及以上版本 3、yum安装gcc相关 CentOS7能上外网\n1 2 yum -y install gcc yum -y install gcc-c++ 4、卸载旧版本 1 yum -y remove docker docker-common docker-selinux docker-engine 2018.3官网版本\n1 2 3 4 5 6 7 8 9 10 11 yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 5、安装需要的软件包 1 yum install -y yum-utils device-mapper-persistent-data lvm2 6、设置stable镜像仓库 大坑\n1 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 报错：\n1 [Errno 14] curl#35 - TCP connection reset by peer 2 [Errno 12] curl#35 - Timeout\n注意：因为这是Docker的官网的，是外国的，所以下载慢。\n解决：推荐使用阿里云源，或者网易的源\n推荐\n1 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 注意：这里是阿里云的源，国内的，速度很快。\n7、更新yum软件包索引 1 yum makecache fast 8、安装DOCKER-CE 1 yum -y install docker-ce 9、启动docker 1 systemctl start docker 10、测试 1 docker version 1 docker run hello-world 11、配置镜像加速 1、创建目录（docker会自动创建）\n1 mkdir -p /etc/docker 2、使用阿里云的加速地址\n1 vim /etc/docker/daemon.json 1 systemctl daemon-reload 3、重启Docker\n1 systemctl restart docker 12、卸载 停止Docker\n1 systemctl stop docker yum卸载Docker\n1 yum -y remove docker-ce rm删除Docker的目录\n1 rm -rf /var/lib/docker 底层原理 1、Docker是怎么工作的 Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 容器，是一个运行时环境，就是我们前面说到的集装箱。\n2、为什么Docker比较比VM快 (1)docker有着比虚拟机更少的抽象层。由亍docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。\n(2)docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载Guest OS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一个docker容器只需要几秒钟。\n","date":"2019-12-09T11:04:32Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/201912091104/","title":"Centos7系统Docker安装"},{"content":"目录\n镜像\n容器\n仓库\n总结\nDocker的基本组成三要素\n镜像\n容器\n仓库\n镜像 Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。\n容器 Docker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序 容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 仓库 仓库（Repository）是集中存放镜像文件的场所。\n仓库(Repository)和仓库注册服务器（Registry）是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。\n仓库分为公开仓库（Public）和私有仓库（Private）两种形式。\n最大的公开仓库是 Docker Hub(https://hub.docker.com/)，\n存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云 等\n总结 需要正确的理解仓储/镜像/容器这几个概念:\nDocker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就似乎 image镜像文件。只有通过这个镜像文件才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 * image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 * 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 * 至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。 ","date":"2019-12-09T08:42:08Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/201912090842/","title":"Docker组成三要素"},{"content":"目录\n一、Git的安装\n1.1 图形化界面\n1.2 命令行界面\n二、本地仓库的创建与提交\n2.1 图形化界面\n2.1.1 首先在电脑上有一个空白目录\n2.1.2 打开SourceTree\n2.1.3 点击左边\u0026quot;克隆/新建\u0026quot;,创建本地仓库\n2.1.4 选择第一步中的空白目录，点击\u0026quot;创建\u0026quot;按钮\n2.1.5 此时左边会出现这个，代表本地仓库创建完成\n2.1.6 打开空白目录，在空白目录下新建文件，文件(内容/名称)随便输入\n2.1.7 返回SourceTree，会发现未暂存文件中有你刚才修改或增加的文件\n2.1.8 右键“未暂存文件”中的文件，点击添加\n2.1.9 会发现“未暂存文件”中的文件进入了“已暂存文件”中\n2.1.10 在下方输入“本次提交的描述”，点击“提交按钮”\n2.1.11 点击master分支，会显示本次提交的详细信息\n2.2 命令行界面\n2.2.1 点击SourceTree右上角的“命令行模式”即可打开命令行窗口\n2.2.2 命令识别\n三、工作流\n3.1 图形化界面\n3.1.1 首先在电脑上有一个空白目录\n3.1.2 打开SourceTree\n3.1.3 点击左边\u0026quot;克隆/新建\u0026quot;,创建本地仓库\n3.1.4 选择第一步中的空白目录，点击\u0026quot;创建\u0026quot;按钮\n3.1.5 打开刚才创建的demo2目录，在里面添加一个文件(内容自定)\n3.1.6 打开SourceTree，将刚才修改的文件添加进暂存区\n3.1.7 第一次提交\n3.1.8 需求变更\n3.1.9 需求撤销\n3.1.10 第二天正式需求\n3.1.11 第二天的需求提交到git仓库\n3.1.12 第三天撤销需求\n3.1.13 项目不需要了\n3.2 命令行界面\n3.2.1 先有一个空目录\n3.2.2 在该空目录下打开git_bush\n3.2.3 产品经理临时变更需求\n3.2.4 将变更后的需求提交到暂存区\n3.2.5 第二天上班之后，被产品经理告知该需求不需要\n3.2.6 第二天结束\n3.2.7 将第二天的工作内容提交到仓库\n3.2.8 产品经理突然告知这个需求不要了\n3.2.9 这个项目都不需要了\n3.3 总结\n四、远程仓库\n4.1 github密钥生成\n4.2 添加远程仓库\n4.2.1 命令行界面\n4.2.2 图形化界面\n五、克隆仓库\n5.1 命令行操作\n5.2 图形化操作\n六、标签管理\n6.1 标签命令\n6.2 命令行方式实现\n6.3 图形化方式实现\n七、分支管理\n7.1 命令行方式\n7.1.1 提交到本地仓库\n7.1.2 创建分支\n7.1.2 删除分支\n7.2 图形化界面\n7.2.1 首先需要有一个本地目录，里面有一个文件，文件中有内容\n7.2.2 打开SourceTree创建本地仓库，并将代码提交到本地仓库\n7.2.3 新建分支\n7.2.4 删除分支\n八、问题归纳总结\n8.1 报错\n8.1.1 如果你本地有远程仓库的ssh的话，按照下方步骤来添加\n8.1.2 如果你本地没有远程仓库的ssh的话，先来添加\n一、Git的安装 1.1 图形化界面 https://pan.baidu.com/s/1oAf6Eu9iha6TPzaGHNsADQ\n安装过程：\n安装完成之后,在C:\\Users\\Administrator\\AppData\\Local\\Atlassian\\SourceTree目录下创建accounts.json文件，里面内容如下：\n再次打开SourceTree下载只被SourceTree识别的Git，不需要moun\u0026hellip;(会有提示，没有则不用管)\n1.2 命令行界面 https://pan.baidu.com/s/1SPqrbKJLRkzzz2c0jHkuJA\n二、本地仓库的创建与提交 2.1 图形化界面 2.1.1 首先在电脑上有一个空白目录 2.1.2 打开SourceTree 2.1.3 点击左边\u0026quot;克隆/新建\u0026quot;,创建本地仓库 2.1.4 选择第一步中的空白目录，点击\u0026quot;创建\u0026quot;按钮 2.1.5 此时左边会出现这个，代表本地仓库创建完成 2.1.6 打开空白目录，在空白目录下新建文件，文件(内容/名称)随便输入 2.1.7 返回SourceTree，会发现未暂存文件中有你刚才修改或增加的文件 2.1.8 右键“未暂存文件”中的文件，点击添加 2.1.9 会发现“未暂存文件”中的文件进入了“已暂存文件”中 2.1.10 在下方输入“本次提交的描述”，点击“提交按钮” 2.1.11 点击master分支，会显示本次提交的详细信息 2.2 命令行界面 2.2.1 点击SourceTree右上角的“命令行模式”即可打开命令行窗口 2.2.2 命令识别 三、工作流 3.1 图形化界面 3.1.1 首先在电脑上有一个空白目录 3.1.2 打开SourceTree 3.1.3 点击左边\u0026quot;克隆/新建\u0026quot;,创建本地仓库 3.1.4 选择第一步中的空白目录，点击\u0026quot;创建\u0026quot;按钮 3.1.5 打开刚才创建的demo2目录，在里面添加一个文件(内容自定) 3.1.6 打开SourceTree，将刚才修改的文件添加进暂存区 3.1.7 第一次提交 3.1.8 需求变更 背景：开发完成，要下班了，但是临时有个需求变更，将变更后的文件提交的暂存区\n修改内容，添加“需求变更”\n将修改后的文件添加到暂存区\n3.1.9 需求撤销 背景：第二天上班之后，产品经理说，昨天的需求不需要了\n直接丢弃掉暂存区的文件\n此时的需求变更已撤销\n3.1.10 第二天正式需求 背景：第二天下班之后，当天的需求已经完成，准备提交到git仓库\n3.1.11 第二天的需求提交到git仓库 3.1.12 第三天撤销需求 背景：第三天上班，发现第二天的需求没用，但是已经提交到线上仓库，可以通过重置当前分支到此次提交\n3.1.13 项目不需要了 在工作区直接删除该文件，打开sourceTree，虽然在本地删除了，但是线上仓库还有遗留。\n再次打开SourceTree，将动作提交至暂存区，然后再次提交到仓库\n到此，仓库才算干净\n3.2 命令行界面 3.2.1 先有一个空目录 3.2.2 在该空目录下打开git_bush 3.2.3 产品经理临时变更需求 3.2.4 将变更后的需求提交到暂存区 3.2.5 第二天上班之后，被产品经理告知该需求不需要 此时的工作区已经干净\n3.2.6 第二天结束 3.2.7 将第二天的工作内容提交到仓库 3.2.8 产品经理突然告知这个需求不要了 此时的工作区只有第一次提交的信息\n3.2.9 这个项目都不需要了 3.3 总结 四、远程仓库 4.1 github密钥生成 https://www.cnblogs.com/xue-shuai/p/11555150.html\n4.2 添加远程仓库 4.2.1 命令行界面 4.2.2 图形化界面 1）首先先提交到本地仓库\n** 2）将本地仓库与远程仓库关联，右键master分支，点击“创建拉去请求”**\n添加远程仓库地址\n点击确定\n3）出现origin说明本地仓库与远程仓库关联成功\n4）推送到远程仓库\n右键master，点击推送到origin\n直接推送即可\n五、克隆仓库 5.1 命令行操作 找一个空目录，输入\n1 git clone git@github*******.git 克隆远程仓库到本地\n5.2 图形化操作 点击克隆，输入远程仓库地址，找到放置的目录，点击克隆即可\n六、标签管理 6.1 标签命令 6.2 命令行方式实现 首先需要在远程GitHub上有一个仓库，并且克隆到本地，在该项目中执行下列命令\n查看远程GitHub中的标签\n更换Branches分支为Tags标签，即可查看拥有的标签，可进行版本的查看与切换。\n** 删除远程标签**\n6.3 图形化方式实现 首先先提交本地仓库到远程仓库，这里就不多说了，详情查看4.2.2\n推送完成之后，点击标签\n填写“标签名称”，选择“指定的提交”，勾选“推送标签”\n选择最新的修改，点击确定\n添加标签\n添加完成之后即可在GitHub上查看添加成功的标签\n删除标签，在sourceTree中右键所要删除的标签\n勾选“移除所有远程标签”，即可将远程仓库中该标签删除掉\n七、分支管理 假设需要实现一个功能，但是这个功能需要两周完成，第一周完成了50%，如果直接提交到远程仓库，别人的代码可能会出问题，但是如果等到全部完成在提交，可能进度上会有问题，所以只需要创建一个属于你自己的分支，自己所有的代码在该分支上完成。为保持工作树清洁，所创建的分支在合成到主分支之后，该分支便不再需要，可删除。\n7.1 命令行方式 在一个空目录下打开git bash\n7.1.1 提交到本地仓库 7.1.2 创建分支 7.1.2 删除分支 7.2 图形化界面 7.2.1 首先需要有一个本地目录，里面有一个文件，文件中有内容 7.2.2 打开SourceTree创建本地仓库，并将代码提交到本地仓库 具体就不细说了，详情请看2.1\n7.2.3 新建分支 点击“分支”\n** 输入新的分支名称，点击“创建分支”**\n右键仓库，选择“在资源管理器里打开”\n在新的分支上添加一些内容\n返回sourceTree，选择“未提交的更改”，将下方的“为暂存文件”提交到本地仓库\n接下来合并分支。双击master分支就会切换至master分支\n点击“合并”，选择要合并的分支，勾选“立即提交合并”，点击确定\n此时通过右键该仓库点击“在资源管理器中打开”，查看该文件\n** 我们发现原来futureY中修改的文件已经合并到了master分支上**\n7.2.4 删除分支 右键要删除的分支名，点击删除futureY\n勾选“强制删除”，ok即可删除该分支。\n八、问题归纳总结 8.1 报错 如果出现下类问题不要慌，是因为你的git没有ssh密钥与远程仓库关联\n8.1.1 如果你本地有远程仓库的ssh的话，按照下方步骤来添加 会自动帮你提取本地的ssh\n8.1.2 如果你本地没有远程仓库的ssh的话，先来添加 https://www.cnblogs.com/xue-shuai/p/11555150.html\n","date":"2019-12-05T13:45:27Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/201912051345/","title":"Git入门基础教程和SourceTree应用"},{"content":"1、什么是 nginx 高可用 （1）需要两台 nginx 服务器 （2）需要 keepalived （3）需要虚拟 ip 2、配置高可用的准备工作 （1）需要两台服务器 192.168.17.129 和 192.168.17.131\n（2）在两台服务器安装 nginx\n**（3）在两台服务器安装 keepalived **\n3、在两台服务器安装 keepalived （1）使用 yum 命令进行安\n1 yum install keepalived –y **（2）安装之后，在 etc 里面生成目录 keepalived，有文件 keepalived.conf **\n4、完成高可用配置（主从配置） **（1）修改/etc/keepalived/keepalivec.conf 配置文件 **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.17.129 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_script chk_http_port { script \u0026#34;/usr/local/src/nginx_check.sh\u0026#34; interval 2 #（检测脚本执行的间隔） weight 2 } vrrp_instance VI_1 { state BACKUP # 备份服务器上将 MASTER 改为 BACKUP interface ens33 //网卡 virtual_router_id 51 # 主、备机的 virtual_router_id 必须相同 priority 90 # 主、备机取不同的优先级，主机值较大，备份机值较小 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.17.50 // VRRP H 虚拟地址 } } （2）在/usr/local/src 添加检测脚本\n1 2 3 4 5 6 7 8 9 #!/bin/bash A=`ps -C nginx –no-header |wc -l` if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived fi fi （3）把两台服务器上 nginx 和 keepalived 启动\n启动 nginx：./nginx\n**启动 keepalived：systemctl start keepalived.service **\n5、最终测试 **（1）在浏览器地址栏输入 虚拟 ip 地址 192.168.17.50 **\n**（2）把主服务器（192.168.17.129）nginx 和 keepalived 停止，再输入 192.168.17.50 **\n相关博文： Nginx 简介与安装、常用的命令和配置文件 nginx 配置实例-反向代理 nginx 配置实例-负载均衡 Nginx 配置实例-动静分离 Nginx 配置高可用的集群","date":"2019-12-04T18:05:19Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/201912041805/","title":"Nginx 配置高可用的集群"},{"content":"**1、什么是动静分离 ** **通过 location 指定不同的后缀名实现不同的请求转发。通过 expires 参数设置，可以使浏 览器缓存过期时间，减少与服务器之前的请求和流量。具体 Expires 定义：是给一个资源 设定一个过期时间，也就是说无需去服务端验证，直接通过浏览器自身确认是否过期即可， 所以不会产生额外的流量。此种方法非常适合不经常变动的资源。（如果经常更新的文件， 不建议使用 Expires 来缓存），我这里设置 3d，表示在这 3 天之内访问这个 URL，发送一 个请求，比对服务器该文件最后更新时间没有变化，则不会从服务器抓取，返回状态码 304， 如果有修改，则直接从服务器重新下载，返回状态码 200。 **\n**2、准备工作 ** **（1）在 liunx 系统中准备静态资源，用于进行访问 **\n**3、具体配置 ** **（1）在 nginx 配置文件中进行配置 **\n4、最终测试 **（1）浏览器中输入地址 http://192.168.17.129/image/01.jpg **\n**（2）在浏览器地址栏输入地址 http://192.168.17.129/www/a.html **\n相关博文： Nginx 简介与安装、常用的命令和配置文件 nginx 配置实例-反向代理 nginx 配置实例-负载均衡 Nginx 配置实例-动静分离 Nginx 配置高可用的集群","date":"2019-12-04T17:57:02Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/201912041757/","title":"Nginx 配置实例-动静分离"},{"content":"我已经CSDN博客迁移到博客园\n博客园地址：https://www.cnblogs.com/heian99/\n","date":"2019-11-30T13:44:52Z","image":"https://www.ownit.top/title_pic/28.jpg","permalink":"https://www.ownit.top/p/201911301344/","title":"将博客搬至博客园"},{"content":"1、实现效果 （1）浏览器地址栏输入地址 http://www.123.com/edu/a.html，负载均衡效果，平均 8080 和 8081 端口中\n2、准备工作 （1）准备两台 tomcat 服务器，一台 8080，一台 8082\n（2）在两台 tomcat 里面 webapps 目录中，创建名称是 edu 文件夹，在 edu 文件夹中创建 页面 a.html，用于测试 3、在 nginx 的配置文件中进行负载均衡的配置 1 2 3 4 upstream heian{ server localhost:8080; server localhost:8082; } 4、nginx 分配服务器策略\n第一种 轮询（默认）\n每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除 **第二种 weight **\nweight 代表权重默认为 1,权重越高被分配的客户端越 指定轮询几率，weight 和访问比率成正比，用于后端服务器性能不均的情况。 例如： 1 2 3 4 upstream server_pool{ server 192.168.5.21 weight=10; server 192.168.5.22 weight=10; } **3、ip_hash **\n每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题。 例如： 1 2 3 4 upstream server_pool{ ip_hash; server 192.168.5.21:80; server 192.168.5.22:80; } **4、fair（第三方） **\n按后端服务器的响应时间来分配请求，响应时间短的优先分配\n1 2 3 4 5 upstream server_pool{ server 192.168.5.21:80; server 192.168.5.22:80; fair; } 相关博文： Nginx 简介与安装、常用的命令和配置文件 nginx 配置实例-反向代理 nginx 配置实例-负载均衡 Nginx 配置实例-动静分离 Nginx 配置高可用的集群","date":"2019-11-28T18:03:28Z","image":"https://www.ownit.top/title_pic/77.jpg","permalink":"https://www.ownit.top/p/201911281803/","title":"nginx 配置实例-负载均衡"},{"content":"反向代理实例一 虚拟机IP：192.168.116.129\n**实现效果：使用 nginx 反向代理，访问 www.123.com 直接跳转到 虚拟机的192.168.116.129:8080 **\n**实验代码 ** 1） 启动一个 tomcat，浏览器地址栏输入 192.168.116.129**:8080，出现如下界面 **\n2） 通过修改本地 host 文件，将 www.123.com 映射到****192.168.116.129\n配置完成之后，我们便可以通过 www.123.com:8080 访问到第一步出现的 Tomcat 初始界 面。那么如何只需要输入 www.123.com 便可以跳转到 Tomcat 初始界面呢？便用到 nginx 的反向代理。\n3） 在 nginx.conf 配置文件中增加如下配置 注意：修改配置文件后，需要重启nginx\n如上配置，我们监听 80 端口，访问域名为 www.123.com，不加端口号时默认为 80 端口，故 访问该域名时会跳转到 127.0.0.1:8080 路径上。在浏览器端输入 www.123.com 结果如下：\n反向代理实例二 虚拟机ip:192.168.116.129\n实现效果：使用 nginx 反向代理，根据访问的路径跳转到不同端口的服务中 nginx 监听端口为 9001，\n访问 http://192.168.116.129:9001/edu/ 直接跳转到 **192.168.116.129.**0.0.1:8080\n访问 http://192.168.116.129:9001/vod/ 直接跳转到 192.168.116.129**:8082 **\n实验代码 1、准备工作\n（1）准备两个 tomcat 服务器，一个 8080 端口，一个 8082 端口\n**（2）创建文件夹和测试页面 **\n2、具体配 修改 nginx 的配置文件\n**在 http 块中添加 server{} **\n1 2 3 4 5 6 7 8 9 10 11 server{ listen 9001; server_name localhost; location ~ /edu/ { proxy_pass http://localhost:8080; } location ~ /dev/ { proxy_pass http://localhost:8082; } } 重启\n**location 指令说明 ** 语法如下：\n** 1、= ：用于不含正则表达式的 uri 前，要求请求字符串与 uri 严格匹配，如果匹配 成功，就停止继续向下搜索并立即处理该请求。 ** ** 2、~：用于表示 uri 包含正则表达式，并且区分大小写。 ** ** 3、~*：用于表示 uri 包含正则表达式，并且不区分大小写。 ** ** 4、^~：用于不含正则表达式的 uri 前，要求 Nginx 服务器找到标识 uri 和请求字 符串匹配度最高的 location 后，立即使用此 location 处理请求，而不再使用 location 块中的正则 uri 和请求字符串做匹配。 ** ** 注意：如果 uri 包含正则表达式，则必须要有 ~ 或者 ~* 标识。 ** 我改了一行配置，会实现下面修改。 有兴趣的朋友可以试试（猜猜我改的那个地方）\n相关博文： Nginx 简介与安装、常用的命令和配置文件 nginx 配置实例-反向代理 nginx 配置实例-负载均衡 Nginx 配置实例-动静分离 Nginx 配置高可用的集群","date":"2019-11-28T15:15:18Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/201911281515/","title":"nginx 配置实例-反向代理"},{"content":"1、nginx 简介 （1）介绍 nginx 的应用场景和具体可以做什么事情\n（2）介绍什么是反向代理\n（3）介绍什么是负载均衡\n（4）介绍什么是动静分离\n2、nginx 安装 （1）介绍 nginx 在 linux 系统中如何进行安装\n3、nginx 常用的命令和配置文件 （1）介绍 nginx 启动、关闭、重新加载命令\n（2）介绍 nginx 的配置文件\n1.1 Nginx 概述 Nginx (\u0026ldquo;engine x\u0026rdquo;) 是一个高性能的 HTTP 和反向代理服务器,特点是占有内存少，并发能 力强，事实上 nginx 的并发能力确实在同类型的网页服务器中表现较好，中国大陆使用 nginx 网站用户有：百度、京东、新浪、网易、腾讯、淘宝等\n1.2 Nginx 作为 web 服务器 Nginx 可以作为静态页面的 web 服务器，同时还支持 CGI 协议的动态语言，比如 perl、php 等。但是不支持 java。Java 程序只能通过与 tomcat 配合完成。Nginx 专为性能优化而开发， 性能是其最重要的考量,实现上非常注重效率 ，能经受高负载的考验,有报告表明能支持高 达 50,000 个并发连接数。 https://lnmp.org/nginx.html\n1.3 正向代理 Nginx 不仅可以做反向代理，实现负载均衡。还能用作正向代理来进行上网等功能。 正向代理：如果把局域网外的 Internet 想象成一个巨大的资源库，则局域网中的客户端要访 问 Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理。\n1.4 反向代理 反向代理，其实客户端对代理是无感知的，因为客户端不需要任何配置就可以访问，我们只 需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后，在返 回给客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器 地址，隐藏了真实服务器 IP 地址。\n1.5 负载均衡 客户端发送多个请求到服务器，服务器处理请求，有一些可能要与数据库进行交互，服 务器处理完毕后，再将结果返回给客户端。\n这种架构模式对于早期的系统相对单一，并发请求相对较少的情况下是比较适合的，成 本也低。但是随着信息数量的不断增长，访问量和数据量的飞速增长，以及系统业务的复杂 度增加，这种架构会造成服务器相应客户端的请求日益缓慢，并发量特别大的时候，还容易 造成服务器直接崩溃。很明显这是由于服务器性能的瓶颈造成的问题，那么如何解决这种情 况呢？\n我们首先想到的可能是升级服务器的配置，比如提高 CPU 执行频率，加大内存等提高机 器的物理性能来解决此问题，但是我们知道摩尔定律的日益失效，硬件的性能提升已经不能 满足日益提升的需求了。最明显的一个例子，天猫双十一当天，某个热销商品的瞬时访问量 是极其庞大的，那么类似上面的系统架构，将机器都增加到现有的顶级物理配置，都是不能 够满足需求的。那么怎么办呢？\n上面的分析我们去掉了增加服务器物理配置来解决问题的办法，也就是说纵向解决问题 的办法行不通了，那么横向增加服务器的数量呢？这时候集群的概念产生了，单个服务器解 决不了，我们增加服务器的数量，然后将请求分发到各个服务器上，将原先请求集中到单个 java 课程系列服务器上的情况改为将请求分发到多个服务器上，将负载分发到不同的服务器，也就是我们 所说的负载均衡\n1.6 动静分离 为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速 度。降低原来单个服务器的压力。\n第 2 章 Nginx 安装 2.1 进入 nginx 官网，下载 http://nginx.org/\n2.2 安装 nginx 第一步，安装 pcre\n1 wget http://downloads.sourceforge.net/project/pcre/pcre/8.37/pcre-8.37.tar.gz 解压文件， ./configure 完成后，回到 pcre 目录下执行 make， 再执行 make install 第二步，安装 openssl\n第三步，安装 zlib\n1 yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 第四步，安装 nginx\n1、 解压缩 nginx-xx.tar.gz 包。 2、 进入解压缩目录，执行./configure。 3、 make \u0026amp;\u0026amp; make install 查看开放的端口号\nfirewall-cmd \u0026ndash;list-all 设置开放的端口号\nfirewall-cmd \u0026ndash;add-service=http –permanent sudo firewall-cmd \u0026ndash;add-port=80/tcp \u0026ndash;permanent 重启防火墙\nfirewall-cmd –reload 第 3 章 nginx 常用的命令和配置文件 3.1 nginx 常用的命令： （1）启动命令\n在/usr/local/nginx/sbin 目录下执行 ./nginx\n（2）关闭命令\n在/usr/local/nginx/sbin 目录下执行 ./nginx -s stop\n（3）重新加载命令\n在/usr/local/nginx/sbin 目录下执行 ./nginx -s reload\n3.2 nginx.conf 配置文件 nginx 安装目录下，其默认的配置文件都放在这个目录的 conf 目录下，而主配置文件 nginx.conf 也在其中，后续对 nginx 的使用基本上都是对此配置文件进行相应的修改\n配置文件中有很多#， 开头的表示注释内容，我们去掉所有以 # 开头的段落，精简之后的 内容如下：\n根据上述文件，我们可以很明显的将 nginx.conf 配置文件分为三部分： 第一部分：全局块 从配置文件开始到 events 块之间的内容，主要会设置一些影响 nginx 服务器整体运行的配置指令，主要包括配 置运行 Nginx 服务器的用户（组）、允许生成的 worker process 数，进程 PID 存放路径、日志存放路径和类型以 及配置文件的引入等。\n比如上面第一行配置的：\n1 worker_processes 1; 这是 Nginx 服务器并发处理服务的关键配置，worker_processes 值越大，可以支持的并发处理量也越多，但是 会受到硬件、软件等设备的制约\n第二部分：events 块 比如上面的配置：\n1 2 3 events { worker_connections 1024; } events 块涉及的指令主要影响 Nginx 服务器与用户的网络连接，常用的设置包括是否开启对多 work process 下的网络连接进行序列化，是否允许同时接收多个网络连接，选取哪种事件驱动模型来处理连接请求，每个 word process 可以同时支持的最大连接数等。\n上述例子就表示每个 work process 支持的最大连接数为 1024. 这部分的配置对 Nginx 的性能影响较大，在实际中应该灵活配置。 第三部分：http 块 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 http { include mime.types; default_type application/octet-stream; #log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; # \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; # \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; } } 这算是 Nginx 服务器配置中最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里。 需要注意的是：http 块也可以包括 http 全局块、server 块。\n①、http 全局块\nhttp 全局块配置的指令包括文件引入、MIME-TYPE 定义、日志自定义、连接超时时间、单链接请求数上限等。 ②、server 块\n这块和虚拟主机有密切关系，虚拟主机从用户角度看，和一台独立的硬件主机是完全一样的，该技术的产生是为了 节省互联网服务器硬件成本。\n每个 http 块可以包括多个 server 块，而每个 server 块就相当于一个虚拟主机。 而每个 server 块也分为全局 server 块，以及可以同时包含多个 locaton 块。 1、全局 server 块\n最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或 IP 配 2、location 块\n一个 server 块可以配置多个 location 块。 这块的主要作用是基于 Nginx 服务器接收到的请求字符串（例如 server_name/uri-string），对虚拟主机名称 （也可以是 IP 别名）之外的字符串（例如 前面的 /uri-string）进行匹配，对特定的请求进行处理。地址定向、数据缓 存和应答控制等功能，还有许多第三方模块的配置也在这里进行。\n相关博文： Nginx 简介与安装、常用的命令和配置文件 nginx 配置实例-反向代理 nginx 配置实例-负载均衡 Nginx 配置实例-动静分离 Nginx 配置高可用的集群","date":"2019-11-26T20:42:29Z","image":"https://www.ownit.top/title_pic/14.jpg","permalink":"https://www.ownit.top/p/201911262042/","title":"Nginx 简介与安装、常用的命令和配置文件"},{"content":"Centos7安装elasticSearch6 上面讲述了elasticSearch6的安装和使用教程。\n下面讲一下elasticsearch6的管理工具Kibana。\nKibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。\n你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。\n你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。\nKibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化\nKibana安装教程 1、拷贝kibana-5.6.4-linux-x86_64.tar 到/opt下 2、解压缩 3、进入kibana主目录的config目录下 4、修改配置文件 1 vim kibana.yml 5、启动，在 kibana主目录bin目录下执行 1 nohup ./kibana \u0026amp; 6、然后ctrl+c退出 执行ps -ef\n如上图,1757号进程就是kibana的进程\n7、用浏览器打开 点击左边菜单DevTools\n在Console中\n执行 get _cluster/health 右边的结果中，status为yellow或者green。\n表示es启动正常，并且与kibana连接正常。\nanalysis-ik安装教程【简单】 1、上传文件到服务器 2、解压 3、移动到ElasticSearch6下的plugins 4、重新启动就可以了 ","date":"2019-11-26T09:48:45Z","image":"https://www.ownit.top/title_pic/69.jpg","permalink":"https://www.ownit.top/p/201911260948/","title":"kibana和中文分词器analysis-ik的安装使用"},{"content":"好吧，这个问题比较low，但是记录一下，以免后期遗忘。\n说白了，这个问题就是端口被占用了。\n问题：\n1 2 qos-server can not bind localhost:22222, dubbo version: 2.6.0, current host: 127.0.0.1 java.net.BindException: Address already in use: bind 看翻译：【22222端口被占用】\n解决办法：\n解决端口被占用的问题（80、222222、3306）等等\n查看占用程序的pid\n1 netstat -ano | findstr 22222 通过pid，停止程序\n1 taskkill /f /pid 20720 看截图：\n","date":"2019-11-19T16:36:56Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/201911191636/","title":"qos-server can not bind localhost-22222, dubbo version- 2.6.0, current host- 127.0.0.1【问题解决】"},{"content":"今天，启动dubbo，开始写项目。\n在一个调用dubbo里面的一个方法时，程序一直调用，每次显示报红。\n很难搞。\n问题代码\n1 2 3 4 5 6 7 com.alibaba.dubbo.rpc.RpcException: Failed to invoke the method getAllSku in the service com.atguigu.gmall.service. SkuService. Tried 3 times of the providers [192.168.116.1:53684] (1/1) from the registry 47.98.231.26:2181 on the consumer 192.168.116.1 using the dubbo version 2.6.0. Last error is: Invoke remote method timeout. method: getAllSku, provider: dubbo://192.168.116.1:53684/com.atguigu.gmall.service 这么烦，已经搞崩心态了\n扔百度翻译去。\n【bug，真TM智障】\n靠，方法调用超时。\n解决办法：\n1 2 3 4 # 设置超时时间 spring.dubbo.consumer.timeout=600000 # 设置是否检查服务存在 spring.dubbo.consumer.check=false 可以了，终于找出bug了。真是不容易\n","date":"2019-11-19T16:18:24Z","image":"https://www.ownit.top/title_pic/40.jpg","permalink":"https://www.ownit.top/p/201911191618/","title":"Dubbo启动，调用方法失败【问题：调用超时】"},{"content":"刚刚写完一个项目，准备打包，却发现无法打包。\n然后认真排查了一下问题。发现少引入了一个插件。\n1 2 3 4 5 6 7 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;skip\u0026gt;true\u0026lt;/skip\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 引入这个插件就可以完美运行打包了。\n","date":"2019-11-18T09:42:22Z","image":"https://www.ownit.top/title_pic/42.jpg","permalink":"https://www.ownit.top/p/201911180942/","title":"Tests in error-BlogApplicationTests.initializationError » IllegalState Unable to find a @Spri...【解决】"},{"content":"作为一个程序员 经常会遇到端口被占用的问题，而无法启动项目。 这个事情让人很烦。【两种解决办法】 1、首先，需要确定那个端口被占用 示例：8080端口被占用【命令查看程序占用的pid】\n1 netstat -ano | findstr 8080 2、通过CMD命令利用PID可以杀掉占用的程序 1 taskkill /f /pid 92076 3、通过CMD查出那个凶手占用端口的名称 1 tasklist|findstr \u0026#34;80\u0026#34; 4、通过凶手的名称，杀掉程序 1 taskkill /f /im 程序名 ","date":"2019-11-15T17:53:34Z","image":"https://www.ownit.top/title_pic/55.jpg","permalink":"https://www.ownit.top/p/201911151753/","title":"解决端口被占用的问题（80、8080、3306）等等"},{"content":"Java实现MD5加密 为了保护有些数据，就需要采取一些手段来进行数据的加密，防止被别人破解。\nMD5简介 md5的全称是md5信息摘要算法（英文：MD5 Message-Digest Algorithm ），一种被广泛使用的密码散列函数，可以产生一个128位（16字节，1字节8位）的散列值（常见的是用32位的16进制表示，比如：0caa3b23b8da53f9e4e041d95dc8fa2c），用于确保信息传输的完整一致。\n功能实现 1、MD5的工具类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import java.security.MessageDigest; import java.security.NoSuchAlgorithmException; /** * 南宫乘风 */ public class MD5Utils { /** * MD5加密类 * @param str 要加密的字符串 * @return 加密后的字符串 */ public static String code(String str){ try { MessageDigest md = MessageDigest.getInstance(\u0026#34;MD5\u0026#34;); md.update(str.getBytes()); byte[]byteDigest = md.digest(); int i; StringBuffer buf = new StringBuffer(\u0026#34;\u0026#34;); for (int offset = 0; offset \u0026lt; byteDigest.length; offset++) { i = byteDigest[offset]; if (i \u0026lt; 0) i += 256; if (i \u0026lt; 16) buf.append(\u0026#34;0\u0026#34;); buf.append(Integer.toHexString(i)); } //32位加密 return buf.toString(); // 16位的加密 //return buf.toString().substring(8, 24); } catch (NoSuchAlgorithmException e) { e.printStackTrace(); return null; } } /** * 测试方法 * @param args */ public static void main(String[] args) { System.out.println(code(\u0026#34;111111\u0026#34;)); } } 最下面这个是测试方法\n2、项目中调用 我在项目的接口中调用MD5的方法\n1 2 3 4 public User checkUser(String username, String password) { User user = userRepository.findByUsernameAndPassword(username, MD5Utils.code(password)); return user; } 解析：这就是用户登录把密码传递过来，进行MD5加密后，在和数据库中的密码进行对比（数据库中的密码是MD5格式存储的）\n3、思路拓展（防止重要信息被盗用） 账号注册 密码登录 信息保护 资料加密 等等，许多的功能都可以用到这样的方法来进行加密。\n","date":"2019-11-14T18:00:16Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/201911141800/","title":"Java使用MD5加密算法，实现等登陆功能"},{"content":"我们准备设计博客，那就要设计数据库。\n我们可以使用Hibernate来自动生成数据库。\n博客数据库的结构：\n实体类：\n博客 Blog 博客分类 Type 博客标签 Tag 博客评论 Comment 用户 User 项目截图：\n首先，在pom.xml中添加以下的一些依赖:\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 这样就可以使用Hibernate框架了，下面实现自动创建数据库表的功能:\n打开application.properties文件添加以下的代码:\n1 2 3 4 jpa: hibernate: ddl-auto: update show-sql: true 这里除了update参数外还有其他的参数，这里解释一下:\n//1：value=\u0026ldquo;create-drop\u0026rdquo; 表示当JPA应用的时候自动创建表，在解应用的时候删除相应的表，这个在做测试的时候比较有用，但在开发过程中不这么用\n//2：value=\u0026ldquo;create\u0026quot;这个在每次应用启动的时候都会创建数据库表（会删除以前数据库里的数据。\n//3：value=\u0026ldquo;update\u0026rdquo; 这个属性的作用是a:每次只会更新数据库表里的信息\n下面是创建实体类的代码:\nBlog 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 package com.lrm.po; import javax.persistence.*; import java.util.ArrayList; import java.util.Date; import java.util.List; @Entity @Table(name = \u0026#34;t_blog\u0026#34;) public class Blog { @Id @GeneratedValue private Long id; private String title; private String content; private String firstPicture; private String flag; private Integer views; private boolean appreciation; private boolean shareStatement; private boolean commentabled; private boolean published; private boolean recommend; private Date createTime; private Date updateTime; @ManyToOne private Type type; @ManyToMany(cascade = {CascadeType.PERSIST}) private List\u0026lt;Tag\u0026gt; tags=new ArrayList\u0026lt;\u0026gt;(); @ManyToOne private User user; @OneToMany(mappedBy = \u0026#34;blog\u0026#34;) private List\u0026lt;Comment\u0026gt; comments = new ArrayList\u0026lt;\u0026gt;(); public Blog() { } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getTitle() { return title; } public void setTitle(String title) { this.title = title; } public String getContent() { return content; } public void setContent(String content) { this.content = content; } public String getFirstPicture() { return firstPicture; } public void setFirstPicture(String firstPicture) { this.firstPicture = firstPicture; } public String getFlag() { return flag; } public void setFlag(String flag) { this.flag = flag; } public Integer getViews() { return views; } public void setViews(Integer views) { this.views = views; } public boolean isAppreciation() { return appreciation; } public void setAppreciation(boolean appreciation) { this.appreciation = appreciation; } public boolean isShareStatement() { return shareStatement; } public void setShareStatement(boolean shareStatement) { this.shareStatement = shareStatement; } public boolean isCommentabled() { return commentabled; } public void setCommentabled(boolean commentabled) { this.commentabled = commentabled; } public boolean isPublished() { return published; } public void setPublished(boolean published) { this.published = published; } public boolean isRecommend() { return recommend; } public void setRecommend(boolean recommend) { this.recommend = recommend; } public Date getCreateTime() { return createTime; } public void setCreateTime(Date createTime) { this.createTime = createTime; } public Date getUpdateTime() { return updateTime; } public void setUpdateTime(Date updateTime) { this.updateTime = updateTime; } public Type getType() { return type; } public void setType(Type type) { this.type = type; } public List\u0026lt;Tag\u0026gt; getTags() { return tags; } public void setTags(List\u0026lt;Tag\u0026gt; tags) { this.tags = tags; } public User getUser() { return user; } public void setUser(User user) { this.user = user; } public List\u0026lt;Comment\u0026gt; getComments() { return comments; } public void setComments(List\u0026lt;Comment\u0026gt; comments) { this.comments = comments; } @Override public String toString() { return \u0026#34;Blog{\u0026#34; + \u0026#34;id=\u0026#34; + id + \u0026#34;, title=\u0026#39;\u0026#34; + title + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, content=\u0026#39;\u0026#34; + content + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, firstPicture=\u0026#39;\u0026#34; + firstPicture + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, flag=\u0026#39;\u0026#34; + flag + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, views=\u0026#34; + views + \u0026#34;, appreciation=\u0026#34; + appreciation + \u0026#34;, shareStatement=\u0026#34; + shareStatement + \u0026#34;, commentabled=\u0026#34; + commentabled + \u0026#34;, published=\u0026#34; + published + \u0026#34;, recommend=\u0026#34; + recommend + \u0026#34;, createTime=\u0026#34; + createTime + \u0026#34;, updateTime=\u0026#34; + updateTime + \u0026#39;}\u0026#39;; } } Comment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 package com.lrm.po; import javax.persistence.*; import java.util.ArrayList; import java.util.Date; import java.util.List; @Entity @Table(name = \u0026#34;t_comment\u0026#34;) public class Comment { @Id @GeneratedValue private Long id; private String nickname; private String email; private String content; private String avatar; @Temporal(TemporalType.TIMESTAMP) private Date createTime; @ManyToOne private Blog blog; @OneToMany(mappedBy = \u0026#34;parentComment\u0026#34;) private List\u0026lt;Comment\u0026gt; replyComments = new ArrayList\u0026lt;\u0026gt;(); @ManyToOne private Comment parentComment; public Comment() { } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getNickname() { return nickname; } public void setNickname(String nickname) { this.nickname = nickname; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; } public String getContent() { return content; } public void setContent(String content) { this.content = content; } public String getAvatar() { return avatar; } public void setAvatar(String avatar) { this.avatar = avatar; } public Date getCreateTime() { return createTime; } public void setCreateTime(Date createTime) { this.createTime = createTime; } public Blog getBlog() { return blog; } public void setBlog(Blog blog) { this.blog = blog; } public List\u0026lt;Comment\u0026gt; getReplyComments() { return replyComments; } public void setReplyComments(List\u0026lt;Comment\u0026gt; replyComments) { this.replyComments = replyComments; } public Comment getParentComment() { return parentComment; } public void setParentComment(Comment parentComment) { this.parentComment = parentComment; } @Override public String toString() { return \u0026#34;Comment{\u0026#34; + \u0026#34;id=\u0026#34; + id + \u0026#34;, nickname=\u0026#39;\u0026#34; + nickname + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, email=\u0026#39;\u0026#34; + email + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, content=\u0026#39;\u0026#34; + content + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, avatar=\u0026#39;\u0026#34; + avatar + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, createTime=\u0026#34; + createTime + \u0026#39;}\u0026#39;; } } Tag 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package com.lrm.po; import javax.persistence.*; import java.util.ArrayList; import java.util.List; @Entity @Table(name = \u0026#34;t_tag\u0026#34;) public class Tag { @Id @GeneratedValue private Long id; private String name; @ManyToMany(mappedBy = \u0026#34;tags\u0026#34;) private List\u0026lt;Blog\u0026gt; blogs = new ArrayList\u0026lt;\u0026gt;(); public Tag() { } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public List\u0026lt;Blog\u0026gt; getBlogs() { return blogs; } public void setBlogs(List\u0026lt;Blog\u0026gt; blogs) { this.blogs = blogs; } @Override public String toString() { return \u0026#34;Tag{\u0026#34; + \u0026#34;id=\u0026#34; + id + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } Type 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package com.lrm.po; import javax.persistence.*; import java.util.ArrayList; import java.util.List; @Entity @Table(name = \u0026#34;t_type\u0026#34;) public class Type { @Id @GeneratedValue private long id; private String name; @OneToMany(mappedBy = \u0026#34;type\u0026#34;) private List\u0026lt;Blog\u0026gt; blogs = new ArrayList\u0026lt;\u0026gt;(); public Type() { } public long getId() { return id; } public void setId(long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public List\u0026lt;Blog\u0026gt; getBlogs() { return blogs; } public void setBlogs(List\u0026lt;Blog\u0026gt; blogs) { this.blogs = blogs; } @Override public String toString() { return \u0026#34;Type{\u0026#34; + \u0026#34;id=\u0026#34; + id + \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } User 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 package com.lrm.po; import javax.persistence.*; import java.util.ArrayList; import java.util.Date; import java.util.List; /** * Created by limi on 2017/10/14. */ @Entity @Table(name = \u0026#34;t_user\u0026#34;) public class User { @Id @GeneratedValue private Long id; private String nickname; private String username; private String password; private String email; private String avatar; private Integer type; @Temporal(TemporalType.TIMESTAMP) private Date createTime; @Temporal(TemporalType.TIMESTAMP) private Date updateTime; @OneToMany(mappedBy = \u0026#34;user\u0026#34;) private List\u0026lt;Blog\u0026gt; blogs = new ArrayList\u0026lt;\u0026gt;(); public User() { } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getNickname() { return nickname; } public void setNickname(String nickname) { this.nickname = nickname; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; } public String getAvatar() { return avatar; } public void setAvatar(String avatar) { this.avatar = avatar; } public Integer getType() { return type; } public void setType(Integer type) { this.type = type; } public Date getCreateTime() { return createTime; } public void setCreateTime(Date createTime) { this.createTime = createTime; } public Date getUpdateTime() { return updateTime; } public void setUpdateTime(Date updateTime) { this.updateTime = updateTime; } public List\u0026lt;Blog\u0026gt; getBlogs() { return blogs; } public void setBlogs(List\u0026lt;Blog\u0026gt; blogs) { this.blogs = blogs; } @Override public String toString() { return \u0026#34;User{\u0026#34; + \u0026#34;id=\u0026#34; + id + \u0026#34;, nickname=\u0026#39;\u0026#34; + nickname + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, username=\u0026#39;\u0026#34; + username + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, password=\u0026#39;\u0026#34; + password + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, email=\u0026#39;\u0026#34; + email + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, avatar=\u0026#39;\u0026#34; + avatar + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, type=\u0026#34; + type + \u0026#34;, createTime=\u0026#34; + createTime + \u0026#34;, updateTime=\u0026#34; + updateTime + \u0026#39;}\u0026#39;; } } OK，既然，实体类已将创建完毕，运行接口。就可以生成数据库。但是前提，你是配置数据库\n1 2 3 4 5 6 spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/blog?useUnicode=true\u0026amp;characterEncoding=utf-8 username: root password: root 测试结果： 真的很nice，确实是一波骚操作。\n","date":"2019-11-11T17:52:14Z","image":"https://www.ownit.top/title_pic/13.jpg","permalink":"https://www.ownit.top/p/201911111752/","title":"SpringBoot使用Hibernate，实现自动创建数据库表【博客数据库设计】"},{"content":"Elasticsearch6.0 1、Elasticsearch： Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。\n特点：\n分布式的实时文件存储，每个字段都被索引并可被搜索 分布式的实时分析搜索引擎--做不规则查询 可以扩展到上百台服务器，处理PB级结构化或非结构化数据 Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。\nES能做什么？\n全文检索（全部字段）、模糊查询（搜索）、数据分析（提供分析语法，例如聚合）\n2、Elasticsearch使用案例 （1）2013年初，GitHub抛弃了Solr，采取ElasticSearch 来做PB级的搜索。 “GitHub使用ElasticSearch搜索20TB的数据，包括13亿文件和1300亿行代码”\n（2）维基百科：启动以elasticsearch为基础的核心搜索架构SoundCloud：“SoundCloud使用ElasticSearch为1.8亿用户提供即时而精准的音乐搜索服务”\n（3）百度：百度目前广泛使用ElasticSearch作为文本数据分析，采集百度所有服务器上的各类指标数据及用户自定义数据，通过对各种数据进行多维分析展示，辅助定位分析实例异常或业务层面异常。目前覆盖百度内部20多个业务线（包括casio、云分析、网盟、预测、文库、直达号、钱包、风控等），单集群最大100台机器，200个ES节点，每天导入30TB+数据\n（4）新浪使用ES 分析处理32亿条实时日志\n（5）阿里使用ES 构建挖财自己的日志采集和分析体系\n3、同类产品 Solr、ElasticSearch、Hermes（腾讯）（实时检索分析）\nSolr、ES 1. 源自搜索引擎，侧重搜索与全文检索。\n2. 数据规模从几百万到千万不等，数据量过亿的集群特别少。\n有可能存在个别系统数据量过亿，但这并不是普遍现象（就像Oracle的表里的数据规模有可能超过Hive里一样，但需要小型机）。\nHermes 1. 一个基于大索引技术的海量数据实时检索分析平台。侧重数据分析。\n2. 数据规模从几亿到万亿不等。最小的表也是千万级别。\n在 腾讯17 台TS5机器，就可以处理每天450亿的数据(每条数据1kb左右)，数据可以保存一个月之久。\nSolr、ES区别 全文检索、搜索、分析。基于lucene\nSolr 利用 Zookeeper 进行分布式管理，而 Elasticsearch 自身带有分布式协调管理功能; Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式； Solr 官方提供的功能更多，而 Elasticsearch 本身更注重于核心功能，高级功能多有第三方插件提供； Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch\u0026mdash;\u0026ndash;附近的人 Lucene是一个开放源代码的全文检索引擎工具包，但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎\n搜索引擎产品简介\n1 搜索引擎 elasticSearch6（和elasticSearch5的区别在于，root用户权限、一个库能否建立多个表）\n软件链接：https://www.lanzous.com/i73b6pc\n2 搜索引擎 文本搜索(以空间换时间算法)\n于同类产品相比(solr、hermes),和solr一样都是基于lucene(apache)，默认以集群方式工作\n搜索引擎(以百度和goole为例)的工作原理是什么？\na 爬虫\nb 分析\nc 查询\n3 elasticSearch(搜索引擎)的算法 倒排索引(在内容上建立索引，用内容去匹配索引)\nBtree（balance tree b-tree）\nB+tree\n4、Elasticsearch安装教程 1、准备工作 安装Centos7、建议内存2G以上、安装java1.8环境\n2、设置IP地址（要有固定的IP） 3、Java环境安装 解压安装包 1 [root@localhost jdk1.8]# tar -zxvf jdk-8u171-linux-x64.tar.gz 2、在文件最后添加\n1 2 3 4 export JAVA_HOME=/opt/es/jdk1.8.0_152 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/LIB:$JRE_HOME/LIB:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 3、查看java安装成功\n4、配置文件 elasticSearch.yml（集群配置文件）、jvm.Opitons(jvm配置文件)\n5、 创建目录 ，上传文件和解压 创建\n1 mkdir -p /opt/es 上传\n解压\n1 tar -zxvf elasticsearch-6.3.1.tar.gz 6、 配置 es使用最大线程数、最大内存数、访问的最大文件数\n需要改成其他非用户启动 创建新用户\n1 [root@wei bin]# adduser es 切换es用户下（成功）\n1 2 [root@wei bin]# su es [es@wei bin]$ 7、启动 Es的权限问题：\n首先用root用户解压\n然后用root用户授权\n整个文件夹全部授权（R：表示循环授权）\n1 [root@wei es]# chmod 777 -R elasticsearch-6.3.1 启动后配置\nelasticSearch.yml、jvm.Opitons\nes使用的jvm的内存大小\nelasticSearch.yml中配置es的host地址(配成本机地址，允许访问)\n配置完毕启动es(必须切换到非root用户下)\n8、启动后会报错(linux的默认线程数、最大文件数、最大内存数都不够) 9、 修改linux的配置(配合es的启动需求) 两处修改\nA修改linux的limits配置文件，设置内存线程和文件\n1 2 3 4 * hard nofile 655360 * soft nofile 131072 * hard nproc 4096 * soft nproc 2048 B修改linux的sysctl配置文件，配置系统使用内存\n1 2 vm.max_map_count=655360 fs.file-max=655360 整个es的配置有四处文件需要修改 elasticSearch.yml es的启动host地址 jvm.options配置es的虚拟机内存 limits.conf配置linux的线程内存和文件 sysctl.conf配置系统允许的软件运行内存 10、成功访问 ","date":"2019-10-31T13:26:55Z","image":"https://www.ownit.top/title_pic/03.jpg","permalink":"https://www.ownit.top/p/201910311326/","title":"Centos7安装elasticSearch6"},{"content":"Redsi常见问题 缓存在高平发和安全压力下的一些问题\n缓存击穿 是某一个热点key在高并发访问的情况下，突然失效，导致大量的并发大金mysql数据库的情况\n缓存穿透 是利用redis和mysql的机制（redis缓存一旦不存在，就访问mysql），直接让过缓存访问mysql，而制造的db请求压力\n一般在代码中防止\n解决： 为防止缓存穿透，将null或者空字符串设置给redis\n缓存雪崩 缓存是采用了相同的过期时间，导致缓存在某一时刻同时全部失效，导致的db崩溃\n解决：设计不同的缓存失效时间\n如何解决缓存击穿的问题 ？ 分布式锁\n穿透：利用不存在的key去攻击mysql数据库\n雪崩：缓存中的很多key失效，导致数据库负载过重宕机\n击穿：在正常的访问情况下，如果缓存失效，如果保护mysql，重启缓存的过程\n使用redis数据库的分布式锁，解决mysql的访问压力问题\n第一种分布式锁：redis自带一个分布式锁，set px nx 用完之后需要删除，不然别人不能访问。\n错误自旋代码（B为孤儿线程）\n正确自旋代码\n问题1 如果在redis中的锁已经过期了，然后锁过期的那个请求又执行完毕，回来删锁,删除了其他线程的锁，怎么办？\n问题2 如果碰巧在查询redis锁还没删除的时候，正在网络传输时，锁过期了，怎么办？\n1 2 String script =\u0026#34;if redis.call(\u0026#39;get\u0026#39;, KEYS[1]) == ARGV[1] then return redis.call(\u0026#39;del\u0026#39;, KEYS[1]) else return 0 end\u0026#34;; jedis.eval(script, Collections.singletonList(\u0026#34;lock\u0026#34;),Collections.singletonList(token)); 第二种分布式锁：redisson框架，一个redis的带有juc的lock功能的客户端（既有jedis的功能，又有juc的功能） 整合 引入pom\n1 2 3 4 5 6 \u0026lt;!-- https://mvnrepository.com/artifact/org.redisson/redisson --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.10.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置\n1 2 spring.redis.host=192.168.159.130 spring.redis.port=6379 配置类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Configuration public class GmallRedissonConfig { @Value(\u0026#34;${spring.redis.host}\u0026#34;) private String host; @Value(\u0026#34;${spring.redis.port}\u0026#34;) private String port; @Bean public RedissonClient redissonClient(){ Config config = new Config(); config.useSingleServer().setAddress(\u0026#34;redis://\u0026#34;+host+\u0026#34;:\u0026#34;+port); RedissonClient redisson = Redisson.create(config); return redisson; } } Redisson实现了juc的lock锁，并且可以在分布式的redis环境下使用\n添加gmall-redisson-text 把需要的依赖添加的pom上\n配置application.properties\n1 2 3 4 # 服务端口 server.port=8082 # 日志级别 logging.level.root=info RedissonController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package com.atguigu.gmallredisson.redissonTest; import com.atguigu.gmall.util.RedisUtil; import org.apache.commons.lang3.StringUtils; import org.redisson.api.RLock; import org.redisson.api.RedissonClient; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; import redis.clients.jedis.Jedis; @Controller public class RedissonController { @Autowired RedisUtil redisUtil; @Autowired RedissonClient redissonClient; @RequestMapping(\u0026#34;testRedisson\u0026#34;) @ResponseBody public String testRedisson(){ Jedis jedis = redisUtil.getJedis(); RLock lock = redissonClient.getLock(\u0026#34;lock\u0026#34;);// 声明锁 lock.lock();//上锁 try { String v = jedis.get(\u0026#34;k\u0026#34;); if (StringUtils.isBlank(v)) { v = \u0026#34;1\u0026#34;; } System.out.println(\u0026#34;-\u0026gt;\u0026#34; + v); jedis.set(\u0026#34;k\u0026#34;, (Integer.parseInt(v) + 1) + \u0026#34;\u0026#34;); }finally { jedis.close(); lock.unlock();// 解锁 } return \u0026#34;success\u0026#34;; } } 设置非单例模式启动 点击取消单一示例\n启动3个示例，端口分别为8080,8081,8082 配置nginx 下载安装apache测试工具(apache) 1 下载地址 https://www.apachehaus.com/cgi-bin/download.plx\n2 安装即解压 3 修改apache服务的配置文件(服务器的根目录) 修改服务的根目录路径：\n4 启动服务 查看443端口是否被占用\n1 D:\\ap\\Apache24\\bin\u0026gt;netstat -ano | findstr \u0026#34;443\u0026#34; 没有被占用，启动httpd.exe\n1 D:\\ap\\Apache24\\bin\u0026gt;httpd.exe 5 压测命令(另起cmd输入命令) D:\\apache24\\bin\u0026gt;ab -c 200 -n 1000 http:nginx负载均衡/压力方法\n测试：\n1 D:\\ap\\Apache24\\bin\u0026gt;ab -c 100 -n 10000 http://www.der-matech.com.cn/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public String testRedisson(){ Jedis jedis = redisUtil.getJedis(); RLock lock = redissonClient.getLock(\u0026#34;lock\u0026#34;);// 声明锁 lock.lock();//上锁 try { String v = jedis.get(\u0026#34;k\u0026#34;); if (StringUtils.isBlank(v)) { v = \u0026#34;1\u0026#34;; } System.out.println(\u0026#34;-\u0026gt;\u0026#34; + v); jedis.set(\u0026#34;k\u0026#34;, (Integer.parseInt(v) + 1) + \u0026#34;\u0026#34;); }finally { jedis.close(); lock.unlock();// 解锁 } return \u0026#34;success\u0026#34;; } ","date":"2019-10-31T12:45:14Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/201910311245/","title":"Redsi缓存问题（穿透，击穿，雪崩）以及解决办法（分布式锁）【高并发问题】"},{"content":"目录\nRedis缓存\n使用缓存Redis解决首页并发问题\n1、缓存使用的简单设计\n2、Redis的整合步骤\nA 将Redis整合到项目中（Redis+Spring）\nB 设计一个数据存储策越\n3、Redis的整合过程\n1、引入pom依赖信息（将本工程所有的Redis统一放入service-util里）\n2、写一个Redis的工具类（用来将Redis的池初始化到spring容器）\n3、写一个spring整合Redis的配置类\n4、每隔引用工程引入service-util后，单独配置只能的redis的配置文件\n代码\n查询结果\n查看Redis数据库的数据\nRedis缓存 重点要讲的是另外一个层面：尽量避免直接查询数据库。\n解决办法就是：缓存\n缓存可以理解是数据库的一道保护伞，任何请求只要能在缓存中命中，都不会直接访问数据库。而缓存的处理性能是数据库10-100倍。\n使用缓存Redis解决首页并发问题 用户第一次请求到redis 如果redis没有数据，redis会请求mysql mysql会把数据返回给用户，同时会传到redis上 第二次用户访问时，redis有数据，就不需要访问mysql。节省时间，降低消耗 1、缓存使用的简单设计 连接缓存 查询缓存 如果缓存没有，查询mysql mysql查询结果存入redis 2、Redis的整合步骤 A 将Redis整合到项目中（Redis+Spring） B 设计一个数据存储策越 企业中的存储策越（核心是：如何设计k）\n数据对象名：数据对象id：对象属性\nUser:123:password 用户ID为123的密码\nUser:123:userename 用户ID为123的名字\n3、Redis的整合过程 1、引入pom依赖信息（将本工程所有的Redis统一放入service-util里） 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 创建两个类RedisConfig和RedisUtil\nRedisConfig负责在spring容器启动时自动注入，而RedisUtil就是被注入的工具类以供其他模块调用。\n2、写一个Redis的工具类（用来将Redis的池初始化到spring容器） RedisUtil\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class RedisUtil { private JedisPool jedisPool; public void initPool(String host,int port ,int database){ JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(200); poolConfig.setMaxIdle(30); poolConfig.setBlockWhenExhausted(true); poolConfig.setMaxWaitMillis(10*1000); poolConfig.setTestOnBorrow(true); jedisPool=new JedisPool(poolConfig,host,port,20*1000); } public Jedis getJedis(){ Jedis jedis = jedisPool.getResource(); return jedis; } } 3、写一个spring整合Redis的配置类 将Redis的链接池创建到spring的容器中\nRedisConfig\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Configuration public class RedisConfig { //读取配置文件中的redis的ip地址 @Value(\u0026#34;${spring.redis.host:disabled}\u0026#34;) private String host; @Value(\u0026#34;${spring.redis.port:0}\u0026#34;) private int port; @Value(\u0026#34;${spring.redis.database:0}\u0026#34;) private int database; @Bean public RedisUtil getRedisUtil(){ if(host.equals(\u0026#34;disabled\u0026#34;)){ return null; } RedisUtil redisUtil=new RedisUtil(); redisUtil.initPool(host,port,database); return redisUtil; } } 4、每隔引用工程引入service-util后，单独配置只能的redis的配置文件 Service-util的配置文件没有作用\n同时，任何模块想要调用redis都必须在application.properties配置，否则不会进行注入\n1 2 3 4 5 6 7 #Redis配置 //读取配置文件中的redis的ip地址 spring.redis.host=192.168.1.111 #Redis端口号 spring.redis.port=6379 #数据库 spring.redis.database=0 代码 这是从数据库调用mysql，查询数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 /** * 从数据库调用 * * @param skuId * @return */ public PmsSkuInfo getSkuByIdFromDb(String skuId) { //sku的商品对象 PmsSkuInfo pmsSkuInfo = new PmsSkuInfo(); pmsSkuInfo.setId(skuId); PmsSkuInfo skuInfo = pmsSkuInfoMapper.selectOne(pmsSkuInfo); try { //sku的图片集合 PmsSkuImage pmsSkuImage = new PmsSkuImage(); List\u0026lt;PmsSkuImage\u0026gt; pmsSkuImages = pmsSkuImageMapper.select(pmsSkuImage); skuInfo.setSkuImageList(pmsSkuImages); } catch (Exception e) { e.printStackTrace(); } return skuInfo; } 这个是Redis的代码，判断redis中是否有数据，\n如果没有，就调用上面的代码，查询mysql数据库。返回结果，在写入redis数据库中。\n如果有，直接调用redis数据库中的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /** * 商品详细图 * 主要是item前端的东西，调用此处的服务，方便 * 使用Redis缓存，解决高并发 * * @param skuId * @return */ @Override public PmsSkuInfo getSkuById(String skuId) { PmsSkuInfo pmsSkuInfo = new PmsSkuInfo(); //链接缓存 Jedis jedis = redisUtil.getJedis(); //查询缓存 String skuKey = \u0026#34;sky:\u0026#34; + skuId + \u0026#34;:info\u0026#34;; String skuJson = jedis.get(\u0026#34;skuKey\u0026#34;); //可以吧json的字符串转换成jav的对象类 if (StringUtils.isNotBlank(skuJson)) {// if (skuJson!=null\u0026amp;\u0026amp;!skuJson.equals(\u0026#34;\u0026#34;)) pmsSkuInfo = JSON.parseObject(skuJson, PmsSkuInfo.class); } else { //如果缓存没有，查询mysql pmsSkuInfo = getSkuByIdFromDb(skuId); if (pmsSkuInfo != null) { //mysql查询结果存入redis jedis.set(\u0026#34;sku\u0026#34; + skuId + \u0026#34;:info\u0026#34;, JSON.toJSONString(pmsSkuInfo)); } } jedis.close(); return pmsSkuInfo; } 查询结果 查看Redis数据库的数据 ","date":"2019-10-28T10:10:11Z","image":"https://www.ownit.top/title_pic/14.jpg","permalink":"https://www.ownit.top/p/201910281010/","title":"Redis缓存实战教程"},{"content":"目录\nRedis安装教程\nredis介绍\nredis的应用场景 yum安装redis\n安装\n安装完毕后，使用下面的命令启动redis服务\n进入redis服务\n修改redis默认端口和密码\n查看Redis进程\n重启redis服务器\n登录Redis服务器\n语法\n语法\nRedis安装教程 redis介绍 redis是用C语言开发的一个开源的高性能键值对（key-value）数据库。它通过提供多种键值数据类型来适应不同场景下的存储需求，目前为止redis支持的键值数据类型如下字符串、列表（lists）、集合（sets）、有序集合（sorts sets）、哈希表（hashs）\nredis的应用场景 缓存（数据查询、短连接、新闻内容、商品内容等等）。（最多使用） 分布式集群架构中的session分离。 聊天室的在线好友列表。 任务队列。（秒杀、抢购、12306等等） 应用排行榜。 网站访问统计。 数据过期处理（可以精确到毫秒） yum安装redis 安装 1 2 3 4 5 6 #检查是否有redis yum 源 yum install redis #下载fedora的epel仓库 yum install epel-release #安装redis数据库 yum install redis 安装完毕后，使用下面的命令启动redis服务 1 2 3 4 5 6 7 8 # 启动redis service redis start # 停止redis service redis stop # 查看redis运行状态 service redis status # 查看redis进程 ps -ef | grep redis 进入redis服务 1 2 3 4 # 进入本机redis redis-cli # 列出所有key keys * 修改redis默认端口和密码 1、打开配置文件\n1 vi /etc/redis.conf 2、修改默认端口，查找 port 6379 修改为相应端口即可\n3、修改默认密码，查找 requirepass foobared 将 foobared 修改为你的密码\n4、使用配置文件启动 redis\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 查找Redis配置（注意不是安装目录下的redis.conf） # 打开第五步设计的Redis配置，默认为：/etc/redis/6379.conf # 修改配置文件如下几项，其它保持不变 daemonize yes #bind 127.0.0.1 （注释，不限制IP） protected-mode no 将 requirepass foobared前的“#”去掉，密码改为你想要设置的密码（我设置为123456） # 重启服务 [root@172 redis-3.2.11]# service redis_6379 restart Stopping ... Redis stopped Starting Redis server... # 开放6379端口 firewall-cmd --zone=public --add-port=6379/tcp --permanent # 重启防火墙，否则开放端口不起作用 firewall-cmd --reload 查看Redis进程 1 ps -ef|grep redis 重启redis服务器 1 systemctl restart redis 登录Redis服务器 Redis 命令用于在 redis 服务上执行操作。\n要在 redis 服务上执行命令需要一个 redis 客户端。Redis 客户端在我们之前下载的的 redis 的安装包中。\n语法 Redis 客户端的基本语法为：\n1 $ redis-cli 本地登录\n1 redis-cli 如果需要在远程 redis 服务上执行命令，同样我们使用的也是 redis-cli 命令。\n语法 1 $ redis-cli -h host -p port -a password 远程登录\n1 2 3 redis-cli -h 192.168.116.129 -p 6379 192.168.116.129:6379\u0026gt; keys * 1) \u0026#34;hello\u0026#34; ","date":"2019-10-26T17:38:01Z","image":"https://www.ownit.top/title_pic/63.jpg","permalink":"https://www.ownit.top/p/201910261738/","title":"Redis安装教程"},{"content":"目录\n安装步骤\n1、查看Docker的版本\n​\n2、安装 Docker\n3、启动Docker\n4、设置为开启启动\n5、查看Docker安装信息\n6、使用Docker 中国加速器\n安装步骤 安装操作系统，最小化安装（mini）\n禁用Selinux, 修改/etc/selinux/config 文件，将SELINUX=enforcing改为SELINUX=disabled，重启机器即可\n1、查看Docker的版本 1 [root@wei ~]# yum list docker 2、安装 Docker 1 [root@wei ~]# yum install -y docker 3、启动Docker 1 [root@wei ~]# systemctl start docker 4、设置为开启启动 1 [root@wei ~]# systemctl enable docker 5、查看Docker安装信息 1 [root@wei ~]# docker version 6、使用Docker 中国加速器 由于网络原因，我们在pull Image 的时候，从Docker Hub上下载会很慢。\n修改文件\n1 2 3 4 5 6 vi /etc/docker/daemon.json #添加后： { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://registry.docker-cn.com\u0026#34;], \u0026#34;live-restore\u0026#34;: true } 重起docker服务\n1 systemctl restart docker ","date":"2019-10-26T09:50:55Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/201910260950/","title":"CentOS 7上安装Docker"},{"content":"目录\nDocker介绍\nDocker优点\n虚拟机缺点\n容器和虚拟机\nDocker产生的目的就是解决以下问题\nDocker的用途\nDocker镜像\nDocker容器\n仓库\nDocker介绍 Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。\nDocker优点 灵活：即使是复杂的应用程序也可封装。 轻量级：容器利用并共享主机内核。 便携式：您可以在本地构建，部署到云上并在任何地方运行。 可扩展性：您可以增加和自动分发容器副本。 可堆叠：您可以垂直堆叠服务并及时并及时堆叠服务。 虚拟机缺点 **资源占用多：**虚拟机会独占一部分内存和硬盘空间。它运行的时候，其他程序就不能使用这些资源了。哪怕虚拟机里面的应用程序，真正使用的内存只有1M，虚拟机依然需要几百MB的内容才能运行。 **冗余步骤多：**虚拟机是完整的操作系统，一些系统级别的操作步骤，往往无法跳过，比如用户登录。 **启动慢：**启动操作系统需要多久，启动虚拟机就需要多久。可能要等几分钟，应用陈故乡才能真正运行。 容器和虚拟机 一个容器中运行原生Linux和共享主机与其它容器的内核，它运行一个独立的进程，不占用任何其它可执行文件的内存，使其轻量化。\n相比之下，虚拟机(VM)运行一个完整的“客户”操作系统，通过虚拟机管理程序虚拟访问主机资源。一般来说，虚拟机提供的环境比大多数应用程序需要的资源多\nDocker产生的目的就是解决以下问题 环境管理复杂：从各种OS到各个中间件再到各种App,一款产品能够成功发布，作为开发者需要关心的东西太多，且难于管理，这个问题在软件兴业中普遍存在并需要直接面对。Docker可以简化部署多种应用实例工作，比如Web应用、后台应用、数据库应用、大数据应用比如Hadoop集群、消息队列等等都可以打包成一个image部署。 云时代的到来：AWS的成功，引到开发者将应用转移到云上，解决来硬件管理的问题，然而软件配置和管理香瓜的问题依然存在。Docker的出现正好能帮助软件开发着开阔思路，尝试新的软件管理的方法解决这个问题。 虚拟化手段的变化：云时代采用标配硬件来降低成本，采用虚拟化手段来满足用户按需分配的资源需求以及保证可用性和隔离性。然而无论是KVM还是Xen，在Docker看来都是在浪费资源，又难于管理，更加轻量级大LXC更加灵活和快速： LXC的便携性：LXC在Linux 2.6的Kernel里就已经存在了，但是其设计之初并非为云计算考虑，缺少标准化的描述手段和容器的可便携性，决定其构建出的环境难于分发和标准化管理(相对于KVM之类的image和sanpshot的概念)。Docker就在这个问题上作出了实质性的创新方法。 Docker的用途 Docker的主要用途，目前又三大类：\n**提供了一次性的环境：**比如，本地测试他人的软件、持续集成的时候提供单元测试和构建的环境。 **提供弹性的云服务：**因为Docker容器可以随开随关，很适合动态扩容和所容。 **组建微服务架构：**通过多个容器，一台机器可以跑多个服务，因此在本机就可以模拟出微服务架构。 Docker镜像 操作系统分为内核和用户空间，对于Linux而言，内核启动后，会挂载root文件系统为其提供用户空间支持。而Docker镜像(Image),就相当于是一个root文件系统。 Docker镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数(如匿名卷、环境变量、用户等)。镜像不包含任何动态数据，其内容在构建之后也不会被改变。\nDocker容器 镜像(image)和容器(container)的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。\n容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的root文件系统、自己的网络配置、自己的进程空间，甚至自己的用户ID空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立宿主的系统下操作一样。这种特性使容器封装的应用比直接在宿主运行更加安全。\n前面讲过镜像使用的是分层储存，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，可以称这个味容器运行时读写而准备的存储层为容器存储层。\n容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。\n按照Docker最佳实践的要求，容器不应该向其存储层写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷(volume)、或者绑定宿主目录，在这些位置的读写会跳过存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。\n数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。\n仓库 镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。\n一个Docker Registry中可以包含多个仓库(Repository);每个仓库可以包含多个标签(tag)；每个标签对应一个镜像。\n通常，一个仓库会包含一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt;的格式来指定具体是这个软件那个版本的镜像。如果不给出标签，将以laest作为默认标签。\n以ubuntu镜像为例，ubuntu是仓库的名字，其包含有不同的版本标签，如，14.04,16.04。我们可以通过ubuntu:14.04或者ubuntu:16.04来具体指定所需要哪个版本的镜像。如果忽略了标签，比如ubuntu,那将视为ubuntu:latest。\n仓库名经常以两段式路径形式出现，比如jwilder/nginx-proxy,前者意味着Docker Registry多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体Docker Registry的软件或服务。U和内存的利用率。\n企业级 新浪\n美团\n蘑菇街\n去哪下 1、官网 docker官网：http://www.docker.com\ndocker中文网站：https://www.docker-cn.com/\n2、仓库 Docker Hub官网: https://hub.docker.com/\n","date":"2019-10-26T09:26:28Z","image":"https://www.ownit.top/title_pic/26.jpg","permalink":"https://www.ownit.top/p/201910260926/","title":"Docker简介"},{"content":"我们部署Fastdfs，就是为了实现文件的上传。 现在使用idea整合Fastdfs，实现图片上传 部署环境：Centos7部署分布式文件存储(Fastdfs) 利用Java客户端调用FastDFS 服务器安装完毕后，咱们通过Java调用fastdfs\n加载Maven****依赖\nfastdfs 没有在中心仓库中提供获取的依赖坐标。\n只能自己通过源码方式编译，打好jar 包，安装到本地仓库。\n官方仓库地址：\nhttps://github.com/happyfish100/fastdfs-client-java\n直接用idea 直接把这个源码作为模块导入工程\n别的不用改，只把pom.xml中的版本改成1.27。\n然后右边 执行install 就好了\n安装好了 ，别的模块就可以直接使用这个坐标了。\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.csource\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fastdfs-client-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.27-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 而这个fastdfs-client-java模块可以从idea 中删除。\n然后可以进行一下上传的测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package com.atguigu.gmall.manage; import org.csource.common.MyException; import org.csource.fastdfs.ClientGlobal; import org.csource.fastdfs.StorageClient; import org.csource.fastdfs.TrackerClient; import org.csource.fastdfs.TrackerServer; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; import java.io.IOException; @RunWith(SpringRunner.class) @SpringBootTest public class GmallManageWebApplicationTests { @Test public void contextLoads() throws IOException, MyException { //配置fdfs的全局连接地址 String tracker = GmallManageWebApplicationTests.class.getResource(\u0026#34;/tracker.conf\u0026#34;).getPath();//获取配置文件路径 ClientGlobal.init(tracker); TrackerClient trackerClient = new TrackerClient(); //获得一个trackerserver的实例 TrackerServer trackerServer = trackerClient.getConnection(); //通过tracker获得storage客户端 StorageClient storageClient = new StorageClient(trackerServer, null); String[] uploadInfos = storageClient.upload_file(\u0026#34;g:/9.gif\u0026#34;, \u0026#34;gif\u0026#34;, null); String url=\u0026#34;http://192.168.116.129\u0026#34;; for (String uploadInfo : uploadInfos){ url+=\u0026#34;/\u0026#34;+uploadInfo; } System.out.println(url); } } 加入tracker.conf文件 1 2 3 4 5 6 7 tracker_server=192.168.67.162:22122 # 连接超时时间，针对socket套接字函数connect，默认为30秒 connect_timeout=30000 # 网络通讯超时时间，默认是60秒 network_timeout=60000 打印结果\n这个打印结果实际上就是我们访问的路径，加上服务器地址我们可以拼接成一个字符串\nhttp://192.168.116.129/group1/M00/00/00/wKh0gV2dHmGAFpUzAA7-f54U48M105.gif\n直接放到浏览器去访问\n","date":"2019-10-10T15:20:18Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/201910101520/","title":"SpringBoot整合Fastdfs，实现图片上传（IDEA）"},{"content":"\n目录\nFastDFS介绍\n楼主目标：前可H5撩妹，后可Linux搞运维\n环境：Centos7\n软件：\n软件链接：\n安装前所有准备，上传软件到Centos7上的/opt的目录下\n安装依赖软件和类库(安装前的准备)\n1 fdfs的依赖库\nA 解压Libfastcommon\nB 进入Libfastcommon目录下\nC make编译\nD make install 安装\nE libfastcommon.so复制文件到/usr/lib/\n2 fastdfs软件(tracker、storage)\nA 新建目录mkdir /opt/fastdfs\nB 解压FastDFS_v5.05.tar.gz\nC 进入解压目录\nD make编译\nE make install 安装\nF 进入conf配置目录将文件都拷贝到/etc/fdfs下cp * /etc/fdfs/（安装时自动生成）\nG 进入/etc/fdfs/，配置tracker.conf\nH storage的配置(storage不需要安装，因为安装tracker时已经同时安装)\n3 配置tracker和storage的启动服务\n配置tracker启动服务\n配置storage启动服务\n将启动脚本加入linux服务\n​\n启动服务\n检查服务启动状态\n​\n4 测试上传\n修改/etc/fdfs/client.conf\nFastDFS整合nginx\n5 安装nginx整合插件fastdfs-nginx-module\nA 解压FastDFS-nginx-module插件\nB 修改插件读取fdfs的目录（插件自己的配置文件）\nC 将FastDFS-nginx-module插件整合fdfs的配置文件拷贝到fdfs的配置目录下(整合fdfs的配置文件)\nD 修改/etc/fdfs/mod_fastdfs.conf配置文件\n6 安装nginx\n创建nginx/client目录\n安装环境：\n解压nginx\n进入nginx目录 配置安装环境\n配置成功\n编译\n安装\n编辑nginx.conf\n启动nginx\n设置开机启动\n需要关闭防火墙\n测试\n浏览器打开链接\n完美成功，OK了\n楼主目标：前可H5撩妹，后可Linux搞运维\nFastDFS介绍 FastDFS是一个开源的轻量级分布式文件系统，它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等\n楼主目标：前可H5撩妹，后可Linux搞运维 OK，废话不多说开始部署\n环境：Centos7 软件： FastDFS_v5.05.tar.gz\nfastdfs-nginx-module_v1.16.tar.gz\nlibfastcommonV1.0.7.tar.gz\nnginx-1.12.2.tar.gz\n软件链接： https://www.lanzous.com/b0c1xw7hi\n安装前所有准备，上传软件到Centos7上的/opt的目录下 安装依赖软件和类库(安装前的准备) 1 yum install gcc-c++ -y 1 yum -y install zlib zlib-devel pcre pcre-devel gcc gcc-c++ openssl openssl-devel libevent libevent-devel perl unzip net-tools wget 1 yum install perl* 1 fdfs的依赖库 Libfastcommon安装过程\nA 解压Libfastcommon 1 tar -zxvf libfastcommon B 进入Libfastcommon目录下 1 cd Libfastcommon C make编译 1 make D make install 安装 1 make install E libfastcommon.so复制文件到/usr/lib/ 1 cp /usr/lib64/libfastcommon.so /usr/lib/ 2 fastdfs软件(tracker、storage) 配置tracker\n配置storage\n(依赖于：Gcc、libevent、perl)\nA 新建目录mkdir /opt/fastdfs 1 mkdir /opt/fastdfs B 解压FastDFS_v5.05.tar.gz 1 tar -zxvf FastDFS_v5.05.tar.gz C 进入解压目录 1 cd FastDFS D make编译 1 ./make.sh E make install 安装 1 ./make.sh install F 进入conf配置目录将文件都拷贝到/etc/fdfs下cp * /etc/fdfs/（安装时自动生成） 1 2 cd conf cp * /etc/fdfs/ G 进入/etc/fdfs/，配置tracker.conf vim /etc/fdfs/tracker.conf ，设置软件数据和日志目录\nH storage的配置(storage不需要安装，因为安装tracker时已经同时安装) vim /etc/fdfs/storage.conf\n软件目录\nStorage存储文件的目录（新建mkdir /opt/fastdfs/fdfs_storage）\n1 mkdir /opt/fastdfs/fdfs_storage Storage的trackerip\n3 配置tracker和storage的启动服务 配置tracker启动服务 进入/etc/init.d启动脚本目录，默认fastdfs已经生成\nVi fdfs_trackerd脚本文件\n因为启动脚本还在安装目录下，所以我们新建/usr/local/fdfs目录，并且将启动脚本cp到该目录\n1 mkdir /usr/local/fdfs 进入安装目录/opt/FastDFs\n1 2 3 cd /opt/FastDFs cp restart.sh /usr/local/fdfs/ cp stop.sh /usr/local/fdfs/ 配置storage启动服务 (restart和stop脚本已经拷贝到/usr/local/fdfs下，所以storage只需要配置/etc/init.d/fdfs_storage脚本就可以了)\nvim /etc/init.d/fdfs_storage\n将启动脚本加入linux服务 1 2 3 4 cd /etc/init.d/ chkconfig --add fdfs_trackerd chkconfig --add fdfs_storaged 启动服务 1 2 3 service fdfs_trackerd start service fdfs_storaged start 检查服务启动状态 1 ps -ef |grep fdfs 4 测试上传 FastDFS安装成功可通过/usr/bin/fdfs_test测试上传、下载等操作。\n修改/etc/fdfs/client.conf 1 2 3 4 5 [root@localhost ~]# vim /etc/fdfs/client.conf base_path=/opt/fastdfs tracker_server=192.168.67.163:22122 比如将/root下的图片上传到FastDFS中：\n1 /usr/bin/fdfs_test /etc/fdfs/client.conf upload preview.jpg 对应的上传路径：\n/opt/fastdfs/fdfs_storage/data /00/00/wKhDo1qipbiAJC6iAAB1tayPlqs094_big.jpg\nFastDFS整合nginx 5 安装nginx整合插件fastdfs-nginx-module A 解压FastDFS-nginx-module插件 1 tar -zxvf fastdfs-nginx-module_v1.16.tar.gz B 修改插件读取fdfs的目录（插件自己的配置文件） Vi fastdfs-nginx-module/src/config\n删除圈中里面的local，就上面两个就可以了 C 将FastDFS-nginx-module插件整合fdfs的配置文件拷贝到fdfs的配置目录下(整合fdfs的配置文件) FastDFS-nginx-module/src下的mod_fastdfs.conf拷贝至/etc/fdfs/下(这里面是两个路径)\n1 cp /opt/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/ D 修改/etc/fdfs/mod_fastdfs.conf配置文件 软件安装目录\nTracker_server地址\nWeb的url是否包含group的路径名\n上传文件存储目录\n6 安装nginx 创建nginx/client目录 1 mkdir -p /var/temp/nginx/client 安装环境： 安装pcre库\nyum -y install pcre-devel\n安装zlib库\nyum install -y zlib-devel\n解压nginx 1 tar -zxvf nginx-1.12.2.tar.gz 进入nginx目录 配置安装环境 添加fastdfs-nginx-module模块\ncd nginx-1.8.0\n1 2 3 4 5 6 7 8 9 10 11 12 13 ./configure \\ --prefix=/usr/local/nginx \\ --pid-path=/var/run/nginx/nginx.pid \\ --lock-path=/var/lock/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --with-http_gzip_static_module \\ --http-client-body-temp-path=/var/temp/nginx/client \\ --http-proxy-temp-path=/var/temp/nginx/proxy \\ --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\ --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\ --http-scgi-temp-path=/var/temp/nginx/scgi \\ --add-module=/opt/fastdfs-nginx-module/src 配置成功 编译 1 make 安装 1 make install 编辑nginx.conf vim /usr/local/nginx/conf/nginx.conf\n启动nginx 1 /usr/local/nginx/sbin/nginx 设置开机启动 vim /etc/rc.d/rc.local\n需要关闭防火墙 1 service iptables stop 永久关闭 chkconfig iptables off\n测试 1 /usr/bin/fdfs_test /etc/fdfs/client.conf upload preview.jpg 浏览器打开链接 完美成功，OK了 楼主目标：前可H5撩妹，后可Linux搞运维 ","date":"2019-10-10T12:21:17Z","image":"https://www.ownit.top/title_pic/40.jpg","permalink":"https://www.ownit.top/p/201910101221/","title":"Centos7部署分布式文件存储(Fastdfs)"},{"content":"1 前端127.0.0.1:8888 2 后端127.0.0.1:8080 前端和后端因为来自不同的网域，所以在http的安全协议策略下，不信任 3 解决方案，在springmvc的控制层加入@CrossOrigin跨域访问的注解 ","date":"2019-10-08T17:57:15Z","image":"https://www.ownit.top/title_pic/68.jpg","permalink":"https://www.ownit.top/p/201910081757/","title":"Java前后端的跨域问题"},{"content":"目录\n环境： 第一步：安装jdk，并且配置环境变量\n1.解压jdk：\n2.配置环境变量：\n3.保存并使文件立即生效：\n4.立即重启虚拟机，进行下面的安装\n第二步：安装注册中心zookeeper 1.解压zookeeper：\n2.在zookeeper目录下创建data和logs目录：\n3.将/usr/local/zookeeper3.4.6/zookeeper-3.4.6/conf 目录下的 zoo_sample.cfg拷贝：\n4.修改配置文件：\n​\n5.在vi /etc/profile末尾添加zookeeper配置\n6.配置文件立即生效：\n7.关闭防火墙，并且启动测试\n第三步：安装dubbo-admin-war和tomcat 1.解压tomcat:\n2.解压dubbo文件\n3.进入tomcat的conf目录下修改server.xml\n4.修改server.xml\n5.启动tomcat服务，进入tomcat的bin下\n6.启动zookeeper服务，进入zookeeper的bin下\n第四步：在浏览器中输入地址显示如下： 环境： 1.centos7 2.jdk-7u76-linux-x64.tar.gz 2.tomcat:apache-tomcat-7.0.59.tar.gz 3.zookeeper-3.4.6.tar.gz 4.dubbo-admin-2.5.3.war\n具体的流程： 第一步：安装jdk，并且配置环境变量\nxshell5命令:\n1.解压jdk：\n1 tar zxvf jdk-7u76-linux-x64.tar.gz 2.配置环境变量： /opt/jdk1.8.0_152 :为jdk解压的路径\n1 [root@localhost~]# vi /etc/profile 1 2 3 export JAVA_HOME=/opt/jdk1.8.0_152 export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH 3.保存并使文件立即生效： 1 2 保存：点击ESC键，并且输入:wq; 立即生效：source /etc/profile 4.立即重启虚拟机，进行下面的安装 1 shutdown -r now 第二步：安装注册中心zookeeper 1.解压zookeeper：\n1 tar zxvf zookeeper-3.4.6.tar.gz 2.在zookeeper目录下创建data和logs目录： 1 mkdir data 3.将/usr/local/zookeeper3.4.6/zookeeper-3.4.6/conf 目录下的 zoo_sample.cfg拷贝： 1 cp zoo_sample.cfg zoo.cfg 4.修改配置文件： 1 vi zoo.cfg 5.在vi /etc/profile末尾添加zookeeper配置 1 2 export ZOOKEEPER_HOME=/opt/zookeeper-3.4.11 export PATH=$ZOOKEEPER_HOME/bin:$PATH 6.配置文件立即生效： 1 source /etc/profile 7.关闭防火墙，并且启动测试 1 2 systemctl stop firewalld.service 在zookeeper的bin目录下执行： ./zkServer.sh start 第三步：安装dubbo-admin-war和tomcat 1.解压tomcat:\n1 tar zxvf apache-tomcat-7.0.59.tar.gz 2.解压dubbo文件 1 unzip dubbo-admin-2.6.0.war -d dubbo 3.进入tomcat的conf目录下修改server.xml 4.修改server.xml 1 \u0026lt;Context path=\u0026#34;/dubbo\u0026#34; docBase=\u0026#34;/opt/dubbo\u0026#34; debug=\u0026#34;0\u0026#34; privileged=\u0026#34;true\u0026#34; /\u0026gt; 5.启动tomcat服务，进入tomcat的bin下\n1 startup.sh 6.启动zookeeper服务，进入zookeeper的bin下 1 2 3 bash zkServer.sh start bash zkServer.sh status 第四步：在浏览器中输入地址显示如下： 账号：root\n密码：root\n","date":"2019-09-28T14:22:52Z","image":"https://www.ownit.top/title_pic/38.jpg","permalink":"https://www.ownit.top/p/201909281422/","title":"Centos7安装dubbo与zookeeper服务配置"},{"content":"dubbo与zookeeper的关系 dubbo是动物..zookeeper是动物园的管理员！\n按我的理解，您可以把dubbo服务想象成学校里的一个学生，并且对应有一个学号，zookeeper则是想象成一个教务网管理系统。我们可以通过教务网管理系统，查找到对应的学生。我们首先通过注册入学，将学生和学号对应绑定。\n比方说项目是一个分布式的项目，web层与 service层被拆分了开来， 部署在不同的tomcat中， 我在web层 需要调用 service层的接口，但是两个运行在不同tomcat下的服务无法直接互调接口，那么就可以通过zookeeper和dubbo实现。 我们通过dubbo 建立ItemService这个服务，并且到zookeeper上面注册，填写对应的zookeeper服务所在 的IP及端口号。【按照我上面的比喻就是，学生注册入学（接口是学号，学生本人是impl实现），填写学校教务网网址（就是zookeeper）】\nDubbo建议使用Zookeeper作为服务的注册中心。\n1. Zookeeper的作用：\nzookeeper用来注册服务和进行负载均衡，哪一个服务由哪一个机器来提供必需让调用者知道，简单来说就是ip地址和服务名称的对应关系。当然也可以 通过硬编码的方式把这种对应关系在调用方业务代码中实现，但是如果提供服务的机器挂掉调用者无法知晓，如果不更改代码会继续请求挂掉的机器提供服务。 zookeeper通过心跳机制可以检测挂掉的机器并将挂掉机器的ip和服务对应关系从列表中删除。至于支持高并发，简单来说就是横向扩展，在不更改代码 的情况通过添加机器来提高运算能力。通过添加新的机器向zookeeper注册服务，服务的提供者多了能服务的客户就多了。\n2. dubbo：\n是管理中间层的工具，在业务层到数据仓库间有非常多服务的接入和服务提供者需要调度，dubbo提供一个框架解决这个问题。\n注意这里的dubbo只是一个框架，至于你架子上放什么是完全取决于你的，就像一个汽车骨架，你需要配你的轮子引擎。这个框架中要完成调度必须要有一个分布式的注册中心，储存所有服务的元数据，你可以用zk，也可以用别的，只是大家都用zk。\n3. zookeeper和dubbo的关系：\nDubbo的将注册中心进行抽象，是得它可以外接不同的存储媒介给注册中心提供服务，有ZooKeeper，Memcached，Redis等。\n引入了ZooKeeper作为存储媒介，也就把ZooKeeper的特性引进来。首先是负载均衡，单注册中心的承载能力是有限的，在流量达到一定程度的时 候就需要分流，负载均衡就是为了分流而存在的，一个ZooKeeper群配合相应的Web应用就可以很容易达到负载均衡；资源同步，单单有负载均衡还不 够，节点之间的数据和资源需要同步，ZooKeeper集群就天然具备有这样的功能；命名服务，将树状结构用于维护全局的服务地址列表，服务提供者在启动 的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。 其他特性还有Mast选举，分布式锁等。\n","date":"2019-09-28T14:10:02Z","image":"https://www.ownit.top/title_pic/08.jpg","permalink":"https://www.ownit.top/p/201909281410/","title":"dubbo与zookeeper的关系"},{"content":"随着技术的更新对于开发速度的追求，我们越来越不能忍受的是Spring框架对于集成开发以后大量的配置问题。所以SprigBoot应运而生，SpringBoot框架其实就是在Spring框架的外边包裹上了一层纸，包括减少配置文件，内置Tomcat服务器等等。在这里我们就使用IDEA工具为代表讲解一下SpringBoot在开发过程中会使用到的开发技术。官方推荐的编辑器是STS，STS就是对Eclipes做了封装，其实没有什么具体的改变，所以这里就是用更加快捷方便的开发工具IDEA，没有多大的影响。\n创建项目：\n接下来就是给项目命名了，我偷懒了选择默认吧 (=v=)\nweb项目开发就少不了它啦\n项目名称、项目位置\n点击Finish后，idea就帮我们创建项目\n目录结构\njava -\u0026mdash;源码，要注意的是Application要放在当前工程groupId下，举个栗子（=.=）\n\u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt;\n所以上面的 DemoApplication 位置是要放在 com.example 目录下\nresource\n-\u0026mdash;-static ：web的静态资源\n-\u0026mdash;-templates ：页面模板（.html / .ftl）\n-\u0026mdash;-application.properties ：配置文件 ，不过常用的是以 .yml 为后缀，application.yml\n接下来写一个简单的测试代码\nUserController.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package com.demo.ssm.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @Controller @RequestMapping(\u0026#34;\u0026#34;) public class UserController { @RestController public class TestController { @RequestMapping(\u0026#34;/test\u0026#34;) public String hello() { System.out.println(\u0026#34;TestController的方法被调用了\u0026#34;); return \u0026#34;welcome to the new age !\u0026#34;; } } } 因为Springboot 内嵌 web 服务器（有很多，可根据情况所需配置），默认是 Tomcat，因此直接运行 Application类\napplication.properties\n1 2 3 4 5 6 7 # 服务端口 server.port=8080 # 日志级别 logging.level.root=error 打开浏览器，在地址栏输入请求URL\n控制台输出\n打开浏览器，在地址栏输入请求URL\n控制台输出\n没有spring繁琐的配置，也不用部署到Tomcat，开发也可以如此快捷方便。\n","date":"2019-09-26T17:52:53Z","image":"https://www.ownit.top/title_pic/19.jpg","permalink":"https://www.ownit.top/p/201909261752/","title":"IEDA创建Springboot项目"},{"content":" 数据结构和设计的介绍 ezdml软件：https://www.lanzous.com/i6ew2pe 1 用ezdml设计数据表然后导出到mysql数据库中 点击生成模型 2 选择库 3 点击开始生成 ","date":"2019-09-26T17:11:33Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201909261711/","title":"ezdml设计数据库"},{"content":"目录\nCentos7下zabbix安装与部署\n1.Zabbix介绍\n2.LAMP/LNMP介绍\n3.Zabbix安装与部署\n1、临时关闭\n2、永久关闭\n3 、安装环境\n4、安装zabbix\nCentos7下zabbix安装与部署 1.Zabbix介绍 zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。\nzabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位/解决存在的各种问题。\nzabbix由2部分构成，zabbix server与可选组件zabbix agent。\nzabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视，数据收集等功能，它可以运行在Linux，Solaris，HP-UX，AIX，Free BSD，Open BSD，OS X等平台上。\n2.LAMP/LNMP介绍 LAMP：Linux+Apache+Mysql/MariaDB+Perl/PHP/Python一组常用来搭建动态网站或者服务器的开源软件，本身都是各自独立的程序，但是因为常被放在一起使用，拥有了越来越高的兼容度，共同组成了一个强大的Web应用程序平台。\nLNMP：LNMP指的是一个基于CentOS/Debian编写的Nginx、PHP、MySQL、phpMyAdmin、eAccelerator一键安装包。可以在VPS、独立主机上轻松的安装LNMP生产环境。\nL：linux\nA：apache\nN：nginx\nM：mysql,mariaDB\nP：php,python,perl\n3.Zabbix安装与部署 Zabbix的安装\n1 2 3 关闭SeLinux 临时关闭：setenforce 0 1 永久关闭：vi /etc/selinux/config 关闭防火墙\n1、临时关闭 1 systemctl stop firewalld.service 2、永久关闭 1 systemctl disable firewalld.service 3 、安装环境 LAMP 大家可以看下面的博客，安装LAMP环境\nLinux(Centos7)搭建LAMP(Apache+PHP+Mysql环境)\n4、安装zabbix (1)下载包\n1 rpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-2.el7.noarch.rpm (2)安装zabbix的包\n1 yum install -y zabbix-server-mysql zabbix-get zabbix-web zabbix-web-mysql zabbix-agent zabbix-sender 4、创建一个zabbix库并设置为utf8的字符编码格式\n1 create database zabbix character set utf8 collate utf8_bin; 创建账户并且授权设置密码\n1 grant all privileges on zabbix.* to zabbix@localhost identified by \u0026#39;zabbix\u0026#39;; 给来自loclhost的用户zabbxi分配可对数据库zabbix所有表进行所有操作的权限，并且设定密码为zabbix\n刷新\n1 flush privileges; exit退出\n5、导入表\n切换到此目录下\n1 cd /usr/share/doc/zabbix-server-mysql-3.2.10/ 进行解压\n1 gunzip create.sql.gz 对表进行导入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [root@wei zabbix-server-mysql-3.4.15]# mysql -u zabbix -p zabbix Enter password: Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 10 Server version: 5.6.45 MySQL Community Server (GPL) Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | zabbix | +--------------------+ 2 rows in set (0.00 sec) mysql\u0026gt; use zabbix; Database changed mysql\u0026gt; source create.sql 6、配置zabbix server配置文件\n配置文件目录\n1 cd /etc/zabbix 对zabbix_server.conf进行配置\n运行zabbix-server服务\n开机自启zabbix-server服务\n7、配置php\n1 cd /etc/httpd/conf.d 配置时间\n1 vim zabbix.conf 1 Systemctl restart httpd 8、登陆zabbix网址设置\n192.168.85.11/zabbix\npassword是我们设置的数据库密码zabbix\n登陆账户是Admin\n密码是zabbix\n9、设置中文\n10、对服务器自身进行监控\n11、解决中文乱码无法显示的问题\n从我们电脑win7里面找到黑体右键复制到桌面然后拉到zabbix服务器上面\n直接修改字体名字\n切换到这个目录下面: /usr/share/zabbix/fonts\n现在的中文字体是显示正常的了\n","date":"2019-09-24T10:47:15Z","image":"https://www.ownit.top/title_pic/36.jpg","permalink":"https://www.ownit.top/p/201909241047/","title":"Centos7下zabbix安装与部署"},{"content":"目录\n宝塔面板安装和使用图文教程\n1,通过ssh工具登录服务器\n2，安装宝塔面板\n2，登录宝塔面板\n3，设置宝塔面板\n3.1，首先我们进入面板设置\n3.2，更改面板端口\n3.3，绑定域名\n3.4，绑定ip\n3.5，更改默认的面板用户和密码\n3.5，绑定宝塔账号\n3.6，绑定微信小程序\n4，宝塔面板安全设置\n5，安装面板环境\n6，创建网站\n7，购买插件\n8，升级为专业版\n宝塔面板安装和使用图文教程 如果你要安装宝塔linux面板,你要准备好一个纯净版的linux操作系统，没有安装过其它环境带的Apache/Nginx/php/MySQL（已有环境不可安装）。支持的操作系统有CentOS，Ubuntu、Debian、Fedora。这里给大家演示的是centos7.5。\n1,通过ssh工具登录服务器 这里推荐大家使用xshell进行登录。注意要开放ssh连接的端口，一般默认是22，为了网站安全推荐大家更换ssh登录端口。设置为不常用的端口。\n输入账号和密码，注意密码在输入时是不显示的，大家不要以为密码没输入。\n2，安装宝塔面板 执行以下代码进行安装宝塔6.9免费版。宝塔6.9版本已经很稳定了，推荐大家直接安装6.9版本（注意：宝塔linux6.0版本是基于centos7开发的，务必使用centos7.x 系统\n1 [root@wei ~]# yum install -y wget \u0026amp;\u0026amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh \u0026amp;\u0026amp; sh install.sh 如果大家系统是centos7以下的大家还是乖乖使用宝塔5.9的安装脚本（Centos官方已宣布在2020年停止对Centos6的维护更新，推荐大家装系统直接安装centos7）\n1 yum install -y wget \u0026amp;\u0026amp; wget -O install.sh http://download.bt.cn/install/install.sh \u0026amp;\u0026amp; sh install.sh 回车进行安装。\n输入y，并回车。接下来便是等待宝塔面板进行安装。\n我们得到登录宝塔面板的URL,账号和密码。\n2，登录宝塔面板 安装完成宝塔面板后，我们就可以在浏览器中访问了。复制Bt-panel中的URL到浏览器上访问。注意要打开服务器上的8888端口，关于如何打开服务器端口，你可以在本站中搜索答案。\n输入默认的账号和密码进行登录。\n3，设置宝塔面板 登陆后进入宝塔面板我们可以看到如下图所示，你可以选择LNMP或者LAMP进行安装。看大家网站需要什么环境进行选择。如果是生产环境推荐大家使用编译安装，如果只是测试环境选择极速安装。两者的区别是编译安装慢但稳定，极速安装虽然慢但是没编译安装稳定。\n推荐大家首次进入宝塔面板前不要进行环境的安装，因为在安装环境不能更改宝塔面板的设置。推荐大家先更改宝塔面板的默认设置，编译安装环境将近一个小时。在这段时间里我们先将宝塔面板设置好提高面板的安全性。\n3.1，首先我们进入面板设置 3.2，更改面板端口 将端口更改为不常用的端口。\n3.3，绑定域名 你可以绑定一个域名绑定完域名后只能通过你绑定的域名来访问面板。\n3.4，绑定ip 如果你有固定的ip，你可绑定ip访问，绑定了ip访问你只能通过绑定得这个ip进行访问。如果你是家用电脑就不要绑定ip了，因为家用电脑的ip是动态的。这就会造成ip发生改变面板访问不了。\n3.5，更改默认的面板用户和密码 更改宝塔安装完成时的默认用户名和密码，设置一个自己能记住的用户名和密码，密码不要太简单了。\n3.5，绑定宝塔账号 如果你有宝塔账号你可以绑定下，没有的话可以去宝塔官网申请。宝塔账号在购买付费插件，开通专业版时要用到。要去注册账号\n3.6，绑定微信小程序 由于微信小程序是付费插件，你只有购买了或者开通专业版才能使用。微信小程序能够监控服务器，方便用户随时查看服务器状态。\n4，宝塔面板安全设置 在这里你可以开启和禁用一些端口。推荐大家更改ssh端口，和禁用ping。更改FTP端口。更改phpadmin默认端口。不常用的端口可以把它关闭，等要使用了在开启。\n5，安装面板环境 在软件管理选择你所需要的网站环境进行安装。\n在这里老杨选择LNMP进行安装，即Linux+Nginx+Mysql+Php。\n6，创建网站 等网站环境安装完成后便可以创建网站，有两种方法可以创建网站。第一种直接在选择网站，选择添加站点，进行创建网站。\n第二种在软件管理中的宝塔插件中安装宝塔一键部署源码插件进行创建网站。\n7，购买插件 如果你在使用过程中需要用到某款插件你可以到软件管理\u0026gt;付费插件进行购买。\n选择购买时间进行购买。\n8，升级为专业版 如果你在使用过程中需要使用到多款付费插件推荐大家升级专业版。\n选择时间并进行支付。\n如果你有账号有购买过专业版你可以选择代金劵进行支付。\n刷新下面板在到期时间可以看到永久授权四个字。\n如果升级不成功可以ssh登录到服务器执行升级代码进行升级。\n1 2 3 4 5 6 7 8 9 ``` 1. wget -O update.sh http://download.bt.cn/install/update\\_pro.sh \\\u0026amp;\\\u0026amp; bash update.sh pro [![](format,png)](https://www.laoyangblog.com/wp-content/uploads/2018/09/20180924011530.png) 或者进入文件管理器，打开终端，粘贴升级代码，然后点击“发送”，手动升级到专业版。 [![](format,png)](https://www.laoyangblog.com/wp-content/uploads/2018/09/20180924011916.png) ","date":"2019-09-24T09:48:52Z","image":"https://www.ownit.top/title_pic/57.jpg","permalink":"https://www.ownit.top/p/201909240948/","title":"Centos7安装宝塔控制面板"},{"content":"目录\nLinux搭建LAMP(Apache+PHP+Mysql环境)Centos7\n一、 检查系统环境\n1、确认centos版本\n2、检查是否安装过apache\n3、检查是否安装过Mysql\n4、清理Mysql痕迹\n5、卸载Apache包\n二、安装Apache、PHP、Mysql\n1、安装apache\n2、安装Php\n3、安装php-fpm\n4、安装Mysql\n5、安装 mysql-server\n6、安装 php-mysql\n三、安装基本常用扩展包\n1、安装Apache扩展包\n2、安装PHP扩展包\n3、安装Mysql扩展包\n四、配置Apache、mysql开机启动\n五、配置Mysql\n六、测试环境\nLinux搭建LAMP(Apache+PHP+Mysql环境)Centos7 LAMP是指一组通常一起使用来运行动态网站或者服务器的自由软件名称首字母缩写：\nLinux，操作系统\nApache，网页服务器\nMariaDB或MySQL，数据库管理系统（或者数据库服务器）\nPHP、Perl或Python，脚本语言\n一、 检查系统环境 1、确认centos版本\n1 2 [root@wei ~]# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) 2、检查是否安装过apache\n1 2 [root@wei ~]# rpm -qa |grep httpd 或者：\napachectl -v\n或者：\nhttpd -v 3、检查是否安装过Mysql\n1 2 [root@wei ~]# rpm -qa | mysql 4、清理Mysql痕迹\n1 2 3 4 5 [root@wei ~]# yum remove mysql 已加载插件：fastestmirror 参数 mysql 没有匹配 不删除任何软件包 [root@wei ~]# rm -rf /etc/my.cnf 5、卸载Apache包\n1 [root@wei ~]# rpm -e httpd --nodeps 注意：如果是新的系统或者你从来没有尝试安装过，则以上步骤省略。\n二、安装Apache、PHP、Mysql 1、安装apache\n1 [root@wei ~]# yum install httpd -y 直到返回\n安装完成\n查看安装httpd\n1 2 3 [root@wei ~]# rpm -qa |grep httpd httpd-tools-2.4.6-90.el7.centos.x86_64 httpd-2.4.6-90.el7.centos.x86_64 表示安装成功！\n2、安装Php\n[root@localhost ~]# yum -y install php\n直到返回：\n查看安装php的软件\n1 2 3 4 [root@wei ~]# rpm -qa |grep php php-common-5.4.16-46.el7.x86_64 php-5.4.16-46.el7.x86_64 php-cli-5.4.16-46.el7.x86_64 3、安装php-fpm\n1 [root@wei ~]# yum -y install php-fpm 直到返回：\n4、安装Mysql 1 [root@wei ~]# yum -y install mysql 直到返回：\nComplete!\n7.2版本的Centos已经把mysql更名为mariadb，表示安装成功！\n5、安装 mysql-server 1 2 3 4 5 6 7 8 [root@wei ~]# yum -y install mysql-server 已加载插件：fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.tuna.tsinghua.edu.cn 没有可用软件包 mysql-server。 错误：无须任何处理 返回错误！！！\n分析解决方案\nCentOS 7+ 版本将MySQL数据库软件从默认的程序列表中移除，用mariadb代替了，entos7配置教程上，大多都是安装mariadb，因为centos7默认将mariadb视作mysql。 因为mysql被oracle收购后，原作者担心mysql闭源，所以又写了一个mariadb，这个数据库可以理解为mysql的分支。如果需要安装mariadb，只需通过yum就可。 有两种解决方案：\n一是安装mariadb\n[root@localhost ~]# yum install -y mariadb 二是从官网下载mysql-server\n采用第二种方案：\n1 [root@wei ~]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm 1 [root@wei ~]# rpm -ivh mysql-community-release-el7-5.noarch.rpm 1 [root@wei ~]# yum -y install wget 下载中\u0026hellip;\u0026hellip;.\n安装成功！！！\n6、安装 php-mysql\n1 [root@wei ~]# yum -y install php-mysql 直到返回：\n安装成功！！！\n三、安装基本常用扩展包 1、安装Apache扩展包\n1 [root@wei ~]# yum -y install httpd-manual mod_ssl mod_perl mod_auth_mysql 安装成功！！！\n2、安装PHP扩展包\n1 [root@wei ~]# yum -y install php-gd php-xml php-mbstring php-ldap php-pear php-xmlrpc php-devel 安装成功！！！\n3、安装Mysql扩展包 1 [root@wei ~]# yum -y install mysql-connector-odbc mysql-devel libdbi-dbd-mysql 安装成功！！！\n四、配置Apache、mysql开机启动 重启Apache、mysql服务(注意这里和centos6有区别,Cenots7+不能使用6的方式)\nsystemctl start httpd.service #启动apache\nsystemctl stop httpd.service #停止apache\nsystemctl restart httpd.service #重启apache\nsystemctl enable httpd.service #设置apache开机启动\n如果是采用方法一安装的mariadb,安装完成以后使用下面的命令开启数据库服务：\n#启动MariaDB\n1 [root@wei~]# systemctl start mariadb.service #停止MariaDB\n1 [root@wei~]# systemctl stop mariadb.service #重启MariaDB\n1 [root@wei~]# systemctl restart mariadb.service #设置开机启动\n1 [root@wei~]# systemctl enable mariadb.service 重启对应服务\nservice mysqld restart\nservice php-fpm start\nservice httpd restart\n五、配置Mysql\n注意：要启动mysql才能进去\n初次安装mysql是没有密码的,我们要设置密码，mysql的默认账户为root\n设置 MySQL 数据 root 账户的密码：\n1 [root@wei etc]# mysql -u root -p 六、测试环境 注意：要启动httpd才能进去\n1、我们在浏览器地址栏输入http://localhost/如下图，说明我们的apache测试成功\n2、测试PHP\n进入apache的web根目录：/var/www/html 中写一个最简单的php测试页面\n1 2 [root@wei ~]# cd /var/www/html/ [root@wei html]# vi phpinfo.php 3、进入到了控制模式之后按键盘字母 i 进入到编辑模式，将如下代码输入到文件中\n1 2 3 4 \u0026lt;?php echo \u0026#34;\u0026lt;title\u0026gt;Phpinfo Test.php\u0026lt;/title\u0026gt;\u0026#34;; phpinfo() ?\u0026gt; 按 esc 退出编辑模式，回到控制模式，输入 :wq 然后回车，\n重启apache服务器\n1 [root@wei html]# systemctl restart httpd 在浏览器中输入服地址http://localhost/phpinfo.php\n出现下图则成功。\n","date":"2019-09-23T15:09:01Z","image":"https://www.ownit.top/title_pic/51.jpg","permalink":"https://www.ownit.top/p/201909231509/","title":"Linux(Centos7)搭建LAMP(Apache+PHP+Mysql环境)"},{"content":"目录\n1.创建web Maven项目\n2.创建java源码文件和resources资源文件\n3.创建数据库配置文件：jdbc.properties\n4.项目总体目录：\n5.添加spring配置文件：applicationContext.xml\n6.添加springMVC配置文件：springMVC.xml\n7.修改web.xml\n8.创建数据库相关表\n9.根据数据库表创建pojo类：User.java\n[10.编写dao层-\u0026gt;mapper接口和xml文件 ：UserMapper.java、UserMapper.xml](\u0026lt;#10.编写dao层-\u0026gt;mapper接口和xml文件 ：UserMapper.java、UserMapper.xml\u0026gt;)\n11.编写service层-\u0026gt;UserService.java、UserServiceImpl.java\n12.编写controller层-\u0026gt;UserController.java\n13.在jsp文件夹下创建edit.jsp和view.jsp\n14.配置tomcat，启动服务器（记得加载项目名）\n15.查看运行结果（已经成功）\n16.项目源码\n前言：学习maven后，感觉很厉害就搭建个项目小项目玩玩。\n功能：实现表的增删改查\n工具：IDEA jdk 1.8 mysql 整体项目结构：\n1.创建web Maven项目 ，接下来在pom文件加入项目所需依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 \u0026lt;dependencies\u0026gt; \u0026lt;!--Spring框架核心库 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-context\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-tx\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Spring Web --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-webmvc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--连接驱动--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.46\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--数据源--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;druid\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- mybatis ORM框架 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--mybatis-spring适配器 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-spring\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--整合mybatis如果不加这个包会报错 java.lang.NoClassDefFoundError: org/springframewor--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--测试--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--引入JSTL标签--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;taglibs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;standard\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 2.创建java源码文件和resources资源文件 3.创建数据库配置文件：jdbc.properties resources目录下添加：jdbc.properties\n1 2 3 4 5 #数据库配置文件 jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=utf8 jdbc.username=root jdbc.password=root 4.项目总体目录： 先给出项目总体目录\n创建好java目录下的分层的包，resources下创建存放mapper的包，WEB-INF下创建jsp放跳转的页面（下面的配置文件中会配置这些路径）\n5.添加spring配置文件：applicationContext.xml resources目录下添加：applicationContext.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:context=\u0026#34;http://www.springframework.org/schema/context\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- 导入数据库配置文件 --\u0026gt; \u0026lt;context:property-placeholder location=\u0026#34;classpath:jdbc.properties\u0026#34;/\u0026gt; \u0026lt;!-- 扫描包 这里的包需要根据自己java目录下的包名修改 --\u0026gt; \u0026lt;context:component-scan base-package=\u0026#34;com.demo.service\u0026#34; /\u0026gt; \u0026lt;!-- 配置数据库连接池 --\u0026gt; \u0026lt;bean id=\u0026#34;dataSource\u0026#34; class=\u0026#34;com.alibaba.druid.pool.DruidDataSource\u0026#34;\u0026gt; \u0026lt;!-- 基本属性 url、user、password --\u0026gt; \u0026lt;property name=\u0026#34;url\u0026#34; value=\u0026#34;${jdbc.url}\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;username\u0026#34; value=\u0026#34;${jdbc.username}\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;password\u0026#34; value=\u0026#34;${jdbc.password}\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!--Mybatis的SessionFactory配置--\u0026gt; \u0026lt;bean id=\u0026#34;sqlSession\u0026#34; class=\u0026#34;org.mybatis.spring.SqlSessionFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;typeAliasesPackage\u0026#34; value=\u0026#34;com.demo.pojo\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;mapperLocations\u0026#34; value=\u0026#34;classpath:mapper/*.xml\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!--Mybatis的Mapper文件识别--\u0026gt; \u0026lt;bean class=\u0026#34;org.mybatis.spring.mapper.MapperScannerConfigurer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;basePackage\u0026#34; value=\u0026#34;com.demo.dao\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 6.添加springMVC配置文件：springMVC.xml resources目录下添加：springMVC.xml，并且根据文件中‘视图定位’的配置，所以在WEB-INF下创建jsp文件夹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:context=\u0026#34;http://www.springframework.org/schema/context\u0026#34; xmlns:mvc=\u0026#34;http://www.springframework.org/schema/mvc\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.2.xsd\u0026#34;\u0026gt; \u0026lt;context:component-scan base-package=\u0026#34;com.demo.controller\u0026#34; /\u0026gt; \u0026lt;!-- 配置如果没有\u0026lt;mvc:annotation-driven/\u0026gt;， 那么所有的Controller可能就没有解析，所有当有请求时候都没有匹配的处理请求类， 就都去\u0026lt;mvc:default-servlet-handler/\u0026gt;即default servlet处理了。 添加上\u0026lt;mvc:annotation-driven/\u0026gt;后， 相应的do请求被Controller处理，而静态资源因为没有相应的Controller就会被default servlet处理。 总之没有相应的Controller就会被default servlet处理就ok了。 --\u0026gt; \u0026lt;mvc:annotation-driven /\u0026gt; \u0026lt;!--开通静态资源的访问--\u0026gt; \u0026lt;mvc:default-servlet-handler /\u0026gt; \u0026lt;!-- 视图定位 --\u0026gt; \u0026lt;bean class=\u0026#34;org.springframework.web.servlet.view.InternalResourceViewResolver\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;viewClass\u0026#34; value=\u0026#34;org.springframework.web.servlet.view.JstlView\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;prefix\u0026#34; value=\u0026#34;/WEB-INF/view/\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;suffix\u0026#34; value=\u0026#34;.jsp\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 7.修改web.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://java.sun.com/xml/ns/javaee\u0026#34; xsi:schemaLocation=\u0026#34;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\u0026#34; version=\u0026#34;2.5\u0026#34;\u0026gt; \u0026lt;!-- spring的配置文件--\u0026gt; \u0026lt;context-param\u0026gt; \u0026lt;param-name\u0026gt;contextConfigLocation\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;classpath:applicationContext.xml\u0026lt;/param-value\u0026gt; \u0026lt;/context-param\u0026gt; \u0026lt;listener\u0026gt; \u0026lt;listener-class\u0026gt;org.springframework.web.context.ContextLoaderListener\u0026lt;/listener-class\u0026gt; \u0026lt;/listener\u0026gt; \u0026lt;!--中文过滤器--\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;filter-name\u0026gt;CharacterEncodingFilter\u0026lt;/filter-name\u0026gt; \u0026lt;filter-class\u0026gt;org.springframework.web.filter.CharacterEncodingFilter\u0026lt;/filter-class\u0026gt; \u0026lt;init-param\u0026gt; \u0026lt;param-name\u0026gt;encoding\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;utf-8\u0026lt;/param-value\u0026gt; \u0026lt;/init-param\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;filter-mapping\u0026gt; \u0026lt;filter-name\u0026gt;CharacterEncodingFilter\u0026lt;/filter-name\u0026gt; \u0026lt;url-pattern\u0026gt;/*\u0026lt;/url-pattern\u0026gt; \u0026lt;/filter-mapping\u0026gt; \u0026lt;!-- spring mvc核心：分发servlet --\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;mvc-dispatcher\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;org.springframework.web.servlet.DispatcherServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;!-- spring mvc的配置文件 --\u0026gt; \u0026lt;init-param\u0026gt; \u0026lt;param-name\u0026gt;contextConfigLocation\u0026lt;/param-name\u0026gt; \u0026lt;param-value\u0026gt;classpath:springMVC.xml\u0026lt;/param-value\u0026gt; \u0026lt;/init-param\u0026gt; \u0026lt;load-on-startup\u0026gt;1\u0026lt;/load-on-startup\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;mvc-dispatcher\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; \u0026lt;/web-app\u0026gt; 8.创建数据库相关表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /* Navicat Premium Data Transfer Source Server : phpStudy Source Server Type : MySQL Source Server Version : 50553 Source Host : localhost:3306 Source Schema : test Target Server Type : MySQL Target Server Version : 50553 File Encoding : 65001 Date: 16/09/2019 19:46:40 */ SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for user -- ---------------------------- DROP TABLE IF EXISTS `user`; CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET gbk COLLATE gbk_chinese_ci NULL DEFAULT NULL, `password` varchar(255) CHARACTER SET gbk COLLATE gbk_chinese_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE ) ENGINE = MyISAM AUTO_INCREMENT = 9 CHARACTER SET = gbk COLLATE = gbk_chinese_ci ROW_FORMAT = Dynamic; -- ---------------------------- -- Records of user -- ---------------------------- INSERT INTO `user` VALUES (1, \u0026#39;cheng\u0026#39;, \u0026#39;128946\u0026#39;); INSERT INTO `user` VALUES (3, \u0026#39;eff\u0026#39;, \u0026#39;f\u0026#39;); INSERT INTO `user` VALUES (4, \u0026#39;fwefw\u0026#39;, \u0026#39;wef\u0026#39;); INSERT INTO `user` VALUES (8, \u0026#39;fwef\u0026#39;, \u0026#39;13\u0026#39;); SET FOREIGN_KEY_CHECKS = 1; 9.根据数据库表创建pojo类：User.java pojo的编写规则：数据库字段对应类字段，字段类型要一直 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 package com.demo.pojo; import java.io.Serializable; public class User implements Serializable { private Integer id; private String name; private String password; public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name == null ? null : name.trim(); } } 10.编写dao层-\u0026gt;mapper接口和xml文件 ：UserMapper.java、UserMapper.xml UserMapper.xml放在resources\\mapper下，UserMapper.java放在com.demo.dao这个包下：\n注意namespace=\u0026ldquo;com.demo.dao.UserMapper\u0026rdquo;，这里一定写对应的mapper接口具体路径\nUserMapper接口的编写规则：\n(1）方法名和对应的mapper配置文件中查询语句的id相同\n（2）返回类型和resultType的类型一致，没有就是void。\n（3）方法中的参数列表中的类型和parameterType一致。\n（4）mapper配置文件的namespace对应mapper接口类的全路径。\nUserMapper.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.demo.dao; import com.demo.pojo.User; import java.util.List; public interface UserMapper { List\u0026lt;User\u0026gt; list(); void del(int id); void update(User user); void add(User user); User get(int id); } UserMapper.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34; \u0026gt; \u0026lt;mapper namespace=\u0026#34;com.demo.dao.UserMapper\u0026#34; \u0026gt; \u0026lt;select id=\u0026#34;list\u0026#34; resultType=\u0026#34;user\u0026#34;\u0026gt; select * from user \u0026lt;/select\u0026gt; \u0026lt;update id=\u0026#34;update\u0026#34; parameterType=\u0026#34;user\u0026#34;\u0026gt; update user set name=#{name} ,password=#{password} where id=#{id} \u0026lt;/update\u0026gt; \u0026lt;insert id=\u0026#34;add\u0026#34; parameterType=\u0026#34;user\u0026#34;\u0026gt; insert into user values (null,#{name},#{password}); \u0026lt;/insert\u0026gt; \u0026lt;delete id=\u0026#34;del\u0026#34;\u0026gt; delete from user where id=#{id} \u0026lt;/delete\u0026gt; \u0026lt;select id=\u0026#34;get\u0026#34; resultType=\u0026#34;user\u0026#34;\u0026gt; select * from user where id =#{id} \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; 11.编写service层-\u0026gt;UserService.java、UserServiceImpl.java UserService.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package com.demo.service; import com.demo.pojo.User; import java.util.List; public interface UserService { List\u0026lt;User\u0026gt; list(); void del(int id); void update(User user); void add(User user); User get(int id); } UserServiceImpl.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package com.demo.service.impl; import com.demo.dao.UserMapper; import com.demo.pojo.User; import com.demo.service.UserService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import java.util.List; @Service public class UserServiceImpl implements UserService{ @Autowired UserMapper userMapper; public List\u0026lt;User\u0026gt; list() { return userMapper.list(); } public void add(User user) { userMapper.add(user); } public void del(int id) { userMapper.del(id); } public void update(User user) { userMapper.update(user); } public User get(int id) { return userMapper.get(id); } } 12.编写controller层-\u0026gt;UserController.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 package com.demo.controller; import com.demo.pojo.User; import com.demo.service.UserService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; import java.util.List; @Controller @RequestMapping(\u0026#34;\u0026#34;) public class UserController { @Autowired UserService userService; @RequestMapping(\u0026#34;list\u0026#34;) public String list(Model model){ List\u0026lt;User\u0026gt; us= userService.list(); model.addAttribute(\u0026#34;us\u0026#34;, us); return \u0026#34;view\u0026#34;; } @RequestMapping(\u0026#34;add\u0026#34;) public String add(User user,Model model){ userService.add(user); return \u0026#34;redirect:list\u0026#34;; } @RequestMapping(\u0026#34;del\u0026#34;) public String del(int id){ userService.del(id); return \u0026#34;redirect:list\u0026#34;; } @RequestMapping(\u0026#34;editUI\u0026#34;) public String editUI(int id,Model model){ User user = userService.get(id); model.addAttribute(\u0026#34;user\u0026#34;,user); return \u0026#34;edit\u0026#34;; } @RequestMapping(\u0026#34;update\u0026#34;) public String update(User user){ userService.update(user); return \u0026#34;redirect:list\u0026#34;; } } 13.在jsp文件夹下创建edit.jsp和view.jsp edit.jsp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;%@ page contentType=\u0026#34;text/html;charset=UTF-8\u0026#34; language=\u0026#34;java\u0026#34; isELIgnored=\u0026#34;false\u0026#34; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; #edit{width:300px;height:500px;margin: 100px auto} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;edit\u0026#34;\u0026gt; \u0026lt;form action=\u0026#34;/update\u0026#34;\u0026gt; 姓名：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34; value=\u0026#34;${user.name}\u0026#34;/\u0026gt;\u0026lt;br/\u0026gt; 密码：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;password\u0026#34; value=\u0026#34;${user.password}\u0026#34;/\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; value=\u0026#34;${user.id}\u0026#34; name=\u0026#34;id\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34; value=\u0026#34;\u0026#34;\u0026gt;修改\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; view.jsp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \u0026lt;%@ page contentType=\u0026#34;text/html;charset=UTF-8\u0026#34; language=\u0026#34;java\u0026#34; isELIgnored=\u0026#34;false\u0026#34; %\u0026gt; \u0026lt;%@ taglib prefix=\u0026#34;c\u0026#34; uri=\u0026#34;http://java.sun.com/jsp/jstl/core\u0026#34; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; table,table tr th, table tr td { border:1px solid rgba(41, 36, 35, 0.96); } #mytable{width:300px;margin: 100px auto} #add{width:300px;height:500px;margin: 100px auto} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;list\u0026#34;\u0026gt; \u0026lt;table id=\u0026#34;mytable\u0026#34;\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;th\u0026gt;id\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;名字\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;密码\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;修改\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;删除\u0026lt;/th\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;c:forEach items=\u0026#34;${us}\u0026#34; var=\u0026#34;user\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;${user.id}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${user.name}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;${user.password}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\u0026#34;editUI?id=${user.id}\u0026#34;\u0026gt;edit\u0026lt;/a\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\u0026#34;del?id=${user.id}\u0026#34;\u0026gt;delete\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/c:forEach\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;add\u0026#34;\u0026gt; \u0026lt;form action=\u0026#34;/add\u0026#34;\u0026gt; 姓名：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34; value=\u0026#34;\u0026#34;/\u0026gt;\u0026lt;br/\u0026gt; 密码：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;password\u0026#34; value=\u0026#34;\u0026#34;/\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;添加\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 最后把index.jsp内容修改为:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;%@ page contentType=\u0026#34;text/html;charset=UTF-8\u0026#34; language=\u0026#34;java\u0026#34; isELIgnored=\u0026#34;false\u0026#34; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;% response.sendRedirect(request.getContextPath()+\u0026#34;/list\u0026#34;); %\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 14.配置tomcat，启动服务器（记得加载项目名） 15.查看运行结果（已经成功） 16.项目源码 源码+sql：https://www.lanzous.com/i679e2d\n如果有什么问题，可以私聊我，看见解决\n欢迎不会的可以相互交流。\n交流群：629383010（免费）\n","date":"2019-09-16T20:26:56Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/201909162026/","title":"Maven+SSM框架，实现单表简单的增删改查"},{"content":"urllib包 urllib是一个包含几个模块来处理请求的库： - urllib.request发送http请求 - urllib.error处理请求过程中出现的异常 - urllib.parse解析url - urllib.robotparser解析robots.txt文件\n一般我们爬虫只需要常用的几个，下面只列出比较常用的函数\n我们使用urllib模块，那就要引用模块\n1 import urllib.request urlreteieve：直接下载网页到本地 格式 urlreteieve（网址，本地的文件）\n示例：\n1 2 3 import urllib.request urllib.request.urlretrieve(\u0026#34;https://read.douban.com/provider/all\u0026#34;,\u0026#34;F:/test/down.html\u0026#34;) print(\u0026#34;下载完成\u0026#34;) urlcleanup：清楚系统缓存 1 2 3 4 import urllib.request urllib.request.urlcleanup() urllib.request.urlretrieve(\u0026#34;https://read.douban.com/provider/all\u0026#34;,\u0026#34;F:/test/down.html\u0026#34;) print(\u0026#34;下载完成\u0026#34;) info() ：看相应情况的简介\n1 2 3 import urllib.request file=urllib.request.urlopen(\u0026#34;https://read.douban.com/provider/all\u0026#34;) print(file.info()) getcode() 返回网页爬取状态码 geturl() 获取当前访问的网页的url ","date":"2019-09-07T17:05:43Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/201909071705/","title":"Python的爬虫利器之urllib"},{"content":"目录\n# 正则表达式\n#普通字符作为原子\n#非打印字符作为原子\n# 通用字符作为原子（作用非常大）\n# 原子表(几个原子组成一个)\n# 元字符\n# 模式修正符\n# 贪婪模式和懒惰模式\n# 正则表达式函数\n#示例：匹配.com和.cn网址​​​​​\n#示例：匹配电话号码\n# 正则表达式 1 2 import re str =\u0026#34;weidongliang\u0026#34; #普通字符作为原子 1 2 3 pat =\u0026#34;wei\u0026#34; rsr=re.search(pat,str) print(rsr) #非打印字符作为原子 #\\n:换行符 \\t：制表符\n1 2 3 4 5 6 str =\u0026#39;\u0026#39;\u0026#39;baidusdfs baidu \u0026#39;\u0026#39;\u0026#39; pat=\u0026#34;\\n\u0026#34; rest=re.search(pat,str) print(rest) # 通用字符作为原子（作用非常大）\n\\w:匹配任何字母，数字，下划线 \\W:除匹配任何字母，数字，下划线 \\d:十进制数字 \\D:除十进制数字 \\s:空白字符 \\S:除空白字符 1 2 3 string=\u0026#34;taobao4 59454baidu\u0026#34; pat=\u0026#34;\\w\\d\\s\\d\\d\u0026#34; print(re.search(pat,string)) # 原子表(几个原子组成一个) 1 2 3 4 string=\u0026#34;taobao4 59454baidu\u0026#34; pat=\u0026#34;tao[zub]ao\u0026#34; pat=\u0026#34;tao[^zuq]ao\u0026#34; print(re.search(pat,string)) # 元字符 . :出换行外任意一个字符 \\^ ：开始位置 \\$ :结束位置 \\* ：0\\\\1\\\\多次 ？：0\\\\1次 \\+ ：1\\\\多次 \\{n\\} ：恰好n次 \\{n,\\} ：至少n次 \\{n，m\\}：至少n，最多m | ：模式选择符 \\(\\) :模式单元 1 2 3 4 5 wei=\u0026#34;taoyun14524baidu\u0026#34; pat=\u0026#34;tao.un\u0026#34; pat=\u0026#34;bai..$\u0026#34; ce=re.search(pat,wei) print(ce) # 模式修正符 I 匹配时忽悠大小写 \\* M 多行匹配 \\* L 本地化识别匹配 U Unicode S 让.匹配包括换行符 \\* 1 2 3 4 str=\u0026#34;Python\u0026#34; pat=\u0026#34;pyt\u0026#34; ce=re.search(pat,str,re.I) print(ce) # 贪婪模式和懒惰模式 # 默认是贪婪模式\n1 2 3 4 5 6 7 str=\u0026#34;pythony\u0026#34; pat=\u0026#34;p.*y\u0026#34; #贪婪模式，模糊 pat2=\u0026#34;p.*?y\u0026#34; #懒惰模式，精准 ce=re.search(pat,str,re.I) ce2=re.search(pat2,str,re.I) print(ce) print(ce2) # 正则表达式函数 #1，match 重头开始匹配\n1 2 3 4 str=\u0026#34;poyajgsdabskjdbaiush\u0026#34; pat=\u0026#34;p.*?y\u0026#34; ce=re.match(pat,str,re.I) print(ce) #2，search 任何地方都可以匹配\n#3，全局匹配函数\n1 2 str=\u0026#34;efdrpoyajgspnyskjdbapyth\u0026#34; pat=\u0026#34;p.*?y\u0026#34; # 全局匹配格式re.compile(正则表达式).findall(数据)\n1 2 ce=re.compile(pat).findall(str) print(ce) #示例：匹配.com和.cn网址 ​​​​​\n1 2 3 4 string=\u0026#34;\u0026lt;a href=\u0026#39;http://www.baidu.com\u0026#39;\u0026gt;百度\u0026lt;/a\u0026gt;\u0026#34; pat=\u0026#34;[a-zA-Z]+://[^\\s]*[.com|.cn]\u0026#34; ce=re.search(pat,string) print(ce) #示例：匹配电话号码 1 2 3 4 string=\u0026#34;adsgdiasdiauhdaj012-754745745dasd0773-46853415adasda\u0026#34; pat=\u0026#34;\\d{4}-\\d{7}|\\d{3}-\\d{8}\u0026#34; ce=re.compile(pat).findall(string) print(ce) ","date":"2019-09-07T16:45:11Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/201909071645/","title":"Python的正则表达式"},{"content":"异常处理 由于我经常爬虫，会因为网络，字符编码集等原因让程序崩溃，从而导致代码停止。\n为了解决这个问题，我们可以使用异常处理，从而使程序跳过异常，保证程序可以不停止运行。 异常处理的格式（这个是最常见的一种，也是最实用的一种） 1 2 3 4 try： 程序 except Exception as 异常名称： 异常处理部分 示例\n1 2 3 4 5 6 7 8 try: for i in range(0,10): print(i) if(i==4): print(ij) print(\u0026#34;hello\u0026#34;) except Exception as err: print(err) 异常处理过后 1 2 3 4 5 6 7 8 #让异常后的程序继续 for i in range(0,10): try: print(i) if(i==4): print(ij) except Exception as err: print(err) ","date":"2019-09-05T14:17:46Z","image":"https://www.ownit.top/title_pic/72.jpg","permalink":"https://www.ownit.top/p/201909051417/","title":"Python的异常处理"},{"content":"类和对象 class 类名\n类里面的东西\n1 2 class c1: pass 实例化一个类\n1 a=c1() 构造函数 （构造方法）\n#self：在类中的方法必须加上seif参数\n#__init__(self,参数)\n构造函数实际意义：初始化\n1 2 3 class c2: def __init__(self): print(\u0026#34;南宫乘风\u0026#34;) 给类加上参数：给构造方法加上参数 1 2 3 class c3: def __init__(self,name,job): print(\u0026#34;我的名字\u0026#34;+name+\u0026#34;工作是\u0026#34;+job) 属性：类里面的变量：self.属性名\n1 2 3 4 class c4: def __init__(self,name,job): self.myname=name self.myjob=job 方法：类里面的函数：def 方法名（self，参数） 1 2 3 class c5: def fun1(self,name): print (\u0026#34;hello\u0026#34;+name) 1 2 3 4 5 class c6: def __init__(self,name): self.myname=name def fun2(self): print (\u0026#34;hello\u0026#34;+self.myname) 继承（单继承，多继承）\n#某一个家庭有父亲，母亲，儿子，女儿\n#父亲可以说话，母亲可以写字\n#儿子继承了父亲，女儿同时继承了父母,并且可听东西\n#小儿子继承了父亲，但是优化了父亲的说话能力\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #父亲类 class father(): def speak(self): print(\u0026#34;I can speak\u0026#34;) #单继承：class子类（父亲） #儿子类 class son(father): pass #母亲类 class mother(): def write(self): print(\u0026#34;I can write\u0026#34;) #多继承 #女儿类 class daugther(father,mother): def listen(self): print(\u0026#34;I can listen\u0026#34;) #重载：（重载） #小儿子类 class son2(father): def speak(self): print(\u0026#34;I can speak2\u0026#34;) ","date":"2019-09-05T14:06:48Z","image":"https://www.ownit.top/title_pic/03.jpg","permalink":"https://www.ownit.top/p/201909051406/","title":"Python的类和对象"},{"content":"乘法口诀表 1 2 3 4 5 6 print(\u0026#34;乘法口诀表\u0026#34;) for i in range(1,10): for j in range(1,i+1): print(str(i)+str(\u0026#34;*\u0026#34;)+str(j)+\u0026#34;=\u0026#34;+str(i*j),end=\u0026#34; \u0026#34;) print() 逆向乘法口诀表\n1 2 3 4 5 6 print(\u0026#34;逆向乘法口诀表\u0026#34;) for i in range(9,0,-1): for j in range(i,0,-1): print(str(i)+str(\u0026#34;*\u0026#34;)+str(j)+\u0026#34;=\u0026#34;+str(i*j),end=\u0026#34; \u0026#34;) print() ","date":"2019-09-05T13:57:11Z","image":"https://www.ownit.top/title_pic/07.jpg","permalink":"https://www.ownit.top/p/201909051357/","title":"Python乘法口诀表"},{"content":"#文件操作 #打开 #open(\u0026ldquo;文件地址\u0026rdquo;，\u0026ldquo;操作形式’) 常用四种操作模式 \u0026rsquo;\u0026rsquo;\u0026rsquo;\nw：写入\nr：读取\nb：二进制\na+:追加\n\u0026rsquo;''\n1 2 fh=open(\u0026#34;E:/Code/Python/python.txt\u0026#34;,\u0026#34;r\u0026#34;) date=fh.read() #文件读取 1 date=fh.read() #读取一行类容 1 line=fh.readline() #关闭文件 1 fh.close() #文件写入 1 2 3 4 date=\u0026#34;一起去学习\u0026#34; fh2=open(\u0026#34;E:/Code/Python/python2.txt\u0026#34;,\u0026#34;w\u0026#34;) fh2.write(date) fh2.close() ","date":"2019-09-05T13:51:17Z","image":"https://www.ownit.top/title_pic/24.jpg","permalink":"https://www.ownit.top/p/201909051351/","title":"Python的文件操作"},{"content":"springmvc是什么? Spring Web MVC是一种基于Java的实现了Web MVC设计模式的请求驱动类型的轻量级Web框架，即使用了MVC架构模式的思想，将web层\n进行职责解耦，基于请求驱动指的就是使用请求-响应模型，框架的目的就是帮助我们简化开发，Spring Web MVC也是要简化我们日常Web开发的。\n服务器端分成三层框架 表现层：SpringMVC 包含JSP和Servlet等与WEB相关的内容\n业务层：Spring框架 业务层中不包含JavaWeb API，它只关心业务逻辑\n持久层：MyBatis 封装了对数据库的访问细节\nMVC设计模型 MVC 是一种使用 MVC（Model View Controller 模型-视图-控制器）设计创建 Web 应用程序的模式：\nModel（模型）表示应用程序核心（比如数据库记录列表）。\nView（视图）显示数据（数据库记录）。\nController（控制器）处理输入（写入数据库记录）。\nMVC 模式同时提供了对 HTML、CSS 和 JavaScript 的完全控制。\n**Model（模型）**是应用程序中用于处理应用程序数据逻辑的部分。\n通常模型对象负责在数据库中存取数据。\n**View（视图）**是应用程序中处理数据显示的部分。\n通常视图是依据模型数据创建的。\n**Controller（控制器）**是应用程序中处理用户交互的部分。\n通常控制器负责从视图读取数据，控制用户输入，并向模型发送数据。\n","date":"2019-08-30T15:03:46Z","image":"https://www.ownit.top/title_pic/47.jpg","permalink":"https://www.ownit.top/p/201908301503/","title":"三层架构介绍和MVC设计模型介绍"},{"content":" 源代码下载：https://www.lanzous.com/i5p4mvc * 组件扫描 * @Component：表示这个类需要在应用程序中被创建\n* @ComponentScan：自动发现应用程序中被创建的类\n*\n* 自动装配\n* @Autowired：自动满足bean之间的依赖\n*\n* 定义配置类\n* @Configuration：表示当前类是一个配置类\n* @Autowired的使用方法 * 1：用在构造函数上（多种依赖的情况下）\n* 2：用在成员变量上\n* 3：用在setter方法上\n* 4：用在任意方法上\n* 使用单元测试的方案 *\n* 引入Spring单元测试模块\n* maven：junit，spring-test\n*\n* @RunWith(SpringJUnit4ClassRunner.class) 加载配置类\n* @ContextConfiguration(class=AppConfig.class)\n","date":"2019-08-21T15:07:21Z","image":"https://www.ownit.top/title_pic/57.jpg","permalink":"https://www.ownit.top/p/201908211507/","title":"spring的组件使用"},{"content":"spring框架 Spring框架是由于软件开发的复杂性而创建的。Spring使用的是基本的JavaBean来完成以前只可能由EJB完成的事情。然而，Spring的用途不仅仅限于服务器端的开发。从简单性、可测试性和松耦合性角度而言，绝大部分Java应用都可以从Spring中受益。\n◆目的：解决企业应用开发的复杂性\n◆功能：使用基本的JavaBean代替EJB，并提供了更多的企业应用功能\n◆范围：任何Java应用\nSpring是一个轻量级控制反转(IoC)和面向切面(AOP)的容器框架。\n优点 ◆JAVA EE应该更加容易使用。\n◆面向对象的设计比任何实现技术（比如JAVA EE）都重要。\n◆面向接口编程，而不是针对类编程。Spring将使用接口的复杂度降低到零。（面向接口编程有哪些复杂度？）\n◆代码应该易于测试。Spring框架会帮助你，使代码的测试更加简单。\n◆JavaBean提供了应用程序配置的最好方法。\n◆在Java中，已检查异常（Checked exception）被过度使用。框架不应该迫使你捕获不能恢复的异常。\nIDEA使用maven搭建spring项目 idea建立spring项目相当方便 , 可以自动生成spring配置文件 , 和自动导入Spring所需jar包.\nFile—\u0026gt;new—\u0026gt;project—\u0026gt;Maven\n选择本地的jdk，下一步\n可以根据自己需求填写（没有什么限制）\n选择项目存储的位置，在点击完成就可以。此时一个Maven已经建立成功。\n下面，根据自己的需求添加spring依赖和jar包\n在pom.xml的文件下添加依赖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-context --\u0026gt; \u0026lt;!--spring的核心依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-context\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.3.13.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--log4j的日志文件--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.12\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 加入这段代码，idea的右下角会出现让你导包的选项，你可以点击第一个，导入jar包 好的，现在spring的核心包已经导入，下面开始练习。\n整体项目结果图\n首先，建两个类\nMessagesService ：消息\nMessagePrinter ： 打印机\n就是使用打印机打印消息，就这么简单。\n也就是要MessagePrinter这个类调用MessagesService 类来输出消息。\nMessagesService 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package hello; /** * 打印 */ public class MessagesService { // 无参构造函数 public MessagesService() { super(); System.out.println(\u0026#34;MessageService..\u0026#34;); } public String getMessage(){ return \u0026#34;Hello Word\u0026#34;; } } MessagePrinter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package hello; /** * 打印机 */ public class MessagePrinter { /** * 无参构造函数 */ public MessagePrinter() { super(); System.out.println(\u0026#34;MessagePinter..\u0026#34;); } /** * 建立和MessageService的关联关系 */ private MessagesService service; /** * 设置service的值 * @param service */ public void setService(MessagesService service) { this.service = service; } public void printMessage(){ System.out.println(this.service.getMessage()); } } （1）使用传统的调用方法 创建Application类。\nApplication\n1 2 3 4 5 6 7 8 9 10 11 12 13 package hello; public class Application { public static void main(String[] args) { System.out.println(\u0026#34;appliction\u0026#34;); //创建打印机对象 MessagePrinter printer=new MessagePrinter(); //创建消息服务对象 MessagesService service=new MessagesService(); //设置打印机对象的service属性 printer.setService(service); printer.printMessage(); } } 点击绿色图标，点击第一个运行\n运行结构\n已经成功运行，并输出。\n（2）spring的方法来进行调用 在resources下建立applicationContext.xml配置文件\napplicationContext.xml\n1 2 3 4 5 6 7 8 9 10 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;service\u0026#34; class=\u0026#34;hello.MessagesService\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;printer\u0026#34; class=\u0026#34;hello.MessagePrinter\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;service\u0026#34; ref=\u0026#34;service\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 在java的hello下创建Applicationspring类\nApplicationspring\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package hello; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; public class Applicationspring { public static void main(String[] args) { System.out.println(\u0026#34;applictionspring\u0026#34;); //初始化spring容器 ApplicationContext context; context = new ClassPathXmlApplicationContext(\u0026#34;applicationContext.xml\u0026#34;); //从容器中获取MessagePrinter对象 MessagePrinter printer=context.getBean(MessagePrinter.class); printer.printMessage(); } } 点击运行\n结果\n（3）添加log4j.properties日志\nlog4j.properties\n1 2 3 4 5 6 7 log4j.rootCategory=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{ABSOLUTE} %5p %t %c{2}:%L - %m%n log4j.category.org.springframework.beans.factory=DEBUG 再次运行，会出现各种相关的日志\n","date":"2019-08-21T14:32:48Z","image":"https://www.ownit.top/title_pic/32.jpg","permalink":"https://www.ownit.top/p/201908211432/","title":"IDEA使用maven搭建spring项目"},{"content":"MarkdownPad 2 是一款较不错的Markdown编辑器，可快速将文本转换为美观的HTML/XHTML的网页格式代码，且操作方便，用户可以通过键盘快捷键和工具栏按钮来使用或者移除Markdown格式，左右栏的分割方式令用户可以实时看到 HTML 格式的 Markdown 文档。如果你喜欢在简书、CSDN等平台发表文章，也想掌握这门简单的轻量级的标记语言Markdown，又担心离线无处编辑，欢迎尝试。\n具体方法和资源如下：\nstep1.解压MarkdownPad 2\n地址：https://www.lanzous.com/i47m59c\nstep2.找到根目录下MarkdownPad 2应用程序，运行并破解\n1 2 3 4 5 Email address : Soar360@live.com License key : GBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5/sQytXJUQl/QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ== step3.如果是win10还需要安装一个组件 awesomium_v1.6.6_sdk_win，否则会出现错误提示\n下载地址：https://pan.baidu.com/s/1qY7LKba##step\nstep4:最后重启MarkdownPad 2就可以用了!\n注：如果是英文版，可到tools-\u0026gt;options修改，再重启即可\n","date":"2019-05-18T21:47:15Z","image":"https://www.ownit.top/title_pic/13.jpg","permalink":"https://www.ownit.top/p/201905182147/","title":"MarkdownPad 2破解"},{"content":"前面搭建Java环境和tomcat环境。\n下面进行实战，搭建ejforum论坛\nejforum论坛源码：https://www.lanzous.com/i45rcoh\nCentos7安装MySQL数据库 Centos7安装JDK环境配置 Centos7安装和配置Tomcat8 ok，下面进行论坛搭建\n前提：以上三个配置已经搭建完成。\n（1）使用mysql创建数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mysql\u0026gt; create database ejforum; Query OK, 1 row affected (0.00 sec) mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | ejforum | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.00 sec) mysql\u0026gt; GRANT all ON ejforum.* TO \u0026#34;ejforum\u0026#34;@\u0026#34;localhost\u0026#34; IDENTIFIED BY \u0026#34;ejforum\u0026#34;; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; flush privileges; Query OK, 0 rows affected (0.00 sec) （2）安装mysql连接器（驱动包）下载地址：https://dev.mysql.com/downloads/connector/j/5.1.html\n1 2 3 4 5 6 7 [root@wei mysql-connector-java-5.1.47]# tar zxf mysql-connector-java-5.1.47.tar.gz [root@wei mysql-connector-java-5.1.47]# ls build.xml mysql-connector-java-5.1.47-bin.jar README.txt CHANGES mysql-connector-java-5.1.47.jar src COPYING README [root@wei mysql-connector-java-5.1.47]# cp mysql-connector-java-5.1.47.jar /usr/local/tomcat/lib/ （3）解压ejforum压缩包，并拷贝到网站的目录\n1 2 3 [root@wei ejforum-2.3]# rm -rf /usr/local/tomcat/webapps/ROOT/* [root@wei ~]# unzip ejforum2.3.zip 1 2 3 4 5 6 [root@wei ~]# cd ejforum-2.3/ [root@wei ejforum-2.3]# ls ejforum install [root@wei ~]# cd ejforum [root@wei ejforum-2.3]# cp -r * /usr/local/tomcat/webapps/ROOT/ （4）编辑WEB-INF文件，指定连接MYSQL数据库用户\n1 2 3 [root@wei ejforum]# vim /usr/local/tomcat/webapps/ROOT/WEB-INF/conf/config.xml [root@wei conf]# pwd /usr/local/tomcat/webapps/ROOT/WEB-INF/conf 1 2 3 4 5 6 DB Connection Pool - Mysql \u0026lt;database maxActive=\u0026#34;10\u0026#34; maxIdle=\u0026#34;10\u0026#34; minIdle=\u0026#34;2\u0026#34; maxWait=\u0026#34;10000\u0026#34; username=\u0026#34;ejforum\u0026#34; password=\u0026#34;ejforum\u0026#34; driverClassName=\u0026#34;com.mysql.jdbc.Driver\u0026#34; url=\u0026#34;jdbc:mysql://localhost:3306/ejforum?characterEncoding=gbk\u0026amp;amp;autoReconnect=true\u0026amp;amp;autoReconnectForPools=true\u0026amp;amp;zeroDateTimeBehavior=convertToNull\u0026#34; sqlAdapter=\u0026#34;sql.MysqlAdapter\u0026#34;/\u0026gt; （5）重启tomcat服务\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@wei conf]# /usr/local/tomcat/bin/shutdown.sh Using CATALINA_BASE: /usr/local/tomcat Using CATALINA_HOME: /usr/local/tomcat Using CATALINA_TMPDIR: /usr/local/tomcat/temp Using JRE_HOME: /usr/java/jdk-12.0.1 Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar NOTE: Picked up JDK_JAVA_OPTIONS: --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.rmi/sun.rmi.transport=ALL-UNNAMED [root@wei conf]# /usr/local/tomcat/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcat Using CATALINA_HOME: /usr/local/tomcat Using CATALINA_TMPDIR: /usr/local/tomcat/temp Using JRE_HOME: /usr/java/jdk-12.0.1 Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar Tomcat started. ","date":"2019-05-14T23:48:18Z","image":"https://www.ownit.top/title_pic/41.jpg","permalink":"https://www.ownit.top/p/201905142348/","title":"Centos7部署ejforum论坛（Java+tomcat+mysql）"},{"content":"第一步：下载Tomcat8压缩包\n进入 http://tomcat.apache.org/download-80.cgi\n下载tar.gz压缩包\n第二步：用xshell工具把压缩包上传到/home/data/下\n第三步：解压以及新建目录\n1 [root@wei data]# tar -zxvf apache-tomcat-8.5.40.tar.gz 我们新建/home/tomcat/目录 把tomcat剪切进去\n第四步：配置tomcat server.xml\nserver.xml可以配置端口，编码以及配置项目等等，我们这里就配置一个端口，把默认的8080\nvi /home/tomcat/apache-tomcat-8.5.16/conf/server.xml\n第五步：配置防火墙，开放8080端口\nfirewall-cmd --zone=public --add-port=8080/tcp --permanent\nfirewall-cmd --reload\n第六步：启动tomcat\n1 [root@wei home]# /home/tomcat/apache-tomcat-8.5.40/bin/startup.sh 下面是tomcat启动的界面（端口8080）\n下面是httpd启动的界面（端口80）\n","date":"2019-05-14T22:31:52Z","image":"https://www.ownit.top/title_pic/34.jpg","permalink":"https://www.ownit.top/p/201905142231/","title":"Centos7安装和配置Tomcat8"},{"content":"作为一名程序员，各种环境搭建都要会。\n下面介绍关于Linux操作系统之centos7（64位）安装JDK以及环境配置。\n下面开始学习吧\n查看并卸载CentOS自带的OpenJDK 安装好的CentOS会自带OpenJdk,用命令 java -version ，会有下面的信息： 1 2 3 java version \u0026#34;1.6.0\u0026#34; OpenJDK Runtime Environment (build 1.6.0-b09) OpenJDK 64-Bit Server VM (build 1.6.0-b09, mixed mode) 最好还是先卸载掉openjdk,在安装sun公司的jdk.\n先查看 rpm -qa | grep java 显示如下信息：\n1 2 java-1.4.2-gcj-compat-1.4.2.0-40jpp.115 java-1.6.0-openjdk-1.6.0.0-1.7.b09.el5 卸载：\n1 2 rpm -e --nodeps java-1.4.2-gcj-compat-1.4.2.0-40jpp.115 rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.7.b09.el5 STEP 1：\n现在JDK12都出来了，所以我们也要紧跟着技术的潮流走。\n所有下载的jdk-12.0.1_linux-x64_bin.rpm\n官网链接：JDK官网下载\n选择与自己系统相匹配的版本，我的是Centos7 64位的，所以如果是我的是jdk-12.0.1_linux-x64_bin.rpm\nSTEP 2：\n利用Xshell工具进行Linux命令处理，进行安装。\n进行rpm安装jdk\n1 [root@wei ~]# rpm -ivh jdk-12.0.1_linux-x64_bin.rpm 下面以及安装成功过jdk。\n注意：安装成功，但是环境变量没有配置，需要配置环境变量。\n（1）找到jdk安装路径\n1 2 3 4 5 [root@wei ~]# cd /usr/java/jdk-12.0.1/ [root@wei jdk-12.0.1]# ls bin conf include jmods legal lib man release [root@wei jdk-12.0.1]# pwd /usr/java/jdk-12.0.1 （2）复制路径，开始配置环境变量（/usr/java/jdk-12.0.1）\n编辑：/etc/profile文件，在最后面加入下面的代码\n1 2 export JAVA_HOME=/usr/java/jdk-12.0.1 export PATH=$PATH:JAVA_HOME/bin 1 [root@wei jdk-12.0.1]# source /etc/profile （3）保存，重新加载配置文件\n（4）查看jdk的配置信息，看是否配置完成\n1 [root@wei jdk-12.0.1]# java -version ok，jdk已经成功安装成功。恭喜你有学会一个新的技能。 加油。","date":"2019-05-14T21:52:01Z","image":"https://www.ownit.top/title_pic/59.jpg","permalink":"https://www.ownit.top/p/201905142152/","title":"Centos7安装JDK环境配置"},{"content":"一、python的优缺点 优点： 优美、清晰、简单 高级语言 开发效率高 可移植性、可拓展性、可嵌入性 缺点： 运行速度慢 代码不能加密 线程不能利用多CPU 二、python2与python3的区别 代码： python2：代码混乱，重复代码较多，冗余 python3：代码崇尚优美、清晰、简单 print： python2：print是一个语句 python3：print是一个函数 input： python2：raw_input()接收字符串，input()接收数字 python3：input()接收的全部是字符串 编码方式： python2：默认编码是ASCII码（若想使用中文：#_*_coding:utf-8_*_） python3：默认编码是utf-8，支持中文 不等运算符： python2：可以使用!=或者\u0026gt;\u0026lt; python3：只能使用!= 创建迭代计数器： python2：xrange python3：range repr： python2：repr可以是语句 python3：只允许使用repr()函数 文件： python2：可以使用!=或者\u0026gt;\u0026lt; python3：只能使用!= 整型： python2：存在long型 python3：全部为int型 修改语法： python2：字典的keys，values，items以及map，filter，reduce返回的都是一个列表 python3：字典的keys，values，items以及map，filter，reduce返回一个可迭代对象 新增语法： python2：print和exec语句，无nolocal等方法 python3：print和exec改为函数，新增nolocal等方法 继承： python2：默认经典类（新式类需要(object)） python3：只有新式类 三、开发的种类 编译型 缺点：排错慢，开发效率低，不可移植\n优点：执行效率高\n典型：C语言，go语言\n解释型 缺点：执行效率低\n优点：排错快，开发效率高，可移植\n典型：python，PHP\n混合型 典型：java，C#\n四、python的种类 Cpython：基于C语言开发的\nlpython\nJpython\nPyPy：目前执行最快的\n五、变量与常量 常量：一直不变的量，约定俗称，全部大写为常量\n变量：把程序的运行结果存放在内存中，以便后期代码的调用\n要求：\n必须由数字、字母、下划线组成 不能以数字开头 不能是关键字 不能是中文，不能太长，要有可描述性 官网推荐下划线old_boy和驼峰体OldBoy ","date":"2019-05-12T21:48:08Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/201905122148/","title":"Python基础知识"},{"content":"环境：Centos7\n工具：mysql，php，httpd\n目的：熟练掌握httpd服务器搭建和个服务器之间的配合。\n有兴趣的朋友可以来实践一下，我会提供各种源码进行搭建。\n网络家园和论坛****源码：https://www.lanzous.com/i3yqq3c\n（1）准备一台centos服务器，我是在虚拟机搭建的centos7.\n（2）关闭防火墙和selinux等（centos7关闭防火墙和selinux）\n（3）搭建myql数据库（MySQL的rpm安装教程）\n（4）搭建httpd服务器（centos7自带httpd，只需要启动即可用）\n（5）安装PHP服务器\n**安装** 1 [root@wei ~]# yum install php –y 安装php-mysql\n1 [root@wei ~]# yum install php-mysql –y 2.测试php和apache协同\n测试协同\n1 [root@localhost ~]# cd /var/www/html/ 1 2 3 4 5 6 7 8 9 [root@localhost html]# vim phpinfo.php \u0026lt;?php phpinfo(); ?\u0026gt; ** 测试： http://IP/phpinfo.php**\n测试php和MySQL协同 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@localhost html]# vim php_mysql.php \u0026lt;?php $servername = \u0026#34;localhost\u0026#34;; $username = \u0026#34;admin\u0026#34;; $password = \u0026#34;123456\u0026#34;; // 创建连接 //$con = mysql_connect($servername,$username,$password); $conn = new mysqli($servername, $username, $password); // 检测连接 if ($conn-\u0026gt;connect_error) { die(\u0026#34;连接失败: \u0026#34; . $conn-\u0026gt;connect_error); } echo \u0026#34;连接成功\u0026#34;; ?\u0026gt; ** 测试： http://IP/php_mysql.php**\n（6）部署应用\n1.上传代码（代码在上面）（代码上传到/var/www/html/目录）\n2.解压\n安装解压软件：\n1 [root@localhost html]# yum install unzip –y 3.配置\n改名：\n1 [root@wei html]# mv upload/ farm 在线安装： http://192.168.196.131/farm/install/index.php\n1.问题一\n修改/etc/php.ini， 将short_open_tag = On\n1 vim /etc/php.ini 修改完毕，重启httpd服务。\n1 [root@wei html]# systemctl restart httpd 2.问题二\n修改目录权限：\n1 [root@localhost html]# chmod -R 777 farm 让后进项下面步骤，进行在线安装\n步骤一：\n步骤二：\n创建farm数据库和用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [root@wei html]# mysql -u root -proot ##登录数据库 Warning: Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 6 Server version: 5.6.44 MySQL Community Server (GPL) Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. mysql\u0026gt; create database farm; ##创建farm数据库 Query OK, 1 row affected (0.00 sec) mysql\u0026gt; grant all on farm.* to farm@\u0026#39;localhost\u0026#39; identified by \u0026#39;farm\u0026#39;; #创建用户，进行授权 Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; flush privileges; ##刷新权限表 Query OK, 0 rows affected (0.00 sec) 步骤三：\n进行安装\n步骤四：\n进行测试\nhttp://192.168.196.131/farm/bbs/\nhttp://192.168.196.131/farm/\nhttp://192.168.196.131/farm/home/space.php?do=home\n","date":"2019-04-29T23:43:33Z","image":"https://www.ownit.top/title_pic/71.jpg","permalink":"https://www.ownit.top/p/201904292343/","title":"Centos7服务器搭建网络家园和论坛"},{"content":"Navicat是一套数据库管理工具，专为简化数据库的管理及降低系统管理成本而设。\n**Navicat 是以直觉化的图形用户界面而建的，可以安全和简单地创建、组织、访问并共用信息。 Navicat Premium 是 Navicat 的产品成员之一，能简单并快速地在各种数据库系统间传输数据，或传输一份指定 SQL 格式及编码的纯文本文件。其他功能包括导入向导、导出向导、查询创建工具、报表创建工具、资料同步、备份、工作计划及更多。 本文介绍在Navicat Premium中进行简单的数据库管理。 在下载安装完 Navicat Premium 之后，进行以下操作。**\nNavicat Premium 软件：（提供32位\u0026amp;\u0026amp;64位【破解文件】）\n链接：https://pan.baidu.com/s/1bt_W1FuB-uzlmkPaYfj-2g 密码：op4h\n如果软件链接失效，可留言提供最新链接\nNavicat Premium 安装破解教程\nhttps://blog.csdn.net/loveer0/article/details/82016644\nNavicat Premium 官网详细教程\n**http://www.formysql.com/premium/ **\n在下载安装完 Navicat Premium 之后，进行以下操作。\n连接MySQL数据库 快捷键 功能 ctrl+F 搜索本页数据 Ctrl+Q 打开查询窗口 Ctrl+/ 注释sql语句 Ctrl+Shift +/ 解除注释 Ctrl+R 运行查询窗口的sql语句 Ctrl+Shift+R 只运行选中的sql语句 F6 打开一个mysql命令行窗口 Ctrl+L 删除一行 Ctrl+N 打开一个新的查询窗口 Ctrl+W 关闭一个查询窗口 Ctrl+D 表的数据显示显示页面切换到表的结构设计页面，但是在查询页面写sql时是复制当前行 ","date":"2019-04-27T21:35:58Z","image":"https://www.ownit.top/title_pic/17.jpg","permalink":"https://www.ownit.top/p/201904272135/","title":"Navicat Premium 详解"},{"content":"Mycat实现MySQL主从复制读写分离\nMyCAT的安装及部署\n1、部署jdk环境\nMyCAT用Java开发，需要有JAVA运行环境，mycat依赖jdk1.7的环境\n1）上传jdk\n1 2 [root@localhost tools]# ll jdk-7u45-linux-x64.tar.gz -rw-r--r-- 1 root root 138094686 10月 24 2013 jdk-7u45-linux-x64.tar.gz 2）安装jdk\n1 2 [root@localhost tools]# mkdir /usr/java [root@localhost tools]# tar xf jdk-7u45-linux-x64.tar.gz -C /usr/java/ 3）设置环境变量\n1 [root@localhost tools]# vim /etc/profile.d/java.sh 内容如下：\n1 2 3 export JAVA_HOME=/usr/java/jdk1.7.0_45/ export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 使环境变量当前终端生效\n1 [root@localhost tools]# source /etc/profile.d/java.sh 4）测试\n1 2 3 4 [root@localhost tools]# java -version java version \u0026#34;1.7.0_45\u0026#34; Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode) 2、安装Mycat\n1）上传mycat包\n1 2 [root@localhost tools]# ll Mycat-server-1.5.1-RELEASE-20161130213509-linux.tar.gz -rw-r--r--. 1 root root 11499865 12月 15 16:33 Mycat-server-1.5.1-RELEASE-20161130213509-linux.tar.gz 2）解压\n1 [root@localhost tools]# tar xf Mycat-server-1.5.1-RELEASE-20161130213509-linux.tar.gz -C /usr/local/ 解压后内容如下：\n1 [root@localhost tools]# ll /usr/local/mycat 总用量 16\n1 2 3 4 5 6 drwxr-xr-x 2 root root 4096 12月 15 11:36 bin drwxrwxrwx 2 root root 6 3月 1 2016 catlet drwxrwxrwx 4 root root 4096 12月 15 11:36 conf drwxr-xr-x 2 root root 4096 12月 15 11:36 lib drwxrwxrwx 2 root root 6 10月 28 20:47 logs -rwxrwxrwx 1 root root 217 10月 28 20:47 version.txt 3）添加环境变量\n1 2 3 4 [root@localhost tools]# vim /etc/profile.d/mycat.sh export PATH=$PATH:/usr/local/mycat/bin [root@localhost tools]# source /etc/profile.d/mycat.sh 3、读写分离配置\n1）不使用Mycat托管MySQL主从服务器，简单使用如下配置\n#注意：配置前备份下配置文件\n1 2 [root@localhost tools]# cd /usr/local/mycat/conf [root@localhost conf]# cp schema.xml{,.bak} (1)\u0026lt;schema name=\u0026ldquo;TESTDB\u0026rdquo; checkSQLschema=\u0026ldquo;false\u0026rdquo; sqlMaxLimit=\u0026ldquo;100\u0026rdquo; dataNode=\u0026ldquo;dn1\u0026rdquo;\u0026gt;\n这里的TESTDB就是我们所宣称的数据库名称，必须和server.xml中的用户指定的数据库名称一致。添加一个dataNode=\u0026ldquo;dn1\u0026rdquo;，是指定了我们这个库只有在dn1上，没有分库。\n(2)\u0026lt;dataNode name=\u0026ldquo;dn1\u0026rdquo; dataHost=\u0026ldquo;localhost1\u0026rdquo; database=\u0026ldquo;db1\u0026rdquo; /\u0026gt;\n这里只需要改database的名字，就是你真是的数据库上的数据库名，可根据自己的数据库名称修改。\n(3) \u0026lt;dataHost name=\u0026ldquo;localhost1\u0026rdquo; maxCon=\u0026ldquo;1000\u0026rdquo; minCon=\u0026ldquo;10\u0026rdquo; balance=\u0026ldquo;1\u0026rdquo;\nwriteType=\u0026ldquo;0\u0026rdquo; dbType=\u0026ldquo;mysql\u0026rdquo; dbDriver=\u0026ldquo;native\u0026rdquo; switchType=\u0026ldquo;1\u0026rdquo; slaveThreshold=\u0026ldquo;100\u0026rdquo;\u0026gt;\n需要配置的位置：\nbalance=\u0026ldquo;1\u0026rdquo; writeType=\u0026ldquo;0\u0026rdquo; switchType=\u0026ldquo;1\u0026rdquo;\nbalance\n1、balance=0 不开启读写分离机制，所有读操作都发送到当前可用的writehostle .\n2、balance=1 全部的readhost与stand by writeHost 参与select语句的负载均衡。简单的说，双主双从模式(M1-\u0026gt;S1,M2-\u0026gt;S2，并且M1和M2互为主备)，正常情况下，M1，S1，S2都参与select语句的复杂均衡。\n3、balance=2 所有读操作都随机的在readhost和writehost上分发\nwriteType\n负载均衡类型，目前的取值有3种：\n1、writeType=\u0026ldquo;0\u0026rdquo;, 所有写操作发送到配置的第一个writeHost。\n2、writeType=\u0026ldquo;1\u0026rdquo;，所有写操作都随机的发送到配置的writeHost。\n3、writeType=\u0026ldquo;2\u0026rdquo;，不执行写操作。\nswitchType\n1、switchType=-1 表示不自动切换\n2、switchType=1 默认值，自动切换\n3、switchType=2 基于MySQL 主从同步的状态决定是否切换\n(4)\u0026lt;writeHost host=\u0026ldquo;hostM1\u0026rdquo; url=\u0026ldquo;192.168.95.120:3306\u0026rdquo; user=\u0026ldquo;mycat\u0026rdquo; password=\u0026ldquo;123456\u0026rdquo;\u0026gt;\n\u0026lt;!– can have multi read hosts –\u0026gt;\n\u0026lt;readHost host=\u0026ldquo;hostS2\u0026rdquo; url=\u0026ldquo;192.168.95.140:3306\u0026rdquo; user=\u0026ldquo;mycat_r\u0026rdquo; password=\u0026ldquo;123456\u0026rdquo; /\u0026gt;\n\u0026lt;readHost host=\u0026ldquo;hostS3\u0026rdquo; url=\u0026ldquo;192.168.95.140:3307\u0026rdquo; user=\u0026ldquo;mycat_r\u0026rdquo; password=\u0026ldquo;123456\u0026rdquo; /\u0026gt;\n\u0026lt;!\u0026ndash;\u0026lt;writeHost host=\u0026ldquo;hostS1\u0026rdquo; url=\u0026ldquo;localhost:3316\u0026rdquo; user=\u0026ldquo;root\u0026rdquo;\npassword=\u0026ldquo;123456\u0026rdquo; /\u0026gt;\u0026ndash;\u0026gt;\n配置好文件内容如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mycat:schema SYSTEM \u0026#34;schema.dtd\u0026#34;\u0026gt; \u0026lt;mycat:schema xmlns:mycat=\u0026#34;http://org.opencloudb/\u0026#34; \u0026gt; \u0026lt;schema name=\u0026#34;TESTDB\u0026#34; checkSQLschema=\u0026#34;false\u0026#34; sqlMaxLimit=\u0026#34;100\u0026#34; dataNode=\u0026#34;dn1\u0026#34;\u0026gt; \u0026lt;/schema\u0026gt; \u0026lt;dataNode name=\u0026#34;dn1\u0026#34; dataHost=\u0026#34;localhost1\u0026#34; database=\u0026#34;db1\u0026#34; /\u0026gt; \u0026lt;dataHost name=\u0026#34;localhost1\u0026#34; maxCon=\u0026#34;1000\u0026#34; minCon=\u0026#34;10\u0026#34; balance=\u0026#34;1\u0026#34; writeType=\u0026#34;0\u0026#34; dbType=\u0026#34;mysql\u0026#34; dbDriver=\u0026#34;native\u0026#34; switchType=\u0026#34;1\u0026#34; slaveThreshold=\u0026#34;100\u0026#34;\u0026gt; \u0026lt;heartbeat\u0026gt;select user()\u0026lt;/heartbeat\u0026gt; \u0026lt;writeHost host=\u0026#34;hostM1\u0026#34; url=\u0026#34;192.168.95.120:3306\u0026#34; user=\u0026#34;root\u0026#34; password=\u0026#34;123456\u0026#34;\u0026gt; \u0026lt;readHost host=\u0026#34;hostR1\u0026#34; url=\u0026#34;192.168.95.140:3306\u0026#34; user=\u0026#34;root\u0026#34; password=\u0026#34;123456\u0026#34; /\u0026gt; \u0026lt;readHost host=\u0026#34;hostR2\u0026#34; url=\u0026#34;192.168.95.140:3307\u0026#34; user=\u0026#34;root\u0026#34; password=\u0026#34;123456\u0026#34; /\u0026gt; \u0026lt;/writeHost\u0026gt; \u0026lt;/dataHost\u0026gt; \u0026lt;/mycat:schema\u0026gt; 4、创建管理用户\n主库上对mycat用户授权如下：\n用户：mycat 密码：123456 端口：3306\n权限：insert,delete,update,select\n命令：grant insert,delete,update,select on TD_OA.* to mycat@\u0026lsquo;192.168.95.%\u0026rsquo; identified by \u0026lsquo;123456\u0026rsquo;;\nflush privileges;\n从库上mycat_r用户授权如下：\n用户：mycat_r 密码：123456 端口：3306/3307\n权限： select\ngrant select on TD_OA.* to mycat@\u0026lsquo;192.168.95.%\u0026rsquo; identified by \u0026lsquo;123456\u0026rsquo;;\nflush privileges;\n测试环境可以直接使用root用户，授予所有权限：\n1 2 3 4 mysql\u0026gt; grant all on *.* to root@\u0026#39;192.168.95.%\u0026#39; identified by \u0026#39;123456\u0026#39;; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; grant all on *.* to root@\u0026#39;localhost\u0026#39; identified by \u0026#39;123456\u0026#39;; 5、修改mycat配置文件\n采用默认配置\n注意：\n①这里配置的是可以连接主库的两个用户\n用户：test 密码：test 给予此用户TESTDB数据库增删改查的权限。\n用户：user 密码：password 给予此用户TESTDB数据库读的权限。\n②这里的TESTDB，不一定是你数据库上的真实库名，可以任意指定，只要接下来和schema.xml的配置文件的库名统一即可。\n6、启动Mycat\n方法一：# mycat console #\u0026lt;=通过console命令启动mycat，这样方便提取信息\n方法二：# mycat start\n方法三：# startup_nowrap.sh #服务脚本方式启动\n[root@localhost conf]# netstat -lnupt | egrep \u0026ldquo;(8|9)066\u0026rdquo;\ntcp6 0 0 :::9066 :::* LISTEN 3342/java\ntcp6 0 0 :::8066 :::* LISTEN 3342/java\n重启：# mycat restart\n7、在客户端连接mysql主库服务器：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # mysql -uuser -puser -h192.168.95.130 -P8066 -DTESTDB Warning: Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB) Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. mysql\u0026gt; 8、主从同步读写分离测试\n管理端创建表:\n1 2 [root@localhost ~]# mysql -utest -ptest -h192.168.95.130 -P8066 -DTESTDB CREATE TABLE test1 (id int(10),name varchar(10),address varchar(20) DEFAULT NULL); 手动停止主从同步: stop slave;\n分别在主从库插入数据:\n**master: insert into test1 values(1,\u0026rsquo;test1\u0026rsquo;,\u0026lsquo;master\u0026rsquo;);\nslave1: insert into test1 values(2,\u0026rsquo;test1\u0026rsquo;,\u0026lsquo;slave1\u0026rsquo;);\nslave2: insert into test1 values(3,\u0026rsquo;test1\u0026rsquo;,\u0026lsquo;slave2\u0026rsquo;);**\n管理端验证\n负载均衡:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mysql\u0026gt; select * from test1; +------+-------+---------+ | id | name | address | +------+-------+---------+ | 2 | test1 | slave1 | +------+-------+---------+ 1 row in set (0.00 sec) mysql\u0026gt; select * from test1; +------+-------+---------+ | id | name | address | +------+-------+---------+ | 3 | test1 | slave2 | +------+-------+---------+ 1 row in set (0.00 sec) 读写功能:\n管理端再次插入数据 insert into test1 values(4,\u0026rsquo;test1\u0026rsquo;,\u0026lsquo;write\u0026rsquo;);\n1 2 mysql\u0026gt; insert into test1 values(4,\u0026#39;test1\u0026#39;,\u0026#39;write\u0026#39;); Query OK, 1 row affected (0.00 sec) #注意：测试完毕启动主从同步功能。\n9、管理命令与监控\nmycat自身有类似其他数据库的管理监控方式，可通过mysql命令行，登陆端口9066执行相应的SQL操作，也可通过jdbc的方式进行远程连接管理。\n登录：目前mycat有两个端口，8066数据端口，9066管理端口。命令行登录时通过9066管理端口来执行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # mysql -uuser -puser -h192.168.95.130 -P9066 -DTESTDB Warning: Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 2 Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (monitor) Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. mysql\u0026gt; 选项：\n-h 后面接主机\n-u mycat server.xml配置的逻辑库用户\n-p mycat server.xml配置的逻辑库密码\n-P 后面接的端口9066，注意P大写\n-D Mycat server.xml中配置的逻辑库\n1）查看所有的命令，如下：\n1 mysql\u0026gt; show @@help; 2）显示mycat数据库的列表，对应的在scehma.xml配置的逻辑库\n1 2 3 4 5 6 7 mysql\u0026gt; show @@databases; +----------+ | DATABASE | +----------+ | TESTDB | +----------+ 1 row in set (0.00 sec) 3）显示mycat数据节点的列表，对应的是scehma.xml配置文件的dataNode节点\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; show @@datanode; +------+----------------+-------+-------+--------+------+------+---------+------------+----------+---------+---------------+ | NAME | DATHOST | INDEX | TYPE | ACTIVE | IDLE | SIZE | EXECUTE | TOTAL_TIME | MAX_TIME | MAX_SQL | RECOVERY_TIME | +------+----------------+-------+-------+--------+------+------+---------+------------+----------+---------+---------------+ | dn1 | localhost1/db1 | 0 | mysql | 0 | 0 | 1000 | 0 | 0 | 0 | 0 | -1 | | dn2 | localhost1/db2 | 0 | mysql | 0 | 0 | 1000 | 0 | 0 | 0 | 0 | -1 | | dn3 | localhost1/db3 | 0 | mysql | 0 | 0 | 1000 | 0 | 0 | 0 | 0 | -1 | +------+----------------+-------+-------+--------+------+------+---------+------------+----------+---------+---------------+ 3 rows in set (0.00 sec) 其中，NAME表示datanode的名称；dataHost 对应的是dataHost属性的值，数据主机的名称，ACTIVE表示活跃的连接数，IDIE表示闲置的连接数，SIZE对应的是总连接的数量。\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; show @@heartbeat; +--------+-------+----------------+------+---------+-------+--------+---------+--------------+---------------------+-------+ | NAME | TYPE | HOST | PORT | RS_CODE | RETRY | STATUS | TIMEOUT | EXECUTE_TIME | LAST_ACTIVE_TIME | STOP | +--------+-------+----------------+------+---------+-------+--------+---------+--------------+---------------------+-------+ | hostM1 | mysql | 192.168.95.120 | 3306 | 1 | 0 | idle | 0 | 1,0,0 | 2016-12-15 14:25:35 | false | | hostS2 | mysql | 192.168.95.140 | 3306 | 1 | 0 | idle | 0 | 1,1,1 | 2016-12-15 14:25:35 | false | | hostS3 | mysql | 192.168.95.140 | 3307 | 1 | 0 | idle | 0 | 1,1,1 | 2016-12-15 14:25:35 | false | +--------+-------+----------------+------+---------+-------+--------+---------+--------------+---------------------+-------+ 3 rows in set (0.01 sec) RS_CODE状态为1，正常状态\n4、获取当前mycat的版本\n1 mysql\u0026gt; show @@version; 5、显示mycat前端连接状态\n1 mysql\u0026gt; show @@connection; 6、显示mycat后端连接状态\n1 mysql\u0026gt; show @@backend; 7、显示数据源\n1 mysql\u0026gt; show @@datasource ","date":"2019-04-26T21:07:54Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/201904262107/","title":"MySQL读写分离之MyCAT"},{"content":"MySQL Proxy：\n========================================================\nMySQL_Proxy Master Slave1 Slave2\n＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝=\nIP 192.168.1.250 192.168.1.215 192.168.1.66\nServer_ID 6 215 66\n实现步骤：\nMySQL主/备复制\n安装并配置MySQL Proxy\n测试Proxy\n一、MySQL主/备复制（略）\n二、安装并配置MySQL Proxy\n1. 安装并配置\n1 2 3 4 5 6 7 8 9 10 [root@MySQL_Proxy ~]# service mysqld stop [root@MySQL_Proxy ~]# chkconfig mysqld off [root@MySQL_Proxy ~]# rpm -qa |grep lua lua-5.1.4-4.1.el6.x86_64 [root@MySQL_Proxy ~]# tar xf mysql-proxy-0.8.4-linux-el6-x86-64bit.tar.gz -C /usr/local/ [root@MySQL_Proxy ~]# cd /usr/local/ [root@MySQL_Proxy ~]# ln -s mysql-proxy-0.8.4-linux-el6-x86-64bit mysql-proxy [root@MySQL_Proxy ~]# vim /usr/local/mysql-proxy/share/doc/mysql-proxy/rw-splitting.lua min_idle_connections = 1, max_idle_connections = 1, 2. 启动mysql-proxy\n1 2 [root@MySQL_Proxy ~]# lsof -i TCP:3306 [root@MySQL_Proxy ~]# /usr/local/mysql-proxy/bin/mysql-proxy --help-proxy -P 指定proxy服务器工作的地址和端口\n-b 指定写服务器的地址和端口\n-r 指定读服务器的地址和端口\n-s 指定判断的脚本\n--daemon 以后台进程的方式启动\n调整最大打开的文件数\n1 2 3 4 [root@MySQL_Proxy ~]# ulimit -a |grep \u0026#39;open files\u0026#39; [root@MySQL_Proxy ~]# ulimit -n 10240 [root@MySQL_Proxy ~]# ulimit -a |grep \u0026#39;open files\u0026#39; open files (-n) 10240 1 2 [root@MySQL_Proxy ~]# /usr/local/mysql-proxy/bin/mysql-proxy -P 192.168.1.250:3306 -b 192.168.1.27:3306 -r 192.168.1.215:3306 -r 192.168.1.66:3306 -s /usr/local/mysql-proxy/share/doc/mysql-proxy/rw-splitting.lua --daemon 2014-02-13 17:15:54: (critical) plugin proxy 0.8.4 started 1 2 [root@MySQL_Proxy ~]# netstat -tnlp |grep :3306 tcp 0 0 192.168.10.137:3306 0.0.0.0:* LISTEN 16620/mysql-proxy 1 2 3 [root@MySQL_Proxy ~]# vim /etc/rc.local ulimit -n 10240 /usr/local/mysql-proxy/bin/mysql-proxy -P 192.168.1.250:3306 -b 192.168.1.27:3306 -r 192.168.1.215:3306 -r 192.168.1.66:3306 -s /usr/local/mysql-proxy/share/doc/mysql-proxy/rw-splitting.lua --daemon 三、测试\n1. 主库\n1 2 3 4 5 mysql\u0026gt; grant ALL on bbs.* to bbs@\u0026#39;192.168.1.%\u0026#39; identified by \u0026#39;localhost\u0026#39;; mysql\u0026gt; flush privileges; mysql\u0026gt; create database bbs; mysql\u0026gt; create table bbs.t1 (name varchar(50)); 2. 备库\nmysql\u0026gt; stop slave; //暂时断掉和主库的连接\n3. 从客户端测试\na. 读 ====主 or 备\nb. 写 ====主\n4. 备库\nmysql\u0026gt; start slave;\n=======================================================\n","date":"2019-04-24T00:00:40Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/201904240000/","title":"MySQL读写分离之Proxy"},{"content":"MySQL高可用方案\n投票选举机制，较复杂\nMySQL本身没有提供replication failover的解决方案，自动切换需要依赖MHA脚本\n可以有多台从库，从库可以做报表和备份\nMySQL复制技术\n===========================================================================\n重置数据库：\n1 2 3 # service mysqld stop # rm -rf /usr/local/mysql/data/* # /usr/local/mysql/scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 复制拓扑： 复制原理： 1. 在主库上把数据更改记录到二进制日志（Binary Log）中。 2. 备库将主库上的日志复制到自己的中继日志（Relay Log）中。\n3. 备库读取中继日志中的事件，将其重放到备库数据库之上。\n一、主/备均为刚初始的数据库\n单主到多备： Master-MultiSlave\nMaster Slave1 Slave2\n＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\nIP 192.168.1.27 192.168.1.66 192.168.1.251\nServer_ID 27 66 251\n如何实现主从复制？\n在主服务器（master）上\n启用二进制日志\n选择一个唯一的server-id\n创建具有复制权限的用户\n在从服务器（slave）上\n启用中继日志（二进制日志可开启，也可不开启）\n选择一个唯一的server-id\n连接至主服务器，并开始复制\n1. 主库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@master ~]# vim /etc/my.cnf [mysqld] log-bin=master-bin binlog_format = row sync_binlog = 1 skip_name_resolv = 1 log_slave_updates = 1 server_id = 27 [root@master ~]# service mysqld start [root@master ~]# mysql mysql\u0026gt; reset master; mysql\u0026gt; grant replication slave, replication client on *.* -\u0026gt; to rep@\u0026#39;192.168.1.%\u0026#39; identified by \u0026#39;localhost\u0026#39;; mysql\u0026gt; flush privileges; 2. 备库\na. 测试复制账号\n1 [root@slave1 ~]# mysql -h 192.168.1.27 -urep -plocalhost b. 配置复制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 [root@slave1 ~]# vim /etc/my.cnf log-bin=slave1-bin binlog_format = row sync_binlog = 1 skip_name_resolv = 1 log_slave_updates = 1 server_id = 66 [root@slave1 ~]# service mysqld start [root@slave1 ~]# mysql mysql\u0026gt; reset master; mysql\u0026gt; change master to -\u0026gt; master_host=\u0026#39;192.168.1.27\u0026#39;, -\u0026gt; master_user=\u0026#39;rep\u0026#39;, -\u0026gt; master_password=\u0026#39;localhost\u0026#39;, -\u0026gt; master_log_file=\u0026#39;master-bin.000001\u0026#39;, -\u0026gt; master_log_pos=0; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Master_Host: 192.168.1.27 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 4 Relay_Log_File: mysql-relay-bin.000001 Relay_Log_Pos: 4 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: No Slave_SQL_Running: No mysql\u0026gt; start slave; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.1.27 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 354 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 500 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes 3. 测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 Master： mysql\u0026gt; show processlist\\G *************************** 2. row *************************** Id: 2 User: rep Host: 192.168.10.37:50915 db: NULL Command: Binlog Dump Time: 324 State: Master has sent all binlog to slave; waiting for binlog to be updated Info: NULL 2 rows in set (0.00 sec) mysql\u0026gt; create database bbs; Query OK, 1 row affected (0.00 sec) mysql\u0026gt; create table bbs.t1(id int); Query OK, 0 rows affected (0.03 sec) mysql\u0026gt; insert into bbs.t1 values(1); Query OK, 1 row affected (0.02 sec) mysql\u0026gt; select * from bbs.t1; +------+ | id | +------+ | 1 | +------+ 1 row in set (0.00 sec) Slave： mysql\u0026gt; show processlist\\G *************************** 2. row *************************** Id: 2 User: system user Host: db: NULL Command: Connect Time: 356 State: Waiting for master to send event Info: NULL *************************** 3. row *************************** Id: 3 User: system user Host: db: NULL Command: Connect Time: -173772 State: Slave has read all relay log; waiting for the slave I/O thread to update it Info: NULL 3 rows in set (0.00 sec) mysql\u0026gt; select * from bbs.t1; +------+ | id | +------+ | 1 | +------+ 1 row in set (0.03 sec) 二、针对已经运行一段时间的主库实现主/备\n单主到多备： Master-MultiSlave\nMaster Slave1 Slave2\n＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\nIP 192.168.1.27 192.168.1.66 192.168.1.251\nServer_ID 27 66 251\n1. 主库\n1 2 3 4 5 6 7 8 9 10 11 12 [root@master ~]# vim /etc/my.cnf log-bin=master-bin binlog_format = row sync_binlog = 1 skip_name_resolv = 1 log_slave_updates = 1 server_id = 27 [root@master ~]# service mysqld restart [root@master ~]# mysql mysql\u0026gt; grant replication slave, replication client on *.* -\u0026gt; to rep@\u0026#39;192.168.1.%\u0026#39; identified by \u0026#39;localhost\u0026#39;; mysql\u0026gt; flush privileges; ========================================================\n初始化备库（使其和主库数据一致）： 逻辑备份，物理备份\n主库：\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; flush tables with read lock; //主服务器锁定表 [root@master ~]# mysqldump --all-databases \u0026gt; all.sql [root@master ~]# mysql -e \u0026#39;show master status\u0026#39; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | master-bin.000001 | 699 | | | +------------------+----------+--------------+------------------+ mysql\u0026gt; unlock tables; //解锁表 [root@master ~]# rsync -va all.sql 192.168.1.66:/ ========================================================\n2. 备库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [root@slave1 ~]# vim /etc/my.cnf server-id = 66 [root@slave1 ~]# service mysqld start [root@slave1 ~]# mysql mysql\u0026gt; reset master; mysql\u0026gt; source /all.sql mysql\u0026gt; change master to master_host=\u0026#39;192.168.1.27\u0026#39;, master_user=\u0026#39;rep\u0026#39;, master_password=\u0026#39;localhost\u0026#39;, master_log_file=\u0026#39;master-bin.000001\u0026#39;, master_log_pos=699; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; start slave; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.1.27 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-bin.000001 Read_Master_Log_Pos: 354 Relay_Log_File: mysql-relay-bin.000002 Relay_Log_Pos: 500 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes MySQL主从复制的状况监测\n主从状况监测主要参数\nSlave_IO_Running: IO线程是否打开 YES/No/NULL\nSlave_SQL_Running: SQL线程是否打开 YES/No/NULL\nSeconds_Behind_Master: NULL #和主库比同步的延迟的秒数\n可能导致主从延时的因素\n主从时钟是否一致\n网络通信是否存在延迟\n是否和日志类型，数据过大有关\n从库性能，有没开启binlog\n从库查询是否优化\n常见状态错误排除\n发现IO进程错误，检查日志，排除故障：\n# tail localhost.localdomain.err\n\u0026hellip;2015-11-18 10:55:50 3566 [ERROR] Slave I/O: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work. Error_code: 1593\n找到原因：从5.6开始复制引入了uuid的概念，各个复制结构中的server_uuid得保证不一样\n解决方法：(从库是克隆机器)修改从库的uuid\n# vim auto.cnf server-uuid=\nshow slave status;报错：Error xxx doesn’t exist\n解决方法：\nstop slave;\nset global sql_slave_skip_counter = 1;\nstart slave;\n三、常见复制拓朴\n1. 一主库多备库\n2. 主库，分发主库以及备库\n3. 主——主复制（双主）\n四、MySQL 主主同步\n重置数据库：\n1 2 3 # service mysqld stop # rm -rf /usr/local/mysql/data/* # /usr/local/mysql/scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data 1. mysql1 192.168.1.4:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@mysql1 ~]# vim /etc/my.cnf log-bin=mysql-bin server-id = 4 [root@mysql1 ~]# service mysqld start [root@mysql1 ~]# mysql mysql\u0026gt; reset master; mysql\u0026gt; grant replication slave, replication client on *.* -\u0026gt; to rep@\u0026#39;192.168.1.%\u0026#39; identified by \u0026#39;localhost\u0026#39;; mysql\u0026gt; flush privileges; mysql\u0026gt; change master to -\u0026gt; master_host=\u0026#39;192.168.1.251\u0026#39;, -\u0026gt; master_user=\u0026#39;rep\u0026#39;, -\u0026gt; master_password=\u0026#39;localhost\u0026#39;, -\u0026gt; master_log_file=\u0026#39;mysql-bin.000001\u0026#39;, -\u0026gt; master_log_pos=0; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; show slave status\\G 2. mysql2 192.168.1.251:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@mysql2 ~]# vim /etc/my.cnf log-bin=mysql-bin server-id = 251 [root@mysql2 ~]# service mysqld start [root@mysql2 ~]# mysql mysql\u0026gt; reset master; mysql\u0026gt; grant replication slave, replication client on *.* -\u0026gt; to rep@\u0026#39;192.168.1.%\u0026#39; identified by \u0026#39;localhost\u0026#39;; mysql\u0026gt; flush privileges; mysql\u0026gt; change master to -\u0026gt; master_host=\u0026#39;192.168.1.4\u0026#39;, -\u0026gt; master_user=\u0026#39;rep\u0026#39;, -\u0026gt; master_password=\u0026#39;localhost\u0026#39;, -\u0026gt; master_log_file=\u0026#39;mysql-bin.000001\u0026#39;, -\u0026gt; master_log_pos=0; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; show slave status\\G 3. mysql1,mysql2\n1 2 mysql\u0026gt; slave start; mysql\u0026gt; show slave status\\G 4. 测试\n5. 建立用于客户连接用户\n1 2 mysql\u0026gt; grant ALL on *.* to admin@\u0026#39;192.168.1.%\u0026#39; identified by \u0026#39;localhost\u0026#39;; mysql\u0026gt; flush privileges; ===========================================================================\n生产环境其他常用设置\n1、配置忽略权限库同步参数\nbinlog-ignore-db=\u0026lsquo;information_schema mysql test\u0026rsquo;\n2、从库备份开启binlog\nlog-slave-updates\nlog_bin = mysql-bin\nexpire_logs_days = 7\n应用场景：级联复制或从库做数据备份。\n3、从库只读read-only来实现\ninnodb_read_only = ON或1，或者innodb_read_only\n结论：当用户权限中没有SUPER权限(ALL权限是包括SUPER的)时，从库的read-only生效！\n","date":"2019-04-22T22:17:01Z","image":"https://www.ownit.top/title_pic/40.jpg","permalink":"https://www.ownit.top/p/201904222217/","title":"MySQL复制技术"},{"content":"复制数据文件方式，可以使用cp或tar\n1、停止服务\n1 2 [root@localhost mysql]# systemctl stop mysqld [root@localhost mysql]# netstat -lnupt | grep 3306 2、备份数据文件\n1 2 3 cd /var/lib/mysql [root@localhost mysql]# mkdir -p /server/backup [root@localhost mysql]#tar czf /server/backup/all.`date +%F`.tar.gz * 3、将备份文件拷贝到目标服务器\n1 scp /server/backup/all.`date +%F`.tar.gz 192.168.95.12:/tmp 4、目标服务器停止服务\n1 # systemctl stop mysqld 5、解压文件至目标服务器数据文件夹\n1 # tar xf /tmp/all.tar.gz -C /usr/local/mysql/data 修改权限\n1 # chown -R mysql.mysql /usr/local/mysql/data 6、目标服务器启动服务测试\n1 # systemctl start mysqld ","date":"2019-04-21T22:28:12Z","image":"https://www.ownit.top/title_pic/06.jpg","permalink":"https://www.ownit.top/p/201904212228/","title":"MySQL数据物理备份之tar打包备份"},{"content":"使用lvm快照实现物理备份 优点：\n几乎是热备(创建快照前把表上锁，创建完后立即释放)\n支持所有存储引擎\n备份速度快\n无需使用昂贵的商业软件(它是操作系统级别的)\n缺点：\n可能需要跨部门协调(使用操作系统级别的命令，DBA一般没权限)\n无法预计服务停止时间\n数据如果分布在多个卷上比较麻烦(针对存储级别而言)\n操作流程：\n1、flush table with read locak;\n2、create snapshot\n3、show master status;　show slave status;\n4、unlock tables;\n5、copy data from cow to backup\n6、remove snapshot\n正常安装MySQL： 1. 安装系统\n2. 准备LVM，例如 /dev/vg_localhost/lv-mysql，mount /usr/local/mysql\n3. 源码安装MySQL到 /usr/local/mysql\n可选操作：　将现在的数据迁移到LVM\n1. 准备lvm及文件系统\n1 2 [root@localhost ~]# lvcreate -L 2G -n lv-mysql vg_localhost [root@localhost ~]# mkfs.ext4 /dev/vg_localhost/lv-mysql 2. 将数据迁移到LVM\n1 2 3 [root@localhost ~]# service mysqld stop [root@localhost ~]# mount /dev/vg_localhost/lv-mysql /mnt/ //临时挂载点 [root@localhost ~]# rsync -va /usr/local/mysql/ /mnt/ //将MySQL原数据镜像到临时挂载点 1 2 3 4 5 [root@localhost ~]# umount /mnt/ [root@localhost ~]# mount /dev/vg_localhost/lv-mysql /usr/local/mysql //加入fstab开机挂载 [root@localhost ~]# df -Th /dev/mapper/vg_localhost-lv--mysql ext4 2.0G 274M 1.7G 15% /usr/local/mysql [root@localhost ~]# service mysqld start 手动基于LVM快照实现备份： 1. 加锁\n1 mysql\u0026gt; flush table with read lock; 2.创建快照\n1 2 3 4 5 6 7 8 9 10 # lvcreate -L 500M -s -n lv-mysql-snap /dev/vg_localhost/lv-mysql # mysql -uroot -p123 -e \u0026#39;show master status\u0026#39; \u0026gt; /backup/`date +%F`_position.txt mysql\u0026gt; show master status; +-------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-------------------+----------+--------------+------------------+-------------------+ | mysqld-bin.000016 | 542 | | | | +-------------------+----------+--------------+------------------+-------------------+ 3. 释放锁\n1 mysql\u0026gt; unlock tables; 4. 从快照中备份\n1 2 3 [root@localhost ~]# mount -o ro /dev/vg_localhost/lv-mysql-snap /mnt/ [root@localhost ~]# mkdir /backup/`date +%F` [root@localhost ~]# rsync -a /mnt/ /backup/2014-09-02/ 5. 移除快照\n1 2 [root@localhost ~]# umount /mnt/ [root@localhost ~]# lvremove -f /dev/vg_localhost/lv-mysql-snap 脚本 + Cron 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash #LVM backmysql... back_dir=/backup/`date +%F` [ -d $back_dir ] || mkdir -p $back_dir mysql -uroot -p678 -e \u0026#39;flush table with read lock\u0026#39; lvcreate -L 500M -s -n lv-mysql-snap /dev/vg_localhost/lv-mysql mysql -uroot -p678 -e \u0026#39;show master status\u0026#39; |grep mysql \u0026gt; $back_dir/position.txt mysql -uroot -p678 -e \u0026#39;flush logs\u0026#39; mysql -uroot -p678 -e \u0026#39;unlock tables\u0026#39; mount -o ro /dev/vg_localhost/lv-mysql-snap /mnt/ rsync -a /mnt/ $back_dir if [ $? -eq 0 ];then umount /mnt/ lvremove -f /dev/vg_localhost/lv-mysql-snap fi mylvmbackup 功能：利用LVM快照实现物理备份，即LVM快照备份的自动版\n安装perl模块\n1. 在线安装\n1 http://www.lenzg.net/mylvmbackup 它依赖于perl 模块，可用以下命令安装\n1 perl -MCPAN -e \u0026#39;install Config::IniFiles\u0026#39; 2. 离线安装\n1 # yum -y install atrpms-77-1.noarch.rpm perl-Config-IniFiles-2.72-3.em.el6.noarch.rpm perl-File-Copy-Recursive-0.38-1.el6.rfx.noarch.rpm perl-IO-stringy-2.110-8.el6.noarch.rpm 安装mylvmbackup软件包\n1 # yum -y install mylvmbackup-0.15-0.noarch.rpm 备份方法一：\n1 2 # mylvmbackup --user=root --password=111 --host=localhost --mycnf=/etc/my.cnf --vgname=vg_localhost --lvname=lv-mysql --backuptype=tar --lvsize=100M --backupdir=/ backup 1 2 3 4 [root@localhost backup]# tar xf backup-20140903_000236_mysql.tar.gz [root@localhost backup]# ls backup backup-cnf-20140903_000236_mysql backup-20140903_000236_mysql.tar.gz backup-pos 备份方法二：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [root@server ~]# vim /etc/mylvmbackup.conf [mysql] #连接数据库配置 user=root password=123456 host=localhost port=3306 socket=/tmp/mysql.sock mycnf=/etc/my.cnf [lvm] #LVM逻辑卷的配置 vgname=vg_server #卷组名称 lvname=lv_mysql #逻辑卷名称 backuplv=mysql_snap #快照卷名称 lvsize=500M [fs] #文件系统配置 xfs=0 mountdir=/var/tmp/mylvmbackup/mnt/ #挂载目录 backupdir=/backup #备份目录，也可以备份到行程主机 [misc] #定义备份选项 backuptype=tar #定义备份的类型 backupretention=0 prefix=backup #定义备份文件名前缀 suffix=_mysql #定义备份文件名后缀 tararg=cvf #定义tar参数，默认为cvf tarfilesuffix=.tar.gz #定义备份文件后缀名格式 datefmt=%Y%m%d_%H%M%S #定义备份文件名时间戳格式 keep_snapshot=0 #是否保留snaphot keep_mount=0 #是否卸载snaphot quiet=0 #定义记录日志类型 注释：其他配置保持输入即可 然后直接执行mylvmbackup即可\n","date":"2019-04-21T22:24:57Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/201904212224/","title":"MySQL数据物理备份之lvm快照"},{"content":"percona-xtrabackup 它是开源免费的支持MySQL 数据库热备份的软件，它能对InnoDB和XtraDB存储引擎的数据库非阻塞地备份。它不暂停服务创建Innodb热备份；\n为mysql做增量备份；在mysql服务器之间做在线表迁移；使创建replication更加容易；备份mysql而不增加服务器的负载。\npercona是一家老牌的mysql技术咨询公司。它不仅提供mysql的技术支持、培训、咨询，还发布了mysql的分支版本\u0026ndash;percona Server。并围绕\npercona Server还发布了一系统的mysql工具。\n=================================================================================\n完全备份 增量备份 差异备份 xtrabackup\nXtrabackup是一个对InnoDB做数据备份的工具，支持在线热备份（备份时不影响数据读写），是商业备份工具InnoDB Hotbackup的一个很好的替代品。\nXtrabackup有两个主要的工具：xtrabackup、innobackupex\nxtrabackup 只能备份InnoDB和XtraDB两种数据表，而不能备份MyISAM数据表。 innobackupex 是参考了InnoDB Hotbackup的innoback脚本修改而来的.innobackupex是一个perl脚本封装，封装了xtrabackup。主要是为了方便的同时备份InnoDB和MyISAM引擎的表，但在处理myisam时需要加一个读锁。并且加入了一些使用的选项。如slave-info可以记录备份恢复后作为slave需要的一些信息，根据这些信息，可以很方便的利用备份来重做slave。支持完全备份和增量备份\n备份过程快速、可靠； 备份过程不会打断正在执行的事务； 能够基于压缩等功能节约磁盘空间和流量； 自动实现备份检验； 还原速度快； 使用innobakupex备份时，其会调用xtrabackup备份所有的InnoDB表，复制所有关于表结构定义的相关文件(.frm)、以及MyISAM、MERGE、CSV和ARCHIVE表的相关文件，同时还会备份触发器和数据库配置信息相关的文件。这些文件会被保存至一个以时间命令的目录中。\n(1)xtrabackup_checkpoints —— 备份类型（如完全或增量）、备份状态（如是否已经为prepared状态）和LSN(日志序列号)范围信息；每个InnoDB页(通常为16k大小)都会包含一个日志序列号，即LSN。LSN是整个数据库系统的系统版本号，每个页面相关的LSN能够表明此页面最近是如何发生改变的。\n(2)xtrabackup_binlog_info —— mysql服务器当前正在使用的二进制日志文件及至备份这一刻为止二进制日志事件的位置。(3)xtrabackup_binlog_pos_innodb —— 二进制日志文件及用于InnoDB或XtraDB表的二进制日志文件的当前position。\n(4)xtrabackup_binary —— 备份中用到的xtrabackup的可执行文件；\n(5)backup-my.cnf —— 备份命令用到的配置选项信息；\n安装xtrabackup\n1 2 3 4 5 6 7 8 9 [root@localhost ~]# yum -y install percona-xtrabackup-2.1.9-744.rhel6.x86_64.rpm [root@localhost ~]# rpm -ql percona-xtrabackup |grep bin /usr/bin/innobackupex 支持myisam、innodb /usr/bin/innobackupex-1.5.1 /usr/bin/xbcrypt /usr/bin/xbstream /usr/bin/xtrabackup 仅适用于percona Server /usr/bin/xtrabackup_55 适用mysql 5.5数据库 /usr/bin/xtrabackup_56 适用mysql5.6数据库 完整备份实例 ：\n==备份==\n1 2 3 4 5 6 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 /mysqlbackup/full [root@localhost ~]# ls /mysqlbackup/full/2015-09-15_11-03-03/ backup-my.cnf company mysql school test xtrabackup_binary xtrabackup_checkpoints bbs ibdata1 performance_schema shop weibo xtrabackup_binlog_info xtrabackup_logfile [root@localhost 2015-09-15_11-03-03]# cat xtrabackup_binlog_info localhost-bin.000003 2090096 ==恢复==\na. 准备新环境\n1 2 3 4 5 [root@localhost ~]# rm -rf /usr/local/mysql/data [root@localhost ~]# chown -R mysql.mysql /usr/local/mysql [root@localhost ~]# /usr/local/mysql/scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data [root@localhost ~]# killall -9 mysqld [root@localhost ~]# service mysqld start b. 恢复\n1 2 3 4 5 6 7 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --apply-log /mysqlbackup/full/2015-09-15_11-03-03/ [root@localhost ~]# rm -rf /usr/local/mysql/data/* [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --copy-back /mysqlbackup/full/2015-09-15_11-03-03/ [root@localhost ~]# cd /usr/local/mysql [root@localhost ~]# chown -R mysql.mysql . [root@localhost ~]# killall -9 mysqld [root@localhost ~]# service mysqld start 增量备份实例： ==备份==\n1、完整备份：周一\n1 2 3 4 5 create database testdb; use testdb; create table test(id int); insert into test values(1); select * from test; 1 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 /mysqlbackup/full 2、增量备份：周二　——　周六\n1 2 3 insert into testdb.test values(2); [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --incremental /mysqlbackup/incremental --incremental-basedir=完全备份目录 1 2 3 insert into testdb.test values(3); [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --incremental /mysqlbackup/incremental --incremental-basedir=上次增量目录 1 2 3 insert into testdb.test values(4); [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --incremental /mysqlbackup/incremental --incremental-basedir=上次增量目录 ==恢复== 1.恢复全量的redo log\n1 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --apply-log --redo-only /mysqlbackup/full/... 2.恢复增量的redo log\n1 2 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --apply-log --redo-only /mysqlbackup/full/... --incremental-dir=/mysqlbackup/incremental/第一次增量 1 2 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --apply-log --redo-only /mysqlbackup/full/... --incremental-dir=/mysqlbackup/incremental/第二次增量 1 2 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --apply-log --redo-only /mysqlbackup/full/... --incremental-dir=/mysqlbackup/incremental/第Ｎ次增量 3.关闭mysqld，替换数据文件(cp,rsyn,innobackupex copy-back)，修改权限\n4.启动mysqld\n5.通过binlog增量恢复\n1 2 3 4 5 6 7 create database testdb; use testdb; create table test(id int); insert into test values(1); select * from test; [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 /mysqlbackup/full 差异备份实例： ==备份==\n1、完整备份：周一\n2、差异备份：周二　——　周六\n1 2 3 insert into testdb.test values(2); [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --incremental /mysqlbackup/incremental --incremental-basedir=完全备份目录 1 2 3 insert into testdb.test values(3); [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --incremental /mysqlbackup/incremental --incremental-basedir=完全备份目录 1 2 3 insert into testdb.test values(4); [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --incremental /mysqlbackup/incremental --incremental-basedir=完全备份目录 ==恢复== 1.恢复全量的redo log\n1 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --apply-log --redo-only /mysqlbackup/full/... 2.恢复差异的redo log\n1 2 [root@localhost ~]# innobackupex --host=localhost --socket=/tmp/mysql.sock --defaults-file=/etc/my.cnf --user=root --password=888 --apply-log --redo-only /mysqlbackup/full/... --incremental-dir=/mysqlbackup/incremental/某个差异备份 3.关闭mysqld，替换数据文件(cp,rsyn)，修改权限\n4.启动mysqld\n5.通过binlog增量恢复\n备份单库、多库、多表单数据库备份\ninnobackupex --defaults-file=/etc/my.cnf --socket=/tmp/mysql.sock --databases=uplook --no-timestamp /server/backup/uplook\n多数据库备份innobackupex --user=root --password=123456 --include=\u0026lsquo;dba.*|dbb.*\u0026rsquo; /server/backup\n多表备份方法一：innobackupex --user=root --password=123456 --include=\u0026lsquo;dba.tablea|dbb.tableb\u0026rsquo; /server/backup\n方法二：使用\u0026ndash;tables-file参数，这种方式是将所有要备份的完整表名都写在一个文本文件中，每行一个完整表名，然后程序读取这个文本文件进行备份。完整表名即：databasename.tablename\necho \u0026ldquo;lianxi.Student\u0026rdquo; \u0026gt;/tmp/table1.txt\ninnobackupex --defaults-file=/etc/my.cnf --socket=/tmp/mysql.sock --tables-file=\u0026rsquo;/tmp/table1.txt\u0026rsquo; --no-timestamp /server/backup/table1\n恢复单库、多库、多表单数据库恢复应用日志：\ninnobackupex --apply-log /server/backup/uplook\n删除数据库数据文件：systemctl stop mysql；rm -rf /usr/local/mysql/data/*\n还原数据：innobackupex --copy-back /server/backup/uplook\n授权：chown -R mysql.mysql /usr/local/mysql\n初始化数据库：/usr/local/mysql/scripts/mysql_install_db --user=mysql --datadir=/usr/local/mysql/data --basedir=/usr/local/mysql --explicit_defaults_for_timestamp\n启动数据库：systemctl start mysql\n测试：mysql -e \u0026ldquo;select * from uplook.Student;\u0026rdquo;\n多表数据恢复应用日志：\ninnobackupex --apply-log --export /server/backup/table1/\n定义表\u0026ndash;删除表空间\u0026ndash;拷贝*.ibd/*.cfg文件\u0026ndash;导入表空间\n定义表：模拟删除表，重新定义表结构，过程略\n删除表空间：mysql\u0026gt; ALTER TABLE Student DISCARD TABLESPACE;\n拷贝*.ibd/*.cfg文件：[root@localhost ~]# cp /server/backup/table1/lianxi/Student.{cfg,ibd} /usr/local/mysql/data/lianxi/\n[root@localhost ~]# chown -R mysql.mysql /usr/local/mysql/data/lianxi/\n导入表空间：mysql\u0026gt; ALTER TABLE Student IMPORT TABLESPACE;\n测试：mysql\u0026gt; select * from Student;\n","date":"2019-04-21T22:18:59Z","image":"https://www.ownit.top/title_pic/48.jpg","permalink":"https://www.ownit.top/p/201904212218/","title":"MySQL数据物理备份之xtrabackup"},{"content":"逻辑备份： 备份的是建表、建库、插入等操作所执行SQL语句，适用于中小型数据库，效率相对较低。\nmysqldump\nmydumper\n使用mysqldump实现逻辑备份\n语法：\n# mysqldump -h 服务器 -u用户名 -p密码 数据库名 \u0026gt; 备份文件.sql\n关于数据库名：\n-A, --all-databases 所有库\nschool 数据库名\nschool stu_info t1 school数据库的表stu_info、t1\n-B, --databases bbs test mysql 多个数据库\n关于其它参数说明：\n--single-transaction #基于此项可以实现对InnoDB表做热备份\n-x, --lock-all-tables #执行备份时为所有表请求加锁 MyISAM\n-l, --lock-tables\n-E, --events #备份事件调度器代码\n--opt #同时启动各种高级选项\n-R, --routines #备份存储过程和存储函数\n-F, --flush-logs #备份之前刷新日志\n--triggers #备份触发器\n--master-data=2 #备库，该选项将会记录binlog的日志位置与文件名并追加到文件中，如果为1将会输出CHANGE MASTER命令，主从下有用\n注意：-B 作用：创建数据库和切换到数据库，恢复时不用创建数据库和删表。备份多个库，-B 数据库1 数据库2 .\n-d只备份库结构，不包含数据内容\n备份：mysqldump -u 用户名 -p 数据库名 表名 \u0026gt; 备份的文件名\n备份多个表：mysqldump -u 用户名 -p 数据库名 表名1 表名2 \u0026gt; 备份的文件名\n示例：\n备份jiaowu数据库，名字为hei.sql\n显示数据库\n1 [root@wei ~]# mysql -uroot -proot -e \u0026#39;show databases;\u0026#39; 备份数据库\n1 [root@wei ~]# mysqldump -uroot -proot jiaowu \u0026gt;hei.sql ==备份==\n1 2 3 4 [root@localhost ~]# mysqldump -uroot -proot --single-transaction --master-data=2 company \u0026gt; /tmp/company_`date +%F-%H-%M`.sql [root@localhost ~]# mysqldump -uroot -proot --single-transaction --master-data=2 school \u0026gt; /tmp/school_`date +%F-%H-%M`.sql [root@localhost ~]# mysqldump -uroot -proot --single-transaction --master-data=2 school |gzip \u0026gt; /tmp/school_`date +%F-%H-%M`.gz [root@localhost ~]# mysqldump -uroot -proot --routines --events --triggers --master-data=2 --flush-logs --all-databases \u0026gt; /backup/all_`date +%F`.sql ==恢复==\n1 2 3 4 [root@localhost ~]# mysql -uroot -p888 -e \u0026#39;create database school\u0026#39; [root@localhost ~]# mysql -uroot -p888 school \u0026lt; /tmp/school_2015-09-14-15-40.sql root@(school)\u0026gt; source /tmp/school_2015-09-14-15-40.sql [root@localhost tmp]# gunzip -fc school_2015-09-14-15-09.gz | mysql -uroot -p888 school ==增量备份==\n增量备份前提：\n1）my.cnf，是要开启MySQL log-bin日志功能，重启MySQL log_bin = /data/mysql/data/mysql-bin\n2）存在一个完全备份，生产环境一般凌晨某个时刻进行全备\n示例：\n1 mysqldump -uroot -p --default-character-set=gbk --single-transaction -F -B school |gzip \u0026gt; /server/backup/school_$(date +%F).sql.gz InnoDB 表在备份时，通常启用选项 --single-transaction 来保证备份的一致性\n增量备份\u0026ndash;恢复过程\n1、检查凌晨备份\n2、检查全备后的所有binlog # ls -lrt /usr/local/mysql/data/mysql-bin.*\n3、立即刷新并备份出binlog mysqladmin -uroot -p flush-logs\ncp /usr/local/mysql/data/mysql-bin.000004 /server/backup/\n提示：根据时间点及前一个binlog可以知道发现问题时刻前binlog日志为mysql-bin.000004\n4、恢复binlog生成sql语句mysqlbinlog mysql-bin.000004 \u0026gt; bin.log\n5、恢复凌晨备份\n6、恢复增量备份\nmysqlbinlog增量恢复方式\n基于时间点恢复\n1）指定开始时间到结束时间 myslbinlog mysqlbin.000008 --start-datetime=’2014-10-45 01:10:46’ --stop-datetime=’2014-10-45 03:10:46’-r time.sql\n2）指定开始时间到文件结束 myslbinlog mysqlbin.000008 --start-datetime=’2014-10-45 01:10:46’ -d esen -r time.sql\n3）从文件开头到指定结束时间 myslbinlog mysqlbin.000008 --stop-datetime=’2014-10-45 03:10:46’ -d esen -r time.sql\n基于位置点的增量恢复\n1）指定开始位置到结束位置 myslbinlog mysqlbin.000008 --start-position=510 --stop-position=1312 -r pos.sql\n2）指定开始位置到文件结束 myslbinlog mysqlbin.000008 --start-position=510 -r pos.sql\n3）从文件开始位置到指定结束位置 myslbinlog mysqlbin.000008 --stop-position=1312 -r pos.sql\n==恢复binlog==\n单库？全库\n1 2 3 4 [root@localhost ~]# mysqlbinlog -d school localhost-bin.000002 --start-position=11574908 \u0026gt; school-bin.sql [root@localhost ~]# mysqlbinlog -d school localhost-bin.000003 \u0026gt;\u0026gt; school-bin.sql [root@localhost ~]# mysqlbinlog -d school localhost-bin.000004 \u0026gt;\u0026gt; school-bin.sql [root@localhost ~]# mysql -uroot -p888 school \u0026lt; school-bin.sql 实现自动化备份（数据库小）\n备份计划：\n1. 什么时间 2:00\n2. 对哪些数据库备份\n3. 备份文件放的位置\n备份脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@yang ~]# vim /mysql_back.sql #!/bin/bash back_dir=/backup back_file=`date +%F`_all.sql user=root pass=123 if [ ! -d /backup ];then mkdir -p /backup fi # 备份并截断日志 mysqldump -u${user} -p${pass} --lock-all-tables --routines --events --triggers --master-data=2 --flush-logs --all-databases \u0026gt; /$back_dir/$back_file # 只保留最近一周的备份 cd $back_dir find . -mtime +7 -exec rm -rf {} \\; 手动测试：\n1 2 3 [root@yang ~]# chmod a+x /mysql_back.sql [root@yang ~]# chattr +i /mysql_back.sql [root@yang ~]# /mysql_back.sql 配置cron：\n1 2 [root@yang ~]# crontab -e 0 2 * * * /mysql_back.sql Mydumper介绍 Mydumper是一个针对MySQL和Drizzle的高性能多线程备份和恢复工具。开发人员主要来自MySQL,Facebook,SkySQL公司。目前已经在一些线上使用了Mydumper。\nMydumper主要特性：\n• 轻量级C语言写的\n• 执行速度比mysqldump快10倍\n• 事务性和非事务性表一致的快照(适用于0.2.2以上版本)\n• 快速的文件压缩\n• 支持导出binlog\n• 多线程恢复(适用于0.2.1以上版本)\n• 以守护进程的工作方式，定时快照和连续二进制日志(适用于0.5.0以上版本)\n• 开源 (GNU GPLv3)\n1 2 3 4 5 6 7 #wget https://launchpadlibrarian.net/225370879/mydumper-0.9.1.tar.gz # yum -y install glib2-devel mysql-devel zlib-devel pcre-devel [root@localhost ~]# tar xf mydumper-0.9.1.tar.gz -C /usr/local/src/ [root@localhost ~]# cd /usr/local/src/mydumper-0.9.1/ # cmake . # make # make install mydumper参数介绍：\n-d, --directory 导入备份目录\n-q, --queries-per-transaction 每次执行的查询数量, 默认1000\n-o, --overwrite-tables 如果表存在删除表\n-B, --database 需要备份的库\n-T, --tables-list 需要备份的表，用，分隔\n-o, --outputdir 输出目录\n-s, --statement-size Attempted size of INSERT statement in bytes, default 1000000\n-r, --rows 试图分裂成很多行块表\n-c, --compress 压缩输出文件\n-e, --build-empty-files 即使表没有数据，还是产生一个空文件\n-x, --regex 支持正则表达式\n-i, --ignore-engines 忽略的存储引擎，用，分隔\n-m, --no-schemas 不导出表结构\n-k, --no-locks 不执行临时共享读锁 警告：这将导致不一致的备份\n-l, --long-query-guard 长查询，默认60s\n--kill-long-queries kill掉长时间执行的查询(instead of aborting)\n-b, --binlogs 导出binlog\n-D, --daemon 启用守护进程模式\n-I, --snapshot-interval dump快照间隔时间，默认60s，需要在daemon模式下\n-L, --logfile 日志文件\n-h, --host\n-u, --user\n-p, --password\n-P, --port\n-S, --socket\n-t, --threads 使用的线程数，默认4\n-C, --compress-protocol 在mysql连接上使用压缩\n-V, --version\n-v, --verbose 更多输出, 0 = silent, 1 = errors, 2 = warnings, 3 = info, default 2\nmydumper输出文件：\nmetadata:元数据 记录备份开始和结束时间，以及binlog日志文件位置。\ntable data:每个表一个文件\ntable schemas:表结构文件\nbinary logs: 启用\u0026ndash;binlogs选项后，二进制文件存放在binlog_snapshot目录下\ndaemon mode:在这个模式下，有五个目录0，1，binlogs，binlog_snapshot，last_dump。\n备份目录是0和1，间隔备份，如果mydumper因某种原因失败而仍然有一个好的快照，\n当快照完成后，last_dump指向该备份。\nmydumper用例 ==备份==\n1 2 3 4 5 6 7 #export LD_LIBRARY_PATH=\u0026#34;/usr/local/mysql/lib:$LD_LIBRARY_PATH\u0026#34; [root@localhost ~]# mydumper -h localhost -u root -p 123456 -t 6 -S /tmp/mysql.sock -B uplook -o /mysqlbackup/ [root@localhost ~]# ls /mysqlbackup/ metadata uplooking.Student-schema.sql uplook-schema-create.sql uplook.Student.sql uplooking-schema-create.sql uplooking.Student.sql uplook.Student-schema.sql ** (mydumper:5247): CRITICAL **: Error connecting to database: Can\u0026#39;t connect to local MySQL server through socket \u0026#39;/var/lib/mysql/mysql.sock\u0026#39; (2) 解决方法：建立软连接\n1 2 3 4 5 6 ln -sv /tmp/mysql.sock /var/lib/mysql/mysql.sock [root@localhost ~]# cat /mysqlbackup/metadata [root@localhost ~]# cat /mysqlbackup/metadata Started dump at: 2017-08-05 15:48:09 Finished dump at: 2017-08-05 15:48:09 ==恢复==\n1 2 [root@localhost ~]# mysql -uroot -p -e \u0026#39;drop database uplook;\u0026#39; [root@localhost ~]# myloader -h localhost -u root -p 123456 -S /tmp/mysql.sock -d /mysqlbackup/ -o -B uplook ","date":"2019-04-20T21:20:34Z","image":"https://www.ownit.top/title_pic/31.jpg","permalink":"https://www.ownit.top/p/201904202120/","title":"MySQL数据逻辑备份"},{"content":"MySQL备份类型\n热备份、温备份、冷备份 （根据服务器状态）\n热备份：读、写不受影响；\n温备份：仅可以执行读操作；\n冷备份：离线备份；读、写操作均中止；\n物理备份与逻辑备份 （从对象来分）\n物理备份：复制数据文件；\n逻辑备份：将数据导出至文本文件中；\n完全备份、增量备份、差异备份 （从数据收集来分）\n完全备份：备份全部数据；\n增量备份：仅备份上次完全备份或增量备份以后变化的数据；\n差异备份：仅备份上次完全备份以来变化的数据；\nMySQL数据备份\n逻辑备份： 备份的是建表、建库、插入等操作所执行SQL语句，适用于中小型数据库，效率相对较低。\nmysqldump\nmydumper\n逻辑备份的优点：\n在备份速度上两种备份要取决于不同的存储引擎\n物理备份的还原速度非常快。但是物理备份的最小力度只能做到表\n逻辑备份保存的结构通常都是纯ASCII的，所以我们可以使用文本处理工具来处理\n逻辑备份有非常强的兼容性，而物理备份则对版本要求非常高\n逻辑备份也对保持数据的安全性有保证\n逻辑备份的缺点：\n逻辑备份要对RDBMS产生额外的压力，而裸备份无压力\n逻辑备份的结果可能要比源文件更大。所以很多人都对备份的内容进行压缩\n逻辑备份可能会丢失浮点数的精度信息\n物理备份： 直接复制数据库文件，适用于大型数据库环境，不受存储引擎的限制，但不能恢复到异构系统中如Windows。\nxtrabackup\ninbackup\nlvm snapshot\nmysqlbackup ORACLE公司也提供了针对企业的备份软件MySQL Enterprise Backup简称\nMySQL备份内容\n数据文件日志文件（比如事务日志，二进制日志）\n存储过程，存储函数，触发器\n配置文件（十分重要，各个配置文件都要备份）\n用于实现数据库备份的脚本，数据库自身清理的crontab等……\n建议：\n1 2 3 4 5 6 7 8 9 # vim /etc/my.cnf [mysqld] log-bin=mysql-bin server-id=1 #5.7 必须配置server-id 启用binlog功能 binlog_format=statement #5.7 默认binlog只记录create、drop和alter，要记录insert语句必须设置格式 datadir = /usr/local/mysql/data　#添加此行，数据存放目录 innodb_file_per_table = 1 #启用InnoDB独立表空间，默认所有数据库使用一个表空间 # service mysqld restart ","date":"2019-04-20T19:56:45Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/201904201956/","title":"MySQL数据备份概述"},{"content":"==================== 事务特性\n事务隔离级别\n事务控制语句\nMySQL优化\n====================\n事务的概念\n事务指逻辑上的一组操作，组成这组操作的各个单元，要么全部成功，要么全部不成功。\n例如：A——B转帐，对应于如下两条sql语句\nupdate from account set money=money-100 where name=‘a’;\nupdate from account set money=money+100 where name=‘b’;\n数据库开启事务命令\nstart transaction 开启事务\nrollback 回滚事务\ncommit 提交事务\n事务的特性(ACID)\n原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）事务必须使数据库从一个一致性状态变换到另外一个一致性状态。\n隔离性（Isolation）事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。\n持久性（Durability）持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。\n事务的隔离级别\n多个线程开启各自事务操作数据库中数据时，数据库系统要负责隔离操作，以保证各个线程在获取数据时的准确性。\n如果不考虑隔离性，可能会引发如下问题：\n脏读：指一个事务读取了另外一个事务未提交的数据。 这是非常危险的，假设Ａ向Ｂ转帐100元，对应sql语句如下所示\n1.update account set money=money+100 while name=‘b’;\n2.update account set money=money-100 while name=‘a’;\n不可重复读：在一个事物内读取表中的某一行数据，多次读取结果不同。\n例如银行想查询A帐户余额，第一次查询A帐户为200元，此时A向帐户内存了100元并提交了，银行接着又进行了一次查询，此时A帐户为300元了。银行两次查询不一致，可能就会很困惑，不知道哪次查询是准的。\n和脏读的区别是，脏读是读取前一事务未提交的脏数据，不可重复读是重新读取了前一事务已提交的数据。\n很多人认为这种情况就对了，无须困惑，当然是后面的为准。我们可以考虑这样一种情况，比如银行程序需要将查询结果分别输出到电脑屏幕和写到文件中，结果在一个事务中针对输出的目的地，进行的两次查询不一致，导致文件和屏幕中的结果不一致，银行工作人员就不知道以哪个为准了。\n虚读/幻读是指在一个事务内读取到了别的事务插入的数据，导致前后读取不一致。\n如丙存款100元未提交，这时银行做报表统计account表中所有用户的总额为500元，然后丙提交了，这时银行再统计发现帐户为600元了，造成虚读同样会使银行不知所措，到底以哪个为准。\n事务隔离性的设置语句\n数据库共定义了四种隔离级别：\nSerializable：可避免脏读、不可重复读、虚读情况的发生。（串行化）\nRepeatable read：可避免脏读、不可重复读情况的发生。（可重复读）\nRead committed：可避免脏读情况发生（读已提交）。\nRead uncommitted：最低级别，以上情况均无法保证。(读未提交)\nset transaction isolation level 设置事务隔离级别\nselect @@tx_isolation 查询当前事务隔离级别\nMySQL优化\n系统优化：硬件、架构\n服务优化\n应用优化\n影响性能的因素：\n应用程序、查询、事务管理、数据库设计、数据分布、网络操作系统、硬件\n系统优化：\n1、硬件优化\ncpu 64位 一台机器8-16颗CPU 内存 96-128G 3-4个实例\n硬盘：数量越多越好\n性能：ssd（高并发业务） \u0026gt; sas （普通业务）\u0026gt;sata（线下业务） raid 4块盘，性能 raid0 \u0026gt; raid10 \u0026gt; raid5 \u0026gt; raid1\n网卡：多块网卡bond\n2、软件优化 操作系统：使用64位系统 软件：MySQL 编译优化\n服务优化：\nMySQL配置原则\n配置合理的MySQL服务器，尽量在应用本身达到一个MySQL最合理的使用\n针对 MyISAM 或 InnoDB 不同引擎进行不同定制性配置\n针对不同的应用情况进行合理配置针对 my.cnf 进行配置，后面设置是针对内存为2G的服务器进行的合理设置\n公共选项\n选项缺省值推荐值说明max_connections1001024MySQL服务器同时处理的数据库连接的最大数量query_cache_size0 (不打开）16M查询缓存区的最大长度，按照当前需求，一倍一倍增加，本选项比较重要sort_buffer_size512K16M每个线程的排序缓存大小，一般按照内存可以设置为2M以上，推荐是16M，该选项对排序order by，group by起作用record_buffer128K16M每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区，可以设置为2M以上table_cache64512为所有线程打开表的数量。增加该值能增加mysqld要求的文件描述符的数量。MySQL对每个唯一打开的表需要2个文件描述符。 MyISAM 选项\n选项缺省值推荐值说明key_buffer_size8M256M用来存放索引区块的缓存值, 建议128M以上，不要大于内存的30%read_buffer_size128K16M用来做MyISAM表全表扫描的缓冲大小. 为从数据表顺序读取数据的读操作保留的缓存区的长度myisam_sort_buffer_size16M128M设置,恢复,修改表的时候使用的缓冲大小，值不要设的太大 InnoDB 选项\n选项缺省值推荐值说明innodb_buffer_pool_size32M1GInnoDB使用一个缓冲池来保存索引和原始数据, 这里你设置越大,你在存取表里面数据时所需要的磁盘I/O越少，一般是内存的一半，不超过2G，否则系统会崩溃，这个参数非常重要innodb_additional_mem_pool_size2M128MInnoDB用来保存 metadata 信息, 如果内存是4G，最好本值超过200Minnodb_flush_log_at_trx_commit100 代表日志只大约每秒写入日志文件并且日志文件刷新到磁盘; 1 为执行完没执行一条SQL马上commit; 2 代表日志写入日志文件在每次提交后,但是日志文件只有大约每秒才会刷新到磁盘上. 对速度影响比较大，同时也关系数据完整性innodb_log_file_size8M256M在日志组中每个日志文件的大小, 一般是innodb_buffer_pool_size的25%，官方推荐是 innodb_buffer_pool_size 的 40-50%, 设置大一点来避免在日志文件覆写上不必要的缓冲池刷新行为innodb_log_buffer_size128K8M用来缓冲日志数据的缓冲区的大小. 推荐是8M，官方推荐该值小于16M，最好是 1M-8M 之间 应用优化：\n设计合理的数据表结构：\n适当的数据冗余对数据表建立合适有效的数据库索引数据查询：\n编写简洁高效的SQL语句\n表结构设计原则\n选择合适的数据类型，如果能够定长尽量定长\n使用 ENUM 而不是 VARCHAR,ENUM类型是非常快和紧凑的，在实际上，其保存的是 TINYINT，但其外表上显示为字符串。这样一来，用这个字段来做一些选项列表变得相当的完美 。\n不要使用无法加索引的类型作为关键字段，比如 text类型\n为了避免联表查询，有时候可以适当的数据冗余，比如邮箱、姓名这些不容易更改的数据\n选择合适的表引擎，有时候 MyISAM 适合，有时候InnoDB适合\n为保证查询性能，最好每个表都建立有 auto_increment 字段， 建立合适的数据库索引\n最好给每个字段都设定 default 值\n索引建立原则\n一般针对数据分散的关键字进行建立索引，比如ID、QQ，像性别、状态值等等建立索引没有意义字段唯一，最少，不可为null\n对大数据量表建立聚集索引，避免更新操作带来的碎片。\n尽量使用短索引，一般对int、char/varchar、date/time 等类型的字段建立索引\n需要的时候建立联合索引，但是要注意查询SQL语句的编写\n谨慎建立 unique 类型的索引（唯一索引）\n大文本字段不建立为索引，如果要对大文本字段进行检索， 可以考虑全文索引\n频繁更新的列不适合建立索引\norder by 字句中的字段，where 子句中字段，最常用的sql语句中字段，应建立索引。\n唯一性约束，系统将默认为改字段建立索引。\n对于只是做查询用的数据库索引越多越好，但对于在线实时系统建议控制在5个以内。\n索引不仅能提高查询SQL性能，同时也可以提高带where字句的update，Delete SQL性能。\nDecimal 类型字段不要单独建立为索引，但覆盖索引可以包含这些字段。\n只有建立索引以后，表内的行才按照特地的顺序存储，按照需要可以是asc或desc方式。\n如果索引由多个字段组成将最用来查询过滤的字段放在前面可能会有更好的性能。\n编写高效的 SQL\n能够快速缩小结果集的 WHERE 条件写在前面，如果有恒量条件，也尽量放在前面\n尽量避免使用 GROUP BY、DISTINCT 、OR、IN 等语句的使用，避免使用联表查询和子查询，因为将使执行效率大大下降\n能够使用索引的字段尽量进行有效的合理排列，如果使用了联合索引，请注意提取字段的前后顺序\n针对索引字段使用 \u0026gt;, \u0026gt;=, =, \u0026lt;, \u0026lt;=, IF NULL和BETWEEN 将会使用索引，如果对某个索引字段进行 LIKE 查询，使用 LIKE ‘%abc%’不能使用索引，使用 LIKE ‘abc%’ 将能够使用索引\n如果在SQL里使用了MySQL部分自带函数，索引将失效，同时将无法使用 MySQL 的 Query Cache，比如 LEFT(), SUBSTR(), TO_DAYS()，DATE_FORMAT(), 等，如果使用了 OR 或 IN，索引也将失效\n使用 Explain 语句来帮助改进我们的SQL语句\n不要在where 子句中的“=”左边进行算术或表达式运算，否则系统将可能无法正确使用索引\n尽量不要在where条件中使用函数，否则将不能使用索引\n避免使用 select *, 只取需要的字段\n对于大数据量的查询，尽量避免在SQL语句中使用order by 字句，避免额外的开销，替代为使用ADO.NET 来实现。\n如果插入的数据量很大，用select into 替代 insert into 能带来更好的性能\n采用连接操作，避免过多的子查询，产生的CPU和IO开销\n只关心需要的表和满足条件的数据\n适当使用临时表或表变量\n对于连续的数值，使用between代替in\nwhere 字句中尽量不要使用CASE条件\n尽量不用触发器，特别是在大数据表上\n更新触发器如果不是所有情况下都需要触发，应根据业务需要加上必要判断条件\n使用union all 操作代替OR操作，注意此时需要注意一点查询条件可以使用聚集索引，如果是非聚集索引将起到相反的结果\n当只要一行数据时使用 LIMIT 1\n尽可能的使用 NOT NULL填充数据库\n拆分大的 DELETE 或 INSERT 语句\n批量提交SQL语句\n架构优化\n1）业务拆分：搜索功能，like ，前后都有%，一般不用MySQL数据库\n2）业务拆分：某些应用使用nosql持久化存储，例如memcahcedb、redis、ttserver 比如粉丝关注、好友关系等；\n3）数据库前端必须要加cache，例如memcached，用户登录，商品查询\n4）动态数据静态化。整个文件静态化，页面片段静态化\n5）数据库集群与读写分离；\n6）单表超过2000万，拆库拆表，人工或自动拆分（登录、商品、订单等）\n流程、制度、安全优化\n任何一次人为数据库记录的更新，都要走一个流程\n1）人的流程：开发\u0026ndash;\u0026gt;核心开发\u0026ndash;\u0026gt;运维或DBA\n2）测试流程：内网测试\u0026ndash;\u0026gt;IDC测试\u0026ndash;\u0026gt;线上执行\n3）客户端管理：phpmyadmin等\n","date":"2019-04-19T23:57:56Z","image":"https://www.ownit.top/title_pic/36.jpg","permalink":"https://www.ownit.top/p/201904192357/","title":"MySQL事务优化"},{"content":"MySQL日志管理 ========================================================\n错误日志: 记录 MySQL 服务器启动、关闭及运行错误等信息 二进制日志: 又称binlog日志，以二进制文件的方式记录数据库中除 SELECT 以外的操作 查询日志: 记录查询的信息 慢查询日志: 记录执行时间超过指定时间的操作 中继日志： 备库将主库的二进制日志复制到自己的中继日志中，从而在本地进行重放 通用日志： 审计哪个账号、在哪个时段、做了哪些事件 事务日志 或称redo日志，记录Innodb事务相关的如事务执行时间、检查点等 ========================================================\n日志类型 记录信息 日志文件 记入文件中的信息类型 错误日志 记录启动、运行或停止时出现的问题。 查询日志 记录建立的客户端连接和执行的语句。 二进制日志 记录所有更改数据的语句。主要用于复制和即时点恢复。 慢日志 记录所有执行时间超过long_query_time秒的所有查询或不使用索引的查询。 事务日志 记录InnoDB等支持事务的存储引擎执行事务时产生的日志。 一、错误日志\n错误日志主要记录如下几种日志：\n服务器启动和关闭过程中的信息\n服务器运行过程中的错误信息\n事件调度器运行一个时间是产生的信息\n在从服务器上启动从服务器进程是产生的信息\n错误日志定义：可以用\u0026ndash;log-error[=file_name]选项来指定mysqld保存错误日志文件的位置。\n如果没有给定file_name值，mysqld使用错误日志名host_name.err 并在数据目录中写入日志文件。如果你执行FLUSH LOGS，错误日志用-old重新命名后缀并且mysqld创建一个新的空日志文件。(如果未给出\u0026ndash;log-error选项，则不会重新命名）。\n查看当前错误日志配置 mysql\u0026gt; SHOW GLOBAL VARIABLES LIKE \u0026lsquo;%log_error%\u0026rsquo;;\n是否记录警告日志 mysql\u0026gt; SHOW GLOBAL VARIABLES LIKE \u0026lsquo;%log_warnings%\u0026rsquo;;\n二、bin-log\n1. 启用\n# vim /etc/my.cnf\n[mysqld]\nlog-bin[=dir/[filename]] //目录权限必须mysql用户可写\n# service mysqld restart\n注意：MySQL 5.7.3以后版本，启用bin-log功能，需要设置server-id\n2. 暂停\n//仅当前会话\nSET SQL_LOG_BIN=0;\nSET SQL_LOG_BIN=1;\n3. 查看\n查看二进制日志的工具为：mysqlbinlog\n二进制日志包含了所有更新了数据或者已经潜在更新了数据（例如，没有匹配任何行的一个DELETE）的所有语句语句\n以“事件”的形式保存，它描述数据更改。二进制日志还包含关于每个更新数据库的语句的执行时间信息。它不包含没有修改任何数据的语句\n二进制日志的主要目的是在数据库存在故障时，恢复时能够最大可能地更新数据库（即时点恢复），因为二进制日志包含备份后进行的所有更新。\n二进制日志还用于在主复制服务器上记录所有将发送给从服务器的语句。\n查看全部：\n1 # mysqlbinlog mysql.000002 按时间：\n1 2 3 4 5 # mysqlbinlog mysql.000002 --start-datetime=\u0026#34;2012-12-05 10:02:56\u0026#34; # mysqlbinlog mysql.000002 --stop-datetime=\u0026#34;2012-12-05 11:02:54\u0026#34; # mysqlbinlog mysql.000002 --start-datetime=\u0026#34;2012-12-05 10:02:56\u0026#34; --stop-datetime=\u0026#34;2012-12-05 11:02:54\u0026#34; 按字节数：\n1 2 3 # mysqlbinlog mysql.000002 --start-position=260 # mysqlbinlog mysql.000002 --stop-position=260 # mysqlbinlog mysql.000002 --start-position=260 --stop-position=930 4. 截断bin-log（产生新的bin-log文件）\na. 重启mysql服务器\nb. # mysql -uroot -p123 -e \u0026lsquo;flush logs\u0026rsquo;\n5. 删除bin-log文件\n# mysql -uroot -p123 -e \u0026lsquo;reset master\u0026rsquo;\n二进制日志文件不能直接删除的，如果使用rm等命令直接删除日志文件，可能导致数据库的崩溃。\n必须使用命令PURGE删除日志，语法如下：PURGE { BINARY | MASTER } LOGS { TO \u0026rsquo;log_name\u0026rsquo; | BEFORE datetime_expr }\n三、查询日志\n启用通用查询日志\n1 2 3 4 # vim /etc/my.cnf [mysqld] log[=dir\\[filename]] # service mysqld restart 四、慢查询日志\n启用慢查询日志\n1 2 3 4 5 # vim /etc/my.cnf [mysqld] log-slow-queries[=dir\\[filename]] long_query_time=n # service mysqld restart 1 2 3 4 MySQL 5.6: slow-query-log=1 slow-query-log-file=slow.log long_query_time=3 查看慢查询日志\n测试:BENCHMARK(count,expr)\nSELECT BENCHMARK(50000000,2*3);\n默认与慢查询相关变量：mysql\u0026gt; SHOW GLOBAL VARIABLES LIKE \u0026lsquo;%slow_query_log%\u0026rsquo;;\n默认没有启用慢查询，为了服务器调优，建议开启\n开启方法：SET GLOBAL slow_query_log=ON;\n当前生效，永久有效配置文件中设置\n使用mysqldumpslow命令获得日志中显示的查询摘要来处理慢查询日志\n# mysqldumpslow slow.log 那么多久算是慢呢？如果查询时长超过long_query_time的定义值（默认10秒），即为慢查询：\n1 mysql\u0026gt; SHOW GLOBAL VARIABLES LIKE \u0026#39;long_query_time\u0026#39;; ","date":"2019-04-19T23:49:26Z","image":"https://www.ownit.top/title_pic/68.jpg","permalink":"https://www.ownit.top/p/201904192349/","title":"MySQL日志管理"},{"content":"MySQL安全机制\n========================================================\nMySQL权限表\nMySQL用户管理\nMySQL权限管理\n一、MySQL权限表\nmysql.user Global level\n用户字段\n权限字段\n安全字段\n资源控制字段\nmysql.db、mysql.host Database level\n用户字段\n权限字段\nmysql.tables_priv Table level\nmysql.columns_priv Column level\nmysql.procs_priv\n二、MySQL用户管理\n1. 登录和退出MySQL\n示例：\nmysql -h192.168.5.240 -P 3306 -u root -p123 mysql -e ‘select user,host from user’\n-h 指定主机名\n-P MySQL服务器端口\n-u 指定用户名\n-p 指定登录密码\n此处mysql为指定登录的数据库\n-e 接SQL语句\n查看用户表\n用户管理\n1 mysql\u0026gt;use mysql; 查看\n1 mysql\u0026gt; select host,user,password from user ; 2. 创建用户\n方法一：CREATE USER语句创建\n1 CREATE USER zhang@localhost IDENTIFIED BY \u0026#34;zhang\u0026#34;; 刷新表\n1 flush privileges; 方法二： INSERT语句创建\n1 2 3 Imysql\u0026gt; INSERT INTO mysql.user(user,host, password,ssl_cipher,x509_issuer,x509_subject) -\u0026gt; VALUES(\u0026#34;user2\u0026#34;,\u0026#34;localhost\u0026#34;,password(\u0026#34;123456\u0026#34;),\u0026#34;\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;\u0026#34;); Query OK, 1 row affected (0.00 sec) 1 flush privileges; 方法三： GRANT语句创建\n1 2 mysql\u0026gt; GRANT SELECT ON *.* TO user3@\u0026#34;localhost\u0026#34; IDENTIFIED BY \u0026#34;123456\u0026#34;; Query OK, 0 rows affected (0.00 sec) 1 mysql\u0026gt; FLUSH PRIVILEGES; 3. 删除用户\n方法一：DROP USER语句删除\n1 2 mysql\u0026gt; DROP USER user1@\u0026#34;localhost\u0026#34;; Query OK, 0 rows affected (0.00 sec) 方法二：DELETE语句删除\n1 2 3 mysql\u0026gt; delete from user -\u0026gt; where user=\u0026#34;user2\u0026#34; and host=\u0026#34;localhost\u0026#34;; Query OK, 1 row affected (0.00 sec) 1 mysql\u0026gt; flush privileges; 4. 修改用户密码\n＝＝＝root修改自己密码\n方法一：\n1 # mysqladmin -uroot -p123 password \u0026#39;new_password\u0026#39; //123为旧密码 方法二：\n1 2 3 UPDATE mysql.user SET password=password(‘new_password’) WHERE user=’root’ AND host=’localhost’; FLUSH PRIVILEGES; 方法三：\n1 2 SET PASSWORD=password(‘new_password’); FLUSH PRIVILEGES; ==root修改其他用户密码\n方法一：\n1 2 SET PASSWORD FOR user3@’localhost’=password(‘new_password’); FLUSH PRIVILEGES; 方法二：\n1 2 3 UPDATE mysql.user SET password=password(‘new_password’) WHERE user=’user3’ AND host=’localhost’; FLUSH PRIVILEGES; 方法三：\n1 2 GRANT SELECT ON *.* TO user3@’localhost’ IDENTIFIED BY ‘localhost’; FLUSH PRIVILEGES; ＝＝＝普通用户修改自己密码\n方法一：\n1 SET password=password(‘new_password’); 方法二：\n1 # mysqladmin -uzhuzhu -p123 password \u0026#39;new_password\u0026#39; //123为旧密码 ＝＝＝丢失root用户密码\n1 2 3 4 5 6 7 8 # vim /etc/my.cnf [mysqld] skip-grant-tables # service mysqld restart # mysql -uroot mysql\u0026gt; UPDATE mysql.user SET password=password(‘new_password’) WHERE user=’root’ AND host=’localhost’; mysql\u0026gt; FLUSH PRIVILEGES; 三、MySQL权限管理\n权限应用的顺序：\nuser (Y|N) ==\u0026gt; db ==\u0026gt; tables_priv ==\u0026gt; columns_priv\n查看用户权限\n1 mysql\u0026gt; select Host,User,select_priv,insert_priv,update_priv,delete_priv from user; 语法格式：\ngrant 权限列表 on 库名.表名 to 用户名@\u0026lsquo;客户端主机\u0026rsquo; [identified by \u0026lsquo;密码\u0026rsquo; with option参数];\n==权限列表 all 所有权限（不包括授权权限）\nselect,update\n==数据库.表名 *.* 所有库下的所有表 Global level\nweb.* web库下的所有表 Database level\nweb.stu_info web库下的stu_info表 Table level\nSELECT (col1), INSERT (col1,col2) ON mydb.mytbl Column level\n==客户端主机 % 所有主机\n192.168.2.% 192.168.2.0网段的所有主机\n192.168.2.168 指定主机\nlocalhost 指定主机\nwith_option参数\nGRANT OPTION： 授权选项\nMAX_QUERIES_PER_HOUR： 定义每小时允许执行的查询数\nMAX_UPDATES_PER_HOUR： 定义每小时允许执行的更新数\nMAX_CONNECTIONS_PER_HOUR： 定义每小时可以建立的连接数\nMAX_USER_CONNECTIONS： 定义单个用户同时可以建立的连接数\ngrant 普通数据用户，查询、插入、更新、删除 数据库中所有表数据的权利。\ngrant select on testdb.* to common_user@’%’ grant insert on testdb.* to common_user@’%’ grant update on testdb.* to common_user@’%’ grant delete on testdb.* to common_user@’%’ ** MySQL 命令来替代：**\ngrant select, insert, update, delete on testdb.* to common_user@’%’ grant 数据库开发人员，创建表、索引、视图、存储过程、函数。。。等权限。\ngrant 创建、修改、删除 MySQL 数据表结构权限。\ngrant create on testdb.* to developer@’192.168.0.%’; grant alter on testdb.* to developer@’192.168.0.%’; grant drop on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 外键权限。\ngrant references on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 临时表权限。\ngrant create temporary tables on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 索引权限。\ngrant index on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 视图、查看视图源代码 权限。\ngrant create view on testdb.* to developer@’192.168.0.%’; grant show view on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 存储过程、函数 权限。\ngrant create routine on testdb.* to developer@’192.168.0.%’; \u0026ndash; now, can show procedure status grant alter routine on testdb.* to developer@’192.168.0.%’; \u0026ndash; now, you can drop a procedure grant execute on testdb.* to developer@’192.168.0.%’; grant 普通 DBA 管理某个 MySQL 数据库的权限。\ngrant all privileges on testdb to dba@’localhost’ grant 高级 DBA 管理 MySQL 中所有数据库的权限。\ngrant all on *.* to dba@’localhost’ MySQL grant 权限，分别可以作用在多个层次上。 1. grant 作用在整个 MySQL 服务器上：\ngrant select on *.* to dba@localhost; \u0026ndash; dba 可以查询 MySQL 中所有数据库中的表。 grant all on *.* to dba@localhost; \u0026ndash; dba 可以管理 MySQL 中的所有数据库 2. grant 作用在单个数据库上：\ngrant select on testdb.* to dba@localhost; \u0026ndash; dba 可以查询 testdb 中的表。 3. grant 作用在单个数据表上：\ngrant select, insert, update, delete on testdb.orders to dba@localhost; 4. grant 作用在表中的列上：\ngrant select(id, se, rank) on testdb.apache_log to dba@localhost; 5. grant 作用在存储过程、函数上：\ngrant execute on procedure testdb.pr_add to ’dba’@’localhost’ grant execute on function testdb.fn_add to ’dba’@’localhost’ 注意：修改完权限以后 一定要刷新服务，或者重启服务，刷新服务用：FLUSH PRIVILEGES。\nGrant示例：\nGRANT ALL ON *.* TO admin1@\u0026rsquo;%\u0026rsquo; IDENTIFIED BY \u0026rsquo;localhost\u0026rsquo;;\nGRANT ALL ON *.* TO admin2@\u0026rsquo;%\u0026rsquo; IDENTIFIED BY \u0026rsquo;localhost\u0026rsquo; WITH GRANT OPTION;\nGRANT ALL ON bbs.* TO admin3@\u0026rsquo;%\u0026rsquo; IDENTIFIED BY \u0026rsquo;localhost\u0026rsquo;;\nGRANT ALL ON bbs.user TO admin4@\u0026rsquo;%\u0026rsquo; IDENTIFIED BY \u0026rsquo;localhost';\nGRANT SELECT(col1),INSERT(col2,col3) ON bbs.user TO admin5@\u0026rsquo;%\u0026rsquo; IDENTIFIED BY \u0026rsquo;localhost';\n回收权限REVOKE\n查看权限\nSHOW GRANTS\\G\nSHOW GRANTS FOR admin1@\u0026rsquo;%\u0026rsquo;\\G\n回收权限REVOKE\n语法：\nREVOKE 权限列表 ON 数据库名 FROM 用户名@‘客户端主机’\n示例：\nREVOKE DELETE ON *.* FROM admin1@’%’; //回收部分权限\nREVOKE ALL PRIVILEGES ON *.* FROM admin2@’%’; //回收所有权限\nREVOKE ALL PRIVILEGES,GRANT OPTION ON *.* FROM \u0026lsquo;admin2\u0026rsquo;@\u0026rsquo;%\u0026rsquo;;\n========================================================\n","date":"2019-04-18T23:55:30Z","image":"https://www.ownit.top/title_pic/27.jpg","permalink":"https://www.ownit.top/p/201904182355/","title":"MySQL安全机制"},{"content":"一、概述：\n存储过程和函数是事先经过编译并存储在数据库中的一段SQL语句的集合。\n存储过程和函数的区别：\n• 函数必须有返回值，而存储过程没有。\n• 存储过程的参数可以是IN、OUT、INOUT类型，函数的参数只能是IN\n优点：\n• 存储过程只在创建时进行编译；而SQL语句每执行一次就编译一次，所以使用存储过程可以提高数据库执行速度。\n• 简化复杂操作，结合事务一起封装。\n• 复用性好\n• 安全性高，可指定存储过程的使用权。\n说明：\n并发量少的情况下，很少使用存储过程。\n并发量高的情况下，为了提高效率，用存储过程比较多。\n二、创建与调用\n创建存储过程语法 ：\ncreate procedure sp_name(参数列表)\n[特性\u0026hellip;]过程体\n存储过程的参数形式：[IN | OUT | INOUT]参数名 类型\nIN 输入参数\nOUT 输出参数\nINOUT 输入输出参数\ndelimiter $$\ncreate procedure 过程名(参数列表)\nbegin\nSQL语句\nend $$\ndelimiter ;\n调用：\ncall 存储过程名(实参列表)\n存储过程三种参数类型：IN, OUT, INOUT ===================NONE========================\n1 2 3 4 5 6 7 8 9 mysql\u0026gt; \\d $$ mysql\u0026gt; create procedure p1() -\u0026gt; begin -\u0026gt; select count(*) from mysql.user; -\u0026gt; end$$ Query OK, 0 rows affected (0.51 sec) mysql\u0026gt; \\d ; mysql\u0026gt; call p1() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 mysql\u0026gt; create school.table t1( -\u0026gt; id int, -\u0026gt; name varchar(50) -\u0026gt; ); Query OK, 0 rows affected (2.81 sec) mysql\u0026gt; delimiter $$ mysql\u0026gt; create procedure autoinsert1() -\u0026gt; BEGIN -\u0026gt; declare i int default 1; -\u0026gt; while(i\u0026lt;=20000)do -\u0026gt; insert into school.t1 values(i,md5(i)); -\u0026gt; set i=i+1; -\u0026gt; end while; -\u0026gt; END$$ mysql\u0026gt; delimiter ; ====================IN==========================\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 mysql\u0026gt; create procedure autoinsert2(IN a int) -\u0026gt; BEGIN -\u0026gt; declare i int default 1; -\u0026gt; while(i\u0026lt;=a)do -\u0026gt; insert into school.t1 values(i,md5(i)); -\u0026gt; set i=i+1; -\u0026gt; end while; -\u0026gt; END$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; call autoinsert1(10); Query OK, 1 row affected (1.10 sec) mysql\u0026gt; set @num=20; mysql\u0026gt; select @num; +------+ | @num | +------+ | 20 | +------+ 1 row in set (0.00 sec) mysql\u0026gt; call autoinsert1(@num); ====================OUT=======================\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 mysql\u0026gt; delimiter $$ mysql\u0026gt; CREATE PROCEDURE p2 (OUT param1 INT) -\u0026gt; BEGIN -\u0026gt; SELECT COUNT(*) INTO param1 FROM t1; -\u0026gt; END$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; delimiter ; mysql\u0026gt; select @a; +------+ | @a | +------+ | NULL | +------+ 1 row in set (0.00 sec) mysql\u0026gt; CALL p2(@a); Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SELECT @a; +------+ | @a | +------+ | 3 | +------+ ===================IN 和 OUT=====================\n作用：统计指定部门的员工数 1 2 3 4 5 6 7 8 9 10 11 12 mysql\u0026gt; create procedure count_num(IN p1 varchar(50), OUT p2 int) -\u0026gt; BEGIN -\u0026gt; select count(*) into p2 from employee -\u0026gt; where post=p1; -\u0026gt; END$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; \\d ; mysql\u0026gt; call count_num(\u0026#39;hr\u0026#39;,@a); mysql\u0026gt;select @a; 作用：统计指定部门工资超过例如5000的总人数 1 2 3 4 5 6 7 8 9 mysql\u0026gt; create procedure count_num(IN p1 varchar(50), IN p2 float(10,2), OUT p3 int) -\u0026gt; BEGIN -\u0026gt; select count(*) into p3 from employee -\u0026gt; where post=p1 and salary=\u0026gt;p2; -\u0026gt; END$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; \\d ; mysql\u0026gt; call count_num(\u0026#39;hr\u0026#39;,5000,@a); ====================INOUT======================\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 mysql\u0026gt; create procedure proce_param_inout(inout p1 int) -\u0026gt; begin -\u0026gt; if (p1 is not null) then -\u0026gt; set p1=p1+1; -\u0026gt; else -\u0026gt; select 100 into p1; -\u0026gt; end if; -\u0026gt; end$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select @h; +------+ | @h | +------+ | NULL | +------+ 1 row in set (0.00 sec) mysql\u0026gt; call proce_param_inout(@h); Query OK, 1 row affected (0.00 sec) mysql\u0026gt; select @h; +------+ | @h | +------+ | 100 | +------+begin 1 row in set (0.00 sec) mysql\u0026gt; call proce_param_inout(@h); Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select @h; +------+ | @h | +------+ | 101 | +------+ 1 row in set (0.00 sec) FUNCTION函数 =================================================\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 mysql\u0026gt; CREATE FUNCTION hello (s CHAR(20)) -\u0026gt; RETURNS CHAR(50) RETURN CONCAT(\u0026#39;Hello, \u0026#39;,s,\u0026#39;!\u0026#39;); Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select hello(\u0026#39;localhost\u0026#39;); +------------------+ | hello(\u0026#39;localhost\u0026#39;) | +------------------+ | Hello, localhost! | +------------------+ root@(company)\u0026gt; select hello(\u0026#39;localhost\u0026#39;) return1; +-----------------+ | return1 | +-----------------+ | Hello, localhost! | +-----------------+ 1 row in set (0.00 sec) mysql\u0026gt; create function name_from_employee(x int) -\u0026gt; returns varchar(50) -\u0026gt; BEGIN -\u0026gt; return (select emp_name from employee -\u0026gt; where emp_id=x); -\u0026gt; END$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select name_from_employee(3); mysql\u0026gt; select * from employee where emp_name=name_from_employee(1); +--------+----------+------+------------+------------+-----------------+---------+--------+--------+ | emp_id | emp_name | sex | hire_date | post | job_description | salary | office | dep_id | +--------+----------+------+------------+------------+-----------------+---------+--------+--------+ | 1 | jack | male | 2013-02-02 | instructor | teach | 5000.00 | 501 | 100 | +--------+----------+------+------------+------------+-----------------+---------+--------+--------+ 1 row in set (0.00 sec) ==============================================\n创建函数的语法：\ncreate function 函数名(参数列表) returns 返回值类型\n[特性\u0026hellip;] 函数体\n函数的参数形式：参数名 类型\ndelimiter $$\ncreate function 函数名(参数列表) returns 返回值类型\nbegin\n有效的SQL语句\nend$$\ndelimiter ;\n调用：\nselect 函数名(实参列表)\ndelimiter $$\ncreate function fun1(str char(20)) returns char(50)\nreturn concat(\u0026ldquo;hello\u0026rdquo;,str,\u0026quot;!\u0026quot;);\n$$\ndelimiter ;\nselect fun1(\u0026rsquo; function\u0026rsquo;);\n存储过程与函数的维护：\nshow create procedure pr1 \\G;\nshow create function pr1 \\G;\nshow {procedure|function} status {like \u0026lsquo;pattern\u0026rsquo;}\ndrop {procedure|function} {if exists} sp_name\nmysql变量的术语分类：\n1.用户变量：以\u0026quot;@\u0026ldquo;开始，形式为\u0026rdquo;@变量名\u0026quot;，由客户端定义的变量。\n用户变量跟mysql客户端是绑定的，设置的变量只对当前用户使用的客户端生效，当用户断开连接时，所有变量会自动释放。\n2.全局变量：定义时如下两种形式，set GLOBAL 变量名 或者 set @@global.变量名\n对所有客户端生效，但只有具有super权限才可以设置全局变量。\n3.会话变量：只对连接的客户端有效。\n4.局部变量：设置并作用于begin\u0026hellip;end语句块之间的变量。\ndeclare语句专门用于定义局部变量。而set语句是设置不同类型的变量，包括会话变量和全局变量\n语法：declare 变量名[\u0026hellip;] 变量类型 [default 值]\ndeclare定义的变量必须写在复合语句的开头，并且在任何其它语句的前面。\n变量的赋值：\n直接赋值： set 变量名=表达式值或常量值[\u0026hellip;];\n用户变量的赋值：\n1、set 变量名=表达式或常量值;\n2、也可以将查询结果赋值给变量(要求查询返回的结果只能有一行)\n例：set 列名 into 变量名 from 表名 where 条件;\n3、select 值 into @变量名；\n客户端变量不能相互共享。\n1 2 3 4 5 6 7 8 9 10 11 delimiter $$ create procedure pr2() begin declare xname varchar(50); declare xdesc varchar(100); set xname=\u0026#34;caiwu\u0026#34;; set xdesc=\u0026#34;accouting\u0026#34;; insert into dept(name,desc) values(xname,xdesc); end$$ delimiter ; call pr2(); 1 2 3 4 5 6 7 8 delimiter $$ create procedure pr3(in x int,in y int,out sum int) begin set sum=x+y; end$$ delimiter ; call pr3(3,4,@sum); select @sum; 1 2 3 4 5 6 7 8 9 delimiter // create function fun6(x int,y int) returns int begin declare sum int; set sum=x+y; return sum; end// delimiter ; select fun6(4,3); 1 2 3 4 5 6 7 8 9 10 11 delimiter // create function fun_add_rand(in_int int ) RETURNS int BEGIN declare i_rand int; declare i_return int; set i_rand=floor(rand()*100); set i_return = in_int + i_rand; return i_return; END; // ","date":"2019-04-18T22:33:16Z","image":"https://www.ownit.top/title_pic/53.jpg","permalink":"https://www.ownit.top/p/201904182233/","title":"MySQL存储过程与函数"},{"content":"MySQL触发器Triggers\n========================================================\n触发器简介\n创建触发器\n查看触发器\n删除触发器\n触发器案例\n一、触发器简介\n触发器（trigger）是一个特殊的存储过程，它的执行不是由程序调用，也不是手工启动，而是由事件来触发，\n比如当对一个表进行操作（ insert，delete， update）时就会激活它执行。触发器经常用于加强数据的完整\n性约束和业务规则等。\n例如，当学生表中增加了一个学生的信息时，学生的总数就应该同时改变。因此可以针对学生表创建一个触发\n器，每次增加一个学生记录时，就执行一次学生总数的计算操作，从而保证学生总数与记录数的一致性。\n二、创建Trigger\n语法：\nCREATE TRIGGER 触发器名称 BEFORE|AFTER 触发事件\nON 表名 FOR EACH ROW\nBEGIN\n触发器程序体;\nEND\n\u0026lt;触发器名称\u0026gt; 最多64个字符，它和MySQL中其他对象的命名方式一样\n{ BEFORE | AFTER } 触发器时机\n**{ INSERT | UPDATE | DELETE }**触发的事件\nON \u0026lt;表名称\u0026gt; 标识建立触发器的表名，即在哪张表上建立触发器\nFOR EACH ROW 触发器的执行间隔：FOR EACH ROW子句通知触发器 每隔一行\n执行一次动作，而不是对整个表执行一次\n\u0026lt;触发器程序体\u0026gt; 要触发的SQL语句：可用顺序，判断，循环等语句实现一般程序需要的逻辑功能\n触发器示例 1. 创建表\n1 2 3 4 5 6 7 8 mysql\u0026gt; create table student( -\u0026gt; id int unsigned auto_increment primary key not null, -\u0026gt; name varchar(50) -\u0026gt; ); mysql\u0026gt; insert into student(name) values(\u0026#39;wei\u0026#39;); mysql\u0026gt; create table student_total(total int); mysql\u0026gt; insert into student_total values(1); 2. 创建触发器student_insert_trigger\n创建触发器，实现添加学生信息，数量自动增加 1 2 3 4 5 6 7 8 mysql\u0026gt; \\d $$ mysql\u0026gt; create trigger student_insert_tigger after insert on student for each row begin update nummber set count=count+1; end$$ Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; \\d ; 创建触发器，删除学生信息，数量自动减少 1 2 3 4 5 6 7 8 9 mysql\u0026gt; \\d $ # 修改mysql结束符 mysql\u0026gt; create trigger student_delete_trigger after delete -\u0026gt; on student for each row -\u0026gt; begin -\u0026gt; update number set count=count-1; -\u0026gt; end$ Query OK, 0 rows affected (0.01 sec) mysql\u0026gt; \\d ; 删除一个学生信息 1 mysql\u0026gt; delete from student wheree id=\u0026#34;2\u0026#34;; 查看学生信息\n三、查看触发器\n1. 通过SHOW TRIGGERS语句查看\n1 mysql\u0026gt; show triggers\\G; 2. 通过系统表triggers查看\nUSE information_schema\nSELECT * FROM triggers\\G\nSELECT * FROM triggers WHERE TRIGGER_NAME=\u0026lsquo;触发器名称\u0026rsquo;\\G\n四、删除触发器\n1. 通过DROP TRIGGERS语句删除\nDROP TRIGGER 解发器名称\n1 2 mysql\u0026gt; drop trigger student_delete_trigger; Query OK, 0 rows affected (0.00 sec) ","date":"2019-04-18T20:02:46Z","image":"https://www.ownit.top/title_pic/75.jpg","permalink":"https://www.ownit.top/p/201904182002/","title":"MySQL触发器"},{"content":"视图\n视图就是一个表或多个表的查询结果，它是一张虚拟的表，因为它并不能存储数据。\n视图的作用、优点：\n限制对数据的访问\n让复杂查询变得简单\n提供数据的独立性\n可以完成对相同数据的不同显示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 //创建、修改视图 create or replace view view_temp as select name, age from temp; //通常不对视图的数据做修改操作，因为视图是一张虚拟的表，它并不存储实际数据。如果想让视图不被修改，可以用with check option来完成限制。 create or replace view view_temp as select *from temp with check option; //删除视图 drop view view_temp; //显示创建语法 show create view v_temp; 显示学生成绩单的视图\n1 2 3 4 mysql\u0026gt; create view student_cj as select students.number,students.name,course.math,course.english,course.chinese -\u0026gt; from students,course -\u0026gt; where students.number=course.number; Query OK, 0 rows affected (0.00 sec) 使用student_cj个视图，显示结果\n删除这个视图\n1 2 mysql\u0026gt; drop view student_cj; Query OK, 0 rows affected (0.00 sec) 查看视图的信息\n1 mysql\u0026gt; show create view student_cj\\G 索引\n1.在不读取整个表的情况下，索引使数据库应用程序可以更快地查找数据。[ 是为了快速查询而针对某些字段建立起来的。]\n2.更新一个包含索引的表需要比更新一个没有索引的表更多的时间，这是由于索引本身也需要更新。\n3.表\n01.数据库中的数据都是存储在表中的\n02.表是物理存储的，真实存在的\n2.案例\n创建索引：create index degree_fast on score (degree); 这里的dgeree_fast是索引名，score是表明，degree是表中的一个字段。 创建视图：\n1 2 3 create view [viewName] as select [someFields] from [tableName] 删除索引： - 01.方式一：drop index degree_fast on score; - 02.方式二：alter table score drop index degree_fast;\nMysql cannot drop index needed in a foreign key constraint.Mysql不能在外键约束下删除索引。如果有外键的话，需要先把外键删除，然后再删除索引。\n","date":"2019-04-17T23:46:10Z","image":"https://www.ownit.top/title_pic/42.jpg","permalink":"https://www.ownit.top/p/201904172346/","title":"MySQL视图及索引"},{"content":"select 语句： select 语句一般用法为: select 字段名 from tb_name where 条件 ; **select 查询语句类型一般分为三种： 单表查询，多表查询，子查询**\n最简单的单表查询 : select * from tb_name; *表示，所有字段\n查询特定字段(投影)： select 字段名1，字段名2， from tb_name;\nwhere 语句过滤查询(选择)\nselect * from tb_name where 条件 ;\n使用SELECT子句进行多表查询\nSELECT 字段名 FROM 表1，表2 … WHERE 表1.字段 = 表2.字段 AND 其它查询条件\nSELECT a.id,a.name,a.address,a.date,b.math,b.english,b.chinese FROM tb_demo065_tel AS b,tb_demo065 AS a WHERE a.id=b.id\n注:在上面的的代码中，以两张表的id字段信息相同作为条件建立两表关联，但在实际开发中不应该这样使用，最好用主外键约束来实现\n首先创建一个数据库 学生人数表 学生成绩表 显示每个学生的对应的成绩 方法一：\n1 2 3 mysql\u0026gt; select students.number,students.name,students.sex,course.math,course.english,course.chinese -\u0026gt; from students inner join course -\u0026gt; on students.number=course.number; 方法二：\n1 2 3 mysql\u0026gt; select students.number,students.name,students.sex,course.math,course.english,course.chinese -\u0026gt; from students,course -\u0026gt; where students.number=course.number; 根据学号排名升序输出成绩 1 mysql\u0026gt; select students.number as 学号,students.name as 姓名,course.math as 数学,course.english as 英语,course.chinese as 语文 from students,course where students.number=course.number order by students.number; 求学生的总成绩，并显示出来，成绩按降序排列 1 mysql\u0026gt; select students.number as 学号,students.name as 姓名,(course.math+course.english +course.chinese) as 总成绩 from students,course where students.number=course.number order by 总成绩 desc; ","date":"2019-04-17T16:31:24Z","image":"https://www.ownit.top/title_pic/23.jpg","permalink":"https://www.ownit.top/p/201904171631/","title":"MySQL的select多表查询"},{"content":"MySQL 查询数据 MySQL 数据库使用SQL SELECT语句来查询数据。\n你可以通过 mysql\u0026gt; 命令提示窗口中在数据库中查询数据\n语法 以下为在MySQL数据库中查询数据通用的 SELECT 语法：\nSELECT column_name,column_name FROM table_name [WHERE Clause] [LIMIT N][ OFFSET M]\n查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分割，并使用WHERE语句来设定查询条件。 SELECT 命令可以读取一条或者多条记录。 你可以使用星号（*）来代替其他字段，SELECT语句会返回表的所有字段数据 你可以使用 WHERE 语句来包含任何条件。 你可以使用 LIMIT 属性来设定返回的记录数。 你可以通过OFFSET指定SELECT语句开始查询的数据偏移量。默认情况下偏移量为0。 建一张表用于我们测试： 1 2 3 4 5 6 7 8 create table student ( ids int auto_increment primary key, name varchar(20), chinese float, english float, math float ); 插入如下数据： 1 2 3 4 5 6 7 insert into student values(1,\u0026#39;李明\u0026#39;,89,78,90); insert into student values(2,\u0026#39;乘风\u0026#39;,67,89,56); insert into student values(3,\u0026#39;南宫流云\u0026#39;,87,78,77); insert into student values(4,\u0026#39;南宫皓月\u0026#39;,88,98,90); insert into student values(5,\u0026#39;南宫紫月\u0026#39;,82,84,67); insert into student values(6,\u0026#39;萧炎\u0026#39;,55,85,45); insert into student values(7,\u0026#39;林动\u0026#39;,75,65,30); 1、指定查询列 1 mysql\u0026gt; select id,name,chinese from student; 2、去重查询 用distinct关键字， 如果结果中有完全相同的行，就去除重复行\n1 mysql\u0026gt; select distinct math from student; 3、select语句中进行运算 查询学生总成绩 1 mysql\u0026gt; select id,name,(chinese+math+english) as 总成绩 from student; 查询所有姓南宫人的总成绩。 1 2 mysql\u0026gt; select id,name,(chinese+math+english) as 总成绩 from student -\u0026gt; where name like \u0026#39;南宫%\u0026#39;; 4、where查询过滤 在where子句中有很多经常使用的运算符，如下： （1）查询所有英语成绩大于90的同学成绩： 1 2 mysql\u0026gt; select id,name,english as 英语 from student -\u0026gt; where english \u0026gt; 90; （2）查询所有总分大于200分的同学 注意：where子句后不能用别名 因为数据库中先执行where子句，再执行select子句。 1 2 mysql\u0026gt; select id,name,(chinese+math+english) as 总成绩 from student -\u0026gt; where (chinese+math+english) \u0026gt; 200; （3）查询姓林并且id大于6的学生信息 1 2 mysql\u0026gt; select id,name from student -\u0026gt; where name like \u0026#39;林%\u0026#39; and id \u0026gt;6; （4）查询英语成绩大于语文成绩的同学 1 2 mysql\u0026gt; select id,name from student -\u0026gt; where english \u0026gt; chinese; （5）查询所有总分大于200并且数学成绩小于语文成绩的学生信息 1 2 mysql\u0026gt; select id,name from student -\u0026gt; where (chinese+math+english) \u0026gt;200 and math \u0026lt; chinese; （6）查询所有英语成绩在80到90分的同学 方法一： 1 2 mysql\u0026gt; select id,name,english from student -\u0026gt; where english \u0026gt;=80 and english \u0026lt;= 90; 方法二： 注意：between是闭区间\n1 2 mysql\u0026gt; select id,name,english from student -\u0026gt; where english between 80 and 90; （7）查询数学成绩为89,90,91的同学信息 or: 1 2 mysql\u0026gt; select id,name,math from student -\u0026gt; where math=89 or math=90 or math=91; in: 1 2 mysql\u0026gt; select id,name,math from student -\u0026gt; where math in(89,90,91); 5、order by排序语句 **asc升序(默认)，desc降序 **\norder by 子句应该位于select语句的结尾 eg：对数学成绩进行排序 默认升序： 1 2 mysql\u0026gt; select id,name,math from student -\u0026gt; order by math; 降序： 1 2 mysql\u0026gt; select id,name,math from student -\u0026gt; order by math desc; 对总分进行从高到低输出 1 2 mysql\u0026gt; select (chinese+math+english) as 总成绩 from student -\u0026gt; order by 总成绩 desc; 6、常用函数 （1)count(） count(*)统计null值 count(列名)排除null值 eg :统计当前student表中一共有多少学生 1 mysql\u0026gt; select count(*) as 人数 from student; (2)sum() eg：统计一个班数学总成绩 1 mysql\u0026gt; select sum(math) as 数学总成绩 from student; （3）平均值：avg（） 求数学的平均值\n1 mysql\u0026gt; select sum(math)/count(*) as 数学平均值 from student; 1 mysql\u0026gt; select avg(math) as 数学平均值 from student; 7、group by 子句的使用\n假设有一个职工信息表， EMP:表名 ；部门：depton；sal：工资；job：工作 我们设想： （1）显示每个部门的平均工资和最高工资 select deptno,avg(sal),max(sal) from EMP group by deptno; （2）显示每个部门的每种岗位的平均工资和最低工资 select avg(sal),min(sal),job, deptno from EMP group by deptno, job; 补充：首先按照deptno分组，然后各组再按照job进行分组。 （3）显示平均工资低于2000的部门和它的平均工资 解题思路： 1. 统计各个部门的平均工资 select avg(sal) from EMP group by deptno 2. having往往和group by配合使用，对group by结果进行过滤 select avg(sal) as myavg from EMP group by deptno having myavg\u0026lt;2000;\n","date":"2019-04-16T23:50:12Z","image":"https://www.ownit.top/title_pic/43.jpg","permalink":"https://www.ownit.top/p/201904162350/","title":"MySQL的select详细介绍"},{"content":" SQL语句 ** DLL 数据定义语言** ** create，drop**\n** DML 数据操纵语言** ** insert，delete，select，update**\n** DCL 数据控制语言** ** grant，revoke**\n使用ALTER TABLE修改表结构\n（1）修改表名 ALTER TABLE \u0026lt;表名\u0026gt; RENAME \u0026lt;新表名\u0026gt;\n1 2 mysql\u0026gt; alter table game_account rename account; Query OK, 0 rows affected (0.05 sec) （2）修改表的搜索引擎 1 2 3 mysql\u0026gt; alter table account engine=MyISAM; Query OK, 0 rows affected (0.05 sec) Records: 0 Duplicates: 0 Warnings: 0 查看表的信息\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; show create table account\\G; *************************** 1. row *************************** Table: account Create Table: CREATE TABLE `account` ( `game_name` char(15) NOT NULL, `game_password` char(25) NOT NULL ) ENGINE=MyISAM DEFAULT CHARSET=latin1 1 row in set (0.00 sec) ERROR: No query specified （3）添加字段 ALTER TABLE \u0026lt;表名\u0026gt; ADD \u0026lt;字段名称\u0026gt; \u0026lt;字段定义\u0026gt;\n后面添加： 1 mysql\u0026gt; alter table account add game_sex enum(\u0026#34;M\u0026#34;,\u0026#34;F\u0026#34;) not null; 前面添加：\n1 2 3 mysql\u0026gt; alter table account add game_address varchar(20) not null default \u0026#34;huabei\u0026#34; first; Query OK, 0 rows affected (0.00 sec) Records: 0 Duplicates: 0 Warnings: 0 中间添加： 1 2 3 mysql\u0026gt; alter table account add game_money int after game_name; Query OK, 0 rows affected (0.00 sec) Records: 0 Duplicates: 0 Warnings: 0 （4）删除字段\nALTER TABLE \u0026lt;表名\u0026gt; drop \u0026lt;字段名称\u0026gt;\n1 2 3 mysql\u0026gt; alter table account drop game_wei; Query OK, 0 rows affected (0.00 sec) Records: 0 Duplicates: 0 Warnings: 0 （5）修改字段名称及字段定义 ALTER TABLE \u0026lt;表名\u0026gt; CHANGE \u0026lt;旧字段\u0026gt; \u0026lt;新字段名称\u0026gt; \u0026lt;字段定义\u0026gt;\n1 2 3 mysql\u0026gt; alter table account change game_zhang wei char(25) not null; Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 1 2 3 mysql\u0026gt; alter table account change wei wei varchar(60) ; Query OK, 0 rows affected (0.00 sec) Records: 0 Duplicates: 0 Warnings: 0 （6）修改字段定义\nALTER TABLE \u0026lt;表名\u0026gt; MODIFY \u0026lt;字段名称\u0026gt; \u0026lt;字段定义\u0026gt;\n1 2 3 mysql\u0026gt; alter table account modify wei int ; Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 1 2 3 mysql\u0026gt; alter table account modify wei int not null ; Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 ","date":"2019-04-16T18:55:22Z","image":"https://www.ownit.top/title_pic/40.jpg","permalink":"https://www.ownit.top/p/201904161855/","title":"MySQL使用alter修改表的结构"},{"content":"基本管理指令 mysql登陆 第一种\n1 [root@wei ~]# mysql -u root -p 第二种（带参输入）\n1 [root@wei ~]# mysql -uroot -proot 注意：每个命令后面必须加;\nmysql里面清屏 ** \\! clear** 数据库基本管理操作\n（1）查看数据库 1 2 3 4 5 6 7 8 9 10 11 12 show databases; mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | farm | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.00 sec) （2）创建数据库\nCREATE DATABASE \u0026lt;db_name\u0026gt; [CHARACTER=\u0026lt;字符集\u0026gt; COLLATE=\u0026lt;排序规则\u0026gt;]\n1 2 mysql\u0026gt; create database game; Query OK, 1 row affected (0.01 sec) (3)查看数据库的创建信息\n1 2 3 4 5 6 7 mysql\u0026gt; show create database game; +----------+-----------------------------------------------------------------+ | Database | Create Database | +----------+-----------------------------------------------------------------+ | game | CREATE DATABASE `game` /*!40100 DEFAULT CHARACTER SET latin1 */ | +----------+-----------------------------------------------------------------+ 1 row in set (0.00 sec) （4）查看mysql数据库支持的字符集\n1 mysql\u0026gt; show character set; （5）查看mysql数据库支持字符集的排序规则 1 mysql\u0026gt; show collation; （6）删除数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 mysql\u0026gt; drop database lol; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | farm | | game | | mysql | | performance_schema | +--------------------+ 5 rows in set (0.00 sec) （7）切换数据库 1 2 mysql\u0026gt; use game Database changed 示例：创建一个lol的数据库，字符集为utf8,排序为utf8_general_ci 1 2 3 4 5 6 7 8 9 10 11 12 13 mysql\u0026gt; create database lol -\u0026gt; character set=utf8 -\u0026gt; collate=utf8_general_ci; Query OK, 1 row affected (0.00 sec) mysql\u0026gt; show create database lol; +----------+--------------------------------------------------------------+ | Database | Create Database | +----------+--------------------------------------------------------------+ | lol | CREATE DATABASE `lol` /*!40100 DEFAULT CHARACTER SET utf8 */ | +----------+--------------------------------------------------------------+ 1 row in set (0.00 sec) rpm默认数据目录 ** /var/lib/mysql -\u0026mdash;-\u0026gt;数据目录：rpm默认数据目录**\n**数据库一般存在数据目录下/var/lib/mysql **\n1 2 3 [root@wei ~]# ls /var/lib/mysql auto.cnf game ib_logfile0 lol mysql.sock farm ibdata1 ib_logfile1 mysql performance_schema 数据表的基本操作管理： （1）查看表 1 2 3 4 5 6 7 mysql\u0026gt; show tables; +----------------+ | Tables_in_game | +----------------+ | game_account | +----------------+ 1 row in set (0.00 sec) （2）创建表 CREATE TABLE \u0026lt;表名\u0026gt;(字段名称 数据类型 [属性],字段名称 数据类型 [属性]\u0026hellip;)\n数据类型：\n数值型\n字符型\n日期/时间型\n1 2 3 4 mysql\u0026gt; create table game_account( -\u0026gt; game_name char(15) not null, -\u0026gt; game_passwd char(15) not null, -\u0026gt; ); ** （3）查看创建表的信息**\n1 2 3 4 5 6 7 8 9 10 mysql\u0026gt; show create table game_account\\G; *************************** 1. row *************************** Table: game_account Create Table: CREATE TABLE `game_account` ( `game_name` char(15) NOT NULL, `game_password` char(25) NOT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1 1 row in set (0.00 sec) ERROR: No query specified （4）删除表 1 2 mysql\u0026gt; drop table gam; Query OK, 0 rows affected (0.01 sec) （5）查看表结构 1 2 3 4 5 6 7 8 mysql\u0026gt; desc game_account; +---------------+----------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +---------------+----------+------+-----+---------+-------+ | game_name | char(15) | NO | | NULL | | | game_password | char(25) | NO | | NULL | | +---------------+----------+------+-----+---------+-------+ 2 rows in set (0.00 sec) ","date":"2019-04-16T12:43:20Z","image":"https://www.ownit.top/title_pic/58.jpg","permalink":"https://www.ownit.top/p/201904161243/","title":"MySQL基本库表管理"},{"content":"Linux 上安装 MySQL Linux平台上推荐使用RPM包来安装Mysql,MySQL AB提供了以下RPM包的下载地址：\nMySQL - MySQL服务器。你需要该选项，除非你只想连接运行在另一台机器上的MySQL服务器。 MySQL-client - MySQL 客户端程序，用于连接并操作Mysql服务器。 MySQL-devel - 库和包含文件，如果你想要编译其它MySQL客户端，例如Perl模块，则需要安装该RPM包。 MySQL-shared - 该软件包包含某些语言和应用程序需要动态装载的共享库(libmysqlclient.so*)，使用MySQL。 MySQL-bench - MySQL数据库服务器的基准和性能测试工具。 安装前，我们可以检测系统是否自带安装 MySQL:\n1 rpm -qa | grep mysql 如果你系统有安装，那可以选择进行卸载:\n1 2 rpm -e mysql　// 普通删除模式 rpm -e --nodeps mysql　// 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除 接下来我们在 Centos7 系统下使用 yum 命令安装 MySQL，需要注意的是 CentOS 7 版本中 MySQL数据库已从默认的程序列表中移除，所以在安装前我们需要先去官网下载 Yum 资源包，下载地址为：https://dev.mysql.com/downloads/repo/yum/\n注意：没安装wget的朋友，需要先安装wget\n1 [root@wei ~]# yum install wget 下载安装包 1 wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 安装rpm包 1 rpm -ivh mysql-community-release-el7-5.noarch.rpm 安装mysql-server服务 1 yum install mysql-server 权限设置： 1 chown mysql:mysql -R /var/lib/mysql 初始化 MySQL： 1 mysqld --initialize 启动 MySQL： 1 systemctl start mysqld 查看 MySQL 运行状态： 1 systemctl status mysqld 设置为开机自启 1 systemctl enable mysqld 验证 MySQL 安装 1 2 [root@wei ~]# mysqladmin --version mysqladmin Ver 8.42 Distrib 5.6.42, for Linux on x86_64 ##显示这个信息表示成功 注意： 设置密码 当第一次启动MySQL服务器时，为MySQL根用户生成一个临时密码。 您可以通过运行以下命令找到密码：\n1 [root@wei ~]# grep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log **localhost：后边的就是临时密码，先复制下来 **\n注意：如果没有查到，就是不需要，登陆时，回车即可登陆\n进入mysql，在Enter password：复制粘贴先前的密码就进去了\n1 mysql -u root -p 修改密码方式 1 2 3 4 [root@wei ~]# mysqladmin -u root -p password Enter password: #旧密码 New password: #新密码 Confirm new password: #再输入一次 **注意：**在输入密码时，密码是不会显示了，你正确输入即可。","date":"2019-04-16T09:47:25Z","image":"https://www.ownit.top/title_pic/40.jpg","permalink":"https://www.ownit.top/p/201904160947/","title":"MySQL的rpm安装教程"},{"content":"MySQL数据库 以文本的形式存储数据的劣势:\n** 1、数据冗余(一个文件中出现n次相同的数据)和数据不一致性** ** 2、数据访问困难** ** 3、数据孤立** ** 4、数据完整性问题** ** 5、原子性(多个相关联的操作必须要同时完成** ** 6、并发访问异常** ** 7、安全性问题** DBMS\u0026mdash;-DataBase Management System数据库管理系统\n** 以关系(表)的形式存储数据**\n** 记录Record 表中的每一-行数据;\n字段(属性) column 表中的每一列名字**\n** 数据库\n表\n软件:**\n** MySQL, oracle, MariaDB (https:/ /www.percona.com/)，DB2， SQL Server\nMongeDB**\n约束 Constraint\n** 域约束:数据类型约束\n保证某字段的数据类型一致**\n** 外键约束:引用完整性约束(InnoDB)\n一个表中某字段的数据必须在与之相关的其他表的相关联字段中存在**\n** 主键约束\n某字段能惟一标识此字段所属的实体，并且不允许为空，\n一个表只能有一个主键**\n** 惟一键约束\n某字段能惟一标识此字段所属的实体，可以为空 一个表可以有多个惟一键**\n** 检查性约束\n保证某字段中不能出现违反常理的数据，例如年龄**\nMySQL基础应用 一、数据库特点:结构化,无有害,无重复;\n二、数据库优点:按一定的数据模型组织,描述和储存;可为各种用户共享,冗余度小,节省储存空间易扩展,编写有关数据应用程序。\n三、常用Dos操作指令:\n安装数据库:mysqld -install, 开启/关闭数据库:start mysql/net stop, 监听端口信息:netstat -a, 登陆数据库:mysql -uroot -p, 显示默认数据库:use dbname, 显示所有数据库:show databases, 显示默认数据库中的所有表:show tables, 放弃正在输入的指令:\\c, 显示命令清单:\\h, 退出mysql程序:\\q, 查看mysql服务器状态信息:\\s, mysql版本信息:SELECT version(), 打开表结构:desc table。 四、数据库语法组成:\n1.DDM(Data Definition Language数据库定义语言):create table,drop table,alter table等\n2.DCL(Data Control Language数据控制语言):grant,revoke等;\n3.DML(Data Manipulation Language数据操作语言)查询SELECT、插入insert、update修改、删除delete;\n五、MYSQL三种常用的数据类型:文本:char,varchar,text;数字,日期和时间类型;\n六、常用操作\n显示表结构:DESC 表名; 删除表操作:drop table 表名 除数据库操作:drop database 数据库名 更改表结构操作:alter table 表名 action;(action可以是以下操作) add 列名 建表语句[first/after]\u0026ndash;在表中添加列,制定其位置; add primary key (列名)\u0026ndash; 添加一个主键,如果主键已存在,会报错; add foreign key (列名) reference 表名 （列名）;\u0026ndash; 为表添加一个外键; alter列名 set default 默认值;\u0026ndash; 更改指定列的默认值 drop 列名 \u0026ndash; 删除一列 drop primary key \u0026ndash; 删除主键 engine 类型名 \u0026ndash; 改变表的类型 rename as 新表名 \u0026ndash; 改变表名 change 旧列名 新列名 [first /after]\u0026ndash; 更改列的类型和名称 modify 和change相同; ","date":"2019-04-15T23:27:50Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201904152327/","title":"MySQL基础理论"},{"content":"awk中使用数组 一.数组格式\n数组是一个包含一系列元素的表.\n格式如下：\n** abc[1]=”xiaohong”**\n** abc[2]=”xiaolan”**\n解释：\nabc ：为数组名称\n[1]、[2]：为数组元素下标，可以理解为数组的第1个元素、数组的第2个元素\n”xiaohong”、”xiaolan”： 元素内容\n数组\narrray[index-expression]\n数组下从1开始，也可以使用字符串作为数组的下标\nindex-expression可以使用任意的字符串\n需注意的是：如果某数组元素事先不存在，那么引用其时，awk会自动创建次元素并初始化为0，要判断某数组中是否存在某元素，需要\n使用index in arrary的方式\n要遍历数组中每一个元素，需要使用 如下的特殊结构： for（变量 in 数组名称）{print 数组名称[小标]}\n其中，vae是数组的下标\n统计每个shell的使用次数 1 2 3 4 5 6 [root@wei awk]# awk -F: \u0026#39;{shell[$7]++}END{for(i in shell){print i,shell[i]}}\u0026#39; /etc/passwd /bin/sync 1 /bin/bash 17 /sbin/nologin 20 /sbin/halt 1 /sbin/shutdown 1 ** 统计每个状态下的tcp连接个数\n**\n1 2 3 [root@wei awk]# netstat -antp | awk \u0026#39;/^tcp/{state[$6]++}END{for(i in state){print i,state[i]}}\u0026#39; LISTEN 9 ESTABLISHED 2 ","date":"2019-04-15T23:18:25Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/201904152318/","title":"Linux shell awk数组使用"},{"content":"awk逻辑控制语句 1，if\u0026hellip;else 格式：\nif（条件）{语句；语句} else {语句1；语句2}\n如果statement只有一条语句，{}可以不写\n以冒号为分隔符，判断第一个字段，如果为root，则显示用户为administrator，否则显示用户问common user 1 2 3 [root@wei csdn]# awk -F: \u0026#39;{if($3==0){print $1,\u0026#34;is administrator.\u0026#34;}else {print $1,\u0026#34;is common user\u0026#34;}}\u0026#39; /etc/passwd root is administrator. bin is common user 编写uid大于500的用户个数 1 2 [root@wei csdn]# awk -F: -v count=0 \u0026#39;{if($3\u0026gt;500) {count++}}END{print \u0026#34;uid大于500的用户数量：\u0026#34;,count}\u0026#39; /etc/passwd uid大于500的用户数量： 19 判断系统的bash用户和nologin用户 1 2 [root@wei csdn]# awk -v num1=0 -v num2=0 -F: \u0026#39;/bash$/ || /nologin$/{if($7==\u0026#34;/bin/bash\u0026#34;){num1++} else {num2++}}END{print \u0026#34;bash用户数量：\u0026#34;,num1,\u0026#34;nologin用户数量：\u0026#34;,num2}\u0026#39; /etc/passwd bash用户数量： 17 nologin用户数量： 20 2，while\n格式：\nwhile(条件) {语句1;语句2;\u0026hellip;..}\npasswd前3行进行输出，输出3次 1 2 3 4 5 6 7 8 9 10 [root@wei csdn]# head -n 3 /etc/passwd | awk \u0026#39;{i=1;while(i\u0026lt;=3){print $0; i++}}\u0026#39; root:x:0:0:root:/root:/bin/bash root:x:0:0:root:/root:/bin/bash root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin bin:x:1:1:bin:/bin:/sbin/nologin bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin 以冒号为分割符，判断每一行的每一个字段的长度如果大于4，则显之 1 2 3 4 5 6 7 8 [root@wei awk]# head -n 3 /etc/passwd | awk -F: \u0026#39;{i=1;while(i\u0026lt;=7){if(length($i)\u0026gt;4){print $i};i++}}\u0026#39; /root /bin/bash /sbin/nologin daemon daemon /sbin /sbin/nologin 统计test.txt的文件长度大小为5的单词 1 [root@wei awk]# awk \u0026#39;{i=1;while(i\u0026lt;NF) {if(length($i)\u0026gt;5){print $i};i++}}\u0026#39; print.txt 3，for 遍历数组\n格式：\nfor（变量定义；循环终止的条件；改变循环条件的语句） {语句；语句\u0026hellip;}\n** for(i=1;i\u0026lt;=4;i++) {\u0026hellip;\u0026hellip;}**\n以冒号为分隔符，显示/etc/passwd每一行的前3个字段 1 2 3 4 5 [root@wei awk]# awk -F: \u0026#39;{for (i=1;i\u0026lt;=3;i++) { print $i} }\u0026#39; /etc/passwd root x 0 bin 4， break contiune 用于中断循环\n","date":"2019-04-15T23:14:25Z","image":"https://www.ownit.top/title_pic/19.jpg","permalink":"https://www.ownit.top/p/201904152314/","title":"Linux shell awk逻辑控制语句"},{"content":"awk的PATTERN表示方法： 1，正则表达式，格式为/regex/ 以冒号为分隔符，显示/etc/passwd以r开头的行的第一段\n1 2 [root@wei awk]# awk -F: \u0026#39;/^r/{print $1}\u0026#39; /etc/passwd root 以冒号为分隔符，显示/etc/passwd以nologin结尾的行的第一段\n1 2 3 4 5 6 [root@wei awk]# awk -F: \u0026#39;/nologin$/{print $1}\u0026#39; /etc/passwd bin daemon adm lp mail 以冒号为分隔符，显示/etc/passwd以r或者h开头的行的第一段\n1 2 3 4 [root@wei awk]# awk -F: \u0026#39;/^[rh]/{print $1}\u0026#39; /etc/passwd root halt hei 写出/etc/的软链接的名字\n1 2 3 [root@wei awk]# ls -l /etc/ |awk \u0026#39;/^l/{print $NF}\u0026#39; /usr/share/icons/hicolor/16x16/apps/fedora-logo-icon.png ../boot/grub2/grub.cfg 2，表达式，有下面操作符组成的表达式 awk的操作符\n1 ，算术操作符 -x 负值\n+x 转换为数值，正值\nx^y x**y 次方\nx/y\nx*y\nx-y\nx+y\nx%y\n2 ,字符串操作符 +：实现字符串连接 \u0026ldquo;ab\u0026rdquo;+\u0026ldquo;cd\u0026rdquo; abcd\n3 ,赋值操作符 =\n+=\n-+\n*=\n、=\n%=\n^=\n**=\n4 ,比较操作符 x\u0026lt;y\nx\u0026lt;=y\nx\u0026gt;y\nx\u0026gt;=y\nx==y\nx!=y\nx~y:x为字符串，y为模式，如果x可以被模式匹配则为真，否则为假\nx!~y\n5 ,逻辑关系符 \u0026amp;\u0026amp; 与\n|| 或者\n显示uid大于等于500的用户的及uid\n1 2 3 4 5 [root@wei awk]# awk -F: \u0026#39;$3\u0026gt;=500{print $1,$3}\u0026#39; /etc/passwd polkitd 999 saslauth 998 hei 1200 wei 1001 3 ，指定范围，格式为pattern,pattern2\n以冒号为分隔符，显示uid=0到最后一个字段为nologin结尾中间所有的用户名称，uid及shell\n1 2 3 [root@wei awk]# awk -F: \u0026#39;$3==0,$7~\u0026#34;nologin$\u0026#34;{print $1,$3,$7}\u0026#39; /etc/passwd root 0 /bin/bash bin 1 /sbin/nologin 4 ，BEGIN/END, 特殊模式\nBEGIN表示awk进行处理前执行一次操作\nEND表示awk处理完最后一行结束前执行一次操作\n使用BEGIN打印表头\n1 2 3 4 [root@wei awk]# awk -F: \u0026#39;BEGIN{printf \u0026#34;%-10s%-10s%-20s\\n\u0026#34;,\u0026#34;username\u0026#34;,\u0026#34;uid\u0026#34;,\u0026#34;shell\u0026#34;}$3==0,$7 ~ \u0026#34;nologin$\u0026#34;{printf \u0026#34;%-10s%-10s%-10s\\n\u0026#34;,$1,$3,$7}\u0026#39; /etc/passwd username uid shell root 0 /bin/bash bin 1 /sbin/nologin 使用END打印表尾\n1 2 3 4 5 [root@wei awk]# awk -F: \u0026#39;BEGIN{printf \u0026#34;%-10s%-10s%-20s\\n\u0026#34;,\u0026#34;username\u0026#34;,\u0026#34;uid\u0026#34;,\u0026#34;shell\u0026#34;}$3==0,$7 ~ \u0026#34;nologin$\u0026#34;{printf \u0026#34;%-10s%-10s%-10s\\n\u0026#34;,$1,$3,$7}END{print \u0026#34;END OFFILE...\u0026#34;}\u0026#39; /etc/passwd username uid shell root 0 /bin/bash bin 1 /sbin/nologin END OFFILE... ","date":"2019-04-14T22:59:14Z","image":"https://www.ownit.top/title_pic/49.jpg","permalink":"https://www.ownit.top/p/201904142259/","title":"Linux shell awk模式使用"},{"content":"printf 是 awk 的重要格式化输出命令 printf格式化输出内容 **格式： printf format，item1，item2\u0026hellip;\n要点：**\n1，printf输出时要指定格式format\n2，formay用于指定后面的每个item输出的格式\n3，printf语句不会自动打印换行符\\n\nformat格式： %c:显示单个字符\n%d,%i:十进制整数\n%e,%E:科学计数法显示数值\n%f:显示浮点数\n%g,%G:以科学计数法的格式或浮点数的格式显示数值\n%s:显示字符串\n%u:无符号整数\n%%:显示%自身\n**修饰符: N:显示宽度，N为数字\n-:左对齐，默认为右对齐\n+:显示数值符号**\n1 2 3 weizhang[root@wei awk]# awk \u0026#39;{printf \u0026#34;%s\\n\u0026#34;, $3}\u0026#39; print.txt wei zhang 1 2 3 [root@wei awk]# awk \u0026#39;{printf \u0026#34;%6s\\n\u0026#34;, $3}\u0026#39; print.txt wei zhang 显示前三个用户的用户名，uid，家目录 1 2 3 4 [root@wei awk]# head -n 3 /etc/passwd | awk -F: \u0026#39;{printf \u0026#34;%-8s%-3d%-6s\\n\u0026#34;,$1,$3,$6}\u0026#39; root 0 /root bin 1 /bin daemon 2 /sbin ","date":"2019-04-14T21:29:21Z","image":"https://www.ownit.top/title_pic/03.jpg","permalink":"https://www.ownit.top/p/201904142129/","title":"Linux shell awk中printf使用"},{"content":"**Linux处理文本工具 grep： 过滤文本内容\nsed： 编辑文本内容\nawk: 显示文本\nawk： Aho Peter Weinberger Kernighan\n报告生成器，以特定的条件查找文本内容，在以特定的格式显示**\nawk命令的格式： # awk [option] \u0026lsquo;script\u0026rsquo; file1 file2\u0026hellip;\n# awk [option] \u0026lsquo;PATTERM{action}\u0026rsquo; file1 file2\u0026hellip;\nPATTERN：\n用文本字符与正则表达式元字符描述的条件，可以省略不写\naction:\nprint\nprintf 指定输出项的格式：格式必须写\n**option选项： -F 指定文本分割符\nawk处理文本机制：\nawk将符合PATTERN的文本逐渐取出，并按照指定的分割符（默认为空白，通过—F选项可以指定分割符）进行分割，然后将分割后的每段按照特定的格式输出**\nawk的输出：\n一 print\nprint的使用格式：\nprint item1，item,\u0026hellip;\n注意： 1，各项目间使用逗号分隔开，而输出时以空白字符串为分隔\n2，输出的item可以为字符串，数值，当前的记录的字段（$1）,变量或者awk的表达式，数值会先转换字符串，然后输出\n3，print命令后面的item可以省略，此时其功能相当于print（$0代表未分割的整行文本内容），因此，如果想输出空行，则需要使用print \u0026ldquo;\u0026rdquo;;\n以空白分割，显示文本的第一段及第二段内容\n1 2 3 [root@wei awk]# awk \u0026#39;{print $1,$3}\u0026#39; print.txt i wei i zhang 1 2 3 [root@wei awk]# awk \u0026#39;{print \u0026#34;hello\u0026#34;,$3}\u0026#39; print.txt hello wei hello zhang 显示passwd的用户名称\n1 2 3 4 5 6 [root@wei awk]# awk -F: \u0026#39;{print $1}\u0026#39; /etc/passwd root bin daemon adm lp 显示设备的挂载情况\n1 2 3 4 [root@wei awk]# df -hT | sed \u0026#39;1d\u0026#39; | awk \u0026#39;{print \u0026#34;设备名称：\u0026#34;,$1,\u0026#34;挂载点：\u0026#34;,$7,\u0026#34;总容量：\u0026#34;,$3}\u0026#39; 设备名称： /dev/mapper/centos-root 挂载点： / 总容量： 17G 设备名称： devtmpfs 挂载点： /dev 总容量： 476M 设备名称： tmpfs 挂载点： /dev/shm 总容量： 488M awk变量\n1 awk内置变量之记录变量 FS：指定读取文本时，所使用的行分隔符，默认为空白字符，相当于awk的—F选项\nOFS：指定输出的分隔符，默认为空白字符；\n1 2 [root@wei awk]# head -n 1 /etc/passwd | awk -F: \u0026#39;{print $1,$7}\u0026#39; root /bin/bash FS模式\n1 2 [root@wei awk]# head -n 1 /etc/passwd | awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{print $1,$7}\u0026#39; root /bin/bash OFS模式\n1 2 [root@wei awk]# head -n 1 /etc/passwd | awk -F: \u0026#39;BEGIN{OFS=\u0026#34;---\u0026#34;}{print $1,$7}\u0026#39; root---/bin/bash 模式混合\n1 2 [root@wei awk]# head -n 1 /etc/passwd | awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;;OFS=\u0026#34;---\u0026#34;}{print $1,$7}\u0026#39; root---/bin/bash 2 awk内置变量之数据变量\nNR：记录awk所处理的文本行数，如果有多个文件，所有的文件统一进行计数\n1 2 3 4 5 第 1 行内容： 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 第 2 行内容： ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 第 3 行内容： \\S 第 4 行内容： Kernel \\r on an \\m 第 5 行内容： 注意：\nprint在显示变量值时，不要使用$\nFNR：记录awk所处理的文本行数，如果有多个文件，所有的文件分别进行计数\n1 2 3 4 5 6 [root@wei awk]# awk \u0026#39;{print \u0026#34;第\u0026#34;,FNR,\u0026#34;行内容：\u0026#34;,$0}\u0026#39; /etc/hosts /etc/issue 第 1 行内容： 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 第 2 行内容： ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 第 1 行内容： \\S 第 2 行内容： Kernel \\r on an \\m 第 3 行内容： NF：记录awk正在处理的当前行被分隔成几个字段\n1 2 3 4 5 6 7 8 9 [root@wei awk]# cat print.txt i am wei i am zhang [root@wei awk]# awk \u0026#39;{print NF}\u0026#39; print.txt 3 3 [root@wei awk]# awk \u0026#39;{print $NF}\u0026#39; print.txt wei zhang 3 用户自定义的变量 awk允许用户自定义变量，变量名称不能以数字开头，且区分大小写\n**示例： **\n方法一：使用-v选项\n1 2 3 4 [root@wei awk]# head -n 3 /etc/passwd | awk -v test=\u0026#34;hello\u0026#34; -F: \u0026#39;{print test,$1}\u0026#39; hello root hello bin hello daemon 方法二：在BEGIN{}模式自定义变量\n1 2 3 4 [root@wei awk]# head -n 3 /etc/passwd | awk -F: \u0026#39;BEGIN{test=\u0026#34;hello\u0026#34;}{print test,$1}\u0026#39; hello root hello bin hello daemon ","date":"2019-04-14T20:59:55Z","image":"https://www.ownit.top/title_pic/55.jpg","permalink":"https://www.ownit.top/p/201904142059/","title":"Linux shell awk中print及变量使用"},{"content":"Shell 数组 数组中可以存放多个值。Bash Shell 只支持一维数组（不支持多维数组），初始化时不需要定义数组大小（与 PHP 类似）。\n与大部分编程语言类似，数组元素的下标由0开始。\nShell 数组用括号来表示，元素用\u0026quot;空格\u0026quot;符号分割开，语法格式如下：\n1 array_name=(value1 ... valuen) 我们也可以使用下标来定义数组:\n1 2 3 array_name[0]=value0 array_name[1]=value1 array_name[2]=value2 读取数组\n读取数组元素值的一般格式是：\n1 ${array_name[index]} 数组 Array\n一段连续的内存空间\n** (1)定义数组**\n1 2 3 4 5 6 7 8 [root@wei ~]# aa[0]=hei [root@wei ~]# aa[1]=wei [root@wei ~]# aa[2]=zhang [root@wei ~]# hei=(192.168.196.1 192.168.196.2 192.168.196.3) [root@wei ~]# echo ${hei[1]} 192.168.196.2 [root@wei ~]# echo ${hei[*]} 192.168.196.1 192.168.196.2 192.168.196.3 (2)删除数组 1 2 [root@wei ~]# unset hei [root@wei ~]# echo ${hei[*]} (3)获取数组的长度\n1 2 3 4 5 6 7 [root@wei ~]# aa=(tom hei zhang wei) [root@wei ~]# echo ${aa[*]} tom hei zhang wei [root@wei ~]# echo ${#aa[*]} 4 [root@wei ~]# echo ${#aa[@]} 4 编写脚本，找出数组中最大数 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # number=(53 54 654 45 677 46 999) max=${number[0]} for i in `seq 7`;do if [ ${number[$i]} -gt $max ];then max=${number[$i]} fi done echo $max 随机数 1 2 3 4 [root@wei ~]# echo $RANDOM 10803 [root@wei ~]# echo $RANDOM 5555 ","date":"2019-04-11T23:04:58Z","image":"https://www.ownit.top/title_pic/39.jpg","permalink":"https://www.ownit.top/p/201904112304/","title":"Linux shell 数组"},{"content":"shell脚本中对字符串的处理 1 ${#变量名} 作用：返回字符串的长度\n1 2 3 # foo=\u0026#34;this is a test\u0026#34; # echo ${#foo} //返回字符串的长度 14 2 ${变量名：offset：length} 作用：截取字符串，length指定截取的长度，也可以不写，字符串的第一个字符的索引值为0\n1 2 3 4 5 # foo=“abcdefg” # echo ${foo:3:2} //从下标为3的字符开始截取，共截取2个 de # echo ${foo:3} //从下标为3的字符开始截取到最后的字符 defg 3 ${变量名#pattern} ${变量名##pattern}\npattern：模式，通配符表达式\n作用：清除字符串中符合pattern的字符，从字符串最前匹配\n1 2 3 4 5 # foo=“file.txt.zip” # echo ${foo#*.} //一个#号代表按照最短匹配清除 txt.zip # echo ${foo##*.} //2个#号代表按照最长匹配清除 zip 4 ${变量名%pattern} ${变量名%%pattern} pattern：模式，通配符表达式\n作用：清除字符串中符合pattern的字符，从字符串最后匹配\n1 2 3 4 5 # foo=“file.txt.zip” # echo ${foo%.*} //一个%号代表按照最短匹配清除 file.txt # echo ${foo%%.*} //2个%号代表按照最长匹配清除 file 5 字符串替换操作\n${变量名/old/new}\n1 2 3 4 5 [root@wei ~]# foo=\u0026#34;mp3.txt.txt.mp3.avi\u0026#34; [root@wei ~]# echo ${foo/txt/TXT} mp3.TXT.txt.mp3.avi [root@wei ~]# echo ${foo//txt/TXT} mp3.TXT.TXT.mp3.avi 1 2 3 4 5 [root@wei ~]# foo=\u0026#34;txt.txt.txt\u0026#34; [root@wei ~]# echo ${foo/#txt/TXT} TXT.txt.txt [root@wei ~]# echo ${foo/%txt/TXT} txt.txt.TXT 6 实现大小写字母的转换\n大写转换小写\n1 2 3 4 5 [root@wei ~]# foo=\u0026#34;ABcd\u0026#34; [root@wei ~]# echo ${foo,} aBcd [root@wei ~]# echo ${foo,,} abcd 小写转换大写\n1 2 3 4 [root@wei ~]# echo ${foo^} AbCD [root@wei ~]# echo ${foo^^} ABCD ","date":"2019-04-10T21:47:37Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201904102147/","title":"Linux shell 字符串操作"},{"content":"Linux处理文本文件的工具:\n** grep 过滤文件内容\nsed 编辑文件内容\nawk\n正则表达式Regex\n正则表达式Regex**\nsed在处理文本时是逐行读取文件内容，读到匹配的行就根据指令做操作，不匹配就跳过。\nsed是Linux下一款功能强大的非交互流式文本编辑器，可以对文本文件进行增、删、改、查等操作，支持按行、按字段、按正则匹配文本内容，灵活方便，特别适合于大文件的编辑。本文主要介绍sed的一些基本用法，并通过shell脚本演示sed的使用实例。\n(1)匹配单个字符的元字符 ** .\n[abc] [a-z] [A-Z] [0-9] [a-zA-Z0-9] [^a-z]\n[[:alpha:]] [[:upper:]] [[:lower:]] [[:digit:]]**\n** (2)匹配字符出现位置**\n** ^str 以\u0026hellip;开头\nstr$ 以\u0026hellip;结尾\n^$ 空行**\n(3)匹配字符出现的次数 ** * \\?\n\\+\n\\{3\\}\n\\{2,5\\}\n\\{2,\\}**\n** sed: Stream Editor 流编辑器**\n** 行编辑器 逐行编辑\n将每行内容读入到内存中，在内存中进行处理，将结果返回给屏幕，此段内存空间称为摸索空间**\n默认不编写原文件，仅对模式空间的数据进行处理，处理结束后，将模式空间的内容显示到屏幕\nsed命令的使用格式 # sed [option] scripts file1 file2\u0026hellip;\n# sed [option] \u0026lsquo;AddressCommand\u0026rsquo; file1 file \u0026hellip;\n** Address:表示对那些进行处理\nCommand：操作命令\noption选项：\n-n:不在显示模式空间的内容（默认显示）\n-i：直接修改原文件\n-e：‘AddressCommand’ -e ‘AddressCommand’：同时执行多个匹配操作\n[root0shell ~]+ sed -e ‘/^#/d’ -e \u0026lsquo;/^5/d\u0026rsquo; /etc/fstab\n-f：FILE将多个AddressCommand保存至文件中，每行一个AddressCommand;读取该文件中的操作同时执行多个操作**\n** -r:表示使用扩展正则表达式**\nAddress表示方法:\n(1)StartLine, EndLine\n1, 100\n1,$\n(2)lineNumber\n3\n(3)Startline, tn\n** 5,+2 /root/,+2**\n(4)/正则表达式/\n** /root/\n/bash$/**\n(5)/正则表达式1/, /正则表达式2/\n第1次被Regex1匹配的行升始，到第1次被Regex2匹配的行中同的所有行\nd删除 删除前4行\n1 [root@wei init.d]# sed \u0026#39;1,4d\u0026#39; nginx.sh 删除最后一行\n1 [root@wei init.d]# sed \u0026#39;$d\u0026#39; nginx.sh 删除#开头的行\n1 [root@wei ~]# sed \u0026#39;/^#/d\u0026#39; /etc/fstab 删除/开头的行\n1 [root@wei ~]# sed \u0026#39;/^\\//d\u0026#39; /etc/fstab 删除带数字的行\n1 [root@wei ~]# sed \u0026#39;/[0-9]/d\u0026#39; /etc/fstab p 显示符合条件的行 ** 显示以/开头的行**\n1 [root@wei ~]# sed -n \u0026#39;/^\\//p\u0026#39; /etc/fstab a \\string 在符合条件的行后追加新行，string为追加的内容 ** 在以/开头的行后面追加# hello word\n**\n1 [root@wei ~]# sed \u0026#39;/^\\//a\\# hello word\u0026#39; /etc/fstab ** 在以/开头的行后面分别追加# hello word # hello linux**\n1 [root@wei ~]# sed \u0026#39;/^\\//a\\# hello word\u0026#39; /etc/fstab i \\string 在符合条件的行前追加新行，string为追加的内容 ** 在文件的第一行追加 # hello linux\n**\n1 [root@wei ~]# sed \u0026#39;1i \\# hello linux\u0026#39; /etc/fstab c \\string 替换指定的内容 ** 将文件中最后一行的内容替换为 end of file\n**\n1 [root@wei ~]# sed \u0026#39;$c\\end of file\u0026#39; /etc/fstab ** = 用于显示每一行的行号**\n** 显示/etc/passwd文件最后一行的行号**\n1 [root@wei ~]# sed -n \u0026#39;$=\u0026#39; /etc/passwd r file_name 将指定的文件的内容添加到符合条件的后面\n** 将文件的第二行后面追加/etc/fstab\n**\n1 [root@wei ~]# sed \u0026#39;2r /etc/issue\u0026#39; /etc/fstab w file_name 将符合条件的内容另存到指定的文件中\n** 将以#开头的行另存到/1.txt中\n**\n1 [root@wei ~]# sed \u0026#39;/^#/w /root/1.txt\u0026#39; /etc/fstab 查找并替换\n默认情况下，只替换每一行第一次出现的字符\ns /old/new/[修饰符]\nold:正则表达式/\nnew：替换的内容\n修饰符\ng：替换第一行所有的字符\ni：忽略大小写\n查找文件的UUID，并替换成uuid 1 [root@wei ~]# sed \u0026#39;s/UUID/uuid/\u0026#39; /etc/fstab 将行首的/替换成# 1 [root@wei ~]# sed \u0026#39;s/^\\//#/\u0026#39; /etc/fstab 1 [root@wei ~]# sed \u0026#39;s|/|#|g\u0026#39; /etc/fstab 将每一行出现的所有/替换成@ 1 [root@wei ~]# sed \u0026#39;s/\\//@/g\u0026#39; /etc/fstab ","date":"2019-04-09T22:21:30Z","image":"https://www.ownit.top/title_pic/57.jpg","permalink":"https://www.ownit.top/p/201904092221/","title":"Linux shell sed命令使用"},{"content":"nginx服务控制脚本： 安装ngix 1 2 3 4 5 6 [root@wei function]# yum install gcc pcre-devel openssl-devel [root@wei function]# tar xf nginx-1.14.2.tar.gz [root@wei function]# cd nginx-1.14.2 [root@wei nginx-1.14.2]# ./configure --prefix=/usr/local/nginx [root@wei nginx-1.14.2]# make \u0026amp;\u0026amp; make instal 编写控制nginx的脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #!/bin/bash # nginx_cmd=/usr/local/nginx/sbin/nginx nginx_conf=/usr/local/nginx/conf/nginx.conf nginx_pid_file=/usr/local/nginx/logs/nginx.pid start(){ $nginx_cmd if [ $? -eq 0 ];then echo \u0026#34;服务nginx启动.....[ok]\u0026#34; fi } stop(){ $nginx_cmd -s stop } reload(){ if $nginx_cmd -t \u0026amp;\u0026gt; /dev/null;then $nginx_cmd -s reload else $nginx_cmd -t fi } status(){ if [ -e $nginx_pid_file ];then echo \u0026#34;服务nginx(`cat $nginx_pid_file`) is running\u0026#34; else echo \u0026#34;服务nginx is stopped\u0026#34; fi } if [ -z $1 ];then echo \u0026#34;使用：$0{start|stop|restart|reload|status}\u0026#34; exit 9 fi case $1 in start) start ;; stop) stop ;; restart) stop sleep 2 start ;; reload) reload ;; *) echo \u0026#34;使用：$0{start|stop|restart|reload|status}\u0026#34; exit 9 ;; esac 演示： 1 2 3 4 5 6 7 8 9 [root@wei init.d]# /etc/init.d/nginx status 服务nginx(4974) is running [root@wei init.d]# /etc/init.d/nginx statscdsdc 使用：/etc/init.d/nginx{start|stop|restart|reload|status} [root@wei init.d]# /etc/init.d/nginx stop [root@wei init.d]# /etc/init.d/nginx status 服务nginx is stopped [root@wei init.d]# /etc/init.d/nginx start 服务nginx启动.....[ok] ","date":"2019-04-02T19:39:29Z","image":"https://www.ownit.top/title_pic/38.jpg","permalink":"https://www.ownit.top/p/201904021939/","title":"Linux shell 函数应用示例02"},{"content":"函数Function的使用\n定义函数\n（1） 函数名称() {\n\u0026hellip;\n\u0026hellip;\n}\n(2) function 函数名称{\n\u0026hellip;\n\u0026hellip;\n}\n**调用函数 函数名称**\n也可以通过位置变量的方式给函数传递参数\n1、可以带function fun() 定义，也可以直接fun() 定义,不带任何参数。 2、参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。 return后跟数值n(0-255） 编写脚本，编写函数，并调用 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # sayhei() { echo \u0026#34;$1\u0026#34; } sayhei wei sayhei linux sayhei windows 执行效果 编写脚本，实现下面的功能 ==============\n目录管理\n1：创建目录\n2：删除目录\n3：退出脚本\n==============\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 #!/bin/bash # create_dir(){ read -p \u0026#34;输入目录名称：\u0026#34; dir if [ ! -e $dir ];then mkdir -p $dir echo “目录$dir创建完成” else echo \u0026#34;目录$dir存在\u0026#34; fi } remove_dir(){ read -p \u0026#34;输入目录名称\u0026#34; dir if [ -e $dir ];then rm -r $dir echo \u0026#34;目录$dir删除成功\u0026#34; fi } showmenu(){ cat \u0026lt;\u0026lt; eof ============== 目录管理 1.创建目录 2.删除目录 3.退出脚本 ============== eof } while true;do read -p \u0026#34;请输入你的选择:显示菜单[m] \u0026#34; choice case $choice in 1) create_dir ;; 2) remove_dir ;; 3) exit 0 ;; m) showmenu ;; *) echo \u0026#34;输入错误,请重新选择\u0026#34; ;; esac done 执行效果 ","date":"2019-04-01T23:03:40Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/201904012303/","title":"Linux shell 函数应用示例01"},{"content":"**for ：明确循环次数 while ：不确定循环换次数**\nwhile循环 （1） while CONDITION；do\nstatement\nstatement\n\u0026lt;改变循环条件真假的语句\u0026gt;\ndone\n编写脚本，计算1\u0026ndash;100的和 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # sum=0 i=1 while [ $i -le 100 ];do let sum=$sum+$i let i=$i+1 done echo $sum 编写while循环，输入q退出（不输入q，不退出）\n1 2 3 4 5 6 7 8 #!/bin/bash # read -p \u0026#34;请输入你的选择：\u0026#34; choice while [ $choice != q ];do echo -e \u0026#34;\\033[31m输入错误\\033[0m\u0026#34; #加的颜色代码 read -p \u0026#34;请输入你的选择：\u0026#34; choice done （2）\nwhile true；do\nstatement\nstatement\n\u0026lt;break退出\u0026gt;\ndone\n编写while循环，输入q退出（不输入q，不退出） 1 2 3 4 5 6 7 8 9 #/bin/bash # while true;do read -p \u0026#34;请输入你的选择\u0026#34; str echo \u0026#34;输入错误\u0026#34; if [ $str == q ];then break fi done 编写脚本，每4秒查看系统的内存 1 2 3 4 5 6 #!/bin/bash # while true;do uptime sleep 3 done （3）\nwhile read line;do\nstatement\nstatement\ndone \u0026lt; file\n编写脚本，向系统每个用户打招呼\n1 2 3 4 5 6 7 v#!/bin/bash # while read line;do sh_name=$(echo $line | awk -F: \u0026#39;{print $1}\u0026#39;) echo \u0026#34;Hello $sh_name\u0026#34; done \u0026lt; /etc/passwd 编写脚本，统计/bin/bash /sbin/nologin的个数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@wei while]# cat 6.sh #!/bin/bash # bash_number=0 nologin_number=0 while read line;do sh_name=$(echo $line | awk -F: \u0026#39;{print $7}\u0026#39;) case $sh_name in /bin/bash) let bash_number=$bash_number+1 ;; /sbin/nologin) let nologin_number=$nologin_number+1 ;; esac done \u0026lt; /etc/passwd echo \u0026#34;bash用户数量：$bash_number\u0026#34; echo \u0026#34;nologin_number用户数量：$nologin_number\u0026#34; 执行效果 1 2 3 [root@wei while]# ./6.sh bash用户数量：17 nologin_number用户数量：17 util循环：\nutil CONDITION；do statement\nstatement\ndone\n条件为假时，执行循环，条件为真时，结束循环 重点掌握\nif，case for，while","date":"2019-04-01T19:32:44Z","image":"https://www.ownit.top/title_pic/69.jpg","permalink":"https://www.ownit.top/p/201904011932/","title":"Linux shell while循环语句"},{"content":"无限循环： 循环有限的生命，他们跳出来，一旦条件是 false 还是 false 取决于循环。\n由于所需的条件是不符合一个循环可能永远持续下去。永远不会终止执行一个循环执行无限次数。出于这个原因，这样的循环被称为无限循环。\nbreak语句： break语句用于终止整个循环的执行，完成后所有行代码break语句的执行。然后，它逐级的代码跟在循环结束。\ncontinue 语句: continue语句break命令类似，但它会导致当前迭代的循环退出，而不是整个循环。\n这种参数是有用的，当一个错误已经发生，但你想尝试执行下一个循环迭代。\n中断循环的语句 break 中断整体循环\ncontiune 中断本次循环\nbreak用法：\n编写脚本，判断大于3000的累加和的数 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # sum=0 for i in `seq 100`;do let sum=$sum+$i if [ $sum -ge 3000 ];then echo $i break fi done contiune用法： 编写脚本，求100的奇数的累加和\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # sum=0 for i in `seq 100`;do let ys=$i%2 if [ $ys -eq 0 ];then continue fi let sum=$sum+$i done echo $sum 编写脚本，输出在/bin/bash的前5个用户\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash # number=0 line=$(wc -l /etc/passwd |awk \u0026#39;{print $1}\u0026#39;) for i in `seq $line`;do sh_name=$(head -n $i /etc/passwd | tail -n 1 | awk -F: \u0026#39;{print $7}\u0026#39;) if [ $sh_name = \u0026#34;/bin/bash\u0026#34; ]; then user_name=$(head -n $i /etc/passwd | tail -n 1 | awk -F: \u0026#39;{print $1}\u0026#39; ) echo $user_name let number=$number+1 fi if [ $number -ge 5 ];then break fi done 执行效果 1 2 3 4 5 6 7 8 [root@wei break]# bash 2.sh root mysql hei wei a [root@wei break]# ls /home/ a c d hei user1 user10 user2 user3 user4 user5 user6 user7 user8 user9 wei ","date":"2019-03-23T22:26:25Z","image":"https://www.ownit.top/title_pic/62.jpg","permalink":"https://www.ownit.top/p/201903232226/","title":"Linux shell 中断循环语句"},{"content":"Linux Shell for循环结构 **循环结构 1：循环开始条件\n2：循环操作\n3：循环终止的条件**\n**shell语言 for，while，util**\n** for循环**\n语法： （1）\nfor 变量 in 取值列表；do\nstatement\nstatement\ndone\n（2）\nfor 变量 in 取值列表\ndo\nstatement\nstatement\ndone\n上面两个用法的效果是一样的。\n**取值列表：\n数字\n10 20 30\n使用seq命令生成数字的序列\nseq 10\nseq 3 10\nseq 1 2 10\n字符\naa bb cc\n文件\n**\n示例\nseq可以快速去值，奇数等\n1 2 3 4 5 6 7 8 9 10 11 12 [root@wei for]# seq 5 1 2 3 4 5 [root@wei for]# seq 2 6 2 3 4 5 6 示例：1\u0026ndash;100的累加和\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # sum=0 for i in `seq 1 100` do let sum=$sum+$i done echo $sum [root@wei for]# bash 1.sh 5050 示例：1\u0026ndash;100的奇数累加和\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash sum=0 for i in `seq 100` do let ys=$i%2 if [ $ys -ne 0 ];then let sum=$sum+$i fi done echo $sum [root@wei for]# bash 2.sh 2500 创建10个用户，初始密码为：root，登陆重新修改密码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash # for i in `seq 10` do if ! id user$i \u0026amp;\u0026gt; /dev/null ; then useradd user$i echo \u0026#34;root\u0026#34; | passwd --stdin user$i \u0026amp;\u0026gt; /dev/null passwd -e user$i \u0026amp;\u0026gt; /dev/null echo \u0026#34;用户user$i创建完成，初始密码为：root\u0026#34; else echo \u0026#34;用户user$i已经存在\u0026#34; fi done 以字符作为取值类表 1 2 3 4 5 6 7 #!/bin/bash # for name in a d c d ;do useradd $name echo \u0026#34;$name create finishe\u0026#34; done 以文件作为取值列表\n** `cat file` 编写脚本，读取文本**\n1 2 3 4 5 6 7 8 #!/bin/bash # for i in `cat /shell/for/1.txt`;do echo \u0026#34;line:$i\u0026#34; done [root@wei for]# ./wen.sh line:nangong line:chengfneg ","date":"2019-03-21T22:33:39Z","image":"https://www.ownit.top/title_pic/50.jpg","permalink":"https://www.ownit.top/p/201903212233/","title":"Linux shell for循环结构"},{"content":"case语句使用于需要进行多重分支的应用情况 case分支判断结构\n语法：\ncase 变量名称 in\nvalue1)\nstatement\nstatement\n;;\nvalue2)\nstatement\nstatement\n;;\n*)\nstatement\nstatement\n;; esac\ncase语句结构特点如下：\ncase行尾必须为单词 in 每个模式必须以右括号 ） 结束\n双分号 ;; 表示命令序列结束\ncase语句结构特点如下：\n匹配模式中可是使用方括号表示一个连续的范围，如[0-9]；使用竖杠符号“|”表示或。\n最后的“*）”表示默认模式，当使用前面的各种模式均无法匹配该变量时，将执行“*）”后的命令序列。\n编写脚本，判断用户输入的字符串 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # read -p \u0026#34;输入字符串：\u0026#34; str case $str in linux|Linux) echo \u0026#34;windows\u0026#34; ;; windows|Windows) echo \u0026#34;linux\u0026#34; ;; *) echo \u0026#34;other\u0026#34; ;; esac 运行效果： 1 2 3 [root@wei case]# bash 1.sh 输入字符串：linux windows 特殊变量：\n位置变量\n$1,$2,$3\u0026hellip;\u0026hellip;\u0026hellip;..$9,$1{10}\n$1:命令的第1个参数\n$0 命令本身\n$# 命令参数的个数\n使用位置变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash # case $1 in linux|Linux) echo \u0026#34;windows\u0026#34; ;; windows|Windows) echo \u0026#34;linux\u0026#34; ;; *) echo \u0026#34;other\u0026#34; esac 执行效果\n1 2 [root@wei case]# ./2.sh linux windows 判断字符是为空 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash # if [ -z $1 ];then #判断字符串是否为空 echo \u0026#34;使用：./2.sh{linux/windows}\u0026#34; exit 9 fi case $1 in linux|Linux) echo \u0026#34;windows\u0026#34; ;; windows|Windows) echo \u0026#34;linux\u0026#34; ;; *) echo \u0026#34;other\u0026#34; esac 执行效果 1 2 [root@wei case]# ./2.sh 使用：./2.sh{linux/windows} **$0 命令本身 $# 命令参数的个数**\n示例： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash # if [ $# -ne 1 ];then echo \u0026#34;使用：$0{linux/windows}\u0026#34; exit 9 fi case $1 in linux|Linux) echo \u0026#34;windows\u0026#34; ;; windows|Windows) echo \u0026#34;linux\u0026#34; ;; *) echo \u0026#34;other\u0026#34; esac 执行效果：\n1 2 3 4 [root@wei case]# /shell/case/2.sh 使用：/shell/case/2.sh{linux/windows} [root@wei case]# ./2.sh 使用：./2.sh{linux/windows} 去除文件所在的路径名：\nbasename [路径文件]\n1 2 [root@wei case]# basename /etc/fstab fstab 获取文件所在的路径名：\ndirname [路径文件]\n1 2 [root@wei case]# dirname /etc/fstab /etc 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash # if [ $# -ne 1 ];then echo \u0026#34;使用：$(basename $0){linux/windows}\u0026#34; exit 9 fi case $1 in linux|Linux) echo \u0026#34;windows\u0026#34; ;; windows|Windows) echo \u0026#34;linux\u0026#34; ;; *) echo \u0026#34;other\u0026#34; esac 执行效果 1 2 3 [root@wei case]# /shell/case/2.sh 使用：2.sh{linux/windows} ","date":"2019-03-20T21:03:56Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201903202103/","title":"Linux shell case条件判断及位置变量"},{"content":"前面介绍linux shell的if判断的语法，现在再补充一点。\nLinux shell if条件判断1\n**分支判断结构\nif , case\n**\n下面两个结构语法，已经在前面有过示例。\n结构1：\nif CONDITON； then\nstatement\nstatement\nfi\n结构2：\nif CONDITON； then\nstatement\nstatement\nelse\nstatement\nstatement\nfi\n下面会分享几个我编写的示例，希望对大家有所帮助。\n编写脚本，有用户输入用户名，判断用户是否存在，如果不存在，就显示用户不存在，如果存在，以下面格式输出用户相关信息：\n用户名：\n宿主目录：\nshell程序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@wei shell]# cat if2.sh #!/bin/bash # read -p \u0026#34;请输入用户名：\u0026#34; name if id $name \u0026amp;\u0026gt; /dev/null; then echo \u0026#34;用户名：\u0026#34; $name homedir=`grep \u0026#34;^$name:\u0026#34; /etc/passwd | awk -F: \u0026#39;{print $6}\u0026#39; ` shname=`grep \u0026#34;^$name:\u0026#34; /etc/passwd | awk -F: \u0026#39;{print $7}\u0026#39; ` echo \u0026#34;宿主目录：$homedir\u0026#34; echo \u0026#34;SHELL名称：$shname \u0026#34; else echo \u0026#34;用户$name不存在\u0026#34; fi 编写脚本，判断文件是否存在空行，有则显示空行个数，没有则显示文件类容，并在每一行显示行号\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # read -p \u0026#34;请输入文件的名称：\u0026#34; file if grep \u0026#34;^$\u0026#34; $file \u0026amp;\u0026gt; /dev/null; then number=`grep \u0026#34;^$\u0026#34; $file | wc -l` echo \u0026#34;文件$file中的空行的数量：$number\u0026#34; else echo \u0026#34;文件$file内容如下：\u0026#34; cat -n $file fi 用法3 ：多分支if结构\nif CONDITON； then\nstatement\nstatement\nelif CONDITON； then\nstatement\nstatement\nelif CONDITON； then\nstatement\nstatement\nelse\nstatement\nstatement\nfi\n多个条件的写法：\nAND [conditionl -a condition2] [conditionl ] \u0026amp;\u0026amp; [ condition2]\nOR [conditionl -o condition2] [conditionl ] || [ condition2]\n编写脚本，判断当前系统时间的小时数字 ** 9\u0026ndash;11 morning\n12\u0026ndash;14 noon\n15\u0026ndash;18 afternoon\nnight\n**\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # hour=`date +%H` if [ $hour -ge 9 -a $hour -le 11 ]; then echo \u0026#34;Morning\u0026#34; elif [ $hour -ge 12 -a $hour -le 14 ]; then echo \u0026#34; Noon\u0026#34; elif [ $hour -ge 15 -a $hour -le 18 ]; then echo \u0026#34; Afternoon\u0026#34; else echo \u0026#34;Night\u0026#34; fi 执行效果：\n1 2 3 4 [root@wei shell]# date 2019年 03月 19日 星期二 18:44:50 CST [root@wei shell]# bash shi.sh Afternoon 数学表达式\n字符表达式\n[ str1 == str2 ]\n[ str1 != str2 ]\n[ -z str1 ] 判断字符串是否为空\n**判断两次密码是否相同 ** 1 2 3 4 5 6 7 8 9 #!/bin/bash # read -p \u0026#34;请输入密码：\u0026#34; pwd1 read -p \u0026#34;请在一次输入密码：\u0026#34; pwd2 if [ \u0026#34;$pwd1\u0026#34; == \u0026#34;$pwd2\u0026#34; ];then echo \u0026#34;密码输入正确\u0026#34; else echo \u0026#34;密码两次输入不一致\u0026#34; fi **文件目录表达式：\n[ -e file ] 判断文件目录是否存在\n[ -f file ] 判断是否为文件\n[ -e file ] 判断是否为目录\n[ -r file ] 判断文件是否有r权限\n[ -w file ] 判断文件是否有w权限\n[ -x file ] 判断文件是否有x权限\n**\n双目表达式\n单目表达式 [ -e file ] [ ! -e file ]\n用法4： if的内嵌语法 if CONDITON； then\nif CONDITON； then\nstatement\nstatement\nfi\nelse\nstatement\nstatement\nfi\n判断用户是否存在，如果用户存在，判断他的root的id和group的id是否相同 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bsah # read -p \u0026#34;输入用户名： \u0026#34; name if id $name \u0026amp;\u0026gt; /dev/null; then #获取uid，gid进行判断 user_id=$(id -u $name) group_id=$(id -g $name) if [ $user_id -eq $group_id ];then echo \u0026#34; Good user\u0026#34; else echo \u0026#34;Bad user\u0026#34; fi else echo \u0026#34;用户不存在\u0026#34; fi 判断文件是否存在，如果存在输入到备份的文件去 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # read -p \u0026#34;输入文件的路径：\u0026#34; file if [ -e $file ]; then read -p \u0026#34;输入备份的路径：\u0026#34; dir if [ -e $dir ]; then cp $file $dir echo \u0026#34;文件$file备份到$dir目录\u0026#34; else mkdir -p $dir cp $file $dir echo \u0026#34;文件$file备份到$dir目录\u0026#34; fi else echo “文件$file不存在” fi ","date":"2019-03-19T22:12:21Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/201903192212/","title":"Linux shell if条件判断2"},{"content":"**shell 逻辑控制语句： 分支判断结构\nif\ncase\n循环结构\nfor\nwhile\nuntil\n**\nif语句结构\n用法1 if CONDITON； then\nstatement\nstatement\nfi\nCONDITION条件的写法：\n** COMMAND\n[ expression ]\nexpression表达式：\n数学表达式\n字符表达式\n文件目录表达式\n数学表达式：\n[ number1 -eq number2 ] 等于\n[ number1 -ne number2 ] 不等于\n[ number1 -gt number2 ] 大于\n[ number1 -ge number2 ] 大于等于\n[ number1 -lt number2 ] 小于\n[ number1 -le number2 ] 小于等于\n**\n编写脚本，有用户输入用户名，判断用户不存在则创建 1 [root@wei shell]# vim if.sh 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # read -p \u0026#34;请输入用户名： \u0026#34; name id $name \u0026amp;\u0026gt; /dev/null if [ $? -ne 0 ];then read -p \u0026#34;输入密码：\u0026#34; passwd useradd $name echo \u0026#34;$passwd\u0026#34; | passwd --stdin $name \u0026amp;\u0026gt; /dev/null echo \u0026#34;用户$name创建完成，初始密码为：$passwd\u0026#34; fi 检测语法执行状况\n1 2 3 4 5 6 7 8 9 10 11 12 [root@wei shell]# bash -x if.sh + read -p \u0026#39;请输入用户名： \u0026#39; name 请输入用户名： wei + id wei + \u0026#39;[\u0026#39; 1 -ne 0 \u0026#39;]\u0026#39; + read -p 输入密码： passwd 输入密码：123456 + useradd wei + echo 123456 + passwd --stdin wei + echo 用户wei创建完成，初始密码为：123456 用户wei创建完成，初始密码为：123456 检测语法错误 1 [root@wei shell]# bash -n if.sh 条件语言脚本 用法2： 单分支if if CONDITON； then\nstatement\nstatement\nelse\nstatement\nstatement\nfi\n编写脚本，由用户输入用户名，判断用户不存在则创建，并设置用户第一次登陆系统时需要修改密码。否则提示用户已存在\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # read -p \u0026#34;请输入用户名： \u0026#34; name if id $name \u0026amp;\u0026gt; /dev/null ;then echo \u0026#34; 用户$name已经存在\u0026#34; else useradd $name read -p \u0026#34;输入密码：\u0026#34; passwd echo \u0026#34;$passwd\u0026#34; | passwd --stdin $name \u0026amp;\u0026gt; /dev/null passwd -e $name \u0026amp;\u0026gt; /dev/null echo \u0026#34;用户$name创建完成，初始密码为：$passwd\u0026#34; fi 由用户输入一个用户名，判断用户的UID和GID 判断的方式 [root@wei shell]# grep \u0026ldquo;hei\u0026rdquo; /etc/passwd\nhei:x:1000:1000::/home/hei:/bin/bash\n[root@wei shell]# grep \u0026ldquo;hei\u0026rdquo; /etc/passwd | awk -F: \u0026lsquo;{print $3,$4}\u0026rsquo;\n1000 1000\n[root@wei shell]# id -u hei\n1000\n[root@wei shell]# id -g hei\n1000\n脚本语法 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # read -p \u0026#34;输入用户名：\u0026#34; name user_id=$(id -u $name) group_id=$(id -g $name) if [ $user_id -eq $group_id ];then echo \u0026#34;Good\u0026#34; else echo \u0026#34;Bad\u0026#34; fi 执行结果\n1 2 3 4 5 6 7 8 9 10 11 [root@wei shell]# bash id.sh 输入用户名：hei Good [root@wei shell]# id hei uid=1000(hei) gid=1000(hei) 组=1000(hei) [root@wei shell]# usermod -u 1200 hei [root@wei shell]# id hei uid=1200(hei) gid=1000(hei) 组=1000(hei) [root@wei shell]# bash id.sh 输入用户名：hei Bad ","date":"2019-03-17T20:00:00Z","image":"https://www.ownit.top/title_pic/52.jpg","permalink":"https://www.ownit.top/p/201903172000/","title":"Linux shell if条件判断1"},{"content":"前面介绍简单的shell编写规则。 现在开始编写一个简单的shell脚本。 Linux shell介绍 编写shell脚本 1.创建脚本文件\n2.根据需求，编写脚本\n3.测试执行脚本\n编写脚本，实现创建用户hei，病设置用户密码为root\n1 [root@wei shell]# vim user.sh 1 2 3 4 5 #!/bin/bash # 注释 useradd hei echo \u0026#34;root\u0026#34; | passwd --stdin hei \u0026amp;\u0026gt; /dev/null echo \u0026#34;hei用户创建完成，默认密码是：root\u0026#34; 执行方法：\n（1）利用bash执行\n1 [root@wei shell]# bash user.sh （2）加权限，在执行\n1 2 [root@wei shell]# chmod a+x user.sh [root@wei shell]# ./user.sh 变量名编写 1 [root@wei shell]# vim users.sh 1 2 3 4 5 6 7 8 #!/bin/bash name=wei passwd=root useradd $name echo \u0026#34;$passwd\u0026#34; | passwd --stdin $name \u0026amp;\u0026gt; /dev/null echo \u0026#34;用户$name创建完成，默认密码是：$passwd\u0026#34; 输入字符 1 2 [root@wei shell]# read -p \u0026#34;输入数字：\u0026#34; number 输入数字：99 1 2 3 4 5 6 7 8 #!/bin/bash # read -p \u0026#34;输入用户名：\u0026#34; name read -p \u0026#34;输入密码：\u0026#34; passwd useradd $name echo \u0026#34;$name\u0026#34; | passwd --stdin $name \u0026amp;\u0026gt; /dev/null echo \u0026#34;用户$name创建完成，密码是：$passwd\u0026#34; 以上三种方法都可以。","date":"2019-03-14T22:06:35Z","image":"https://www.ownit.top/title_pic/27.jpg","permalink":"https://www.ownit.top/p/201903142206/","title":"Linux shell简单创建用户脚本"},{"content":"Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。\nShell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。\nKen Thompson 的 sh 是第一种 Unix Shell，Windows Explorer 是一个典型的图形界面 Shell。\nShell 脚本 Shell 脚本（shell script），是一种为 shell 编写的脚本程序。\n业界所说的 shell 通常都是指 shell 脚本，但读者朋友要知道，shell 和 shell script 是两个不同的概念。\nSHELL 变量\n变量（内存空间）\n增加脚本的灵活性，适用性\n类型：\n自定义变量\n环境变量（Path）\n特殊变量\n自定义变量\n变量名称规范：\n命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 1 声明变量 # 变量名称=变量值\n1 [root@wei csdn]# name=wei 2 调用变量的值\n$变量名称\n${变量名称} 变量名称或紧跟数字，字符的时候\n输出时，由变量名，必须用双引号\n1 2 3 4 5 6 7 8 9 [root@wei csdn]# name=cat [root@wei csdn]# echo \u0026#34;this is $name\u0026#34; this is cat [root@wei csdn]# echo \u0026#34;this is ${name}\u0026#34; this is cat [root@wei csdn]# echo \u0026#34;this is ${name}s\u0026#34; this is cats 3 SHELL变量的值默认全做为字符处理 1 2 3 4 5 6 7 8 9 10 11 [root@wei csdn]# a=10 [root@wei csdn]# b=20 [root@wei csdn]# c=a+b [root@wei csdn]# echo $c a+b [root@wei csdn]# a=10 [root@wei csdn]# b=20 [root@wei csdn]# c=$a+$b [root@wei csdn]# echo $c 10+20 数学运算： 方法1：$(())\n1 2 3 4 5 [root@wei csdn]# a=10 [root@wei csdn]# b=20 [root@wei csdn]# c=$((a+b)) [root@wei csdn]# echo $c 30 方法2：关键字：let\n1 2 3 4 5 [root@wei csdn]# a=10 [root@wei csdn]# b=20 [root@wei csdn]# let c=a+b [root@wei csdn]# echo $c 30 方法3：关键字：declare\n1 2 3 4 5 [root@wei csdn]# a=10 [root@wei csdn]# b=20 [root@wei csdn]# declare -i c=a+b [root@wei csdn]# echo $c 30 数学运算符：\n+\n-\n*\n/ 整除\n% 取余\n生成随机数 1 2 3 4 5 6 [root@wei csdn]# echo $RANDOM 11400 [root@wei csdn]# echo $RANDOM 9702 [root@wei csdn]# echo $RANDOM 21328 生成10以内的随机数：\n1 2 3 4 [root@wei csdn]# echo $((RANDOM%10)) 4 [root@wei csdn]# echo $((RANDOM%10)) 2 4 命令引用\n反引号 `COMMAND`\n$(COMMAND)\n1 2 3 4 5 6 7 [root@wei csdn]# a=`ls -ldh /etc/` [root@wei csdn]# echo $a drwxr-xr-x. 77 root root 8.0K 3月 14 16:23 /etc/ [root@wei csdn]# b=$(ls -ldh /etc/) [root@wei csdn]# echo $b drwxr-xr-x. 77 root root 8.0K 3月 14 16:23 /etc/ 提取ip 1 2 [root@wei csdn]# ifconfig ens33 |grep \u0026#34;netmask\u0026#34; | awk \u0026#39;{print $2}\u0026#39; 192.168.196.131 1 2 3 4 [root@wei csdn]# head -n 3 /etc/passwd |awk -F: \u0026#39;{print $1,$6,$7}\u0026#39; root /root /bin/bash bin /bin /sbin/nologin daemon /sbin /sbin/nologin 5 删除变量\n# unset 变量名称\n环境变量 （1）查看环境变量\n1 [root@wei csdn]# env | less （2）定义环境变量：修改环境变量的值\n#export 变量名称=变量值\n1 2 [root@wei csdn]# vim /etc/profile [root@wei csdn]# source /etc/profile $?判断上个命令的执行状态（0\u0026ndash;255）\n0：代表成功\n其余:代表失败\n","date":"2019-03-14T20:16:52Z","image":"https://www.ownit.top/title_pic/44.jpg","permalink":"https://www.ownit.top/p/201903142016/","title":"Linux shell变量详解"},{"content":"shell是一个命令解释器，它在操作系统的最外层，负责直接与用户对话，把用户的输入解释给操作系统，并处理各种各样的操作系统的输出结果，输出到屏幕返回给用户。这种对话方式可以是交互的方式（从键盘输入命令，可以立即得到shell的回应），或非交互（执行脚本程序）的方式。\n下图的黄色部分就是命令解释器shell处于的操作系统中位置形象图解。\nLinux SHELL 脚本 大量重复执行的工作\nshell（Linux壳）， 一类程序的名称\n文本文件\u0026mdash;\u0026ndash;\u0026gt;shell命令，/bin/bash提供逻辑控制语句\n重定向向符号的使用\n/dev/stdin 标准输入设备（键盘） 0\n/dev/stdout 标准输出设备（显示器） 1\n/dev/stderr 标准错误输出设备（显示器） 2\n输出重定向符号\n\u0026gt; 覆盖原文件信息\n\u0026raquo; 往原文件后面追加类容\n\u0026gt; \u0026raquo; 用于重定向标准输出\n1 2 [root@wei ~]# ls -ldh /etc/ /tmp/1.txt [root@wei ~]# ls -ldh /tmp/ \u0026gt;\u0026gt;/tmp/1.txt 2\u0026gt; 2\u0026raquo; 用于重定向标准错误输出\n1 [root@wei ~]# ls -ldh /qwertyuasdfgh 2\u0026gt; /tmp/1.txt \u0026amp;\u0026gt; 同时重定向标准输出及标准错误输出\n特殊设备文件：/dev/null （垃圾站）\n1 2 [root@wei ~]# ls -ldh /etc/ \u0026amp;\u0026gt;/dev/null [root@wei ~]# grep \u0026#34;root\u0026#34; /etc/passwd \u0026amp;\u0026gt; /dev/null 输入重定向符号\n1 2 3 4 [root@wei ~]# cat /tmp/1.txt chengfeng [root@wei ~]# tr \u0026#39;a-z\u0026#39; \u0026#39;A-Z\u0026#39; \u0026lt; /tmp/1.txt CHENGFENG 输出信息：\n1 echo 1 2 3 4 5 6 7 8 9 10 11 12 13 [root@wei ~]# echo \u0026#34;请输出你的选择\u0026#34; #默认会打印换行符 请输出你的选择 [root@wei ~]# echo -n \u0026#34;请输出你的选择\u0026#34; 请输出你的选择[root@wei ~]# [root@wei ~]# echo -e \u0026#34;a\\nbb\\nccc\u0026#34; # \\n 回车 a bb ccc [root@wei ~]# echo -e \u0026#34;a\\tbb\\tccc\u0026#34; # \\t tab键 a bb ccc 2 printf\n1 2 [root@wei ~]# printf \u0026#34;hello wowrd\u0026#34; hello wowrd[root@wei ~]# 3 HERE DOCUMENT -\u0026mdash;-\u0026gt;输出多行信息\n1 2 3 4 5 6 7 8 9 10 [root@wei ~]# cat \u0026lt;\u0026lt; eof （eof为提示符，可以任意定义） \u0026gt; 选择 \u0026gt; 安装 \u0026gt; 重启 \u0026gt; 关机 \u0026gt; eof 选择 安装 重启 关机 双引号和单引号的区别： 单引号：所有字符会失去原有的含义\n双引号：特殊的字符会转义\n如何交互命令: 1 2 3 [root@wei ~]# echo \u0026#34;root\u0026#34; | passwd --stdin hei \u0026amp;\u0026gt; /dev/null [root@wei ~]# echo -e \u0026#34;n\\rp\\r1\\r+100M\\rw\\r\u0026#34; | fdisk /dev/vdb \u0026amp;\u0026gt; /dev/null 显示历史命令\n1 [root@wei ~]# history 执行历史命令的某一条\n1 [root@wei ~]# !254 清空历史命令\n1 [root@wei ~]# history -c ","date":"2019-03-12T22:28:21Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/201903122228/","title":"Linux shell之重定向输入，输出"},{"content":"在服务搭建前，还要了解一下httpd的日志。\n日志有助有工作人员，查看服务器出错状况，更能统计数据分析网页运行情况。\nPV和UV两大分析 PV Page View 页面访问量 UV User View 用户访问量\n1)指定错误日志的名称及级别\n错误日志的路径：/var/log/httpd/error_log\n错误级别： debug, info, notice, warn, error, crit\n访问日志： /var/log/httpd/access_log\n1 [root@wei httpd]# head -n 1 /var/log/httpd/access_log （2）定义访问日志的格式\nLogFormat \u0026ldquo;%h %l %u %t \\\u0026rdquo;%r\\\u0026quot; %\u0026gt;s %b \\\u0026quot;%{Referer}i\\\u0026quot; \\\u0026quot;%{User-Agent}i\\\u0026quot;\u0026quot; combined\n1 192.168.196.1 - - [07/Mar/2019:05:33:39 +0800] \u0026#34;GET / HTTP/1.1\u0026#34; 200 1766 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\u0026#34; %h 远端主机\n%l 远端登录名(由identd而来，如果支持的话)，除非IdentityCheck设为\u0026quot;On\u0026quot;，否则将得到一个\u0026quot;-\u0026quot;。\n%u 远程用户名(根据验证信息而来；如果返回status(%s)为401，可能是假的)\n%t 时间，用普通日志时间格式(标准英语格式)\n%r 请求的第一行\n%\u0026gt;s:HTTP状态码\n%b 以CLF格式显示的除HTTP头以外传送的字节数，也就是当没有字节传送时显示’-\u0026lsquo;而不是0。\n%{Referer}i：记录超链接地址\n%{User-Agent}i：记录客户端的浏览器类型\n（3）指定访问日志的名称及格式\nCustomLog \u0026ldquo;logs/access_log\u0026rdquo; combined\n虚拟主机 VirtualHost 作用：在一台物理服务器上运行多个网站\n类型：\n基于域名的虚拟主机（常用）\n基于IP地址的虚拟主机\n基于端口的虚拟主机\n配置虚拟主机\n1 2 3 4 5 6 7 \u0026lt;VirtualHost 10.1.2.3:80\u0026gt; ServerAdmin webmaster@host.example.com DocumentRoot /www/docs/host.example.com ServerName host.example.com ErrorLog logs/host.example.com-error_log TransferLog logs/host.example.com-access_log \u0026lt;/VirtualHost\u0026gt; 示例：基于主机名的虚拟主机\nwww.a.org 网页目录：/var/www/html/a.org 日志：/var/log/httpd/a.org\nwww.a.org (1)准备目录\n1 2 3 [root@wei ~]# mkdir /var/www/html/a.org [root@wei ~]# vim /var/www/html/a.org/index.html [root@wei ~]# mkdir /var/log/httpd/a.org （2）编写配置文件\n1 [root@wei ~]# vim /etc/httpd/conf.d/a.org.conf 1 2 3 4 5 6 7 \u0026lt;VirtualHost 192.168.196.132:80\u0026gt; DocumentRoot /var/www/html/a.org ServerName www.a.org ErrorLog /var/log/httpd/a.org/error_log CustomLog /var/log/httpd/a.org/access_log combined \u0026lt;/VirtualHost\u0026gt; （3）检测配置文件语法\n1 [root@wei ~]# httpd -t （4）重启服务\n1 [root@wei ~]# systemctl restart httpd ","date":"2019-03-12T19:30:27Z","image":"https://www.ownit.top/title_pic/59.jpg","permalink":"https://www.ownit.top/p/201903121930/","title":"Linux的httpd服务搭建"},{"content":"软件介绍\n客户端代理软件\nIE，firefox，chroome，opera\n服务器端软件\nhttpd，Nginx，Tengine，ISS，Lighthttp\n应用程序服务器\nISS，Tomcat（JSP，open sourec），Websphere（IBM，JSP，commodity）Weblogic(oracle,JSP,commodity)\nJBoss(redhat,JSP),Resin,php\nhttpd安装及配置\nASF：Apache Software Foundation http://www.apache.org/\nweb:httpd\nTomcat\nHadoop\nhttpd: http://httpd.apache.org/\nWeb Server,Open Sourec\n2.4,2.2,2.0\nhttpd软件安装:\nrpm\n源码软件\nhttpd特性:\n事先创建进程\n按需维持适当的进程\n模块化设计，核心较小，各种功能都能通过模块添加:模块可以在运行时启用\n支持运行配置，支持单独编译模块\n支持多种虚拟主机的配置\n基于IP的虚拟主机\n基于端口的虚拟主机\n基于域名的虚拟主机\n支持https协议(mod_ ss1)\n支持用户认证\n支持基于IP或主机名的访问控制机制\n支持每目录的访问控制\n支持URL重写\n安装httpd软件\n1 [root@wei ~]# yum install -y httpd 开启服务\n1 [root@wei ~]# systemctl start httpd 开机自启\n1 [root@wei ~]# systemctl enable httpd 查看端口号\n1 [root@wei ~]# ss -antp |grep httpd 查看它启动的进程\n1 [root@wei ~]# ps aux|grep httpd httpd目录:\n** /etc/httpd/ conf 主配置文件 httpd. conf\n/etc/httpd/conf.d/* . conf 子配置文件\n/var/log/httpd 日志\naccess_ .log 访问日志\nerror_ log 错误日志**\n** /var/www/html 默认静态页面的目录\n/var/www/cgi-bin 默认动态页面的目录**\nCGI: Comnon Gateway Interface通用网关接口\n让web服务器启动某应用程序解析动态页面的机制开发动态网页的语言:\nperl, python, java (Servlet JSP), php\n** PHP LAMP , LNMP\nJSP Tomcat, Weblogical\nPython mod_ wsgi模块**\n** httpd配置文件 \u0026mdash;- /etc/httpd/conf/httpd.conf ;**\ndiretive value\n指令不区分大小写\nvalue区分大小写\n1）设置httpd的主目录\n**ServerRoot \u0026ldquo;/etc/httpd\u0026rdquo; **\n2）监听ip的地址和端口\n** #Listen 12.34.56.78:80\nListen 80\n3）指定子配置文件的路径及名称**\n** Include conf.modules.d/*.conf**\n4)设置运行httpd进程的用户及用户组名称\nUser apache\nGroup apache\n5)长连接相关的配置\nKeepAlive on\nMaxKeepAliveRequests 100\nKeepAliveTimeout 15\n6)设置管理员的邮箱\nServerAdmin root@localhost\n7)设置网站的主机名\nServerName www.a. org\n8)设置网页目录\n**DocumentRoot \u0026ldquo;/var/www/html\u0026rdquo; **\n9)设置网页的首页名称\nDirectoryIndex index. html\n10）针对目录权限\n\u0026lt;Directory \u0026ldquo;/var/www/html\u0026rdquo;\u0026gt;\nOptions Indexes FollowSymLinks\nAllowOverride None\nRequire all granted\n\u0026lt;/Directory\u0026gt;\nA） Require all granted\n允许所有的客户端访问该目录的页面文件\nB）Options Indexes FollowSymLinks\n定义目录下的网页文件被访问时的访问属性\nNone：不支持任何选项\nIndexes：无index.html 时，列出所有的文件，禁用\nFollowSymLinks：存在软链接网页文件时，是否只可以访问对应原网页文件的内容，禁用\nSymLinksifOwnerMatch:允许访问软链接，但所属必须和运行httpd进程的所属一致\nIncludes:允许执行服务器端包含(SSI格式的网页文件)，禁用\nExeCGI:允许运行CGI脚本\nMultiviews:内容协商机制(根据客户端的语言不同显示不同的网页)，多视图:禁用\nA11:启用所有选项\nC）Allowoverride None\n** 是否允许建立.htaccess文件覆盖提权配置**\n查看帮助手册\n[root@wei csdn]# yum -y install httpd-manual\nhttp://192.168.196.131/manual/\n支持用户认证\n示例：客户端通过用户hei访问首页（/var/www/html） (1)创建用户名称和密码\n1 2 3 4 [root@wei ~]# htpasswd -c /etc/httpd/.webuser hei New password: Re-type new password: Adding password for user hei （2）编辑配置文件\n1 [root@wei ~]# vim /etc/httpd/conf/httpd.conf 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;Directory \u0026#34;/var/www/html\u0026#34;\u0026gt; Options Indexes FollowSymLinks AllowOverride AuthConfig AuthType Basic AuthName \u0026#34;Resttrict test\u0026#34; AuthUserFile /etc/httpd/.webuser Require valid-user #Require user 用户名称 只允许指定用户访问 \u0026lt;/Directory\u0026gt; 检测配置文件语法\n1 2 3 4 [root@wei ~]# httpd -t Syntax OK (3)重启服务\n1 [root@wei ~]# systemctl restart httpd （4）再次创建一个用户wei\n创建用户名称和密码\n1 2 3 4 5 [root@wei ~]# htpasswd /etc/httpd/.webuser wei [root@wei ~]# cat /etc/httpd/.webuser hei:$apr1$nBaKumC0$lsSLO6LqDQ58CWLjXIfJT0 wei:$apr1$ovl.gsGg$SHvY5Aksj9MdZv9u8E5XF1 基于客户端ip地址的认证\n1) 允许所有喜户端访问\n** Require all granted**\n2)拒绝所有端访问\n** Require al1 denied3)仅允许某主机访问**\n** Require ip 192.168.1.14) 明确拒绝某主机访问**\n** \u0026lt;RequireAll\u0026gt; Require all granted\nRequire not ip 192.168.96.1\n\u0026lt;/RequireAll\u0026gt;\nwindos访问效果图**\n** 检测配置文件语法\n[root@wei ~]# httpd -t**\nSyntax OK\n重启服务\n**[root@wei ~]# systemctl restart httpd\n**\n","date":"2019-03-08T21:48:28Z","image":"https://www.ownit.top/title_pic/65.jpg","permalink":"https://www.ownit.top/p/201903082148/","title":"Linux的httpd服务介绍和部署"},{"content":"web（World Wide Web）即全球广域网，也称为万维网，它是一种基于超文本和HTTP的、全球性的、动态交互的、跨平台的分布式图形信息系统。是建立在Internet上的一种网络服务，为浏览者在Internet上查找和浏览信息提供了图形化的、易于访问的直观界面，其中的文档及超级链接将Internet上的信息节点组织成一个互为关联的网状结构\n网页类型：\n静态网页 HTML超文本标记语言 *.html\n动态网站\n类似于脚本文件，根据传递的参数不同，返回的页面结果不同的\nPHP *.php\nJava(JSP) *.jsp\nPython(Django模块)*.wsgi\nHTTP\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-HyperText Transfer Protocol 超文本传输协议\nHTTP/0.9：仅纯文本（超链接），ASCLL\nHTTP ：HyperText Mark Language 超文本标记语言\n\u0026lt;h1\u0026gt;I am cehngfneg.\u0026lt;/h1\u0026gt;\nHTTP/1.0\nMIME机制：Multipurpose Internet Mail Extensions，多用途互联网邮件扩展\n将非文本数据在传输前重新编码为文本格式再传输，接收方能够用相反的方式将其还原成以前的格式，还能够调用相应的程序打开此文件\n缓存机制\nHTTP/1.1（无状态连接）\n增强了缓存机制的管理\n长连接keepalive机制\n超时时间\n每个长连接请求文件个数的限制\nHTTP报文：请求报文，响应报文\nHTTP请求报文语法：\n\u0026lt;method\u0026gt;\u0026lt;request-URL\u0026gt;\u0026lt;version\u0026gt;\n\u0026lt;headers\u0026gt;\n\u0026lt;entity-body\u0026gt;\n请求报文：\nGET /1.gif HTTP/1.1\nHost:www.bj.com\nConnection:keep-alive\nHTTP方法：\nGET，PUT，POST，DELETE，HEAD\nURI：Uniform Resource Identifier，统一资源标识符 /.jpg\n全局范围内，唯一标识某个资源的名称\n统一：路径格式上的统一\nURL：Uniform Resource Locator 统一资源定位符\nprotocal://Host:Port/path/to/file\nhttp://192.168.1.1/1.jpg\nhttp://192.168.1.1:8000/1.jpg\nHTTP响应报文语法：\n\u0026lt;version\u0026gt;\u0026lt;status\u0026gt;\u0026lt;reason-phrase\u0026gt;\n\u0026lt;headers\u0026gt;\n\u0026lt;entity-body\u0026gt;\n** 状态代码:\n1xx:纯信息\n2xx:成功类信息\n3xX:重定向类信息I\n301:永久重定向\n302:临时重定向\n304: not-modified, 使用缓存的内容响应客户端\n4xx:客户端错误类信息\n5xx:服务器端错误类信息**\n响应报文:\nHTTP/1.1 200 OK\nX-Powerd=By: PHP/5.2.17\nvary: Accept-Encoding, Cookie, User-Agent\nCache-Control: max-age=3, must-revalidate\nContent-Encoding: gzip\nContent -Length: 6931\n上面两个报文的第一 行通常称为报文的“起始行(start line)\u0026quot;; 后面的标签格式称为报文首部域(Header\nfield)，每个首部域都由名称(name)和值(value)组成，中间用逗号分隔。另外，响应报文通常还有- - 个称作Body的信息主体，即响应给客户端的内容\nweb服务器的主要操作:\n1、建立连接\u0026mdash;-接受或拒绝客户端连接请求\n2、接收请求\u0026mdash;-通过网络读取HTTP请求报文\n3、处理请求\u0026mdash;-解析请求报文并做出相应的动作\n4、访问资源\u0026mdash;-访问请求报文中所请求的资源\n5、构建响应\u0026mdash;-使用正确的首部生成HTTP响应报文\n6、发送响应\u0026mdash;-向客户端发送生成的响应报文\n7、记录日志\u0026mdash;-当已经完成的HTTP事务记录进日志文件\nweb服务器响应并发连接(qps\u0026ndash;\u0026gt; query per second) 的方式:\n1、单进程/单线程机制\n依次处理每个请求\n2、多进程/多线程机制(稳定)\n每个请求生成子进程响应\n3、一一个进程响应多个请求(单进程多线程)\n事件驱动机制\n通知机制\n4、多进程响应多个请求\n","date":"2019-03-06T20:26:44Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/201903062026/","title":"Linux的web服务的介绍"},{"content":"下面的部署是在Linux的DNS正向解析部署上进行修改的。\n如果有什么问题或者错误，可以访问上篇帖子\n下面开始有关DNS的服务部署。\u0026lt;DNS主从服务器\u0026gt;\n环境描述：\n192.168.196.132 DNS主服务器\n192.168.196.131 DNS从服务器\n将主服务器的上的wei.com区域的记录与从服务器同步\n主服务器： （1）编辑主配置文件named.conf\n1 [root@wei named]# vim /var/named/chroot/etc/named.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 options { directory \u0026#34;/var/named\u0026#34;; }; zone \u0026#34;wei.com\u0026#34; { type master; allow-transfer { 192.168.196.131;}; \u0026gt;\u0026gt;\u0026gt;指定从服务器的IP file \u0026#34;wei.com.zone\u0026#34;; }; zone \u0026#34;1.168.192.in-addr.arpa\u0026#34; { type master; file \u0026#34;192.168.1.zone\u0026#34;; }; （2）编辑wei.com的区域的记录文件，添加从服务器的NS记录\n1 [root@wei named]# vim /var/named/chroot/var/named/wei.com.zone 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $TTL 1D @ IN SOA wei.com. 123456.qq.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns01.wei.com. NS dns02.wei.com. dns01 A 192.168.196.132 dns02 A 192.168.196.131 web A 192.168.1.1 web A 192.168.1.11 wei.com. A 192.168.1.1 *.wei.com. A 192.168.1.1 ftp A 192.168.1.2 MX 10 mail.wei.com. mail A 192.168.1.3 （3）重启服务\n1 2 3 [root@wei named]# systemctl restart named-chroot [root@wei named]# systemctl restart named 从服务器：\n（1）安装软件\n1 [root@wei named]# yum install -y bind bind-chroot （2）编辑主配置文件\n1 [root@wei csdn]# vim /var/named/chroot/etc/named.conf 1 2 3 4 5 6 7 8 9 10 options { directory \u0026#34;/var/named\u0026#34;; }; zone \u0026#34;wei.com\u0026#34; { type slave; masters { 192.168.196.132;}; \u0026gt;\u0026gt;\u0026gt;\u0026gt;指向主服务器 file \u0026#34;slaves/wei.com.zone\u0026#34;; }; （3）重启服务\n1 2 3 [root@wei named]# systemctl restart named-chroot [root@wei named]# systemctl restart named （4）测试\n1 2 3 4 [root@wei slaves]# cd /var/named/chroot/var/named/slaves [root@wei slaves]# ls wei.com.zone 已经成功加载\n（5）nslookup检查\n好的，这就已经完成主从服务器的搭建了。\n我们去主服务器新加区域，看能否同步解析\n重启主服务器。\n现在去测试从服务器的同步情况，已经成功了，ok。\n","date":"2019-03-05T21:35:15Z","image":"https://www.ownit.top/title_pic/44.jpg","permalink":"https://www.ownit.top/p/201903052135/","title":"Linux的DNS主从服务器部署"},{"content":"下面的部署是在Linux的DNS正向解析示例上进行修改的。\n如果有什么问题或者错误，可以访问上篇帖子\n下面开始有关DNS的服务部署。\u0026lt;DNS反向解析\u0026gt;\n工具：虚拟机\ncentos7 配置：Linux IP 192.168.196.132\nDSN 192.168.196.132\n建立DAS反向区域，实现反向解析\n（1）编辑主配置文件named.conf\n1 [root@wei named]# vim /var/named/chroot/etc/named.conf 添加下面的代码\n1 2 3 4 zone \u0026#34;1.168.192.in-addr.arpa\u0026#34; { type master; file \u0026#34;192.168.1.zone\u0026#34;; }; （2）建立反向区域\n可以复制正向解析的文件作为模板\n1 [root@wei named]# cp /var/named/chroot/var/named/wei.com.zone /var/named/chroot/var/named/192.168.1.zone 修改192.168.1.zone脚本\n1 [root@wei named]# vim /var/named/chroot/var/named/192.168.1.zone 1 2 3 4 5 6 7 8 9 10 11 12 13 $TTL 1D @ IN SOA wei.com. 123456.qq.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns01.wei.com. dns01 A 192.168.196.132 1 PTR web.wei.com. 11 PTR web.wei.com. 2 PTR ftp.wei.com. 3 PTR mail.wei.com. （3）重启服务\n1 2 3 [root@wei named]# systemctl restart named-chroot [root@wei named]# systemctl restart named （4）使用nslookup测试查看\n","date":"2019-03-05T21:19:25Z","image":"https://www.ownit.top/title_pic/69.jpg","permalink":"https://www.ownit.top/p/201903052119/","title":"Linux的DNS反向解析部署"},{"content":"DNS负载均衡技术的实现原理是在DNS服务器中为同一个主机名配置多个IP地址，在应答DNS查询时，DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果，将客户端的访问引导到不同的机器上去，使得不同的客户端访问不同的服务器，从而达到负载均衡的目的\n下面的部署是在Linux的DNS正向解析部署上进行修改的。\n如果有什么问题或者错误，可以访问上篇帖子\n下面开始有关DNS的服务部署。\u0026lt;DNS实现负载均衡及泛域名\u0026gt;\n工具：虚拟机\ncentos7 配置：Linux IP 192.168.196.132\nDSN 192.168.196.132\n负载平衡 利用DNS记录实现负载均衡效果\nweb A 192.168.1.1\nweb A 192.168.1.11\n同一主机名解析到不同的ip上\n1 [root@wei named]# vim /var/named/chroot/var/named/wei.com.zone 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $TTL 1D @ IN SOA wei.com. 123456.qq.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns01.wei.com. dns01 A 192.168.196.132 web A 192.168.1.1 web A 192.168.1.11 ftp A 192.168.1.2 MX 10 mail.wei.com. mail A 192.168.1.3 重启服务 1 2 3 [root@wei named]# systemctl restart named-chroot [root@wei named]# systemctl restart named 效果图 泛域名记录 wei.com. A 192.168.1.1\n*.wei.com A 192.168.1.1\n1 [root@wei named]# vim /var/named/chroot/var/named/wei.com.zone 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $TTL 1D @ IN SOA wei.com. 123456.qq.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns01.wei.com. dns01 A 192.168.196.132 web A 192.168.1.1 web A 192.168.1.11 wei.com. A 192.168.1.1 # 使用wei.com就可以访问域名 *.wei.com. A 192.168.1.1 # 无论前面加什么，都会解析到192.168.1.1上 ftp A 192.168.1.2 MX 10 mail.wei.com. mail A 192.168.1.3 重启服务 1 2 3 [root@wei named]# systemctl restart named-chroot [root@wei named]# systemctl restart named 效果图 ","date":"2019-03-05T20:02:38Z","image":"https://www.ownit.top/title_pic/05.jpg","permalink":"https://www.ownit.top/p/201903052002/","title":"Linux的DNS实现负载均衡及泛域名部署"},{"content":"前面介绍了DNS的作用及其相关的结果。Linux服务之DNS介绍\n下面开始有关DNS的服务部署。\u0026lt;DNS正向解析示例\u0026gt;\n工具：虚拟机\ncentos7 配置：Linux IP 192.168.196.132\nDSN 192.168.196.132\n要求：\nweb.wei.com 192.168.1.1\nftp.wei.com 192.168.1.2 mail.wei.com 192.168.1.3\n准备工作：（可以看前期教程）\n关闭SElinux，防火墙\n配置yum源\n容易犯错的地方：查看/etc/resolv.conf 确保你的DNS是否正确 DNS正向解析 （1）安装软件 bind bind-chroot\n1 [root@wei csdn]# yum install -y bind bind-chroot 2.编辑DNS的主配置文件，创建区域wei.com\n新建named.conf\n1 [root@wei named]# vim /var/named/chroot/etc/named.conf 填写内容\n1 2 3 4 5 6 7 options{ directory “/var/named\u0026#34;; }; zone \u0026#34;wei.com\u0026#34;{ type master; file \u0026#34;wei.com.zone\u0026#34; }: 3.复制记录文件的模板，并编辑\n模板：/usr/share/doc/bind-9.9.4/sample/var/named/named.localhost\n1 2 3 [root@wei named]# cd /usr/share/doc/bind-9.9.4/sample/var/named/ [root@wei named]# cp named.localhost /var/named/chroot/var/named/wei.com.zone 1 2 3 4 5 6 7 8 9 10 11 12 13 $TTL 1D @ IN SOA wei.com. 123456.qq.com. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns01.wei.com. dns01 A 192.168.196.132 web A 192.168.1.1 ftp A 192.168.1.2 MX 10 mail.wei.com. mail A 192.168.1.3 4.启动named服务\n1 2 3 [root@wei named]# systemctl start named-chroot [root@wei named]# systemctl start named 开机自启\n1 2 3 [root@wei named]# systemctl enable named-chroot [root@wei named]# systemctl enable named 查看端口53\n5.测试\n（1）nslookup\n（2）dig\n[root@wei named]# dig -t A web.wei.com\n","date":"2019-03-05T19:20:34Z","image":"https://www.ownit.top/title_pic/23.jpg","permalink":"https://www.ownit.top/p/201903051920/","title":"Linux的DNS正向解析部署"},{"content":"DNS\u0026mdash;\u0026mdash;-Domain Name System域名系统\n介绍：DNS就是把域名和IP地址联系在一起的服务，有了DNS服务器，你就不用输入IP地址来访问一个网站，可以通过输入网址访问。\n** **\n**作用：（1）将域名，主机名解析对应的ip地址 正向解析\n（2）将IP地址解析成对应的主机名，域名 反向解析 **\n区域zone：\n正向区域 nangong.com\n反向区域 X.X.X.in-addr.arpa 192.168.196.0/24 196.168.1962.in-addr.arpa\n**记录Record\nA记录 主机记录 www.nangong.com A 192.168.1.1\nNS记录 标识DNS服务器自身的名称\nNS dns1.nangong.com.\ndns1.nangong.com A 192.168.1.2\nMX记录 标识邮件服务器的名称\nMX 10 mail.nangong.com.\nmail.nangong.com A 192.168.196.3\nCNAME记录 别名记录 m.mail.com CNAME mail.nangong.com.\nPTR记录 反向指针记录\n192.168.1.1 PTR www.nangong.com.\n**\nDNS域名结构：\n** . 根域\ncom\njd www.jd.com\u0026mdash;\u0026mdash;\u0026mdash;-\u0026gt;www.jd.com.\nbaidu\ntaobao\ncn edu\norg\nDNS解析方式：\n递归\n客户端只需要向DNS服务器发送一次请求\n迭代\n客户端需要发送多次DNS请求**\n","date":"2019-03-05T18:46:29Z","image":"https://www.ownit.top/title_pic/53.jpg","permalink":"https://www.ownit.top/p/201903051846/","title":"Linux服务之DNS介绍"},{"content":"NFS（Network File System）即网络文件系统，\n是FreeBSD支持的文件系统中的一种，它允许网络中的计算机之间通过TCP/IP网络共享资源。\n在NFS的应用中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，就像访问本地文件一样。\nNFS -\u0026mdash;\u0026mdash;\u0026mdash;-Network File Syste 网络文件系统\n作用：在Linux服务器间实现数据共享\n软件：nfs-utils\nrpcbind\n目录导出文件\u0026mdash;-/etc/exports\n文件格式：\n** 目录名称 客户端地址（权限）\n客户端地址：\nIP地址：192.168.196.131\n网关：192.168.196.0/24\n权限：\nro 只读\nrw 读写\nsync 同步\nasync 异步\nall_squash 客户端所有用户上传的文件的所属均为nfsnobody\nroot_squash 客户端root用户上传的文件的所属会被映射为nfsnobody\nno_root_squash 客户端root用户上传的文件所属仍为root\nanonuid=\u0026lt;number\u0026gt;\nanongid=\u0026lt;number\u0026gt;\n**\nLinux服务器：192.168.196.131 Linux客户端：192.168.196.131 ** 示例：\n通过nfs共享本地目录/wedata 允许192.168.196.132以只读方式挂载**\n（1）安装软件\n1 [root@wei ~]# yum -y install rpcbind nfs-utils （2）创建文件\n1 2 [root@wei ~]# mkdir /wedata [root@wei ~]# touch /wedata/{1..10}.html （3）修改配置文件/etc/exports\n1 [root@wei ~]# vim /etc/exports 1 /wedata 192.168.196.132(ro) （4）重启服务\n1 2 [root@wei ~]# systemctl restart rpcbind [root@wei ~]# systemctl restart nfs-server 或者下面的命令\n（5）查看本地共享目录\n1 2 3 [root@wei ~]# showmount -e localhost Export list for localhost: /wedata 192.168.196.132 （6）客户端访问（软件也需要安装）\n1 2 3 [root@zhang ~]# mount 192.168.196.131:/wedata /web/ [root@zhang ~]# ls /web/ 10.html 1.html 2.html 3.html 4.html 5.html 6.html 7.html 8.html 9.html **设置为开机挂载 修改配置文件/etc/fstab **\n1 [root@zhang ~]# vim /etc/fstab 1 192.168.196.131:/wedata /web nfs defaults 0 0 至于权限修改，根据情况可以来修改\n","date":"2019-02-04T19:56:38Z","image":"https://www.ownit.top/title_pic/59.jpg","permalink":"https://www.ownit.top/p/201902041956/","title":"Linux文件服务管理之nfs"},{"content":"简介\nvsftpd是 “very secure FTP deamon”的缩写，是一个完全免费，开源的ftp服务器软件。\n特点\n小巧轻快，安全易用，支持虚拟用户、支持带宽限制等功能。\nFTP -\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;File Transfer Protocol 文件传输协议\nFTP协议的连接模式：\n主动连接\n被动连接\n软件：vsftpd\n配置文件：/etc/vsftpd/vsftpd.conf\n服务：vsftpd\n端口：21/tcp 命令连接端口\n20/tcp 数据连接端口（主动）\nFTP根目录：\n用户宿主目录\n访问方式：\n匿名用户访问（ftp）\n用户认证的访问\n示例：搭建匿名访问的FTP服务器\n（1）安装vsftpd软件\n1 [root@wei csdn]# yum install -y vsftpd （2）开启服务，开机自启\n1 2 [root@wei ftp]# systemctl start vsftpd [root@wei ftp]# systemctl enable vsftpd 已经成功，默认的共享目录是/var/ftp/pub路径\n示例：允许匿名用户上传文件\n1 2 3 [root@wei ~]# chmod o+w /var/ftp/pub/ [root@wei ~]# vim /etc/vsftpd/vsftpd.conf anon_upload_enable=YES \u0026raquo;\u0026raquo;允许上传文件\nanon_mkdir_write_enable=YES \u0026raquo;\u0026raquo;允许上传目录\nanon_umask=022 \u0026raquo;\u0026raquo;允许其他用户能下载匿名用户文件\nanon_other_write_enable=YES \u0026raquo;\u0026raquo;允许修改文件名称，删除文件\nanon_root=/comapng \u0026raquo;\u0026raquo;共享目录修改\n注意：圈住的是匿名用户访问时的权限，可根据上面代码修改权限 重启vsftpd软件\n1 [root@wei ~]# systemctl restart vsftpd 访问方式：\nlinux客户端：\n1 2 3 [root@zhang hei]# lftp 192.168.196.131 lftp 192.168.196.131:~\u0026gt; ls drwxr-xr-x 5 0 0 111 Oct 30 19:45 pub windows客户端：\nftp://192.168.196.131\n本地用户认证的FTP服务 在普通用户家目录创建文件，可以访问这些文件\n示例：\n创建文件\n1 2 [root@wei ~]# ls /home/hei/ [root@wei ~]# touch /home/hei/{1..4}.txt 访问方式：\nlinux客户端：\n1 [root@zhang hei]# lftp 192.168.196.131 -u hei windows客户端：\nftp://192.168.196.131\n由此可见，可以访问用户家目录下的文件。\n","date":"2019-02-03T21:07:37Z","image":"https://www.ownit.top/title_pic/74.jpg","permalink":"https://www.ownit.top/p/201902032107/","title":"Linux文件服务管理之vsftpd"},{"content":"Linux文件服务器的搭建\nSamba\nvsftpd\nnfs\nSamba服务\n作用：共享目录\n软件：Samba 服务器 ，Samba-client 客户端\n配置文件：/etc/smaba/smb.conf\n服务：smb,nmb\n端口：smb -\u0026ndash;\u0026gt;139/tcp , 445/tcp 提供文件共享功能\nnmb -\u0026ndash;\u0026gt;137/udp , 138/udp 提供解析计算机名称\n配置文件：/etc/smaba/smb.conf\n全局配置\nworkgroup = SAMBA -\u0026ndash;设置工作组名称\nserver string =Samba Server Version %v -\u0026mdash;显示samba软件版本信息\ninterface = lo eth0 192.168.196.131？24 -\u0026ndash;samba服务监听的ip地址\nhosts allow=127.192.168.12 192.168.196.0 -\u0026mdash;-设置仅允许那些主机访问\nhosts deny=192.168.12. 192.168.1.1/24 -\u0026mdash;-拒绝那些主机访问\nsecurity =user -\u0026mdash;\u0026mdash;基于用户认证访问\nshare -\u0026mdash;\u0026mdash;匿名访问\n共享目录配置\n[共享名称]\ncomment= ====描述信息\npath = /bj ====指定目录名称\nbrowseable = yes ====可下载文件\nwritable = yes ====可上传文件\npublic = yes ====运行所有用户访问\nwrite list =user1 ====仅允许user1可上传文件\n示例：\n环境描述：\nLinux 192.168.196.131 Centos7 文件共享服务器\nwindows/Linux客户端\n需求：通过samba软件将本地的/caiwu 目录共享，客户端可以通过hei用户访问，仅允许下载文件 前提：selinux和防火墙全部关闭\n（1）安装软件\n1 [root@wei ~]# yum -y install samba samba-client 创建共享用户\n1 2 [root@wei ~]# useradd hei [root@wei ~]# smbpasswd -a hei 查看共享用户\n1 [root@wei ~]# pdbedit -L 配置文件/etc/smaba/smb.conf\n1 [root@wei ~]# vim /etc/samba/smb.conf 1 2 3 4 [caiwu] comment = caiwu path = /caiwu browseable = yes 重启samba服务\n[root@wei ~]# systemctl start smb\n1 [root@wei ~]# systemctl start smb 测试访问： windows访问：\\\\192.168.196.131\n已经共享成功\nLinux客户端： 1 2 [root@wei ~]# yum -y install samba-client [root@zhang ~]# smbclient //192.168.196.131/caiwu-U hei 文件的上传 如果想要上传文件，这需要修改文件权限w为其他共享用户\n如果不给权限会出现下面的情况\nwindows客户端\nLinux客户端\n重点修改文件权限w为其他共享用户\n修改单个用户权限则可以使用下面这段命令\n1 [root@wei ~]# setfacl -m u:hei:rwx /caiwu/ 修改配置文件\n重启就可以上传文件了 多用户示例： ** 通过samba软件将本地的/shanghai目录共享，允许hei用户下载文件，允许admin用户上传文件**\n（1）创建目录，创建共享用户\n1 2 3 4 5 6 [root@zhang ~]# mkdir /shichang [root@zhang ~]# touch /shichang/{1..5}.jpg [root@zhang ~]# useradd admin [root@zhang ~]# useradd zhang [root@zhang ~]# smbpasswd -a zhang [root@zhang ~]# smbpasswd -a admin 配置文件修改下面这样\n1 2 3 4 [shichang] path = /shichang browseable = yes write list = admin （2）重启服务访问\n（3）测试访问：\n清除windows的共享缓存\nnet use * /del\n","date":"2019-02-03T20:38:40Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201902032038/","title":"Linux文件服务管理之Samba"},{"content":"1、DHCP服务简介 DHCP(Dynamic Host Configuration Protocol，动态主机配置协议)是一个局域网的网络协议，使用UDP协议工作， 主要有两个用途:给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段，在RFC 2131中有详细的描述。DHCP有3个端口，其中UDP67和UDP68为正常的DHCP服务端口，分别作为DHCP Server和DHCP Client的服务端口;546号端口用于DHCPv6 Client，而不用于DHCPv4，是为DHCP failover服务，这是需要特别开启的服务，DHCP failover是用来做\u0026quot;双机热备\u0026quot;的。\n2、DHCP服务作用 为大量客户机自动分配地址，提供集中管理\n**减轻管理和维护成本、提高网络配置效率\n原理：\n1.客户端发送DHCP Discovery探索DHCP服务器 2.DHCP服务器发送DHCP Offer (IP/NETMASK/GATEWAY/DNS)\n3.客户端发送DHCP Request\n4.DHCP服务器发送DHCP ACK\n5.客户端发送Gratuation ARP用于检测IP地址是否冲突\n**\n软件：dhcp\n配置文件：/etc/dhcp/dhcpd.conf\n服务：dhcpd\n端口：67/udp（DHCP服务端口） ,68/udp（DHCP客户端端口）\n","date":"2019-02-02T20:00:04Z","image":"https://www.ownit.top/title_pic/60.jpg","permalink":"https://www.ownit.top/p/201902022000/","title":"Linux服务管理之DHCP"},{"content":"NTP是网络时间协议(Network Time Protocol)，它是用来同步网络中各个计算机的时间的协议。\n在计算机的世界里，时间非常地重要，例如对于火箭发射这种科研活动，对时间的统一性和准确性要求就非常地高，是按照A这台计算机的时间，还是按照B这台计算机的时间？NTP就是用来解决这个问题的，NTP（Network Time Protocol，网络时间协议）是用来使网络中的各个计算机时间同步的一种协议。它的用途是把计算机的时钟同步到世界协调时UTC，其精度在局域网内可达0.1ms，在互联网上绝大多数的地方其精度可以达到1-50ms。\n它可以使计算机对其服务器或时钟源（如石英钟，GPS等等）进行时间同步，它可以提供高精准度的时间校正，而且可以使用加密确认的方式来防止病毒的协议攻击。\nNTP服务器\nNTP -\u0026mdash;\u0026mdash;\u0026mdash;- Network Time Protocol 网络时间协议\n软件：ntp\n配置文件： /etc/ntp.conf\n服务：ntp\n端口：123/udp\n示例：配置ntp时间服务器\n1.安装ntp软件\n1 [root@wei ~]# yum install ntp -y 2.编辑ntp配置文件\n1 [root@wei ~]# vim /etc/ntp.conf 添加下面的代码\nrestrict 192.168.196.0 mask 255.255.255.0 nomodify notrap\nserver 127.127.1.0 iburst\n27 fudge 127.127.1.0 stratum 10 3.重启ntpd服务\n1 2 3 [root@wei ~]# systemctl start ntpd #启动服务 [root@wei ~]# systemctl enable ntpd #设置为开机启动 Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service. 在客户端输入：\n1 [root@zhang csdn]# ntpdate 192.168.196.131 即可同步时间\n同步网络时间： 1 [root@wei ~]# /usr/sbin/ntpdate time.windows.com ","date":"2019-02-02T19:50:02Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/201902021950/","title":"Linux服务管理之ntp"},{"content":"Linux服务SSH\nssh服务：\n管理服务器的方式：\n本地管理类 （安装系统，故障修复）\nSHH远程连接方式\nLinux：ssh命令\nwindows:Xshell , SecureCRT , Putty\n提供ssh服务/ssh客户端工具的软件：\nopenssh-clients-7.4p1-16.el7.x86_64\nopenssh-server-7.4p1-16.el7.x86_64\n查看ssh状态\n1 [hei@wei ~]$ systemctl status sshd 查看ssh的端口：\n1 [root@wei ~]# ss -antp | grep sshd 1.远程连接主机\n# shh [user@]host\n** # ssh 192.168.196.132\n# ssh marin@192.168.196.132\n2.远程连接主机执行命令**\n**# ssh 192.168.196.132 ‘hostname’ **\n1 2 3 [root@wei ~]# ssh hei@192.168.196.132 \u0026#39;hostname\u0026#39; hei@192.168.196.132\u0026#39;s password: wei 3.远程复制文件工具\n** scp rsync\n# scp /etc/fstab 192.168.196.132:/tmp# scp 192.168.196.132:/etc/passwd /tmp\n-r: 复制目录\n**\n1 2 3 4 5 6 [root@wei ~]# scp /etc/fstab 192.168.196.132:/tmp root@192.168.196.132\u0026#39;s password: fstab 100% 465 17.4KB/s 00:00 [root@wei ~]# scp 192.168.196.132:/etc/passwd /tmp root@192.168.196.132\u0026#39;s password: passwd rsync\n# rsync -av /bj/ 192.168.196.132:/bj #拷贝文件\n# rsync -av /bj 192.168.196.132:/bj #拷贝目录\n配置文件：/etc/ssh/sshd_config、\n注意：下面几项操作在配置文件中进行\n(1)关闭SSH的主机名解析\nGSSAPIAuthentication no\nUseDNS no\n1 [root@zhang ~]# systemctl restart sshd （2）禁用root用户远程连接\n** PermitRootLogin no**\n（3）修改默认端口\nPort 1999\n注意：此时连接需要加端口（-p） root用户已经登录不上\n1 [root@wei ~]# ssh hei@192.168.196.132 -p 1999 （4）监听ip\n** ListenAddress 192.168.196.131**\nSSH认证方式： ** 基于用户名，密码：默认\n基于密钥\n**\n基于密钥的配置方法：\n1.在客户端生成密钥对\n2.把公钥传给服务器\n（1）在客户端生成密钥对\n注意：默认生成的密钥会存储在/root/.ssh/目录下\n1 [root@wei ~]# ssh-keygen -t rsa （2）把公钥传送给服务器\n注意：传递的公钥会存储在/root/.ssh/目录下\n1 [root@wei .ssh]# ssh-copy-id -i -p 1999 192.168.196.132 注意：可以给普通用户复制公钥，但是要修改目录和公钥的权限为普通用户的\n","date":"2019-02-01T21:34:06Z","image":"https://www.ownit.top/title_pic/61.jpg","permalink":"https://www.ownit.top/p/201902012134/","title":"Linux服务管理之SSH"},{"content":" 计划任务\n**类型： 一次性计划任务\n周期性计划任务**\n** 一次性计划任务**\n前提： atd服务必须运行\n1 2 3 4 5 [root@wei init.d]# yum -y install at #安装atd服务 [root@wei init.d]# systemctl start atd #开启atd服务 [root@wei init.d]# systemctl status atd #查看atd开启状态 18:00关机（以系统时间为准）：\n1 2 3 4 [root@wei init.d]# at 18:00 at\u0026gt; poweroff at\u0026gt; \u0026lt;EOT\u0026gt; # Ctrl+d 提交任务 job 1 at Fri Feb 1 18:00:00 2019 **1分钟后执行的任务：\n**\n1 2 3 4 [root@wei init.d]# at now + 1 minute at\u0026gt; mkdir /root/nangong at\u0026gt; \u0026lt;EOT\u0026gt; job 4 at Thu Jan 31 18:49:00 2019 周期性计划任务 前提：crond服务必须运行\n1 2 3 4 5 [root@wei ~]# yum install crontabs #安装crond服务 [root@wei init.d]# systemctl start crond #开启crond服务 [root@wei ~]# systemctl status crond #查看crond开启状态 **\n制作周期性计划任务**\n**# crontab -e **\n** 时间 COMMAND**\n时间：\n分 时 日 月 周\n分钟： 0\u0026mdash;-59\n时： 0\u0026mdash;-23\n日期： 1\u0026mdash;-31\n月： 1\u0026mdash;12\n周： 0\u0026mdash;-6\n* 表示每周（日 月 周）\n- 连续的时间\n， 不连续的时间\n示例：\n**每天晚上11:30 30 23 * * *\n每天零点 0 0 * * * 每天早上8:10 9:10 10:10 10 8-10 * * * 每隔5分钟 */5 * * * *\n每隔3小时 * */3 * * * **\n**COMMAND命令： 1.建议写命令的完整路径 /bin/mkdir/abc\n2.只能写一条命令（shell）\n**\n**注意： 在写命令时%在周期性计划任务中是结束的意思，因此在使用%时，需要加\\右斜杠转义\n\u0026amp;\u0026gt; /dev/null 不给用户发邮件\n**\n创建计划任务\n示例：\n（1）每分钟在tmp目录下创建文件\n**[root@wei ~]# crontab -e **\n*/1 * * * * /usr/bin/touch /tmp/wei/$(date +\\%F-\\%T).txt\n（2）每分钟分别显示磁盘使用，cpu状态，内存状态的信息\n分析：一行只能写一条命令，但要显示三个命令，则需要借助shell脚本。然后在周期性任务中调用shell脚本。\n（1）创建shell脚本\n1 2 3 4 5 6 7 8 9 10 11 [root@wei ~]# vim hei.sh #!/bin/bash echo echo \u0026#34;CPU负载\u0026#34; uptime echo echo \u0026#34;磁盘容量：\u0026#34; df -hT echo echo \u0026#34;内存容量\u0026#34; free -h 我在次调用演示。\n（2）创建周期性任务\n1 2 3 [root@wei ~]# crontab -e */1 * * * * /usr/bin/bash /root/hei.sh 注意：这个会给root用户发邮件显示shell脚本运行的信息\n1 */1 * * * * /usr/bin/bash /root/hei.sh \u0026amp;\u0026gt; /dev/null ** \u0026amp;\u0026gt; /dev/null 不给用户发邮件**\n查看计划任务\n1 2 3 [root@wei ~]# crontab -l */1 * * * * /usr/bin/touch /tmp/wei/$(date +\\%F-\\%T).txt */1 * * * * /usr/bin/bash /root/hei.sh \u0026amp;\u0026gt; /dev/null 删除计划任务（全部删除）\n1 [root@wei ~]# crontab -r ","date":"2019-01-31T21:08:21Z","image":"https://www.ownit.top/title_pic/67.jpg","permalink":"https://www.ownit.top/p/201901312108/","title":"Linux计划任务管理"},{"content":"Linux运行模式\n自由服务，即不需要用户独立去安装的软件服务，而是在系统安装好之后就可以直接使用的服务（内置服务）。 运行模式也称为运行级别，属于linux的自有服务。 运行模式可以理解为一旦你开机了，计算机就以你设置的运行模式给你展示。 控制运行模式的进程是：init ，初始化进程，进程id是1。 Centos 7系统运行方式（target）：\nmulti—user.target 字符模式（多用户模式）\ngraphical.target 图像模式\n（1）查看当前的默认级别：\n1 2 [root@wei ~]# systemctl get-default multi-user.target （2）修改默认级别\n1 [root@wei ~]# systemctl get-default graphical.target Centos 6系统运行方式：\n0 关机模式\n1 单用户模式（修复）\n2 字符模式（无网络）\n3 完全字符模式（黑底白字）\n4 预留\n5 图形模式\n6 重启模式 查看当前的默认级别\n1 2 3 [root@wei ~]# runlevel N 3 init id 可以来回切换模式\n编辑文件/etc/inittab修改默认的启动模式\n","date":"2019-01-31T20:50:49Z","image":"https://www.ownit.top/title_pic/03.jpg","permalink":"https://www.ownit.top/p/201901312050/","title":"Linux系统运行模式介绍"},{"content":"Linux上进程有5种状态，这5中状态可以与一般操作系统的状态对应起来：\n运行：正在运行或在运行队列中等待。 中断：休眠中， 受阻， 在等待某个条件的形成或接受到信号。 不可中断：收到信号不唤醒和不可运行， 进程必须等待直到有中断发生。 僵死：进程已终止， 但进程描述符存在， 直到父进程调用wait4()系统调用后释放。 停止：进程收到SIGSTOP， SIGSTP， SIGTIN， SIGTOU信号后停止运行运行。 进程控制\n** 信号：Signal\n查看所有的信号**\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@wei csdn]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 常用的信号：\n** 1) SIGHUP 让一个进程不用重启，就可以重读其配置文件，并让新配置生效\n2) SIGINT 硬件中断信号。 Ctrl+c\n9) SIGKILL 杀死一个进程\n15) SIGTERM 终止一个进程\n如何调用一个信号：\n信号号码： kill -9 \u0026lt;PID\u0026gt; （杀死程序后，要删除交互文件，才能正常恢复）\n信号名称： kil -SIGKILL \u0026lt;PID\u0026gt;\n信号名称简写：kill -KILL \u0026lt;PID\u0026gt;**\n# kill \u0026lt;PID\u0026gt;\n**# killall \u0026lt;PROCESS_NAME\u0026gt; **\n1 [root@wei csdn]# killall httpd 控制进程的运行方式（前台/后台）\n前台：占用命令提示符\n（1）控制命令在后台运行\n1 [root@wei csdn]# firefox \u0026amp; （2）查看后台的应用程序\n1 [root@wei csdn]# jobs -l ** （3）将正在运行的指令放入后台，并暂停运行**\n** Ctrl+z**\n（4）将后台的程序调回前台继续运行\n# fg \u0026lt;后台任务编号\u0026gt;\n","date":"2019-01-30T20:51:51Z","image":"https://www.ownit.top/title_pic/50.jpg","permalink":"https://www.ownit.top/p/201901302051/","title":"Linux进程控制"},{"content":"关于Linux进程查看，前面讲解了ps命令，下面拉介绍另一个命令top\nps：静态查看\ntop：动态查看\n动态查看进程的状态 # top\n1 2 3 4 5 6 7 [root@wei ~]# top top - 18:38:46 up 15 min, 2 users, load average: 0.09, 0.07, 0.06 Tasks: 98 total, 1 running, 97 sleeping, 0 stopped, 0 zombie %Cpu(s): 1.0 us, 1.3 sy, 0.0 ni, 97.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 997956 total, 613748 free, 215484 used, 168724 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 605556 avail Mem 7.6%us 用户进程占用的CPU\n1.6%sy 系统进程占用的CPU\n0.0%ni 调整过优先级的进程占用的CPU\n90.6%id CPU空闲\n0.0%wa 等待磁盘IO的进程所占用的CPU\nCpu(s) :显示所有的CPU平均比，按1可现实每个CPU的使用情况\ntop交互式指令：\nM：按内存使用排序\nP：按CPU使用排序 T：按运行时间排序\nl: 是否显示TOP第1行信息\nm: 是否显示内存使用信息\nt: 是否显示CPU及任务信息\nc：是否显示完整的命令行\nq：退出TOP\n显示当前时刻CPU的使用情况\n1 2 [root@wei ~]# uptime 19:03:27 up 39 min, 2 users, load average: 0.00, 0.01, 0.05 top选项：\n-d 1 ：指定top信息刷新的频率\n-b ：以批模式显示进程信息\n-n 2：共显示两批信息\n# top -d 1 -b -n 2\n查看服务器性能 ：\n1.ps ，top\n2.df -hT\n3.free -m 查看磁盘\n使用这三个命令需要安装sysstat\n1 [root@wei csdn]# yum install sysstat # mpstat 查看CPU\n# vmstat 查看内存\n# iostat 查看磁盘\n# sar 查看网卡\n四个基本用法一样，参数设置一样。\n1 2 [root@wei csdn]# mpstat 1 一秒显示一次 [root@wei csdn]# mpstat 1 5 一秒显示一次，显示5次 ","date":"2019-01-29T17:10:43Z","image":"https://www.ownit.top/title_pic/29.jpg","permalink":"https://www.ownit.top/p/201901291710/","title":"Linux进程管理之top"},{"content":"**Linux 是一种动态系统，能够适应不断变化的计算需求。**下面介绍一些 Linux 所提供的工具来进行进程的查看与控制，掌握这些让我们能在某些进程出现异常的时候及时查看相关的指标，从而解决问题。\n进程管理\n进程 process\n某应用程序打开的进程\nPID Process ID\n类型：\n用户空间进程\n内核空间进程\n用户空间进程：通过执行用户程序、应用程序或内核之外的系统程序而产生的进程，此类进程可以在用户的控制下运行或关闭。\n内核空间进程：可以执行内存资源分配和进程切换等管理工作；而且，该进程的运行不受用户的干预，即使是root用户也不能干预系统进程的运行。\n静态查看进程状态\n1 2 3 4 5 6 # ps [root@wei csdn]# ps \u0026gt;\u0026gt;\u0026gt;\u0026gt;查看本终端的进程 PID TTY TIME CMD 1806 pts/1 00:00:00 bash 2805 pts/1 00:00:00 ps 选项的使用方式：\nBSD风格：选项没有横线- ps aux\nSysV风格：选项需要带有横线- ps -elf\nBSD风格：\na ：显示与终端相关的进程\nu : 显示启动进程的用户\nx ：显示与终端无关的程序\n1 2 3 4 5 # ps a # ps u # ps x 1 2 3 4 5 [root@wei csdn]# ps u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1343 0.0 0.2 115436 2032 pts/0 Ss+ 19:59 0:00 -bash root 1674 0.0 0.2 115432 2032 tty1 Ss+ 20:01 0:00 -bash USER运行进程的用户%CPU进程所占欧诺个的CPU百分比\u0026nbsp;%MEM\u0026nbsp;进程所占用的MEM百分比VSZ\u0026nbsp;虚拟内存集，进程独有的内存+共享存在PSS进程独有的内存 STAT 进程的状态\nD: 不可中断的睡眠（等待磁盘IO完成）\nS：可中断的睡眠（不需要等待磁盘IO完成）\nR：运行或就绪\nT: 停止\nZ：僵死 Zombie\n\u0026lt; :高优先级进程\n会被CPU优先执行\n会获取更多的CPU执行时间 N:低优先级进程\n+：前台进程组中的进程\nl:多线程进程（Thread）\ns：会话进程首进程，某一个连接的父进程\n1 2 3 4 5 6 [root@wei csdn]# ps aux |less USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.6 127940 6580 ? Ss 19:58 0:03 /usr/lib/systemd/systemd --switched-root --system --deserialize 22 root 2 0.0 0.0 0 0 ? S 19:58 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? S 19:58 0:00 [ksoftirqd/0] root 5 0.0 0.0 0 0 ? S\u0026lt; 19:58 0:00 [kworker/0:0H] ** 带有方括号为系统进程（Linux内核启动）\n无方括号的（用户进程）**\nSysV风格选项：\n-e 显示所有进程\n-l 详细信息\n-f 以长格式显示（更多字段类容）\n1 2 3 4 5 6 [root@wei csdn]# ps -elf | less F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S root 1 0 0 80 0 - 31985 ep_pol 19:58 ? 00:00:04 /usr/lib/systemd/systemd --switched-root --system --deserialize 22 1 S root 2 0 0 80 0 - 0 kthrea 19:58 ? 00:00:00 [kthreadd] 1 S root 3 2 0 80 0 - 0 smpboo 19:58 ? 00:00:01 [ksoftirqd/0] 进程优先级：\n0\u0026mdash;139\n数据越小，越先级越高\n高优先级进程：\n会获取CPU更多的执行时间\n会被CPU优先执行\nnice值：\n新优先级=旧优先级——nice值\n-20\u0026mdash;-19\n普通用户仅能够调大nice值，既降低进程优先级\nroot用户可以随意调整nice值\n显示进程树\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [root@wei csdn]# yum install psmisc # 安装显示pstree的命令包 [root@wei csdn]# pstree systemd─┬─NetworkManager───2*[{NetworkManager}] ├─VGAuthService ├─auditd───{auditd} ├─crond ├─dbus-daemon───{dbus-daemon} ├─login───bash ├─lvmetad ├─master─┬─pickup │ └─qmgr ├─mysqld_safe───mysqld───21*[{mysqld}] ├─polkitd───5*[{polkitd}] ├─rsyslogd───2*[{rsyslogd}] ├─sshd─┬─2*[sshd───bash] │ └─sshd───bash───pstree ├─systemd-journal ├─systemd-logind ├─systemd-udevd ├─tuned───4*[{tuned}] └─vmtoolsd───{vmtoolsd} 1 2 3 4 5 6 7 [root@wei csdn]# ps aux | grep vim root 2917 0.2 0.5 151600 5136 pts/2 S+ 21:39 0:00 vim list root 2925 0.0 0.0 112720 984 pts/1 R+ 21:43 0:00 grep --color=auto vim [root@wei csdn]# pidof vim 2917 [root@wei csdn]# pidof bash 1828 1806 1674 1343 ","date":"2019-01-29T16:45:00Z","image":"https://www.ownit.top/title_pic/45.jpg","permalink":"https://www.ownit.top/p/201901291645/","title":"Linux进程管理之ps"},{"content":"前面讲解了lLinux 的IP组成，下面就讲一下Linux的网络设置和数据传递。\n其实这地方对运维的人员来说，不会要精通，但还是要了解。必要时刻还会用到的\n电脑之间数据的传递：\n数据的传递要分为下面几层。\nOSI七层模型\n应用层 表示层 会话层 传输层 网络层 数据链路层 物理层\n数据封装过程：\nMAC帧头+IP报头+TCP/UDP报头+数据\nTCP/UDP报头:\n端口号 Port 区分不同的应用程序\n取值范围：1\u0026mdash;65535 基于ip地址\n**数据解包，则反之。 **\ncentos 7 提供network ，NetworkManager服务实现网络参数\n基于network服务\n1.查看操作\n（1）查看网卡IP地址\n# ifconfig\n# ip addr show\n（2）查看网关\n**# route -n **\n# ip route\n（3）查看DNS服务地址\n# cat /etc/resolv.conf\n1 [root@wei ~]# cat /etc/resolv.conf 修改网卡TCP/IP参数\n**配置文件地方 /etc/sysconfig/network-scripts/ifcfg-ens33 **\n内容:\nDEVICE=网卡名称\nNANE=网卡配置文件名称\nONBOOT=yes //设置开机自动启动网卡\nBOOTPROTO=none //手动指定IP\nIPADDR=192.168.196.131 //IP地址 NETMASK=255.255.255.0 //子网掩码 或者PREFIX=24\nGATEWAY=192.168.196.2 //网关\nDNS1=8.8.8.8 //dns服务地址\nDNS2=8.8.4.4\n示例：\n** 为eth0网卡配置多个IP地址 10.1.1.1/24\n临时生效：**\n[root@wei ~]# ifconfig ens33:0 10.1.1.1/24\n[root@wei ~]# ip addr dev ens33 10.1.1.1/24\n永久生效：\n[root@wei ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33:0\n**DEVICE=en33s:0\nNANE=ens33:0\nONBOOT=yes BOOTPROTO=none IPADDR=192.168.196.131 NETMASK=255.255.255.0 **\n[root@wei ~]# systemctl restart NetworkManager\n[root@wei ~]# systemctl restart network\n临时禁用网卡\n# ifdown 网卡名称\n启用网卡\n# if 网卡名称\n端口号（port）：\n（1）查看TCP端口\n[root@wei csdn]# ss -antp\n** a: all 全部\nn：number 数据\np：port 端口号：\nt：tcp 协议\n[root@wei csdn]# netstat -antp**\n（2）查看UDP端口\n[root@wei csdn]# ss -anup\n[root@wei csdn]# netstat -anup\n（3）查看所有的UDP和TCP的端口\n[root@wei csdn]# netstat -anutp\n[root@wei csdn]# ss -anutp\n","date":"2019-01-28T16:08:55Z","image":"https://www.ownit.top/title_pic/20.jpg","permalink":"https://www.ownit.top/p/201901281608/","title":"Linux的网络参数设置"},{"content":"** 俗话说：黑发不知勤学早，白首方悔读书迟。** ** ————正是年轻的我们，更应该吃苦学习，工作。加油吧，**\n前面介绍了linux的软件管理。下面就介绍关于linux的ip详解。\nLinux系统网络参数分别为：\n** 计算机名称\nIP地址\n子网掩码\n默认网关\nDNS服务\n1.计算机名称**\nCentos 7：\n（注意：centos6不是这样修改，需要修改配置文件）\n1 #hostnamectl set-hostname node01.linux.com 【FQDN 完全合格域名】 查看Linux主机名的配置文件\n**# cat /etc/hostname **\n重点\nIP地址 192.168.196.131 点分十进制表示法\n组成：32位二进制数字\nxxxxxxxx.xxxxxxxx.xxxxxxxx.xxxxxxxx\n0\u0026ndash;255 0\u0026ndash;255 0\u0026ndash;255 0\u0026ndash;255\n大致范围：0.0.0.0\u0026ndash;255.255.255.255\n类型：\n1.根据第一个字节的取值范围分类\nA类：0\u0026ndash;127\nB类：128\u0026ndash;191\nC类：192\u0026ndash;223 单播地址 Unicast（前三个）同一个网络内，一个IP地址只能标识一个网络节点\nD类：224\u0026ndash;239 组播地址 Multicast\nE类：240\u0026ndash;255 科学研究\n2.根据IP地址的使用范围\n私网地址（不能直接访问互联网，可重复使用）\n10.0.0.0\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;10.255.255.255\n172.16.0.0\u0026mdash;\u0026mdash;\u0026mdash;172.31.255.255\n192.168.0.0\u0026mdash;\u0026mdash;\u0026mdash;192.168.255.255\n公网地址\n特殊地址：\n127.x.x.x 127.0.0.0\n168.254.x.x\n子网掩码 netmask\n作用： 判断多个地址是否属于同一网络\n原理：分别将IP地址与子网掩码转换为二进制数字，依次进行逻辑与运算（网段）\n默认子网掩码（只是默认，不是固定）\nA类：255.0.0.0 /8\nB类：255.255.0.0 /16\nC类：255.255.255.0 /24\n192.168.1.1 255.255.255.0 -\u0026mdash;\u0026mdash;-\u0026gt;192.168.1.1/24\n","date":"2019-01-27T22:22:32Z","image":"https://www.ownit.top/title_pic/46.jpg","permalink":"https://www.ownit.top/p/201901272222/","title":"Linux的IP详解"},{"content":"前面介绍了软件的管理的方式rpm。但有个缺点，rpm不能解决依赖。\n下面介绍的yum软件管理。可以完美的解决这个问题。\n使用yum的方式管理rpm软件\n优势：自动解决软件的依赖关系\n前提条件：配置yum仓库/yum源\nyum源类型： ** 1.本地yum源 2.ftp源\n3.http源\n**\n** 配置yum的地方：**\n** 阿里云镜像： https://mirrors.aliyun.com\n网易云镜像： http://mirrors.163.com/\nepel源 Centos官网 ：http://vault.centos.org # yum install epel-release 安装epel源\n**\nyum源/yum仓库的配置文件\n/etc/yum.repos.d/*.repo\n示例：配置本地yum源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [root@wei yum.repos.d]# mount /dev/sr0 /mnt/ mount: /dev/sr0 写保护，将以只读方式挂载 [root@wei yum.repos.d]# ls /mnt/ CentOS_BuildTag EULA images LiveOS repodata RPM-GPG-KEY-CentOS-Testing-7 EFI GPL isolinux Packages RPM-GPG-KEY-CentOS-7 TRANS.TBL [root@wei yum.repos.d]# mkdir /etc/yum.repos.d/beifeng [root@wei yum.repos.d]# mv /etc/yum.repos.d/CentOS-* /etc/yum.repos.d/beifeng/ [root@wei yum.repos.d]# ls beifeng mysql-community.repo mysql-community-source.repo [root@wei yum.repos.d]# vim centos.repo [root@wei yum.repos.d]# ls beifeng centos.repo mysql-community.repo mysql-community-source.repo 配置内容： [centos7.2] name=centos7.2 baseurl=file:///mnt enable=1 gpgcheck=0 清除yum缓存：\n1 [root@wei yum.repos.d]# yum clean all 生成yum缓存：\n1 [root@wei yum.repos.d]# yum makecache 查看yum的列表\n1 [root@wei yum.repos.d]# yum repolist yum常规操作\n（1）yum安装软件 # yum install 软件名\n# yum install -y 软件名\n（2）显示yum中所有软件\n1 [root@wei ~]# yum list all （3）显示所有的软件组\n1 [root@wei ~]# yum grouplist （4）安装软件组\n# yum groupinstall -y 软件组的名称\n(英文组，要用“”括起来)\n（5）查询文件所属的软件名称\n# yum provides “*bin/passwd”\n源码软件管理安装 ** 1.配置安装参数 2.编译\n3.安装**\n前提：gcc编译环境（自己安装yum install gcc）\n示例：编译安装htop软件\nhtop软件源码：https://www.lanzous.com/i2zs97g\n**解压\n[root@wei ~]# tar zxf htop-1.0.2.tar.gz **\n切入htop-1.0.2目录\n[root@wei ~]#cd htop-1.0.2\n查看配置\n[root@wei htop-1.0.2]# ./configure --help |less\n配置参数\n[root@wei htop-1.0.2]# ./configure --prefix=/usr/local/htop\n编译\n[root@wei htop-1.0.2]# make\n安装\n[root@wei htop-1.0.2]# make install\n启动（已经安装成功接界面）\n**[root@wei share]# /usr/local/htop/bin/htop **\n出现错误：\nconfigure: error: You may want to use --disable-unicode or install libncursesw.\n**解决办法：\n[root@wei htop-1.0.2]# yum install -y ncurses-devel **\n","date":"2019-01-26T17:04:22Z","image":"https://www.ownit.top/title_pic/54.jpg","permalink":"https://www.ownit.top/p/201901261704/","title":"Linux的yum管理"},{"content":"** 书山有路勤为径，学海无涯苦作舟** 自学linux已经有几天了，感觉还可以。坚持下去，就会有收获。\n每个系统都用相应的软件的管理，Linux也不例外。下面讲解Linux 的rpm管理软件。\nLinux软件管理\n** windows：.exe .mai\ncentos/RHEL/Fedoea ：二进制格式软件(*.rpm) redhat package management\n源码软件(*.tar.gz , *.tar.bz2) **\nrpm软件管理\n** 光盘iso镜像文件\nrpm软件名称的组成\nzlib-devel-1.2.7-17.el7.x86_64.rpm\nzlib-devel：软件名称\n1.2.7：版本\nel7.x86_64 ： 软件运行平台\nel7.noarch ： 无系统架构，可以安装任何版本**\n下面的两个网站可以下载rpm包（网站是国外的，可能比较慢）\nrpm下载： https://pkgs.org/\nrpm下载： http://rpmfind.net/\n注意：你下载的rpm包要对应你服务器的版本号和运行平台\n查看系统平台信息 1 [root@wei ~]# uname -r 注意：本地系统没有rpm软件包，那么只能挂载iso镜像文件 （1）有和运行平台相对应的iso镜像文件（我的centos7的）\n（2）设置虚拟机，加载镜像文件\n（3）光盘挂载\n1 [root@wei dev]# mount /dev/sr0 /mnt/ 光盘卸载\n1 [root@wei dev]# umount /dev/sr0 管理rpm软件\n1.查询软件是否安装\n# rpm -q 软件名称\n# rpm -qa | grep 软件名称\n2.查询软件的说明信息\n#rpm -qi 软件名称\n3.查看软件生成的文件\n# rpm -ql 软件名称\n[root@wei ~]# rpm -ql bash | less\n4.查看文件由那个软件生成\n# rpm -qf 文件名称\n1 2 3 4 [root@wei ~]# which chmod /usr/bin/chmod [root@wei ~]# rpm -qf /usr/bin/chmod coreutils-8.22-21.el7.x86_64 5.查看软件的配置文件\n# rpm -qc 软件名称\n1 2 3 [root@wei ~]# rpm -qc vim-enhanced /etc/profile.d/vim.csh /etc/profile.d/vim.sh 管理操作：\n（1）安装软件\n# rpm -ivh 软件安装包名称\ni: 安装 install\nv：显示详细信息 verbose\nh：显示软件安装进度\n安装vsftpd软件\n1 2 3 4 5 6 [root@wei ~]# mount /dev/sr0 /mnt/ [root@wei ~]# rpm -ivh /mnt/Packages/vsftpd-3.0.2-22.el7.x86_64.rpm 准备中... ################################# [100%] 正在升级/安装... 1:vsftpd-3.0.2-22.el7 ################################# [100%] 安装dhcp软件\n1 2 3 4 [root@wei ~]# rpm -ivh /mnt/Packages/dhcp-4.2.5-68.el7.centos.x86_64.rpm 准备中... ################################# [100%] 正在升级/安装... 1:dhcp-12:4.2.5-68.el7.centos ################################# [100%] 安装软件出现依赖问题\n** 选项\u0026ndash;nodeps 忽略依赖关系安装\n**\n（2）卸载软件\n# rpm -e 软件名称\n1 2 3 4 5 [root@wei ~]# rpm -q dhcp dhcp-4.2.5-68.el7.centos.x86_64 [root@wei ~]# rpm -e dhcp [root@wei ~]# rpm -q dhcp 未安装软件包 dhcp 选项\u0026ndash;nodeps 忽略依赖关系卸载\n（3）升级软件\n# rpm -Uvh 软件安装包名称\n** 注意：自动卸载就版本软件**\n","date":"2019-01-26T15:18:19Z","image":"https://www.ownit.top/title_pic/19.jpg","permalink":"https://www.ownit.top/p/201901261518/","title":"Linux的rpm管理"},{"content":"用户操作环境配置文件：\n从/etc/skel目录复制过来\n**.bashrc 打开新终端 /etc/bashrc\n.bash_profile 用户登录系统 /ect/profile\n.bash_logout 注销系统 **\n示例：设置命令别名\n临时命令别名（关机重启，就没有了） # alias 命令别名=‘命令’\n1 alias ipshow=\u0026#39;cat /etc/sysconfig/network-scripts/ifcfg-ens33\u0026#39; 永久别名（可以一直存在）\n针对单个用户设置别名\n（1）创建hei用户，修改vim /home/hei/.bashrc （2）进入 /home/hei/.bashrc ，在最后一行添加 ** alias ipshow=\u0026rsquo; cat /etc/sysconfig/network-scripts/ifcfg-ens33\u0026rsquo; 保存退出**\n（3）切换到hei用户下查看\n针对所有用户设置别名\n这个方法和上面的一样，只需要修改的文件不一样。下面已经列出\n（1）修改/etc/bashrcde文件\n（2）添加命令别名代码\n（3）刷新文件即可\n1 2 3 4 root@wei ~]# vim /etc/bashrc alias ipshow=\u0026#39; cat /etc/sysconfig/network-scripts/ifcfg-ens33\u0026#39; [root@wei ~]# source /etc/bashrc ","date":"2019-01-25T21:36:47Z","image":"https://www.ownit.top/title_pic/72.jpg","permalink":"https://www.ownit.top/p/201901252136/","title":"Linux用户环境配置文件"},{"content":"** 路漫漫其修远兮，吾将上下而求索** inux文件目录权限管理\n常规权限：\nr read 读取 4\nw write 写入 2\nx execute 执行 1\n文件：\nr 查看文件内容（cat/more/less/head/tail/grep）\nw 编辑文件内容（vim）\nx shell/python脚本\n目录：\nr 查看目录的文件（ls/tmp）\nw 修改目录的文件（新建,删除,mv）\nx 切换目录（cd）\n除去第一个，三个为一组 查看文件权限\n查看目录权限\n设置文件目录权限\n（1）chmod 修改权限\n# chmod {augo}{+-=}{rwx} 文件名称\n** a all 所有\nu user 属主用户\ng group 属组\no other 其他\n# chmod a+x /tmp/1.txt\n# chmod u-x,o+r /tmp/2.txt\n# chmod nnn 文件名称**\n权限设置例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [root@wei ~]# ls -l /test/ 总用量 0 -rw-r--r--. 1 root root 0 1月 25 10:42 1.txt -rw-r--r--. 1 root root 0 1月 25 10:42 2.txt -rw-r--r--. 1 root root 0 1月 25 10:42 3.txt -rw-r--r--. 1 root root 0 1月 25 10:42 4.txt -rw-r--r--. 1 root root 0 1月 25 10:42 5.txt [root@wei ~]# chmod a+x /test/1.txt #设置文件1.txt为所有用户可执行（x） [root@wei ~]# ls -l /test/1.txt -rwxr-xr-x. 1 root root 0 1月 25 10:42 /test/1.txt #查看文件1.txt的权限 [root@wei ~]# chmod g-r,o-r /test/2.txt #设置文件2.txt用户属组权限不可读，其他用户权限不可读 [root@wei ~]# ls -l /test/2.txt #查看文件2.txt的权限 -rw-------. 1 root root 0 1月 25 10:42 /test/2.txt [root@wei ~]# chmod g=rw /test/3.txt #覆盖文件3.txt用户属组的权限为可读可写 [root@wei ~]# ls -l /test/3.txt #查看文件3.txt的权限 -rw-rw-r--. 1 root root 0 1月 25 10:42 /test/3.txt [root@wei ~]# chmod 600 /test/4.txt #设置文件4.txt的权限为主用户可读可写 [root@wei ~]# ls -l /test/4.txt -rw-------. 1 root root 0 1月 25 10:42 /test/4.txt [root@wei ~]# chmod 000 /test/5.txt #设置5.txt的文件权限为所有不可读不可写不可执行 [root@wei ~]# ls -l /test/5.txt ----------. 1 root root 0 1月 25 10:42 /test/5.txt （2）修改文件的属主,属组\n# chown 用户名称.用户组名称 文件名称\n1 2 3 4 5 [root@wei ~]# chown user1.caiwu /test/1.txt [root@wei ~]# chown user1 /test/2.txt [root@wei ~]# chown root.caiwu /test/4.txt 仅修改属组：\n# chgrp 用户组名称 文件名称\n1 [root@wei ~]# ls -l /test/3.txt 属组权限设置例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@wei ~]# ls -l /test/ 总用量 0 -rwxr-xr-x. 1 root root 0 1月 25 10:42 1.txt -rw-------. 1 root root 0 1月 25 10:42 2.txt -rw-rw-r--. 1 root root 0 1月 25 10:42 3.txt -rw-------. 1 root root 0 1月 25 10:42 4.txt ----------. 1 root root 0 1月 25 10:42 5.txt [root@wei ~]# groupadd caiwu # 添加用户属组caiwu [root@wei ~]# chown user1.caiwu /test/1.txt #把1.txt的主用户改为user，属组改为caiwu [root@wei ~]# ls -l /test/1.txt -rwxr-xr-x. 1 user1 caiwu 0 1月 25 10:42 /test/1.txt [root@wei ~]# chown user1 /test/2.txt #把2.txt的主用户改为user1，属组不变 [root@wei ~]# ls -l /test/2.txt -rw-------. 1 user1 root 0 1月 25 10:42 /test/2.txt [root@wei ~]# chgrp caiwu /test/3.txt 把3.txt的属组改为caiwu [root@wei ~]# ls -l /test/3.txt -rw-rw-r--. 1 root caiwu 0 1月 25 10:42 /test/3.txt [root@wei ~]# chown root.caiwu /test/4.txt #把#4.txt的主用户为root，属组为caiwu [root@wei ~]# ls -l /test/4.txt -rw-------. 1 root caiwu 0 1月 25 10:42 /test/4.txt 方法2\nfacl\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;文件访问控制列表\n设置权限：\n针对单个用户设置权限\n# setfacl -m u：用户名：权限 文件名称\n1 [root@wei test]# setfacl -m u:user4:r /test/3.txt 针对单个用户组设置权限\n# setfacl -m g：用户组名称 ：权限 文件名称\n1 [root@wei test]# setfacl -m g:caiwu:rwx /test/3.txt 查看权限\n# getfacl 文件名称\n1 2 3 4 5 6 7 8 9 10 [root@wei test]# getfacl /test/3.txt getfacl: Removing leading \u0026#39;/\u0026#39; from absolute path names # file: test/3.txt # owner: user1 # group: user3 user::rw- user:user4:r-- group::rwx group:caiwu:rwx mask::rwx 删除权限\n针对单个用户设置权限删除\n# setfacl -x u：用户名：文件名称\n1 [root@wei test]# setfacl -x u:user4 /test/3.txt 针对单个用户组设置权限删除\n# setfacl -x g：用户组名称 文件名称\n1 [root@wei test]# setfacl -x g:caiwu /test/3.txt 简单例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [root@wei test]# setfacl -x u:user4 /test/3.txt [root@wei test]# getfacl /test/3.txt getfacl: Removing leading \u0026#39;/\u0026#39; from absolute path names # file: test/3.txt # owner: user1 # group: user3 user::rw- group::rwx group:caiwu:rwx mask::rwx other::r-x [root@wei test]# setfacl -x g:caiwu /test/3.txt [root@wei test]# getfacl /test/3.txt getfacl: Removing leading \u0026#39;/\u0026#39; from absolute path names # file: test/3.txt # owner: user1 # group: user3 user::rw- group::rwx mask::rwx other::r-x 特殊权限\nsuid 4\nsgid 2\nsticky bit 1 1,suid\n作用：普通用户在执行命令期间，会临时获取到命令属主用户对操作系统的权限\n设置suid权限\n# chmod u+s 文件名称\n2，sgid\n针对目录设置\n作用：目录拥有sgid权限后，在目录下创建的文件会继承目录的属组信息\n设置sgid权限\n**# chmod g+s 目录名称 **\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@wei ~]# mkdir /linux [root@wei ~]# ls -dhl /linux drwxr-xr-x. 2 root root 6 1月 25 11:55 /linux [root@wei ~]# chgrp caiwu /linux [root@wei ~]# ls -dhl /linux drwxr-xr-x. 2 root caiwu 6 1月 25 11:55 /linux [root@wei ~]# touch /linux/1.txt [root@wei ~]# ls -l /linux/1.txt -rw-r--r--. 1 root root 0 1月 25 11:56 /linux/1.txt [root@wei ~]# chmod g+s /linux/ [root@wei ~]# ls -dhl /linux drwxr-sr-x. 2 root caiwu 19 1月 25 11:56 /linux [root@wei ~]# touch /linux/2.txt [root@wei ~]# ls -l /linux/2.txt -rw-r--r--. 1 root caiwu 0 1月 25 11:58 /linux/2.txt 3,sticky bit\n针对目录设置\n作用：只用目录下文件的属主用户,目录属主用户及root可删除的文件\n设置sticky bit权限\n# chmod o+t 目录名称\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 公司部门 开发部（经理，员工1，员工2） /project (1)经理可以查看写 (2)员工可以相互看，修改，但不能删除 [root@wei ~]# useradd jl [root@wei ~]# useradd yg1 [root@wei ~]# useradd yg2 [root@wei ~]# mkdir /project [root@wei ~]# chown jl /project/ [root@wei ~]# ls -dhl /project/ drwxr-xr-x. 2 jl root 6 1月 25 12:11 /project/ [root@wei ~]# usermod -G jl yg1 [root@wei ~]# usermod -G jl yg2 [root@wei ~]# chgrp jl /project/ [root@wei ~]# chmod g+w /project/ [root@wei ~]# chmod g+s /project/ [root@wei ~]# chmod o+t /project/ [root@wei ~]# ls -hdl /project/ drwxrwsr-t. 2 jl jl 6 1月 25 12:11 /project/ chmod chown chgrp setfacl\n共同选项： -R 递归修改\n","date":"2019-01-25T20:58:56Z","image":"https://www.ownit.top/title_pic/18.jpg","permalink":"https://www.ownit.top/p/201901252058/","title":"Linux文件权限管理"},{"content":"Linux操作系统： 多用户多任务的操作系统 用户类型分为： 管理员用户 ： root\n普通用户分为：系统用户/程序用户\n用户相关的文件：\n** /etc/passwd 用户信息\n格式：x:UID:GID:说明信息:SHELL\nUID:\n1000\u0026mdash;-60000 创建用户\n0\u0026mdash;\u0026ndash;999 系统用户\nSHELL：\n/bin/bash 默认\n/sbin/nologin 系统用户\n/etc/shadow 用户密码信息**\n用户： 基本组\n附加组 userA\u0026mdash;-\u0026gt; 用户组userA：\n1.创建用户\n# useradd [option] 用户名称\noption选项：\n(1) -u UID 指定用户的uid\n1 2 3 [root@wei ~]# useradd -u 1200 wei2 [root@wei ~]# id wei2 uid=1200(wei2) gid=1200(wei2) 组=1200(wei2) (2)指定用户的基本组,附加组\n-g gid/组名称\n-G gid/组名称,,,\n1 2 3 4 [root@wei ~]# groupadd nan [root@wei ~]# useradd -g wei2 -G nan wei3 [root@wei ~]# id wei3 uid=1201(wei3) gid=1200(wei2) 组=1200(wei2),1201(nan) (3)指定用户shell名称\n-s shell名称\n-M 不创建宿主目录\n1 [root@wei ~]# useradd -s /sbin/nologin -M zhangmiao (4)创建系统用户\n-r 1 [root@wei ~]# useradd -r nangong (5)指定用户的宿主目录\n-d 1 2 3 4 5 [root@wei ~]# useradd -d /tmp/hei hei [root@wei ~]# ls /tmp/ hei vmware-root [root@wei ~]# grep \u0026#34;hei\u0026#34; /etc/passwd hei:x:1203:1203::/tmp/hei:/bin/bash 2，切换用户\n# su - 用户名称\n3，查看用户id\n# id 用户名称\n1 2 [root@wei ~]# id wei uid=1000(wei) gid=1000(wei) 组=1000(wei) id的选项：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@wei ~]# id wei3 uid=1201(wei3) gid=1200(wei2) 组=1200(wei2),1201(nan) [root@wei ~]# id -u wei3 1201 [root@wei ~]# id -g wei3 1200 [root@wei ~]# id -G wei3 1200 1201 [root@wei ~]# id -u -n wei3 wei3 [root@wei ~]# id -g -n wei3 wei2 [root@wei ~]# id -G -n wei3 wei2 nan 3,设置用户密码\n# passwd 用户名\n1 [root@wei ~]# passwd wei (1)查看用户密码状态\n1 2 [root@wei ~]# passwd -S wei wei PS 2019-01-24 0 99999 7 -1 (密码已设置，使用 SHA512 算法。) (2)锁定用户密码\n1 2 3 [root@wei ~]# passwd -l wei 锁定用户 wei 的密码 。 passwd: 操作成功 (3)解锁用户密码\n1 2 3 [root@wei ~]# passwd -u wei 解锁用户 wei 的密码。 passwd: 操作成功 (4)强制用户密码过期\n1 2 3 [root@wei ~]# passwd -e wei 正在终止用户 wei 的密码。 passwd: 操作成功 4,修改用户信息\n# usermod [option] 用户名称 -u UID\n-g 组名称\n-G 组名称\n-s shell名称\n替换原有附加组\n1 2 3 4 5 6 [root@wei ~]# id wei3 uid=1201(wei3) gid=1200(wei2) 组=1200(wei2),1201(nan) [root@wei ~]# groupadd shicahng [root@wei ~]# usermod -G shicahng wei3 [root@wei ~]# id wei3 uid=1201(wei3) gid=1200(wei2) 组=1200(wei2),1204(shicahng) 添加多个附加组\n1 2 3 [root@wei ~]# usermod -aG nan wei3 [root@wei ~]# id wei3 uid=1201(wei3) gid=1200(wei2) 组=1200(wei2),1201(nan),1204(shicahng) 6,删除用户\n# userdel [option] 用户名称\n1 2 3 [root@wei ~]# userdel wei [root@wei ~]# userdel -r wei2 \u0026gt;\u0026gt;同时删除用户的宿主文件 用户组管理\n1,创建用户组\n# groupadd 用户组名称\n2,删除用户组\n# groupdel 用户组名称\n1 2 3 4 5 6 7 8 9 10 11 [root@wei ~]# groupadd jishu [root@wei ~]# useradd tom [root@wei ~]# useradd tom1 [root@wei ~]# usermod -G jishu tom [root@wei ~]# usermod -G jishu tom1 [root@wei ~]# grep \u0026#34;jishu\u0026#34; /etc/group jishu:x:1205:tom,tom1 [root@wei ~]# gpasswd -d tom jishu 正在将用户“tom”从“jishu”组中删除 [root@wei ~]# grep \u0026#34;jishu\u0026#34; /etc/group jishu:x:1205:tom1 ","date":"2019-01-24T22:28:18Z","image":"https://www.ownit.top/title_pic/43.jpg","permalink":"https://www.ownit.top/p/201901242228/","title":"Linux用户权限管理"},{"content":" 文件压缩/解压缩 gzip bzip2 xz 只能压缩文件，不能压缩文件夹（压缩完后，文件会消失） 先建三个文件来进行演示\n1 touch ./{1..3}.txt ** 文件已经创建好，下面就开始介绍文件的压缩和解压（ gzip bzip2 xz ）** gzip介绍： 压缩： gzip 源文件\n1 gzip 1.txt 解压缩 ： gzip -d 源文件\n1 gzip -d 1.txt.gz bzip2介绍 压缩：bzip2 源文件\n1 bzip2 2.txt 解压缩 : bizp2 -d 源文件\n1 bzip2 -d 2.txt.bz2 ** xz介绍** 压缩：xz 源文件\n1 xz 3.txt 解压缩：xz -d 源文件\n1 xz -d 3.txt.xz 创建打包文件 \u0026mdash;\u0026mdash;tar 1）创建打包文件 *.tar # tar cf 打包文件名称 源文件 e：creat创建 f : file文件 2)解包 ** # tar xf 打包文件名称 【-c解压到的目录】** 3）查看包 #tar tvf 打包的文件名称 ** 调用gzip使用压缩 解压缩** 压缩\n#tar czf 打包文件名称 源文件\n** z：调用gzip**\n解压缩：\n# tar xzf 打包文件的名称 【-c 目录名称】\n** 调用bzip2使用压缩 解压缩** 压缩\n#tar cjf 打包文件名称 目录名称\nj：调用bzip2\n解压缩\n#tar xjf 打包文件名称 【-c 目录名称】\n** 调用xz使用压缩 解压缩** 压缩\n#tar cJf 打包文件名称 目录名称\nJ：调用zx\n解压缩\n#tar xJf 打包文件名称 【-c 目录名称】\n","date":"2019-01-23T23:19:14Z","image":"https://www.ownit.top/title_pic/22.jpg","permalink":"https://www.ownit.top/p/201901232319/","title":"压缩，解压缩 和tar详细介绍"},{"content":"过滤文件类容\u0026mdash;grep grep正则表达式应用： #grep 【option】”pattern” 文件名称 pattern模式 由普通字符和正则表达式的元字符组构成的条件 简单例子\n1 grep \u0026#34;root\u0026#34; /etc/passwd 正则表达式的元字符 （1）匹配单个字符的元字符 . 任意单个字符（前面是一个小点）\n1 grep \u0026#34;r..t\u0026#34; /etc/passwd ** 注意：. 代表任意字符，此处有两个 . ，代表两个任意字符，看下面的例子**\n** [ ] 代表或者的关系**\n** 连续的字符范围**\n[a-z] : a到z的所有小写字母\n[A-Z] ： A到Z所有的大写字母\n**[a-zA-Z] ：包含所有的大小写字母 **\n[0-9] ：0到9的所有数字\n[a-zA-Z0-9 ] ：包含所有大小字母和数字\n为了方便后面的练习，在此建立个临时文件，写入字符，当做练习的文件 1 2 3 4 5 6 7 8 vim 1.txt rot rAt rBt r1t root rVCt r4t 1 2 3 grep \u0026#34;r[a-z]t\u0026#34; 1.txt grep \u0026#34;r[A-Z]t\u0026#34; 1.txt ^ 取反\n[^a-z]\n1 grep \u0026#34;r[^0-9]t\u0026#34; 1.txt (2)匹配字符出现的位置 ^string 以string开头\n1 grep \u0026#34;^root\u0026#34; /etc/passwd 对首行[rbh]开头\n1 grep \u0026#34;^[rbh]\u0026#34; /etc/passwd 不是【rbh】开头\n1 grep \u0026#34;^[^rbh]\u0026#34; /etc/passwd string$ 以string$结尾\n以bash结尾的\n1 grep \u0026#34;bash$\u0026#34; /etc/passwd 查看nologin的行数\n1 grep \u0026#34;nologin$\u0026#34; /etc/passwd | wc -l ^$ ： 代表 空行\n查看目录名称（此处是指目录文件）\n1 ls -l /etc/ | grep \u0026#34;^d\u0026#34; 为了方便后面的练习，在此建立个临时文件，写入字符，当做练习的文件 1 2 3 4 5 6 7 vim 2.txt a ab abb abbbb abbbbb abbbbbbb * 匹配其前一个字符出现任意次 ** .*任意字符** 1 grep \u0026#34;ab*\u0026#34; 3.txt \\? 0次或者1次 可有可无 1 grep \u0026#34;ab\\?\u0026#34; 2.txt \\+ 1次或者多次 最少1次 1 grep \u0026#34;ab\\+\u0026#34; 2.txt \\{2\\} 出现两次 1 grep \u0026#34;ab\\{2\\}\u0026#34; 2.txt \\{2，5\\} 最少2次，最多5次 1 grep \u0026#34;ab\\{2,5\\}\u0026#34; 2.txt option选项 1）-i 忽略大小写 1 [root@zhang ~]# grep -i \u0026#34;^r\u0026#34; 1.txt 2）-o 仅显示符合正则表达式的内容，不显示整行 1 2 3 [root@zhang ~]# grep -o \u0026#34;r..t\u0026#34; /etc/passwd root 3）-v 反向过滤 1 2 3 4 5 6 7 8 9 [root@zhang ~]# grep -v \u0026#34;^#\u0026#34; /etc/fstab /dev/mapper/centos-root / xfs defaults 0 0 UUID=20b4a09c-ba00-41d4-a6d5-7dc24bc0a057 /boot xfs defaults 0 0 /dev/mapper/centos-swap swap swap defaults 0 0 4）-e 根据多条件过滤文件 1 2 3 4 5 6 7 8 9 [root@zhang ~]# grep -e \u0026#34;^$\u0026#34; -e \u0026#34;^#\u0026#34; /etc/fstab # # /etc/fstab # Created by anaconda on Mon Jan 7 01:19:06 2019 4）-E 支持扩展正则表达式 1 grep -E \u0026#34;vmx|svm\u0026#34; /proc/cpuinfo 5）-A n 显示符合条件的后2行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@zhang ~]# ifconfig |grep -A 2 \u0026#34;netmask\u0026#34; inet 192.168.196.131 netmask 255.255.255.0 broadcast 192.168.196.255 inet6 fe80::20c:29ff:fe8e:e21b prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 00:0c:29:8e:e2:1b txqueuelen 1000 (Ethernet) -- inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) 6）-B n 显示符合条件的前2行 1 2 3 4 5 6 7 8 9 10 11 12 13 [root@zhang ~]# ifconfig |grep -B 2 \u0026#34;netmask\u0026#34; ens33: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.196.131 netmask 255.255.255.0 broadcast 192.168.196.255 -- lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 博主正在自学Linux云计算，有学习的的小伙伴可以相互交流，增强技术。 ","date":"2019-01-23T21:29:45Z","image":"https://www.ownit.top/title_pic/35.jpg","permalink":"https://www.ownit.top/p/201901232129/","title":"grep基本详细使用"},{"content":"1 Vi、Vim文本编辑器 1．Vi、Vim\nVi是Visual interface的简称。\nVim是Vi的增强版，即Vi Improved。在后面的实例中将介绍Vim的使用。\n为什么学vi？ 1）所有的Unix Like 系统都会内建 vi 文本编辑器，其他的文本编辑器则不一定会存在；\n2）很多个别软件的编辑接口都会主动呼叫 vi (例如未来会谈到的 crontab, visudo, edquota 等指令)；\n3）vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法癿正确性，方便程序设计；\n4）因为程序简单，编辑速度相当快速。\n系统自带教程：vimtutor\nvim [options] [filelist]\n常用选项：\n+[num]\n+/{pat}\n1、工作模式、命令模式、输入模式和末行模式 Vim拥有5种编辑模式：命令模式、输入模式、末行模式、可视化模式、查询模式。\n1）．命令模式（其它模式→ESC）\n2）．输入模式（命令模式→a、i、o、A、I、O）\n3）．末行模式（命令模式→:）\n4）．可视化模式（命令模式→v）\n5）．查询模式（命令模式→?、/）\n2、模式之间切换 命令模式：\n输入模式：\n新增 (append)\na ：从光标所在位置後面开始新增资料，光标後的资料随新增资料向後移动。\nA： 从光标所在列最後面的地方开始新增资料。\n插入 (insert)\ni： 从光标所在位置前面开始插入资料，光标後的资料随新增资料向後移动。\nI ：从光标所在列的第一个非空白字元前面开始插入资料。\n开始 (open)\no ：在光标所在列下新增一列并进入输入模式。\nO: 在光标所在列上方新增一列并进入输入模式\n末行模式：\n3、打开文件 vim /path/to/somefile\nvim +# :打开文件，并定位于第#行\nvim +：打开文件，定位至最后一行\nvim +/PATTERN : 打开文件，定位至第一次被PATTERN匹配到的行的行首\n默认处于编辑模式\n4、关闭文件 1、末行模式关闭文件\n:q 退出\n:wq 保存并退出\n:q! 不保存并退出\n:w 保存\n:w! 强行保存\n:wq --\u0026gt; :x\n2、编辑模式下退出\nZZ: 保存并退出\n4、移动光标(编辑模式) 1、逐字符移动：\nh: 左\nl: 右\nj: 下\nk: 上\n#h: 移动#个字符；\n2、以单词为单位移动\nw: 移至下一个单词的词首\ne: 跳至当前或下一个单词的词尾\nb: 跳至当前或前一个单词的词首\n#w:\n3、行内跳转：\n0: 绝对行首\n^: 行首的第一个非空白字符\n$: 绝对行尾\nhome\nend\n4、行间跳转\n#G：跳转至第#行；\nG：最后一行\n1G：跳转到第1行首===gg\n末行模式下，直接给出行号即可\n:5 直接定位到第5行首\n5、翻屏 Ctrl+f: 向下翻一屏\nCtrl+b: 向上翻一屏\nCtrl+d: 向下翻半屏\nCtrl+u: 向上翻半屏\n6、删除单个字符 x: 删除光标所在处的单个字符\n#x: 删除光标所在处及向后的共#个字符\n7、删除命令: d d命令跟跳转命令组合使用；\n#dw, #de, #db\ndd: 删除当前光标所在行\n#dd: 删除包括当前光标所在行在内的#行；\n末行模式下：\nStartADD,EndADDd\n.: 表示当前行\n$: 最后一行\n+#: 向下的#行\n8、粘贴命令 p p: 如果删除或复制为整行内容，则粘贴至光标所在行的下方，如果复制或删除的内容为非整行，则粘贴至光标所在字符的后面；\nP: 如果删除或复制为整行内容，则粘贴至光标所在行的上方，如果复制或删除的内容为非整行，则粘贴至光标所在字符的前面；\n9、复制命令 y 用法同d命令\nyy 复制1行\n5yy 复制5行\n10、修改：先删除内容，再转换为输入模式 c: 用法同d命令\n11、替换：r R: 替换模式\n12、撤消编辑操作 u u：撤消前一次的编辑操作\n连续u命令可撤消此前的n次编辑操作\n#u: 直接撤消最近#次编辑操作\n撤消最近一次撤消操作：Ctrl+r\n13、可视化模式 v: 按字符选取\n该模式下通过光标移动选择文本，选取后按 y 可以把文本提取到缓冲区（即复制），c 可以剪切。之后可以使用p在光标后粘贴，P粘贴在光标前\nV：按矩形选取\nV是行选取模式，以行为单位进行选取。Ctrl+v是块选取模式，可以选取一块矩形区域中的文本。\n14、查找 /PATTERN\n?PATTERN\nn\nN\n15、查找并替换 1 2 3 4 5 6 7 8 在末行模式下使用s命令 ADDR1,ADDR2s@PATTERN@string@gi 1,$ %：表示全文\t:s/root/admin/ 替换光标所在行第一个root为admin :s/root/admin/g 替换光标所在行所有root为admin :1,5 s/root/admin/g 替换第1-5行所有root为admin :1,$ s/admin/root/g 替换所有行的admin为root ==== 1,$ 等价于% 16、使用vim编辑多个文件 vim FILE1 FILE2 FILE3\n:next 切换至下一个文件\n:prev 切换至前一个文件\n:last 切换至最后一个文件\n:first 切换至第一个文件\n退出\n:qa 全部退出\n17、分屏显示一个文件 Ctrl+w, s: 水平拆分窗口\nCtrl+w, v: 垂直拆分窗口\n在窗口间切换光标：\nCtrl+w, ARROW(表示上下左右箭头)\n1 :qa 关闭所有窗口 18、分窗口编辑多个文件 vim -o : 水平分割显示\nvim -O : 垂直分割显示\n19、将当前文件中部分内容另存为另外一个文件 末行模式下使用w命令\n:w\n:ADDR1,ADDR2w /path/to/somewhere\n20、将另外一个文件的内容填充在当前文件中 :r /path/to/somefile\n21、跟shell交互 :! COMMAND\n22、高级话题 1、显示或取消显示行号\n:set number\n:set nu\n:set nonu\n2、显示忽略或区分字符大小写\n:set ignorecase\n:set ic\n:set noic\n3、设定自动缩进\n:set autoindent\n:set ai\n:set noai\n4、查找到的文本高亮显示或取消\n:set hlsearch\n:set nohlsearch\n5、语法高亮\n:syntax on\n:syntax off\n23、配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /etc/vimrc ~/.vimrc set hlsearch \u0026#34;高亮度反白 set backspace=2 \u0026#34;可随时用退格键删除 set autoindent \u0026#34;自动缩排 set tabstop=4\t\u0026#34;缩进 set softtabstop=4 softtabstop是“逢4空格进1制表符”,前提是你tabstop=4 set shiftwidth=4 自动缩进空格长度 set mouse=a\t\u0026#34;使用鼠标 set selection=exclusive set selectmode=mouse,key set ruler \u0026#34;可显示最后一行的状态 set showmode \u0026#34;左下角那一行的状态 set nu \u0026#34;可以在每一行的最前面显示行号啦！ set bg=dark \u0026#34;显示不同的底色色调 syntax on \u0026#34;进行语法检验，颜色显示\t","date":"2019-01-19T19:03:12Z","image":"https://www.ownit.top/title_pic/03.jpg","permalink":"https://www.ownit.top/p/201901191903/","title":"Vim文本编辑器详细用法"},{"content":"座右铭：长风破浪会有时，直挂云帆济沧海。\nlinux一般查看文件或者目录有几种方法。\n/查看文件类容\u0026mdash;\u0026mdash;\u0026ndash;cat/more/less/head/tail 只能查看文本型（txt）\n（1）查看文件较少的类容\ncat /etc/fstab\ncat -n /etc/fstab\n（2）查看未知类容\n看未知的类容文件，我们不知道有多大，就用more和less查看比较方便使用\n两者相同点：可以 翻页 或者 一行一行查看 ** 不同点：more只能往下翻页，有的翻过了，就不能翻回去了** ** less可以往上往下随意查看翻动** more/less 分页显示类容 more 只能往下翻（空格：翻页 回车：一行一行 q：退出） less 可以上下翻 less /usr/share/dict/words\n（3）head/tail（头部/尾巴）\nhead\n不写-n显示前10行类容\n[root@chengfeng ~]# head -n 3 /etc/passwd(显示前三行)\nTail\n不写-n显示后10行类容\n[root@chengfeng ~]# tail-n 3 /etc/passwd(显示前三行)\n（4）查看文件类型\n1 [root@chengfeng ~]# file /etc/passwd （查看文件类型） ** （5）查看命令所在的路径**\n1 2 3 4 5 [root@chengfeng ~]# which ls alias ls=\u0026#39;ls --color=auto\u0026#39; /usr/bin/ls [root@chengfeng ~]# which cd /usr/bin/cd （6）| :管道符 连接命令 前面的命令给后面命令当参数\n1 2 3 4 5 6 7 8 [root@chengfeng ~]# head -n 5 /etc/passwd root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin [root@chengfeng ~]# head -n 5 /etc/passwd | tail -n 1（显示passwd第5行的类容） lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin ** 显示前4个最大文件**\n1 2 3 4 5 6 root@chengfeng ~]# ls -lhS /etc/ | head -n 5 总用量 1.4M -rw-r--r--. 1 root root 655K 6月 7 2013 services -rw-r--r-- 1 root root 92K 1月 7 14:35 ld.so.cache -rw-r--r-- 1 root root 51K 5月 15 2013 mime.types -rw-r--r-- 1 root root 15K 10月 31 08:17 autofs.conf ** 显示最近修改的文件**\n1 2 3 4 5 6 7 8 9 10 [root@chengfeng ~]# ls -lt /etc/ |head -n 11 总用量 1396 drwxr-xr-x 5 root lp 304 1月 7 19:01 cups -rw-r--r-- 1 root root 57 1月 7 19:01 resolv.conf -rw-r--r-- 1 root root 1992 1月 7 14:51 passwd ---------- 1 root root 1138 1月 7 14:50 shadow ---------- 1 root root 690 1月 7 14:50 gshadow -rw-r--r-- 1 root root 862 1月 7 14:50 group -rw-r--r--. 1 root root 850 1月 7 14:49 group- ----------. 1 root root 682 1月 7 14:49 gshadow- （7）查找文件或目录\n# find 路径 查找方式\n按文件名称查找\n1 2 3 4 [root@chengfeng ~]# find /etc/ -name \u0026#34;*.conf\u0026#34; /etc/resolv.conf /etc/pki/ca-trust/ca-legacy.conf /etc/yum/pluginconf.d/fastestmirror.conf 统计查找的文件个数\n1 2 [root@chengfeng ~]# find /etc/ -name \u0026#34;*.conf\u0026#34; |wc -l 352 按文件大小查找\n查找大于1M的文件\n1 2 3 4 5 6 [root@chengfeng ~]# find /etc/ -size +1M /etc/udev/hwdb.bin /etc/selinux/targeted/contexts/files/file_contexts.bin /etc/selinux/targeted/policy/policy.31 /etc/selinux/targeted/active/policy.kern /etc/selinux/targeted/active/policy.linked ** 按文件的修改时间查找**\n查找7前修改的文件\n1 [root@chengfeng ~]# find / -mtime +7 查找7内修改的文件\n1 [root@chengfeng ~]# find / -mtime -7 ** 按文件的类型查找**\n1 2 3 4 5 6 7 8 [root@chengfeng ~]# find /dev/ -type b /dev/dm-1 /dev/dm-0 /dev/sda2 /dev/sda1 /dev/sda /dev/sr0 [root@chengfeng ~]# find /dev/ -type l 复合条件查文件\n[root@chengfeng ~]# find / -mtime +7 -a -size +100k（a：and并列）\nFind /bj/ -name “*.txt” -exec rm -rf {}\\; 删除\n# Find /bj/ -name “*.txt” -exec cp {} /root \\;复制\n","date":"2019-01-07T23:55:11Z","image":"https://www.ownit.top/title_pic/09.jpg","permalink":"https://www.ownit.top/p/201901072355/","title":"Linux命令查找文件目录"},{"content":"Linux目录/文件增删改\n创建文件\n(1)\n# touch \u0026lt;文件名称\u0026gt;\n(2)\n花括号展开\ntouch /root/{1,3,9}.txt\ntouch /root/{0..100}.txt 批量创建文件\n删除文件\n**rm -f [文件名] **\n-rf 代表强制删除\n批量删除文件\nrm -f *txt\n复制文件\ncp 只能复制文件\ncp -r 可以复制目录\n# cp [option] 源文件 目标文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [root@zhang ~]# mkdir /root/{linux,windown} [root@zhang ~]# ls anaconda-ks.cfg linux mysql57-community-release-el7-8.noarch.rpm windown [root@zhang ~]# touch /root/linux/{1,2,3}.txt [root@zhang ~]# ls anaconda-ks.cfg linux mysql57-community-release-el7-8.noarch.rpm windown [root@zhang ~]# cd linux/ [root@zhang linux]# ls 1.txt 2.txt 3.txt [root@zhang linux]# cd /r root/ run/ [root@zhang linux]# cd /root/ [root@zhang ~]# cp /root/linux/1.txt /root/windown/ [root@zhang ~]# cd windown/ [root@zhang windown]# ls 1.txt [root@zhang windown]# cd .. [root@zhang ~]# ls anaconda-ks.cfg linux mysql57-community-release-el7-8.noarch.rpm windown [root@zhang ~]# cp /root/linux/2.txt /root/windown/2.jpg [root@zhang ~]# cd windown/ [root@zhang windown]# ls 1.txt 2.jpg ** -r 复制文件目录**\n1 2 3 4 5 6 7 8 9 [root@zhang ~]# cp -r /root/linux/ windown/ [root@zhang ~]# ls anaconda-ks.cfg linux mysql57-community-release-el7-8.noarch.rpm windown [root@zhang ~]# cd windown/ [root@zhang windown]# ls 1.txt 2.jpg linux [root@zhang windown]# cd linux/ [root@zhang linux]# ls 1.txt 2.txt 3.txt -fn 强制覆盖\n1 [root@zhang ~]# cp -fn /root/linux/1.txt /root/windown/ 移动文件\n#mv 源文件 目标文件\n文件重名\n","date":"2019-01-06T13:58:06Z","image":"https://www.ownit.top/title_pic/12.jpg","permalink":"https://www.ownit.top/p/201901061358/","title":"Linux文件增删改"},{"content":"Linux文件目录管理\n1：目录管理\n1）切换目录\n# cd [ 目录名称]\n2）退到上一目录\n# cd ..\n2：创建目录\nmkdir [文件名称]\nmkdir -p [文件名称] 递归创建目录\n1 mkdir -p /root/linux/zhang/wei 利用tree来查看结果\n1 tree /root/linux 3：文件管理\n1）查看文件\nls \u0026lt;option\u0026gt; [目录名称]\n【option】：参数\n-l 以详细信息显示出来\n-rw-r\u0026ndash;r\u0026ndash;. 1 root root 16 12月 4 21:42 adjtime\n解释：前面这些- r d l等符合的意思\n- 文件\nd 目录\nl 软链接文件（link）类似快捷方式\nb 块设备文件（block），硬盘，硬盘分区，u盘，光盘\nc 字符设备文件（chaeacter）键盘，鼠标，显示器\nls \u0026lt;option\u0026gt; [目录名称]\n参数：\n-l 更加可视化的显示文件信息\n-d 显示目录自身的信息\n-a 显示目录的隐藏文件（以.开头的文件）\n-t 按文件修改时间降序排列\n-S 按文件大小降序排列\n各个参数可以组合\ndu -sh [目录名称] 用来统计文件和目录占的大小\ndu -ah [目录名称] 用来分享每个文件和目录占的大小\n","date":"2019-01-05T14:53:31Z","image":"https://www.ownit.top/title_pic/30.jpg","permalink":"https://www.ownit.top/p/201901051453/","title":"Linux目录管理"},{"content":"时间久了，我越来越感觉linux比window好用。\n难道这就是传说中的日久生情吗？\n学习是持之以恒的，从基础开始，慢慢来，我相信自己有一天会变成大神的。\nLinux就是那么的方便，干什么就是一个命令就完事，不像window那么麻烦。\nlLinux改主机名称，这个更加的简单。但是为了更深的记忆，还是再次写一下。\n（1）linux一开始安装时就有修改主机名称的选项。\n（2）如果在安装完后，想修改主机名称，一个命令就完成。\n命令：hostnamectl set-hostname [你想修改的名称]\n1 hostnamectl set-hostname zhang ** **\n等你退出从新登录时，你就会发现linux命令字符串的改变；\n","date":"2019-01-04T19:22:56Z","image":"https://www.ownit.top/title_pic/30.jpg","permalink":"https://www.ownit.top/p/201901041922/","title":"Linux修改主机名"},{"content":"Linux创建高级用户并删除\n常见window系统可以创建许多用户，但是linux也可以创建许多用户。\n方法比window方便简单。\n（1）添加一个普通用户 ：nangong（名字自己取）\n1 useradd nangong （2）设置用户nangong的密码\n1 passwd nangong （3）在root权限下修改/etc/passwd 里的用户nangong的权限\n1 2 3 4 nangong:x:1000:1000::/home/nangong:/bin/bash #把两个1000改为0就可以了 nangong:x:0:0::/home/nangong:/bin/bash （4）已经成功创建一个高级用户nangong\n（a）删除一个用户，前提，那个用户没有在运行中，不然会提示下面的错误\n解决方法：（1）切换到那个用户，然后连续按两次ctl+d，退出用户\n** （2）使用vipw命令 ，删除nangong那行，保存**\n** （3）然后使用vipw -s 命令 ，删除nangong那行，保存**\n然后就行下来的步骤即可\n（b）我们来到/home下，可以看到nangong这个用户\n1 cd /home (c)删除这个用户，但是命令不是rm\n正确命令：\n1 userdel -r nangong 可以直接删除/home 的nangong\n命令：\n1 rm -rf nangong ","date":"2019-01-03T19:29:30Z","image":"https://www.ownit.top/title_pic/71.jpg","permalink":"https://www.ownit.top/p/201901031929/","title":"Linux创建高级用户并删除"},{"content":"linux命令提示符\n命令窗口中[root@chengfeng ~]# 表示什么意思？\n** root 当前登录终端的用户**\n** chengfeng 主机名(/etc/sysconfig/network)**\n** ~ 当前用户的家目录**\n** # 表示root用户的终端**\n** $ 普通用户的终端**\n（1）root 当前登录终端的用户\n（2） chengfeng 主机名(/etc/sysconfig/network)\n** **\n（3） ~ 当前用户的家目录\n** **\n（4） # 表示root用户的终端\n** **\n** $ 普通用户的终端**\na：添加一个普通的新用户\n** b：切换到cat用户下**\n","date":"2019-01-03T18:19:46Z","image":"https://www.ownit.top/title_pic/02.jpg","permalink":"https://www.ownit.top/p/201901031819/","title":"linux命令提示符解析"},{"content":"linux系统中一切皆文件。\n有文件，当然少不了目录。\nLinux目录和Windows目录有着很大的不同，Linux目录类似一个树，最顶层是其根目录。\n**在此我介绍一下初学者需要先了解的linux目录，关于跟高深的linux目录，后期会做出相应的讲解 **\n/根目录/rootroot用户的家目录/宿主目录/boot启动分区（内核，启动配置文件）/dev设备文件目录/proc,/sys为根系统/etc应用程序的配置文件 \u0026nbsp;\u0026nbsp; *.conf/home普通用户的家目录/宿主目录带有/bin,/sbin可执行程序（二进制程序）/var放置系统执行过程中经常变化的文件，如随时更改的日志文件/tmp存放临时文件目录，一些命令和应用程序会用的到这个目录 **/：**根目录，位于Linux文件系统目录结构的顶层，一般根目录下只存放目录，不要存放文件，/etc、/bin、/dev、/lib、/sbin应该和根目录放置在一个分区中。\n/bin，/usr/bin：该目录为命令文件目录，也称为二进制目录。包含了供系统管理员及普通用户使用的重要的linux命令和二进制（可执行）文件，包含shell解释器等。\n/boot： 该目录中存放系统的内核文件和引导装载程序文件，/boot/vmlinuz为linux的内核文件，以及/boot/gurb。建议单独分区，分区大小100M即可。\n/dev： 设备（device）文件目录，存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，存放连接到计算机上的设备（终端、磁盘驱动器、光驱及网卡等）的对应文件，包括字符设备和块设备等，常用的是挂载光驱mount /dev/cdrom/mnt。 /etc： 系统配置文件存放的目录，该目录存放系统的大部分配置文件和子目录，不建议在此目录下存放可执行文件，重要的配置文件有/etc/inittab、/etc/fstab、/etc/init.d、/etc/X11（X Window系统有关）、/etc/sysconfig（与网络有关）、/etc/xinetd.d修改配置文件之前记得备份。该目录下的文件由系统管理员来使用，普通用户对大部分文件有只读权限。\n/home： 系统默认的用户宿主目录，新增用户账号时，用户的宿主目录都存放在此目录下，~表示当前用户的宿主目录，~test表示用户test的宿主目录。建议单独分区，并设置较大的磁盘空间，方便用户存放数据。\n/lib，/usr/lib，/usr/local/lib：系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助，该目录下存放了各种编程语言库。典型的linux系统包含了C、C++和FORTRAN语言的库文件。/lib目录下的库映像文件可以用来启动系统并执行一些命令，目录/lib/modules包含了可加载的内核模块，/lib目录存放了所有重要的库文件，其他的库文件则大部分存放在/usr/lib目录下。\n/proc： 此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有/proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/*等。\n/root：系统管理员root的宿主目录，系统第一个启动的分区为/，所以最好将/root和/放置在一个分区下。\n/sbin，/usr/sbin，/usr/local/sbin：放置系统管理员使用的可执行命令，如fdisk、shutdown、mount等。与/bin不同的是，这几个目录是给系统管理员root使用的命令，一般用户只能\u0026quot;查看\u0026quot;而不能设置和使用。\n/usr： 应用程序存放目录，/usr/bin 存放应用程序， /usr/share 存放共享数据，/usr/lib\n存放不能直接运行的，却是许多程序运行所必需的一些函数库文件，/usr/local 存放软件升级包，/usr/share/doc 系统说明文件存放目录。\n/usr/share/man: 程序说明文件存放目录，使用 man ls时会查询/usr/share/man/man1/ls.1.gz的内容建议单独分区，设置较大的磁盘空间。\n/var： 放置系统执行过程中经常变化的文件，如随时更改的日志文件 /var/log。/var/log/message：\n所有的登录文件存放目录。/var/spool/mail： 邮件存放的目录。 /var/run: 程序或服务启动后。建议单独分区，设置较大的磁盘空间。\n/tmp：存放临时文件目录，一些命令和应用程序会用的到这个目录。该目录下的所有文件会被定时删除，以避免临时文件占满整个磁盘。\n","date":"2019-01-03T16:49:46Z","image":"https://www.ownit.top/title_pic/37.jpg","permalink":"https://www.ownit.top/p/201901031649/","title":"linux目录结构"},{"content":"近期要入坑linux运维，在此交流自己的技术和心得。开始自己博客的生涯。（大神勿喷）\n成功安装CentOS7后的基础步骤\n软件：Vmware Workstation\n系统：CentOS7\n安装成功的CentOS7界面\n安装完毕后操作:\n（1） 关闭防火墙\n1 2 systemctl stop firewalld # 当前关闭防火墙 systemctl disable firewalld # 开机禁用防火墙 （2）禁用SELinux\n1 sed -i.bak ‘s/=enforcing/=disabled/’ /etc/selinux/config 1 sed -i.bak ‘s/=enforcing/=disabled/’ /etc/sysconfig/selinux 上面两串代码效果一样，只需要运行一个即可\n（3） 关机\n1 shutdown -h now （4）创建快照，避免重新安装\n（6）安装远程管理软件Xmanager6\n","date":"2019-01-02T16:59:18Z","image":"https://www.ownit.top/title_pic/44.jpg","permalink":"https://www.ownit.top/p/201901021659/","title":"linux基础操作"}]